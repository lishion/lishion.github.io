---
title: 变分自编码器-Variational Auto-Encoding
date: 2018-08-03 15:50:56
tags:
  - 机器学习
  - 生成网络
categories: 机器学习
author: lishion
toc: true
---
最初是在看一篇关于自编码器在异常检测中的应用时接触到的变分自编码器，粗略看了一下之后发现自编码器的内容很好理解，但是变分自编码器始终没看懂。最后又去找了原论文来看还是没看懂。由于变分自编码器的原论文名字其实叫*Auto-Encoding  Variational Bayes*，所以又去找了变分贝叶斯来看，发现变分贝叶斯也没看懂，然后又去看了EM算法。最后把EM算法和变分贝叶斯大概都看懂了之后又回去看原论文发现还是没看懂。。。这个过程可以说是十分的煎熬。不过在这个过程中我名白了一个十分重要的道理，在看论文时，一定要清楚论文描述的模型是生么，得到这个结果之后可以做什么。当你带着自己的目的看一篇论文时，你对论文本身会有一个**知识走势**判定，如果你所带的目的和论文最终的目的不太一样，那么论文的**知识走势**一般来说就和你想的不一样，于是你就越来越云里雾里，到最后看了几遍也没看懂。所以这篇文章最开始我会想讲到 VAE(Variational Auto-Encoding) 这个算法疑惑是模型到底是什么，可以做什么。随后才会讲到它的模型计算。

## VAE到底是什么？

虽然 VAE 的全称是 Variational Auto-Encoding 也就是**变分自编码器**，但它其实和传统的自编码器有很大的不同。VAE的本质是一个生成模型。什么是生成模型? 就是给定一定的观测数据 $X$ ，从中学习到关于 $x$ 的概率模型 $p(x)$。得到 $p(x)$ 后我们可以通过采样的办法从中采样出一些数据，这些数据与已知的 $x$ 相似但是又有不一样的地方。这也就是生成模型名字的由来，它可以 "生成" 一些我们不知道的数据。

举个例子，对于一堆$100×100$的哈士奇的照片，我们可以认为每一张照片是一个100000的向量$x$，它是以概率密度 $p(x)$ 从动物空间中采样出来的。那么如果我们能从这堆图片中学习到 $p(x)$ 具体表达式，我们就能直接从中采样得到所有的哈士奇的照片。

所以不管VAE是不是有编码的功能，它首先是一个**生成模型**。我们要以生成模型的目的去看待这篇论文的结果和目的才能愉快的看下去。

## VAE做了些什么

在这之前先做一些符号上的定义，设 $X$ 为观测数据集，例如刚才提到的哈士奇照片合集，$x$ 为观测数据集中的某一个观测数据点。那么在知道了 VAE 是一个生成模型之后。那么它的终极目的一定是学得一个概率模型 $p(x;\theta)$ 使得我们的观测数据集合概率:
$$
P(X)=\int_xp(x;\theta)dx
$$
的值最大，当然这里是假设每一个观测变量是**独立同分布的**，所以它们的联合概密度可以看作是各自概率密度的乘积。这里出现了两个问题:

1. 概率模型如何选择?
2. 参数如何优化?

当然很多情况直接对这两个问题求解是不太现实的。因此才有了各种优化近似的办法。VAE采用了**潜变量**模型，引入了潜变量及其分布，这里又要继续说一些定义了:

对于潜变量 $z$ 是以一定的概率密度 $p(z)$ 从潜变量空间$Z$ 采样得到的。并且，有一个被参数 $\theta$ 指定的函数族 $f(z;\theta)$ 将观潜变量 $z$ 映射到观测空间 $X$，也就是说，对于每一个$x$，有一个函数使得$x=f(z;\theta)$。我们最终的目地就是求得$\theta$，使得$f(z;\theta)$以最大的概率将潜变量映设到观测空间。那么我们最终优化的目的依然是:
$$
P(X)=\int_x \int_z p(x|z;\theta)p(z)dzdx
$$
只不过多了一个潜变量，其主要的内容还是没有改变。为了简略，这里省去了对$x$的积分。

这里要整理一下出现的几个未知的变量或函数:

1. $p(z)$  
2. $f(z;\theta)$  
3. $p(x|z)$ 

在 VAE 中给出了一个假设，认为$p(x|z)=N(f(z;\theta),I)$。这里是一个比较重要的地方，因为他说明了VAE最终的结果到底是什么。这句话表示了两个含义:

1. 对于一个观测变量，他在潜变量的条件下服从正太分布
2. 正太分布的均值为$f(z;\theta)$

从一开始我们认为 $x=f(z;\theta)$，这里关于 $x$ 的分布为 $N(f(z;\theta),I)$，当然这是合理的。由于$x$为观测值，那么肯定认为观测到的值概率是最大的，而其他的值当然也有可能，只不过概率没有那么大。

