
<!doctype html>
<html class="theme-next use-motion theme-next-mala">
<head>
  

<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>


<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






  <link rel="stylesheet" type="text/css" href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5"/>

<link href="https://cdn.bootcss.com/KaTeX/0.9.0/katex.min.css" type="text/css"  rel="stylesheet">

  <link href='//fonts.googleapis.com/css?family=Lato:300,400,700,400italic&subset=latin,latin-ext' rel='stylesheet' type='text/css'>


<link rel="stylesheet" type="text/css" href="/css/main.css?v=0.4.5.1"/>




  <meta name="keywords" content="Spark,编程," />





  <link rel="shorticon icon" type="image/x-icon" href="/favicon.ico?v=0.4.5.1" />


<meta name="description" content="首先立一个flag，这将是一个长期更新的版块。 写在最开始在我使用spark进行日志分析的时候感受到了spark的便捷与强大。在学习spark初期，我阅读了许多与spark相关的文档，在这个过程中了解了RDD，分区，shuffle等概念，但是我并没有对这些概念有更多具体的认识。由于不了解spark的基本运作方式使得我在编写代码的时候十分困扰。由于这个原因，我准备阅读spark的源代码来解决我对基本">
<meta name="keywords" content="Spark,编程">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark 源码阅读计划 - 第一部分 - 迭代计算">
<meta property="og:url" content="http://yoursite.com/2018/06/06/Spark-源码阅读计划-第一部分-迭代计算/index.html">
<meta property="og:site_name" content="Lishion&#39;s Blog">
<meta property="og:description" content="首先立一个flag，这将是一个长期更新的版块。 写在最开始在我使用spark进行日志分析的时候感受到了spark的便捷与强大。在学习spark初期，我阅读了许多与spark相关的文档，在这个过程中了解了RDD，分区，shuffle等概念，但是我并没有对这些概念有更多具体的认识。由于不了解spark的基本运作方式使得我在编写代码的时候十分困扰。由于这个原因，我准备阅读spark的源代码来解决我对基本">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2018-06-29T17:06:08.923Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark 源码阅读计划 - 第一部分 - 迭代计算">
<meta name="twitter:description" content="首先立一个flag，这将是一个长期更新的版块。 写在最开始在我使用spark进行日志分析的时候感受到了spark的便捷与强大。在学习spark初期，我阅读了许多与spark相关的文档，在这个过程中了解了RDD，分区，shuffle等概念，但是我并没有对这些概念有更多具体的认识。由于不了解spark的基本运作方式使得我在编写代码的时候十分困扰。由于这个原因，我准备阅读spark的源代码来解决我对基本">


<script type="text/javascript" id="hexo.configuration">
  var CONFIG = {
    scheme: 'Mala',
    sidebar: 'post'
  };
</script>

  <title> Spark 源码阅读计划 - 第一部分 - 迭代计算 | Lishion's Blog </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  <!--[if lte IE 8]>
  <div style=' clear: both; height: 59px; padding:0 0 0 15px; position: relative;margin:0 auto;'>
    <a href="http://windows.microsoft.com/en-US/internet-explorer/products/ie/home?ocid=ie6_countdown_bannercode">
      <img src="http://7u2nvr.com1.z0.glb.clouddn.com/picouterie.jpg" border="0" height="42" width="820"
           alt="You are using an outdated browser. For a faster, safer browsing experience, upgrade for free today or use other browser ,like chrome firefox safari."
           style='margin-left:auto;margin-right:auto;display: block;'/>
    </a>
  </div>
<![endif]-->
  



  <div class="container one-column page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><h1 class="site-meta">
  <span class="logo-line-before"><i></i></span>
  <a href="/" class="brand" rel="start">
      <span class="logo">
        <i class="icon-next-logo"></i>
      </span>
      <span class="site-title">LISHION</span>
  </a>
  <span class="logo-line-after"><i></i></span>
</h1>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu ">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            <i class="menu-item-icon icon-next-home"></i> <br />
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            <i class="menu-item-icon icon-next-categories"></i> <br />
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            <i class="menu-item-icon icon-next-archives"></i> <br />
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            <i class="menu-item-icon icon-next-tags"></i> <br />
            Tags
          </a>
        </li>
      

      
      
    </ul>
  

  
</nav>

 </div>
    </header>

    
      

      

    

    <main id="main" class="main">
      <div class="main-inner">
        <div id="content" class="content"> 

  <div id="posts" class="posts-expand">
    

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              Spark 源码阅读计划 - 第一部分 - 迭代计算
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          Posted on
          <time itemprop="dateCreated" datetime="2018-06-06T13:46:45+08:00" content="2018-06-06">
            2018-06-06
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; In
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/Spark源码阅读计划/" itemprop="url" rel="index">
                  <span itemprop="name">Spark源码阅读计划</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        <span itemprop="articleBody"><p>首先立一个flag，这将是一个长期更新的版块。</p>
<h2 id="写在最开始"><a href="#写在最开始" class="headerlink" title="写在最开始"></a>写在最开始</h2><p>在我使用<code>spark</code>进行日志分析的时候感受到了<code>spark</code>的便捷与强大。在学习<code>spark</code>初期，我阅读了许多与<code>spark</code>相关的文档，在这个过程中了解了<code>RDD</code>，<code>分区</code>，<code>shuffle</code>等概念，但是我并没有对这些概念有更多具体的认识。由于不了解<code>spark</code>的基本运作方式使得我在编写代码的时候十分困扰。由于这个原因，我准备阅读<code>spark</code>的源代码来解决我对基本概念的认识。</p>
<h2 id="本部分主要内容"><a href="#本部分主要内容" class="headerlink" title="本部分主要内容"></a>本部分主要内容</h2><p>虽然该章节的名字叫做<strong>迭代计算</strong>，但是本章会讨论包括<strong>RDD、分区、Job、迭代计算</strong>等相关的内容，其中会重点讨论分区与迭代计算。因此在阅读本章之前你至少需要对这些提到的概念有一定的了解。</p>
<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>你需要准备一份 Spark 的源代码以及一个便于全局搜索的编辑器|IDE。</p>
<h2 id="一个简单的例子"><a href="#一个简单的例子" class="headerlink" title="一个简单的例子"></a>一个简单的例子</h2><p>假设现在我们有这样一个需求 : 寻找从 0 to 100 的偶数并输出其总数。利用 spark 编写代码如下:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sc.parallelize(<span class="number">0</span> to <span class="number">100</span>).filter(_%<span class="number">2</span> == <span class="number">0</span>).count</span><br><span class="line"><span class="comment">//res0: Long = 51</span></span><br></pre></td></tr></table></figure>
<p>这是一个非常简单的例子，但是里面却蕴涵了许多基础的知识。这一章的内容也是从这个例子展开的。</p>
<h2 id="迭代计算"><a href="#迭代计算" class="headerlink" title="迭代计算"></a>迭代计算</h2><h3 id="comput-方法与-itertor"><a href="#comput-方法与-itertor" class="headerlink" title="comput 方法与 itertor"></a>comput 方法与 itertor</h3><p>在RDD.scala 定义的类 RDD中，有两个实现迭代计算的核心方法，分别是<code>compute</code>以及<code>itertor</code>方法。其中<code>itertor</code>方法如下:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> <span class="function"><span class="keyword">def</span> <span class="title">iterator</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">  <span class="keyword">if</span> (storageLevel != <span class="type">StorageLevel</span>.<span class="type">NONE</span>) &#123;<span class="comment">//如果有缓存</span></span><br><span class="line">    getOrCompute(split, context)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    computeOrReadCheckpoint(split, context)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们只关心这个方法的第一个参数<strong>分区</strong>，以及返回的迭代器。忽略有缓存的情况，我们继续看<code>computeOrReadCheckpoint</code>这个方法:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">computeOrReadCheckpoint</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>] =</span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">if</span> (isCheckpointedAndMaterialized) &#123;<span class="comment">//如果有checkpoint</span></span><br><span class="line">    firstParent[<span class="type">T</span>].iterator(split, context)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    compute(split, context)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到，在没有缓存和 checkpoint 的情况下，itertor 方法直接调用了 compute 方法。而compute方法定义如下:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Implemented by subclasses to compute a given partition.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure>
<p>从注释可以看出，compute 方法应该由子类实现，用以计算给定分区并生成一个迭代器。如果不考虑缓存和 checkpoint 的情况下，简化一下代码:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> <span class="function"><span class="keyword">def</span> <span class="title">iterator</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">   compute(split,context)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<p>那么实际上 iterator 的功能是: <strong>接受一个分区，对这个分区进行计算，并返回计算结果的迭代器</strong>。而之所以 compute 需要子类来实现，是因为只需要在子类中实现不同的 compute 方法，就能产生不同类型的 RDD。既然不同的RDD 有不同的功能，我们想知道之前的例子是如何进行迭代计算的，就需要知道这个例子中涉及到了哪些 RDD。</p>
<p>首先查看<code>SparkContext</code>中与<code>parallelize</code>相关的部分代码:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parallelize</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">    numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  assertNotStopped()</span><br><span class="line">  <span class="keyword">new</span> <span class="type">ParallelCollectionRDD</span>[<span class="type">T</span>](<span class="keyword">this</span>, seq, numSlices, <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Seq</span>[<span class="type">String</span>]]())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到，<code>parallelize</code>实际返回了一个<code>ParallelCollectionRDD</code>。在<code>ParallelCollectionRDD</code>中并没有对<code>filter</code>方法进行重写，因此我们查看<code>RDD</code>中的<code>filter</code>方法:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(f: <span class="type">T</span> =&gt; <span class="type">Boolean</span>): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">   <span class="keyword">val</span> cleanF = sc.clean(f)</span><br><span class="line">   <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>[<span class="type">T</span>, <span class="type">T</span>](</span><br><span class="line">     <span class="keyword">this</span>,</span><br><span class="line">     (context, pid, iter) =&gt; iter.filter(cleanF),</span><br><span class="line">     preservesPartitioning = <span class="literal">true</span>)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<p>filter 方法返回了<code>MapPartitionsRDD</code>。为了方便起见，我将 ParallelCollectionRDD 类型的 RDD 称为 a，MapPartitionsRDD 类型的 RDD 成为B。那么先看一下 b 的 compute 方法:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">MapPartitionsRDD</span>[<span class="type">U</span>: <span class="type">ClassTag</span>, <span class="type">T</span>: <span class="type">ClassTag</span>](<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    var prev: <span class="type">RDD</span>[<span class="type">T</span>],</span></span></span><br><span class="line"><span class="class"><span class="params">    f: (<span class="type">TaskContext</span>, <span class="type">Int</span>, <span class="type">Iterator</span>[<span class="type">T</span>]</span>) <span class="title">=&gt;</span> <span class="title">Iterator</span>[<span class="type">U</span>],  <span class="title">//</span> (<span class="params"><span class="type">TaskContext</span>, partition index, iterator</span>)</span></span><br><span class="line"><span class="class">    <span class="title">preservesPartitioning</span></span>: <span class="type">Boolean</span> = <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">extends</span> <span class="type">RDD</span>[<span class="type">U</span>](prev) &#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">val</span> partitioner = <span class="keyword">if</span> (preservesPartitioning) firstParent[<span class="type">T</span>].partitioner <span class="keyword">else</span> <span class="type">None</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartitions</span></span>: <span class="type">Array</span>[<span class="type">Partition</span>] = firstParent[<span class="type">T</span>].partitions</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">U</span>] =</span><br><span class="line">    f(context, split.index, firstParent[<span class="type">T</span>].iterator(split, context))</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">clearDependencies</span></span>() &#123;</span><br><span class="line">    <span class="keyword">super</span>.clearDependencies()</span><br><span class="line">    prev = <span class="literal">null</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里可以看到 compute 方法实际上调用 f 这个函数。而 f 这个函数是在 filter 方法中传入的<code>(context, pid, iter) =&gt; iter.filter(cleanF)</code> 。那么实际上 compute 进行的计算为:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(context, split.index, firstParent[<span class="type">T</span>].iterator(split, context)) =&gt; firstParent[<span class="type">T</span>].iterator(split, context).filter(cleanF)</span><br></pre></td></tr></table></figure>
<p>前两个参数并没有用到，也就是最终的方法可以简化为:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">firstParent[<span class="type">T</span>].iterator(split, context) =&gt; firstParent[<span class="type">T</span>].iterator(split, context).filter(cleanF)</span><br></pre></td></tr></table></figure>
<p>这里出现了一个<code>firstParent</code>，我们知道 RDD 之间存在依赖关系，如果rdd3 依赖 rdd2，rdd2 依赖 rdd1。rdd2 与 rdd3 都是rdd1 的一个父rdd, spark也将其称为血缘关系。而故名意思 rdd2 是 rdd3 的 firstParent。在这个例子中 a 是 b的 firstParent。那么在b 的 compute 中又调用了 a 的 iterator 方法。而 a 的 iterator 方法又会调用a 的 comput 方法。用箭头表示调用关系:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b.iterotr -&gt; b.compute -&gt; a.iterotr -&gt; a.compute -&gt;| 调用</span><br><span class="line">b.iterotr &lt;- b.compute &lt;- a.iterotr &lt;- a.compute &lt;-| 返回</span><br></pre></td></tr></table></figure>
<p>而这里我们也可以看出来，b 调用 iterotr 返回的数据，是使用 b 中的方法对 a 返回数据进行处理。也就是b 的计算结果依赖于a的计算结果。如果将一个rdd比作一个函数，大概就是 <code>fb(fa())</code>，这样的。此时，我们已经得到了关于 spark 迭代的最简单的模型，假如我们有 n 个 rdd。那么之间的调用依赖关系便是:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">n.iterotr -&gt; n.compute -&gt; (n-1).iterotr -&gt; (n-1).compute -&gt;...-&gt; 1.iterotr -&gt; 1.compute-&gt;| </span><br><span class="line">n.iterotr &lt;- n.compute &lt;- (n-1).iterotr &lt;- (n-1).compute &lt;-...&lt;- 1.iterotr &lt;- 1.compute&lt;-|</span><br></pre></td></tr></table></figure>
<p>那么细心的同学应该注意到了，所有的数据都是源自于第一个 RDD。这里把它称为源 RDD。例如本例中产生的 RDD 以及 textFile 方法产生的HadoopRDD都属于源RDD。既然属于源 RDD ，那么这个 RDD 一定会与内存或磁盘中的源数据产生交互，否则就没有真实的数据来源。这里的源数据是指内存的数组、本地文件、或者是HDFS上的分布式文件等。</p>
<p>那么我们再继续看 a 的 compute 方法:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(s: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">InterruptibleIterator</span>(context,s.asInstanceOf[<span class="type">ParallelCollectionPartition</span>[<span class="type">T</span>]].iterator)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到 a 的 compute 根据 ParallelCollectionPartition 类的 iterator直接生成了一个迭代器。再看看ParallelCollectionPartition 的定义:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rivate[spark] <span class="class"><span class="keyword">class</span> <span class="title">ParallelCollectionPartition</span>[<span class="type">T</span>: <span class="type">ClassTag</span>](<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    var rddId: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    var slice: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    var values: <span class="type">Seq</span>[<span class="type">T</span>]</span></span></span><br><span class="line"><span class="class"><span class="params">  </span>) <span class="keyword">extends</span> <span class="title">Partition</span> <span class="keyword">with</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">iterator</span></span>: <span class="type">Iterator</span>[<span class="type">T</span>] = values.iterator</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure>
<p>iterator 是来自于 values 的一个迭代器，显然，values是内存中的数据。这说明 a 的 compute 方法直接返回了一个与源数据相关的迭代器。而这里 ParallelCollectionPartition 的数据是如何传入的，又是在什么时候被传入的呢?在<strong>分区</strong>小节将会提到。</p>
<p>到这里，我们已经基本了解了 spark 的迭代计算的原理以及数据来源。但是仍然还有许多不了解的地方，例如 iterotr 会接受一个  Partition 作为参数。那么这个Partition参数到底起到了什么作用。其次，我们知道 rdd 的 iterotr 方法 是从后开始往前调用，那么又是谁调用的最后一个 rdd 的 itertor 方法呢。接下来我们就探索一下与分区相关的内容.</p>
<h3 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h3><p>提到分区，大部分都是类似于“RDD 中最小的数据单元”，“只是数据的抽象分区，而不是真正的持有数据”等等这样的描述。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * An identifier for a partition in an RDD.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Partition</span> <span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">index</span></span>: <span class="type">Int</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">hashCode</span></span>(): <span class="type">Int</span> = index</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">equals</span></span>(other: <span class="type">Any</span>): <span class="type">Boolean</span> = <span class="keyword">super</span>.equals(other)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到分区的类十分的简单，只有一个 index 属性。 当将分区这个参数传入 itertor 时表示我们需要 index 对应的分区的数据经过一系列计算的之后的迭代器。那么显然，在源 RDD 中一定会有更据分区的index获取对应数据的代码部分。而我们又知道，对于<code>ParallelCollectionPartition</code>这个分区类是直接持有数据的迭代器，因此我们只需要知道这个类如何被建立，便知道了是如何根据分区获取数据的。我们在整个工程中搜索 ParallelCollectionPartition，发现:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartitions</span></span>: <span class="type">Array</span>[<span class="type">Partition</span>] = &#123;</span><br><span class="line">   <span class="keyword">val</span> slices = <span class="type">ParallelCollectionRDD</span>.slice(data, numSlices).toArray</span><br><span class="line">   slices.indices.map(i =&gt; <span class="keyword">new</span> <span class="type">ParallelCollectionPartition</span>(id, i, slices(i))).toArray</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<p>中生成了一系列的分区对象。其中 ParallelCollectionRDD.slice() 方法根据总共有多少个分区来将 data  进行切分。而 data 我们在调用 parallelize() 方法时传入的。看到这里就应该明白了，<strong>ParallelCollectionRDD 会将我们输入的数组进行切分，然后将利用分区对象持有数据的迭代器，当调用到 itertor 方法时就返回该分区的迭代器，供后续的RDD</strong>进行计算。虽然这里我们只看了 ParallelCollectionRDD 的原代码，但是其他例如 HadoopRDD 的远离基本相同。 只不过一个是从内存中获取数据进行切分，另一个是从磁盘上获取数据进行切分。</p>
<p>但是直到这里，我们仍然不知道 a 中的 itertor 方法的分区对象是如何传入的，因为这个方法间接被 b 的 itertor 方法调用。 而 b 的  itertor 方法同样也需要在外部被调用 ，因此要解决这个问题只需要找到 b 的 itertor 被调用的地方。不过我们首先可以根据代码猜测一下:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(s: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">InterruptibleIterator</span>(context,s.asInstanceOf[<span class="type">ParallelCollectionPartition</span>[<span class="type">T</span>]].iterator)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>a 中的 compute 方法直接将传入的分区对象转为了 ParallelCollectionPartition 并获取了对应的迭代器。根据对象间强转的</p>
<p>规则，传入的分区对象只能是 ParallelCollectionPartition 类型 ，因为其父类 RDD 并没有 iterator 方法。 并且传入的ParallelCollectionPartition 一定是持有源数据迭代器的对象，否则在 a 的 compute 中就无法向后返回迭代器。而且我们知道，spark 的计算是<strong>惰性计算</strong>，在调用 action 算子之后才会真正的开始计算。那么可以猜测，b 的 iterator 也是在调用了 count 方法之后才被调用的。为了证明这一点，我们继续查看代码:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count</span></span>(): <span class="type">Long</span> = sc.runJob(<span class="keyword">this</span>, <span class="type">Utils</span>.getIteratorSize _).sum</span><br></pre></td></tr></table></figure>
<p>可以看到是调用了 SparkContext 中的runJob方法:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    ...</span><br><span class="line">   dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)</span><br><span class="line">   ...</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<p>而 SparkContext 中的 runJob 又继续调用了 DAGScheduler 中的 runJob 方法， 继而调用了 submitJob 方法。之后由经历了 DAGSchedulerEventProcessLoop.post DAGSchedulerEventProcessLoop.doOnReceive 等方法，最后在 submitMissingTasks 看到建立了 ResultTask 对象。由于一个分区的数据会被一个task 处理，因此我们猜测在 ＲesultTask 中会有关于对应 rdd 调用 iterator 的信息，果然，在ResultTask中找到了 runTask 方法:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">runTask</span></span>(context: <span class="type">TaskContext</span>): <span class="type">U</span> = &#123;</span><br><span class="line">    <span class="comment">// Deserialize the RDD and the func using the broadcast variables.</span></span><br><span class="line">    <span class="keyword">val</span> threadMXBean = <span class="type">ManagementFactory</span>.getThreadMXBean</span><br><span class="line">    <span class="keyword">val</span> deserializeStartTime = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">    <span class="keyword">val</span> deserializeStartCpuTime = <span class="keyword">if</span> (threadMXBean.isCurrentThreadCpuTimeSupported) &#123;</span><br><span class="line">      threadMXBean.getCurrentThreadCpuTime</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="number">0</span>L</span><br><span class="line">    <span class="keyword">val</span> ser = <span class="type">SparkEnv</span>.get.closureSerializer.newInstance()</span><br><span class="line">    <span class="keyword">val</span> (rdd, func) = ser.deserialize[(<span class="type">RDD</span>[<span class="type">T</span>], (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>)](</span><br><span class="line">      <span class="type">ByteBuffer</span>.wrap(taskBinary.value), <span class="type">Thread</span>.currentThread.getContextClassLoader)</span><br><span class="line">    _executorDeserializeTime = <span class="type">System</span>.currentTimeMillis() - deserializeStartTime</span><br><span class="line">    _executorDeserializeCpuTime = <span class="keyword">if</span> (threadMXBean.isCurrentThreadCpuTimeSupported) &#123;</span><br><span class="line">      threadMXBean.getCurrentThreadCpuTime - deserializeStartCpuTime</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="number">0</span>L</span><br><span class="line"></span><br><span class="line">    func(context, rdd.iterator(partition, context))<span class="comment">//这里就 b 的iterator被真正调用的地方</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>那么我们可以看到最后一行有一个rdd的调用，而这个rdd 是反徐序列化得的，很明显，这个rdd就是 b。此时，b 调用 iterator 方法的地方找到了，但是分区对象partition依然没有找到从哪里传入。由于 partition 是 ResultTask 构造时传入的，我们回到 submitMissingTasks 中，创建ResultTask时分区参数对应的为变量 part 。 </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//从需要计算的分区map中获取分区，并生成task</span></span><br><span class="line"> partitionsToCompute.map &#123; id =&gt;</span><br><span class="line">      <span class="keyword">val</span> p: <span class="type">Int</span> = stage.partitions(id)</span><br><span class="line">      <span class="keyword">val</span> part = partitions(p)</span><br><span class="line">      <span class="keyword">val</span> locs = taskIdToLocations(id)</span><br><span class="line">      <span class="keyword">new</span> <span class="type">ResultTask</span>(stage.id, stage.latestInfo.attemptNumber,</span><br><span class="line">        taskBinary, part, locs, id, properties, serializedTaskMetrics,</span><br><span class="line">        <span class="type">Option</span>(jobId), <span class="type">Option</span>(sc.applicationId), sc.applicationAttemptId)</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>继续往上看，可以看到 part 是 从 partitions 中获取的。这里我们也可以看出 <strong>一个任务对应一个分区数据</strong>。继续往上看，发现<code>partitions = stage.rdd.partitions</code>。实际上来自于 Stage 中的 rdd 中的 partitions。那么看Stage 对 rdd 这个属性的描述:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"> *</span><br><span class="line"> * <span class="meta">@param</span> rdd <span class="type">RDD</span> that <span class="keyword">this</span> stage runs on: <span class="keyword">for</span> a shuffle map stage, it<span class="symbol">'s</span> the <span class="type">RDD</span> we run map tasks</span><br><span class="line"> *   on, <span class="keyword">while</span> <span class="keyword">for</span> a result stage, it<span class="symbol">'s</span> the target <span class="type">RDD</span> that we ran an action on</span><br><span class="line"> *   这里的意思是 rdd 表示调用 action 算子的 rdd 也就是 b </span><br><span class="line"> */</span><br><span class="line"><span class="keyword">private</span>[scheduler] <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Stage</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    val id: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val rdd: <span class="type">RDD</span>[_],</span></span></span><br><span class="line"><span class="class"><span class="params">    val numTasks: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val parents: <span class="type">List</span>[<span class="type">Stage</span>],</span></span></span><br><span class="line"><span class="class"><span class="params">    val firstJobId: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val callSite: <span class="type">CallSite</span></span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">Logging</span> </span>&#123;</span><br></pre></td></tr></table></figure>
<p>我们发现一个惊人的事实 ，这个 rdd 居然也是 b。也就是说，我们在 runTask函数中实际调用的方法是这样的:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">func(context, b.iterator(b.partitions(index), context))<span class="comment">//这里就 b 的iterator被真正调用的地方</span></span><br></pre></td></tr></table></figure>
<p>index 表示该task对应需要执行的分区标号。随即我们继续看 b 中的 partitions 的定义:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartitions</span></span>: <span class="type">Array</span>[<span class="type">Partition</span>] = firstParent[<span class="type">T</span>].partitions</span><br></pre></td></tr></table></figure>
<p>说明这个 partitions 来自于 a。 而 a 中的 partitions 实际上又来自于:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartitions</span></span>: <span class="type">Array</span>[<span class="type">Partition</span>] = &#123;</span><br><span class="line">   <span class="keyword">val</span> slices = <span class="type">ParallelCollectionRDD</span>.slice(data, numSlices).toArray</span><br><span class="line">   slices.indices.map(i =&gt; <span class="keyword">new</span> <span class="type">ParallelCollectionPartition</span>(id, i, slices(i))).toArray</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<p>这样最终 runTask 函数实际调用的方法为:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">func(context, b.iterator(a.partitions(index), context))<span class="comment">//这里就 b 的iterator被真正调用的地方</span></span><br></pre></td></tr></table></figure>
<p>这和我们猜测的相同，这个分区确实为 ParallelCollectionPartition 对象，并且也持有了源数据的迭代器。这里其实有一个很巧妙的地方，虽然 RDD  的迭代计算是从后往前调用，但是传入源 RDD 的分区对象依然来自于源 RDD 自身。</p>
<p>到了这里，我们也就明白分区对象是如何和数据关联起来的。ParallelCollectionPartition 对象中存在分区编号 index 以及 源数据的迭代器，通过分区编号就能获取到源数据的迭代器。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>经过如此大篇幅的讲解，终于对 spark 的迭代计算以及分区做了一个简单的分析。虽然还有很多地方不完善，例如还有其它类型的分区对象没有讲解，但是我相信你能完全看懂这篇文章所讲的内容，其他类型的分区对象对于你来说也很简单了。作者水平有限，并且这也是作者的第一篇关于 spark 源码阅读的博客，难免有遗漏和错误。如果想如给作者提建议和意见的话，请联系作者的微信或直接在 github 上提 issue 。</p>
</span>
      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Spark/" rel="tag">#Spark</a>
          
            <a href="/tags/编程/" rel="tag">#编程</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/06/07/使用git+hexo建立个人博客/" rel="prev">使用git + hexo 建立个人博客</a>
            
          </div>

          <div class="post-nav-next post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div>
      
    </div>

    <div class="post-spread">
      
        <div class="bdsharebuttonbox">
	<a href="#" class="bds_more" data-cmd="more"></a>
	<a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a>
	<a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
	<a href="#" class="bds_sqq" data-cmd="sqq" title="分享到QQ好友"></a>
	<a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a>
	<a href="#" class="bds_tqq" data-cmd="tqq" title="分享到腾讯微博"></a>
	<a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a>
</div>
<script>
    window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"16"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script>

      
    </div>
  </div>

 </div>

        

        
          <div class="comments" id="comments">
            
          </div>
        
      </div>

      
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table Of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" src="/images/default_avatar.jpg" alt="lishion" itemprop="image"/>
          <p class="site-author-name" itemprop="name">lishion</p>
        </div>
        <p class="site-description motion-element" itemprop="description"></p>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">12</span>
              <span class="site-state-item-name">posts</span>
            </a>
          </div>

          <div class="site-state-item site-state-categories">
            <a href="/categories">
              <span class="site-state-item-count">5</span>
              <span class="site-state-item-name">categories</span>
              </a>
          </div>

          <div class="site-state-item site-state-tags">
            <a href="/tags">
              <span class="site-state-item-count">7</span>
              <span class="site-state-item-name">tags</span>
              </a>
          </div>

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/lishion" target="_blank">GitHub</a>
              </span>
            
          
        </div>

        <div class="links-of-friendly motion-element">
          
        </div>

        
        

      </section>

      
        <section class="post-toc-wrap sidebar-panel-active">
          <div class="post-toc-indicator-top post-toc-indicator"></div>
          <div class="post-toc">
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#写在最开始"><span class="nav-number">1.</span> <span class="nav-text">写在最开始</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#本部分主要内容"><span class="nav-number">2.</span> <span class="nav-text">本部分主要内容</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#准备工作"><span class="nav-number">3.</span> <span class="nav-text">准备工作</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#一个简单的例子"><span class="nav-number">4.</span> <span class="nav-text">一个简单的例子</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#迭代计算"><span class="nav-number">5.</span> <span class="nav-text">迭代计算</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#comput-方法与-itertor"><span class="nav-number">5.1.</span> <span class="nav-text">comput 方法与 itertor</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#分区"><span class="nav-number">5.2.</span> <span class="nav-text">分区</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结"><span class="nav-number">6.</span> <span class="nav-text">总结</span></a></li></ol></div>
            
          </div>
          <div class="post-toc-indicator-bottom post-toc-indicator"></div>
        </section>
      

    </div>
  </aside>


    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner"> <div class="copyright" >
  
  &copy; &nbsp; 
  <span itemprop="copyrightYear">2018
  </span>
  <span class="with-love">
    <i class="icon-next-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">lishion
  </span>
</div>

<!-- <div class="powered-by">
  Powered by <a class="theme-link" href="http://hexo.io">Hexo</a>
</div> -->

<!-- <div class="theme-info">
  Theme by <a class="theme-link" href="http://blog.guxiangfly.cn">guxiangfly</a>.<a class="theme-link" href="https://github.com/GuXiangFly/next-guxiangfly">next-guxiangfly</a>
</div> -->

<!-- busuanzi -->



 </div>
    </footer>

    <div class="back-to-top"></div>
  </div>

  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  
  
    
    

  


  
  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/js/fancy-box.js?v=0.4.5.1"></script>


  <script type="text/javascript" src="/js/helpers.js?v=0.4.5.1"></script>
  

  <script type="text/javascript" src="/vendors/velocity/velocity.min.js"></script>
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js"></script>

  <script type="text/javascript" src="/js/motion_global.js?v=0.4.5.1" id="motion.global"></script>




  <script type="text/javascript" src="/js/nav-toggle.js?v=0.4.5.1"></script>
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  
<script type="text/javascript" src="/js/bootstrap.scrollspy.js?v=0.4.5.1" id="bootstrap.scrollspy.custom"></script>


<script type="text/javascript" id="sidebar.toc.highlight">
  $(document).ready(function () {
    var tocSelector = '.post-toc';
    var $tocSelector = $(tocSelector);
    var activeCurrentSelector = '.active-current';

    $tocSelector
      .on('activate.bs.scrollspy', function () {
        var $currentActiveElement = $(tocSelector + ' .active').last();

        removeCurrentActiveClass();
        $currentActiveElement.addClass('active-current');

        $tocSelector[0].scrollTop = $currentActiveElement.position().top;
      })
      .on('clear.bs.scrollspy', function () {
        removeCurrentActiveClass();
      });

    function removeCurrentActiveClass () {
      $(tocSelector + ' ' + activeCurrentSelector)
        .removeClass(activeCurrentSelector.substring(1));
    }

    function processTOC () {
      getTOCMaxHeight();
      toggleTOCOverflowIndicators();
    }

    function getTOCMaxHeight () {
      var height = $('.sidebar').height() -
                   $tocSelector.position().top -
                   $('.post-toc-indicator-bottom').height();

      $tocSelector.css('height', height);

      return height;
    }

    function toggleTOCOverflowIndicators () {
      tocOverflowIndicator(
        '.post-toc-indicator-top',
        $tocSelector.scrollTop() > 0 ? 'show' : 'hide'
      );

      tocOverflowIndicator(
        '.post-toc-indicator-bottom',
        $tocSelector.scrollTop() >= $tocSelector.find('ol').height() - $tocSelector.height() ? 'hide' : 'show'
      )
    }

    $(document).on('sidebar.motion.complete', function () {
      processTOC();
    });

    $('body').scrollspy({ target: tocSelector });
    $(window).on('resize', function () {
      if ( $('.sidebar').hasClass('sidebar-active') ) {
        processTOC();
      }
    });

    onScroll($tocSelector);

    function onScroll (element) {
      element.on('mousewheel DOMMouseScroll', function (event) {
          var oe = event.originalEvent;
          var delta = oe.wheelDelta || -oe.detail;

          this.scrollTop += ( delta < 0 ? 1 : -1 ) * 30;
          event.preventDefault();

          toggleTOCOverflowIndicators();
      });
    }

    function tocOverflowIndicator (indicator, action) {
      var $indicator = $(indicator);
      var opacity = action === 'show' ? 0.4 : 0;
      $indicator.velocity ?
        $indicator.velocity('stop').velocity({
          opacity: opacity
        }, { duration: 100 }) :
        $indicator.stop().animate({
          opacity: opacity
        }, 100);
    }

  });
</script>

<script type="text/javascript" id="sidebar.nav">
  $(document).ready(function () {
    var html = $('html');
    var TAB_ANIMATE_DURATION = 200;
    var hasVelocity = $.isFunction(html.velocity);

    $('.sidebar-nav li').on('click', function () {
      var item = $(this);
      var activeTabClassName = 'sidebar-nav-active';
      var activePanelClassName = 'sidebar-panel-active';
      if (item.hasClass(activeTabClassName)) {
        return;
      }

      var currentTarget = $('.' + activePanelClassName);
      var target = $('.' + item.data('target'));

      hasVelocity ?
        currentTarget.velocity('transition.slideUpOut', TAB_ANIMATE_DURATION, function () {
          target
            .velocity('stop')
            .velocity('transition.slideDownIn', TAB_ANIMATE_DURATION)
            .addClass(activePanelClassName);
        }) :
        currentTarget.animate({ opacity: 0 }, TAB_ANIMATE_DURATION, function () {
          currentTarget.hide();
          target
            .stop()
            .css({'opacity': 0, 'display': 'block'})
            .animate({ opacity: 1 }, TAB_ANIMATE_DURATION, function () {
              currentTarget.removeClass(activePanelClassName);
              target.addClass(activePanelClassName);
            });
        });

      item.siblings().removeClass(activeTabClassName);
      item.addClass(activeTabClassName);
    });

    $('.post-toc a').on('click', function (e) {
      e.preventDefault();
      var targetSelector = escapeSelector(this.getAttribute('href'));
      var offset = $(targetSelector).offset().top;
      hasVelocity ?
        html.velocity('stop').velocity('scroll', {
          offset: offset  + 'px',
          mobileHA: false
        }) :
        $('html, body').stop().animate({
          scrollTop: offset
        }, 500);
    });

    // Expand sidebar on post detail page by default, when post has a toc.
    var $tocContent = $('.post-toc-content');
    if (isDesktop() && CONFIG.sidebar === 'post') {
      if ($tocContent.length > 0 && $tocContent.html().trim().length > 0) {
        displaySidebar();
      }
    }
  });
</script>



  <script type="text/javascript">
    $(document).ready(function () {
      if (CONFIG.sidebar === 'always') {
        displaySidebar();
      }
      if (isMobile()) {
        FastClick.attach(document.body);
      }
    });
  </script>

  
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  
  

  
  <script type="text/javascript" src="/js/lazyload.js"></script>
  <script type="text/javascript">
    $(function () {
      $("#posts").find('img').lazyload({
        placeholder: "/images/loading.gif",
        effect: "fadeIn"
      });
    });
  </script>
</body>
</html>
