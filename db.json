{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/next-gux/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next-gux/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next-gux/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next-gux/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next-gux/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next-gux/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next-gux/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next-gux/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next-gux/source/images/loading.gif","path":"images/loading.gif","modified":0,"renderable":1},{"_id":"themes/next-gux/source/images/placeholder.gif","path":"images/placeholder.gif","modified":0,"renderable":1},{"_id":"themes/next-gux/source/images/searchicon.png","path":"images/searchicon.png","modified":0,"renderable":1},{"_id":"themes/next-gux/source/js/bootstrap.scrollspy.js","path":"js/bootstrap.scrollspy.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/js/fancy-box.js","path":"js/fancy-box.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/js/helpers.js","path":"js/helpers.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/js/hook-duoshuo.js","path":"js/hook-duoshuo.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/js/lazyload.js","path":"js/lazyload.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/js/motion_fallback.js","path":"js/motion_fallback.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/js/motion_global.js","path":"js/motion_global.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/js/nav-toggle.js","path":"js/nav-toggle.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/js/ua-parser.min.js","path":"js/ua-parser.min.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-default/icomoon.eot","path":"fonts/icon-default/icomoon.eot","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-default/icomoon.svg","path":"fonts/icon-default/icomoon.svg","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-default/icomoon.ttf","path":"fonts/icon-default/icomoon.ttf","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-default/icomoon.woff","path":"fonts/icon-default/icomoon.woff","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-default/selection.json","path":"fonts/icon-default/selection.json","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-feather/icomoon.eot","path":"fonts/icon-feather/icomoon.eot","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-feather/icomoon.svg","path":"fonts/icon-feather/icomoon.svg","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-feather/icomoon.ttf","path":"fonts/icon-feather/icomoon.ttf","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-feather/icomoon.woff","path":"fonts/icon-feather/icomoon.woff","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-feather/selection.json","path":"fonts/icon-feather/selection.json","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-fifty-shades/icomoon.eot","path":"fonts/icon-fifty-shades/icomoon.eot","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-fifty-shades/icomoon.svg","path":"fonts/icon-fifty-shades/icomoon.svg","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-fifty-shades/icomoon.ttf","path":"fonts/icon-fifty-shades/icomoon.ttf","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-fifty-shades/icomoon.woff","path":"fonts/icon-fifty-shades/icomoon.woff","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-fifty-shades/selection.json","path":"fonts/icon-fifty-shades/selection.json","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-icomoon/icomoon.eot","path":"fonts/icon-icomoon/icomoon.eot","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-icomoon/icomoon.svg","path":"fonts/icon-icomoon/icomoon.svg","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-icomoon/icomoon.ttf","path":"fonts/icon-icomoon/icomoon.ttf","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-icomoon/icomoon.woff","path":"fonts/icon-icomoon/icomoon.woff","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-linecons/icomoon.eot","path":"fonts/icon-linecons/icomoon.eot","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-linecons/icomoon.svg","path":"fonts/icon-linecons/icomoon.svg","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-linecons/icomoon.ttf","path":"fonts/icon-linecons/icomoon.ttf","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-linecons/icomoon.woff","path":"fonts/icon-linecons/icomoon.woff","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-linecons/selection.json","path":"fonts/icon-linecons/selection.json","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/fastclick/LICENSE","path":"vendors/fastclick/LICENSE","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/fastclick/README.md","path":"vendors/fastclick/README.md","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/fastclick/bower.json","path":"vendors/fastclick/bower.json","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/velocity/bower.json","path":"vendors/velocity/bower.json","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/velocity/velocity.min.js","path":"vendors/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/velocity/velocity.ui.js","path":"vendors/velocity/velocity.ui.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/velocity/velocity.ui.min.js","path":"vendors/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/jquery/index.js","path":"vendors/jquery/index.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/blank.gif","path":"vendors/fancybox/source/blank.gif","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/fancybox_loading.gif","path":"vendors/fancybox/source/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/fancybox_loading@2x.gif","path":"vendors/fancybox/source/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/fancybox_overlay.png","path":"vendors/fancybox/source/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/fancybox_sprite.png","path":"vendors/fancybox/source/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/fancybox_sprite@2x.png","path":"vendors/fancybox/source/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/jquery.fancybox.css","path":"vendors/fancybox/source/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/jquery.fancybox.js","path":"vendors/fancybox/source/jquery.fancybox.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/jquery.fancybox.pack.js","path":"vendors/fancybox/source/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/fastclick/lib/fastclick.js","path":"vendors/fastclick/lib/fastclick.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/fastclick/lib/fastclick.min.js","path":"vendors/fastclick/lib/fastclick.min.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/velocity/velocity.js","path":"vendors/velocity/velocity.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/images/background.jpg","path":"images/background.jpg","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/helpers/fancybox_buttons.png","path":"vendors/fancybox/source/helpers/fancybox_buttons.png","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"vendors/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"vendors/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/helpers/jquery.fancybox-media.js","path":"vendors/fancybox/source/helpers/jquery.fancybox-media.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"vendors/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"vendors/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/images/default_avatar.jpg","path":"images/default_avatar.jpg","modified":0,"renderable":1}],"Cache":[{"_id":"themes/next-gux/.bowerrc","hash":"80e096fdc1cf912ee85dd9f7e6e77fd40cf60f10","modified":1528952887131},{"_id":"themes/next-gux/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1528952887131},{"_id":"themes/next-gux/.gitignore","hash":"689a812947ccb5de9ed3ecdc5f927030adfd1d7a","modified":1528952887131},{"_id":"themes/next-gux/.jshintrc","hash":"12c5e37da3432bee2219ed1c667076d54f1639c0","modified":1528952887131},{"_id":"themes/next-gux/README.md","hash":"57c54b159185e346012669a5595b72a5b31cc91f","modified":1528952887131},{"_id":"themes/next-gux/_config.yml","hash":"8956f4f4a2ae603c7d3d5580ab9cb472336c43b2","modified":1528969635000},{"_id":"themes/next-gux/_config.yml.bak","hash":"f18ce03aaee919abef64fd6df1772935b577c3e0","modified":1528952887131},{"_id":"themes/next-gux/bower.json","hash":"057ec1580f78a7adae66bd26fe9e9f924621174f","modified":1528952887131},{"_id":"source/_posts/Spark-源码阅读计划-伪-第三部分-图解-shuffle-read.md","hash":"9fba0917b9c65919a4b28035998a9bdaa9ec8c15","modified":1528900939376},{"_id":"source/_posts/Spark-源码阅读计划-第一部分-迭代计算.md","hash":"1e2c475b21077ccad1516bcc4fcb343ab6cdbbe7","modified":1528900939376},{"_id":"source/_posts/使用git+hexo建立个人博客.md","hash":"6124f36cebc9646c1250eb3b330dde1de66c77d5","modified":1528900939376},{"_id":"source/_posts/常用-linux-命令系列.md","hash":"6e16a5dd589d1beec304e63b84921e2df5ec6831","modified":1528968496763},{"_id":"source/_posts/源码阅读计划-第二部分-shuffle-write.md","hash":"db64f04a37571e1201eb232a357569fb9e7a4ae3","modified":1528900939376},{"_id":"source/about/index.md","hash":"e9ca55c635160ccd3b41404fc77c6d4595573222","modified":1528900939376},{"_id":"themes/next-gux/.idea/compiler.xml","hash":"bff5196ea91a033d64bb5c4d6647a2e3b71bb548","modified":1528952887131},{"_id":"themes/next-gux/.idea/misc.xml","hash":"318a7276f979d7822e16d305da5f7c939a00852d","modified":1528952887131},{"_id":"themes/next-gux/.idea/modules.xml","hash":"64ff052d1dae024fe94ea8e2b1d7f704609f22a6","modified":1528952887131},{"_id":"themes/next-gux/.idea/next-guxiangfly.iml","hash":"980957b57c4f1eae5e85d664d8375f83d47d3e5a","modified":1528952887131},{"_id":"themes/next-gux/.idea/vcs.xml","hash":"6f94fc1df9e8721673d47588ac444667dc9ded06","modified":1528952887131},{"_id":"themes/next-gux/.idea/workspace.xml","hash":"a73aef09b7d8a062a38f2a2f6f0242f91844846b","modified":1528952887131},{"_id":"themes/next-gux/languages/de.yml","hash":"7a8de0e5665c52a1bf168c1e7dd222c8a74fb0ab","modified":1528952887135},{"_id":"themes/next-gux/languages/default.yml","hash":"7e65ef918f16d0189055deb5f1616b9dedcb1920","modified":1528952887135},{"_id":"themes/next-gux/languages/en.yml","hash":"7e65ef918f16d0189055deb5f1616b9dedcb1920","modified":1528952887135},{"_id":"themes/next-gux/languages/fr-FR.yml","hash":"6d097445342a9fb5235afea35d65bf5271b772f0","modified":1528952887135},{"_id":"themes/next-gux/languages/ru.yml","hash":"b4a827b9ddac9d5f6dca096fe513aeafb46a3e93","modified":1528952887135},{"_id":"themes/next-gux/languages/zh-Hans.yml","hash":"8af76df5557561050a950bdd7091d3bb3939c5c0","modified":1528952887135},{"_id":"themes/next-gux/languages/zh-hk.yml","hash":"3fc38103c9efa6f6c37149adbddb014ff85ec849","modified":1528952887135},{"_id":"themes/next-gux/languages/zh-tw.yml","hash":"8897a06e521b36c7a1226c72057c8357611eded8","modified":1528952887135},{"_id":"themes/next-gux/scripts/merge-configs.js","hash":"dfd147d1317e56d283f5e779f00608e913603b51","modified":1528952887135},{"_id":"themes/next-gux/layout/_layout.swig","hash":"cbf2a45d0aca11965f083a732e482141f3f0b1c0","modified":1528952887135},{"_id":"themes/next-gux/layout/archive.swig","hash":"0c3ce594759f347ea90a4ce592a7a18e2ae4cc5c","modified":1528952887135},{"_id":"themes/next-gux/layout/category.swig","hash":"d6b3e1dc5e0b8deade9a084c463126e70188ee9b","modified":1528952887135},{"_id":"themes/next-gux/layout/index.swig","hash":"fdc801f0da71a2eb205ce9c0b12f156b219fdc9c","modified":1528952887135},{"_id":"themes/next-gux/layout/page.swig","hash":"beb1fc9a4e35b602a18b59f895544c6a838a67f2","modified":1528952887135},{"_id":"themes/next-gux/layout/post.swig","hash":"d1fe5e273d5852bbc5009cc1df629248fee54df1","modified":1528952887135},{"_id":"themes/next-gux/layout/tag.swig","hash":"aab44af54fcbc66fea4ad12b2767ffca3eadd451","modified":1528952887135},{"_id":"themes/next-gux/test/.jshintrc","hash":"096ed6df627373edd820f24d46b8baf528dee61d","modified":1528952887143},{"_id":"themes/next-gux/test/helpers.js","hash":"7c8b0c7213ae06ec4e7948971f9b12842207b5c7","modified":1528952887143},{"_id":"themes/next-gux/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1528952887143},{"_id":"source/_posts/Spark-源码阅读计划-伪-第三部分-图解-shuffle-read/part.png","hash":"b2f702f44036d20b43eedec75b1cbbe0a06d5877","modified":1528900939376},{"_id":"source/_posts/Spark-源码阅读计划-伪-第三部分-图解-shuffle-read/sort.png","hash":"2fc1a5957fc0bfc7cf3027e38759fa3c65077768","modified":1528900939376},{"_id":"themes/next-gux/.idea/inspectionProfiles/Project_Default.xml","hash":"e1a86ce90b80bedfa05ec86db802b187d973f133","modified":1528952887131},{"_id":"themes/next-gux/scripts/tags/center-quote.js","hash":"37274f743c2054244dcbbde56fba9ff353414281","modified":1528952887135},{"_id":"themes/next-gux/scripts/tags/full-image.js","hash":"0d69739d1bad5861a4a6ff2db511c3669783e438","modified":1528952887135},{"_id":"themes/next-gux/layout/_macro/post-collapse.swig","hash":"42927bdde998cefd3cf4f19b0476d69bd9e5116a","modified":1528952887135},{"_id":"themes/next-gux/layout/_macro/post.swig","hash":"770c6ed5562205ed2b64d4c02f5418893144a9ff","modified":1528952887135},{"_id":"themes/next-gux/layout/_macro/sidebar.swig","hash":"7af60c855c060c5318df7264eb6860a7fbb7c3ce","modified":1528952887135},{"_id":"themes/next-gux/layout/_partials/footer.swig","hash":"f25e080de405dd5db5b85366eb9257eee5997dce","modified":1528969110240},{"_id":"themes/next-gux/layout/_partials/footer.swig.bak","hash":"5cbf9456977c2087238bfcc47623327fb0e57f20","modified":1528952887135},{"_id":"themes/next-gux/layout/_partials/head.swig","hash":"fc9ab6752cbdd13f563b3969d039ef7cf05ab046","modified":1528952887135},{"_id":"themes/next-gux/layout/_partials/header.swig","hash":"e66b8fca801d5daba31496d4b00bac3018b7c29b","modified":1528952887135},{"_id":"themes/next-gux/layout/_partials/old-browsers.swig","hash":"dbbfea810bf3a2ed9c83b9a6683037175aacfc67","modified":1528952887135},{"_id":"themes/next-gux/layout/_partials/pagination.swig","hash":"d6c7f04eee4388d8f133eb5526b7c0875c321a30","modified":1528952887135},{"_id":"themes/next-gux/layout/_partials/search.swig","hash":"64f14da26792a17bc27836c4e9d83190175f36e6","modified":1528952887135},{"_id":"themes/next-gux/layout/_scripts/analytics.swig","hash":"0ebbf76c2317faa8ba31365adba59331c2e0262c","modified":1528952887135},{"_id":"themes/next-gux/layout/_scripts/bootstrap.scrollspy.swig","hash":"85295f126836b95f0837d03e58228bb3cf8c4490","modified":1528952887135},{"_id":"themes/next-gux/layout/_scripts/fancy-box.swig","hash":"41b4ff1446060c88c33bf666a32277dcf12129f0","modified":1528952887135},{"_id":"themes/next-gux/layout/_scripts/helpers.swig","hash":"4d2cbfca0aaf546a2b5813288073e824c1498fdf","modified":1528952887135},{"_id":"themes/next-gux/layout/_scripts/mathjax.swig","hash":"abc52fefb276c52cbb19de5c214521dfcf2a10fd","modified":1528952887135},{"_id":"themes/next-gux/layout/_scripts/motion.swig","hash":"817705bfd1a1282cb6bf59094afe507e11455aa0","modified":1528952887135},{"_id":"themes/next-gux/source/css/main.styl","hash":"151dccbe683e6a858d8a6ea09df913a2344b417f","modified":1528952887135},{"_id":"themes/next-gux/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1528952887139},{"_id":"themes/next-gux/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1528952887139},{"_id":"themes/next-gux/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1528952887139},{"_id":"themes/next-gux/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1528952887139},{"_id":"themes/next-gux/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1528952887139},{"_id":"themes/next-gux/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1528952887139},{"_id":"themes/next-gux/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1528952887139},{"_id":"themes/next-gux/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1528952887139},{"_id":"themes/next-gux/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1528952887139},{"_id":"themes/next-gux/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1528952887139},{"_id":"themes/next-gux/source/js/bootstrap.scrollspy.js","hash":"ae7bdce88b515aade4eea8bf7407eec458bcd625","modified":1528952887139},{"_id":"themes/next-gux/source/js/fancy-box.js","hash":"b382ba746f4566682948ce92f2588ee940cd1755","modified":1528952887139},{"_id":"themes/next-gux/source/js/helpers.js","hash":"7499b413242a2e75a9308444aade5b72a12cce7d","modified":1528952887139},{"_id":"themes/next-gux/source/js/hook-duoshuo.js","hash":"9881b19132ad90dffd82c53947e4c356e30353e7","modified":1528952887139},{"_id":"themes/next-gux/source/js/lazyload.js","hash":"b92e9acdc7afc15468314c03f4a643b0c93944cf","modified":1528952887139},{"_id":"themes/next-gux/source/js/motion_fallback.js","hash":"a767d522c65a8b2fbad49135c9332135c6785c3e","modified":1528952887139},{"_id":"themes/next-gux/source/js/motion_global.js","hash":"fea8cbb854601b7aee14e51079b3e3f80a1de261","modified":1528952887139},{"_id":"themes/next-gux/source/js/nav-toggle.js","hash":"78b59f1beb12adea0d7f9bcf4377cb699963f220","modified":1528952887139},{"_id":"themes/next-gux/source/js/ua-parser.min.js","hash":"acf0ee6a47ffb7231472b56e43996e3f947c258a","modified":1528952887139},{"_id":"themes/next-gux/source/css/_mixins/Mala.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1528952887135},{"_id":"themes/next-gux/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1528952887135},{"_id":"themes/next-gux/source/css/_mixins/default.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1528952887135},{"_id":"themes/next-gux/source/css/_variables/default.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1528952887135},{"_id":"themes/next-gux/source/images/default_avatar.png","hash":"b86a36d36877ca73a88a77a8eb19291cc3828947","modified":1528967548501},{"_id":"themes/next-gux/layout/_partials/search/swiftype.swig","hash":"00c2b49f6289198b0b2b4e157e4ee783277f32a7","modified":1528952887135},{"_id":"themes/next-gux/layout/_partials/search/tinysou.swig","hash":"2f92046e0b50ebd65abb7045b1cbbfc50abbb034","modified":1528952887135},{"_id":"themes/next-gux/layout/_partials/share/baidu_share.swig","hash":"b4506174e385ee5fb1c94122b45732e3413a0ba2","modified":1528952887135},{"_id":"themes/next-gux/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1528952887135},{"_id":"themes/next-gux/layout/_partials/share/jiathis.swig","hash":"63315fcf210799f894208c9f512737096df84962","modified":1528952887135},{"_id":"themes/next-gux/layout/_partials/suprise/assist.swig","hash":"6b8a25353dbfe9f92e0d48388a6f46996e03b7cb","modified":1528952887135},{"_id":"themes/next-gux/layout/_partials/suprise/donate.swig","hash":"25f196afc193a7b192a49cb7d84db7d727a9e8c2","modified":1528952887135},{"_id":"themes/next-gux/layout/_scripts/analytics/baidu-analytics.swig","hash":"7c43d66da93cde65b473a7d6db2a86f9a42647d6","modified":1528952887135},{"_id":"themes/next-gux/layout/_scripts/analytics/busuanzi.swig","hash":"dee5f8ce80fc34fa2b0c914a45465c79da80612b","modified":1528952887135},{"_id":"themes/next-gux/layout/_scripts/analytics/google-analytics.swig","hash":"30a23fa7e816496fdec0e932aa42e2d13098a9c2","modified":1528952887135},{"_id":"themes/next-gux/layout/_scripts/comments/disqus.swig","hash":"3491d3cebabc8a28857200db28a1be65aad6adc2","modified":1528952887135},{"_id":"themes/next-gux/layout/_scripts/comments/duoshuo.swig","hash":"3351ea62225933f8045d036a79654e19e84d19a7","modified":1528952887135},{"_id":"themes/next-gux/layout/_scripts/pages/post-details.swig","hash":"b63ef233886538f30ced60344ac15d25e5f3e0af","modified":1528952887135},{"_id":"themes/next-gux/source/css/_custom/Mala.styl","hash":"ec81ad093b68bbb121e45d054a646f32397e137d","modified":1528954099393},{"_id":"themes/next-gux/source/css/_custom/Mala.styl.bak","hash":"4ec83e3e4ef02e67dca86615aa013c0b8a4f4b18","modified":1528952887135},{"_id":"themes/next-gux/source/css/_mixins/base.styl","hash":"66985fe77bd323f7f8f634908e17166f51e96e95","modified":1528952887135},{"_id":"themes/next-gux/source/css/_variables/Mala.styl","hash":"360aaa1746bc4032079493ff6027f8431f65b6df","modified":1528952887135},{"_id":"themes/next-gux/source/css/_variables/base.styl","hash":"fb85e5d8e37661c4c333b8aa08a7619cd7b3d046","modified":1528952887135},{"_id":"themes/next-gux/source/css/_variables/custom.styl","hash":"1a3e002602b0dff287b2463d2cd25c22f349a145","modified":1528952887135},{"_id":"themes/next-gux/source/fonts/icon-default/icomoon.eot","hash":"90763e97be18be78e65749075225cceeddc6fa8a","modified":1528952887135},{"_id":"themes/next-gux/source/fonts/icon-default/icomoon.svg","hash":"f92ad8cddc250f0bb5ca466fca95d321987e127e","modified":1528952887135},{"_id":"themes/next-gux/source/fonts/icon-default/icomoon.ttf","hash":"c093408e6030221cafc1f79d897f1fb5283c1178","modified":1528952887135},{"_id":"themes/next-gux/source/fonts/icon-default/icomoon.woff","hash":"dbe0368f2a65d87b13234cfea29d9783892fc7a8","modified":1528952887135},{"_id":"themes/next-gux/source/fonts/icon-default/selection.json","hash":"dc07c29f687315f9458f6b251c214768af865fb2","modified":1528952887139},{"_id":"themes/next-gux/source/fonts/icon-feather/icomoon.eot","hash":"11554b9e9d5b9f535ba96cbb27d45d8c8f1689fd","modified":1528952887139},{"_id":"themes/next-gux/source/fonts/icon-feather/icomoon.svg","hash":"d5eb756eefda9b454dcb23c2b1cefd4051d18d41","modified":1528952887139},{"_id":"themes/next-gux/source/fonts/icon-feather/icomoon.ttf","hash":"b2bbae4b613403cf61ad25037913378da1c07b8f","modified":1528952887139},{"_id":"themes/next-gux/source/fonts/icon-feather/icomoon.woff","hash":"2ea1c59c17422798e64ee6f4e9ce1f7aff1a06a5","modified":1528952887139},{"_id":"themes/next-gux/source/fonts/icon-feather/selection.json","hash":"06ea91e3f98ebe1080087acad4356802bc5b6ebf","modified":1528952887139},{"_id":"themes/next-gux/source/fonts/icon-fifty-shades/icomoon.eot","hash":"da86ba5df72d1288de9e9633e5f528062dd427d5","modified":1528952887139},{"_id":"themes/next-gux/source/fonts/icon-fifty-shades/icomoon.svg","hash":"1a4afd739e1f8eb8d430dbdd29e36a9999802e8d","modified":1528952887139},{"_id":"themes/next-gux/source/fonts/icon-fifty-shades/icomoon.ttf","hash":"72fe82e1f3db52414eed706952d385af241cb196","modified":1528952887139},{"_id":"themes/next-gux/source/fonts/icon-fifty-shades/icomoon.woff","hash":"4de6a74f523dee33d95dde61caae5809f9a5d448","modified":1528952887139},{"_id":"themes/next-gux/source/fonts/icon-fifty-shades/selection.json","hash":"fdd09098d1c3688e2c88cf33fd51e76b383b6d7f","modified":1528952887139},{"_id":"themes/next-gux/source/fonts/icon-icomoon/icomoon.eot","hash":"301fcf00c24750dddf1c529f944ca62c7f1a217d","modified":1528952887139},{"_id":"themes/next-gux/source/fonts/icon-icomoon/icomoon.svg","hash":"e316347805eb93425faa678611c5e42a7152da8f","modified":1528952887139},{"_id":"themes/next-gux/source/fonts/icon-icomoon/icomoon.ttf","hash":"f399713d1c9400d4d3373e38991a7e362a754a94","modified":1528952887139},{"_id":"themes/next-gux/source/fonts/icon-icomoon/icomoon.woff","hash":"05f1ec0bd307da5e731a86eb4961589a6042aebb","modified":1528952887139},{"_id":"themes/next-gux/source/fonts/icon-linecons/icomoon.eot","hash":"e2d7f040428a632f3c50bfa94083b759936effc2","modified":1528952887139},{"_id":"themes/next-gux/source/fonts/icon-linecons/icomoon.svg","hash":"808eaf7d61f7e67c76976265c885e79c36920f0b","modified":1528952887139},{"_id":"themes/next-gux/source/fonts/icon-linecons/icomoon.ttf","hash":"078068206684e4f185b0187ad3cee16f54a287d7","modified":1528952887139},{"_id":"themes/next-gux/source/fonts/icon-linecons/icomoon.woff","hash":"0b07ee6ceda3b1bceb40c1e7379b3aa48dcc15a8","modified":1528952887139},{"_id":"themes/next-gux/source/fonts/icon-linecons/selection.json","hash":"db4ce25d31449ecc6685b32e145252103967bb5c","modified":1528952887139},{"_id":"themes/next-gux/source/vendors/fastclick/.bower.json","hash":"93ebd5b35e632f714dcf1753e1f6db77ec74449b","modified":1528952887143},{"_id":"themes/next-gux/source/vendors/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1528952887143},{"_id":"themes/next-gux/source/vendors/fastclick/README.md","hash":"1decd8e1adad2cd6db0ab50cf56de6035156f4ea","modified":1528952887143},{"_id":"themes/next-gux/source/vendors/fastclick/bower.json","hash":"13379463c7463b4b96d13556b46faa4cc38d81e6","modified":1528952887143},{"_id":"themes/next-gux/source/vendors/jquery/.bower.json","hash":"91745c2cc6c946c7275f952b2b0760b880cea69e","modified":1528952887143},{"_id":"themes/next-gux/source/vendors/velocity/.bower.json","hash":"05f960846f1c7a93dab1d3f9a1121e86812e8c88","modified":1528952887143},{"_id":"themes/next-gux/source/vendors/velocity/bower.json","hash":"2ec99573e84c7117368beccb9e94b6bf35d2db03","modified":1528952887143},{"_id":"themes/next-gux/source/vendors/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1528952887143},{"_id":"themes/next-gux/source/vendors/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1528952887143},{"_id":"themes/next-gux/source/vendors/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1528952887143},{"_id":"themes/next-gux/source/css/_common/_page/home.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1528952887135},{"_id":"themes/next-gux/source/vendors/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1528952887143},{"_id":"themes/next-gux/source/css/_common/_component/back-to-top.styl","hash":"88cd66910260006aa8e9e795df4948d4b67bfa11","modified":1528952887135},{"_id":"themes/next-gux/source/css/_common/_component/buttons.styl","hash":"81063e0979f04a0f9af37f321d7321dda9abf593","modified":1528952887135},{"_id":"themes/next-gux/source/css/_common/_component/comments.styl","hash":"b468e452f29df359957731ee8846e165aef13b3d","modified":1528952887135},{"_id":"themes/next-gux/source/css/_common/_component/duoshuo.styl","hash":"c307f1e4827d7cb82816a5f9de109ae14ed4199c","modified":1528952887135},{"_id":"themes/next-gux/source/css/_common/_component/gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1528952887135},{"_id":"themes/next-gux/source/css/_common/_component/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1528952887135},{"_id":"themes/next-gux/source/css/_common/_component/pagination.styl","hash":"711c8830886619d4f4a0598b0cde5499dce50c62","modified":1528952887135},{"_id":"themes/next-gux/source/css/_common/_component/posts-collapse.styl","hash":"8f9e8f5f65956ccf1d52ff8526392803dff579d3","modified":1528952887135},{"_id":"themes/next-gux/source/css/_common/_component/posts-expand.styl","hash":"e5d24cc3b5486d1c24080161f2ea1d44e6bbcbb9","modified":1528952887135},{"_id":"themes/next-gux/source/css/_common/_component/posts-type.styl","hash":"40b593134bf96d1d6095b3439d47820659d7f10b","modified":1528952887135},{"_id":"themes/next-gux/source/css/_common/_component/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1528952887135},{"_id":"themes/next-gux/source/css/_common/_core/base.styl","hash":"e79a08484b191dca14ccfc005053eb95786dafae","modified":1528952887135},{"_id":"themes/next-gux/source/css/_common/_core/helpers.styl","hash":"41a31d651b60b4f458fc56a1d191dfbbdcb6d794","modified":1528952887135},{"_id":"themes/next-gux/source/css/_common/_core/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1528952887135},{"_id":"themes/next-gux/source/css/_common/_core/scaffolding.styl","hash":"584c636707e0c8bfd6efc936c1b3a0d35d14f29d","modified":1528952887135},{"_id":"themes/next-gux/source/css/_common/_core/tables.styl","hash":"f142a185fda68bc579e89ead9a31bc8fa0f3ca8c","modified":1528952887135},{"_id":"themes/next-gux/source/css/_common/_fonts/icon-default.styl","hash":"8b809aef383bebaeb3f282b47675f3a364ce3569","modified":1528952887135},{"_id":"themes/next-gux/source/css/_common/_fonts/icon-feather.styl","hash":"80413afacfa656322100ce1900fed1ebcd8f8f44","modified":1528952887135},{"_id":"themes/next-gux/source/css/_common/_fonts/icon-fifty-shades.styl","hash":"249f75bafa26b99d272352c0646e7497ea680b39","modified":1528952887135},{"_id":"themes/next-gux/source/css/_common/_fonts/icon-font.styl","hash":"ec3f86739bede393cafcd3e31052c01115ae20d6","modified":1528952887135},{"_id":"themes/next-gux/source/css/_common/_fonts/icon-linecons.styl","hash":"9cdbedb3627ac941cfb063b152abe5a75c3c699a","modified":1528952887135},{"_id":"themes/next-gux/source/css/_common/_page/archive.styl","hash":"dff879f55ca65fa79c07e9098719e53eeea7ac88","modified":1528952887135},{"_id":"themes/next-gux/source/css/_common/_page/categories.styl","hash":"4f696a2eaeee2f214adcf273eab25c62a398077a","modified":1528952887135},{"_id":"themes/next-gux/source/css/_common/_page/post-detail.styl","hash":"73796f6f13caa7151a2ee8e55755627e0d189f55","modified":1528952887135},{"_id":"themes/next-gux/source/css/_common/_section/body.styl","hash":"ca1a4766cbe25baac757c6b47a4858d221afdc40","modified":1528952887135},{"_id":"themes/next-gux/source/css/_common/_section/footer.styl","hash":"8994ffcce84deac0471532f270f97c44fea54dc0","modified":1528952887135},{"_id":"themes/next-gux/source/css/_common/_section/header.styl","hash":"ba501332fb111bd72dc0777f2e1c8a29ad538ff9","modified":1528952887135},{"_id":"themes/next-gux/source/css/_common/_section/layout.styl","hash":"4daaadd156ece64ae05908ad6bb0159c8a27c071","modified":1528952887135},{"_id":"themes/next-gux/source/css/_common/_section/media.styl","hash":"fa9809d2ecc753cf32f70803c1d0821c405211f4","modified":1528952887135},{"_id":"themes/next-gux/source/css/_common/_section/sidebar.styl","hash":"19ba3653e45187c064bbaf8142c2596a83ae7b08","modified":1528952887135},{"_id":"themes/next-gux/source/css/_schemes/Mala/index.styl","hash":"b2c5e70968c381ba9af79247aac4ef2891b0015c","modified":1528952887135},{"_id":"themes/next-gux/source/css/_schemes/default/_menu.styl","hash":"4bba29cece65ffc5122f4e052063dea4439fe4ae","modified":1528952887135},{"_id":"themes/next-gux/source/css/_schemes/default/_search.styl","hash":"c524bccdc554349106d1c8be9c3f275d4c0d4281","modified":1528952887135},{"_id":"themes/next-gux/source/css/_schemes/default/index.styl","hash":"2588e55132e10d82c0608f03c2c72a2bace8fa4e","modified":1528952887135},{"_id":"themes/next-gux/source/vendors/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1528952887139},{"_id":"themes/next-gux/source/vendors/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1528952887139},{"_id":"themes/next-gux/source/vendors/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1528952887139},{"_id":"themes/next-gux/source/vendors/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1528952887139},{"_id":"themes/next-gux/source/vendors/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1528952887139},{"_id":"themes/next-gux/source/vendors/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1528952887139},{"_id":"themes/next-gux/source/vendors/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1528952887139},{"_id":"themes/next-gux/source/vendors/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1528952887143},{"_id":"themes/next-gux/source/vendors/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1528952887143},{"_id":"themes/next-gux/source/vendors/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1528952887143},{"_id":"themes/next-gux/source/vendors/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1528952887143},{"_id":"themes/next-gux/layout/_partials/suprise/ball.swig","hash":"2c18d2cb89a054068fd04a9cf81c28fe3ac48120","modified":1528952887135},{"_id":"themes/next-gux/source/vendors/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1528952887143},{"_id":"themes/next-gux/source/css/_common/_vendor/highlight/highlight.styl","hash":"f3529b7da284c4b859429573c9b1004d32937e40","modified":1528952887135},{"_id":"themes/next-gux/source/css/_common/_vendor/highlight/theme.styl","hash":"ae19721ceee5ba460e131cb2427dae3c1ff39d6f","modified":1528952887135},{"_id":"themes/next-gux/source/images/background.jpg","hash":"a05982daad69991a53f8f707ed8c8dba3ce858a2","modified":1528953313369},{"_id":"themes/next-gux/source/vendors/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1528952887139},{"_id":"themes/next-gux/source/vendors/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1528952887139},{"_id":"themes/next-gux/source/vendors/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1528952887139},{"_id":"themes/next-gux/source/vendors/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1528952887139},{"_id":"themes/next-gux/source/vendors/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1528952887139},{"_id":"themes/next-gux/source/vendors/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1528952887139},{"_id":"public/about/index.html","hash":"a7311615997c1d59b8463c4b02bfe2f31424d838","modified":1528967685195},{"_id":"public/archives/index.html","hash":"5d061352f70c5a6abdcc7072fe466d02b94c8794","modified":1528967685195},{"_id":"public/archives/2018/index.html","hash":"f54f92ec3a3bb1a837a7c14963a5756a91d22d1b","modified":1528967685196},{"_id":"public/archives/2018/06/index.html","hash":"86f43877948cb195fe3ec6a9fd62fff4106c0366","modified":1528967685196},{"_id":"public/categories/Spark源码阅读计划/index.html","hash":"0de181bfee764d482db6387211c3263512f7b30e","modified":1528967685196},{"_id":"public/categories/工具/index.html","hash":"0d439086da04c8de473b291c9a2ad1c04991b4fb","modified":1528967685196},{"_id":"public/tags/Spark/index.html","hash":"01906f15e1fd09cc01bd2b69aab2ba431ba24634","modified":1528967685196},{"_id":"public/tags/编程/index.html","hash":"c42cf96264cf8a7ca9ef0eff05d8f23416024e22","modified":1528967685196},{"_id":"public/tags/工具/index.html","hash":"1b7b4a8a9e104a124dff879473779ccc4a7a84d8","modified":1528967685197},{"_id":"public/tags/教程/index.html","hash":"ec3d1275a0ff68d7f79999f90f7543aaa8704988","modified":1528967685197},{"_id":"public/tags/Linux/index.html","hash":"4f56253814801fcaafdc5fc5ce4e292753a8c885","modified":1528967685197},{"_id":"public/2018/06/13/使用git+hexo建立个人博客/index.html","hash":"313ed0be9d71bd681a573498125aa08c80df5c06","modified":1528967685197},{"_id":"public/2018/06/13/常用-linux-命令系列/index.html","hash":"b7ed256a416290655ffa6292be64c5e3076196f4","modified":1528967685197},{"_id":"public/2018/06/11/Spark-源码阅读计划-伪-第三部分-图解-shuffle-read/index.html","hash":"e51906c0404ee9ef3ed0cb85a494d14ad42ac8bf","modified":1528967685197},{"_id":"public/2018/06/08/源码阅读计划-第二部分-shuffle-write/index.html","hash":"63fed5819390a3afeffda750eefc6e20a31be09d","modified":1528967685198},{"_id":"public/2018/06/06/Spark-源码阅读计划-第一部分-迭代计算/index.html","hash":"4452df9a956a273dfb06760e2f38146fbbd9d168","modified":1528967685198},{"_id":"public/index.html","hash":"012b3527b8df69b6392ad20aaaf798701c752902","modified":1528967685198},{"_id":"public/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1528967685205},{"_id":"public/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1528967685205},{"_id":"public/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1528967685205},{"_id":"public/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1528967685205},{"_id":"public/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1528967685205},{"_id":"public/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1528967685205},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1528967685205},{"_id":"public/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1528967685205},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1528967685205},{"_id":"public/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1528967685205},{"_id":"public/fonts/icon-default/icomoon.eot","hash":"90763e97be18be78e65749075225cceeddc6fa8a","modified":1528967685205},{"_id":"public/fonts/icon-default/icomoon.svg","hash":"f92ad8cddc250f0bb5ca466fca95d321987e127e","modified":1528967685206},{"_id":"public/fonts/icon-default/icomoon.ttf","hash":"c093408e6030221cafc1f79d897f1fb5283c1178","modified":1528967685206},{"_id":"public/fonts/icon-default/icomoon.woff","hash":"dbe0368f2a65d87b13234cfea29d9783892fc7a8","modified":1528967685206},{"_id":"public/fonts/icon-feather/icomoon.eot","hash":"11554b9e9d5b9f535ba96cbb27d45d8c8f1689fd","modified":1528967685206},{"_id":"public/fonts/icon-feather/icomoon.svg","hash":"d5eb756eefda9b454dcb23c2b1cefd4051d18d41","modified":1528967685206},{"_id":"public/fonts/icon-feather/icomoon.ttf","hash":"b2bbae4b613403cf61ad25037913378da1c07b8f","modified":1528967685206},{"_id":"public/fonts/icon-feather/icomoon.woff","hash":"2ea1c59c17422798e64ee6f4e9ce1f7aff1a06a5","modified":1528967685206},{"_id":"public/fonts/icon-fifty-shades/icomoon.eot","hash":"da86ba5df72d1288de9e9633e5f528062dd427d5","modified":1528967685206},{"_id":"public/fonts/icon-fifty-shades/icomoon.svg","hash":"1a4afd739e1f8eb8d430dbdd29e36a9999802e8d","modified":1528967685206},{"_id":"public/fonts/icon-fifty-shades/icomoon.ttf","hash":"72fe82e1f3db52414eed706952d385af241cb196","modified":1528967685206},{"_id":"public/fonts/icon-fifty-shades/icomoon.woff","hash":"4de6a74f523dee33d95dde61caae5809f9a5d448","modified":1528967685206},{"_id":"public/fonts/icon-icomoon/icomoon.eot","hash":"301fcf00c24750dddf1c529f944ca62c7f1a217d","modified":1528967685206},{"_id":"public/fonts/icon-icomoon/icomoon.svg","hash":"e316347805eb93425faa678611c5e42a7152da8f","modified":1528967685206},{"_id":"public/fonts/icon-icomoon/icomoon.ttf","hash":"f399713d1c9400d4d3373e38991a7e362a754a94","modified":1528967685206},{"_id":"public/fonts/icon-icomoon/icomoon.woff","hash":"05f1ec0bd307da5e731a86eb4961589a6042aebb","modified":1528967685206},{"_id":"public/fonts/icon-linecons/icomoon.eot","hash":"e2d7f040428a632f3c50bfa94083b759936effc2","modified":1528967685206},{"_id":"public/fonts/icon-linecons/icomoon.svg","hash":"808eaf7d61f7e67c76976265c885e79c36920f0b","modified":1528967685206},{"_id":"public/fonts/icon-linecons/icomoon.ttf","hash":"078068206684e4f185b0187ad3cee16f54a287d7","modified":1528967685206},{"_id":"public/fonts/icon-linecons/icomoon.woff","hash":"0b07ee6ceda3b1bceb40c1e7379b3aa48dcc15a8","modified":1528967685207},{"_id":"public/vendors/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1528967685207},{"_id":"public/vendors/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1528967685207},{"_id":"public/vendors/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1528967685207},{"_id":"public/vendors/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1528967685207},{"_id":"public/vendors/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1528967685207},{"_id":"public/vendors/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1528967685207},{"_id":"public/vendors/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1528967685207},{"_id":"public/vendors/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1528967685207},{"_id":"public/2018/06/11/Spark-源码阅读计划-伪-第三部分-图解-shuffle-read/part.png","hash":"b2f702f44036d20b43eedec75b1cbbe0a06d5877","modified":1528967685207},{"_id":"public/2018/06/11/Spark-源码阅读计划-伪-第三部分-图解-shuffle-read/sort.png","hash":"2fc1a5957fc0bfc7cf3027e38759fa3c65077768","modified":1528967685207},{"_id":"public/images/default_avatar.png","hash":"b86a36d36877ca73a88a77a8eb19291cc3828947","modified":1528967685511},{"_id":"public/js/hook-duoshuo.js","hash":"9881b19132ad90dffd82c53947e4c356e30353e7","modified":1528967685520},{"_id":"public/js/bootstrap.scrollspy.js","hash":"ae7bdce88b515aade4eea8bf7407eec458bcd625","modified":1528967685520},{"_id":"public/js/fancy-box.js","hash":"b382ba746f4566682948ce92f2588ee940cd1755","modified":1528967685520},{"_id":"public/js/helpers.js","hash":"7499b413242a2e75a9308444aade5b72a12cce7d","modified":1528967685520},{"_id":"public/js/lazyload.js","hash":"b92e9acdc7afc15468314c03f4a643b0c93944cf","modified":1528967685520},{"_id":"public/js/motion_fallback.js","hash":"a767d522c65a8b2fbad49135c9332135c6785c3e","modified":1528967685521},{"_id":"public/js/motion_global.js","hash":"fea8cbb854601b7aee14e51079b3e3f80a1de261","modified":1528967685521},{"_id":"public/js/nav-toggle.js","hash":"78b59f1beb12adea0d7f9bcf4377cb699963f220","modified":1528967685521},{"_id":"public/js/ua-parser.min.js","hash":"acf0ee6a47ffb7231472b56e43996e3f947c258a","modified":1528967685521},{"_id":"public/fonts/icon-default/selection.json","hash":"ff1b9b78eced4d0368d14cc192ac67a0dd498593","modified":1528967685521},{"_id":"public/fonts/icon-feather/selection.json","hash":"d95a90b0d541e48b049902090c0d008ad92b4115","modified":1528967685521},{"_id":"public/fonts/icon-fifty-shades/selection.json","hash":"e5a5042e8e516b1d30fa3b1206d2c74921cec72b","modified":1528967685521},{"_id":"public/fonts/icon-linecons/selection.json","hash":"68da6ea1b3ab9355d42694bf5745071cdefa4a65","modified":1528967685521},{"_id":"public/vendors/fastclick/README.html","hash":"da3c74d484c73cc7df565e8abbfa4d6a5a18d4da","modified":1528967685521},{"_id":"public/vendors/fastclick/bower.json","hash":"4dcecf83afddba148464d5339c93f6d0aa9f42e9","modified":1528967685521},{"_id":"public/vendors/velocity/bower.json","hash":"0ef14e7ccdfba5db6eb3f8fc6aa3b47282c36409","modified":1528967685521},{"_id":"public/vendors/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1528967685521},{"_id":"public/vendors/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1528967685521},{"_id":"public/vendors/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1528967685521},{"_id":"public/vendors/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1528967685521},{"_id":"public/vendors/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1528967685521},{"_id":"public/vendors/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1528967685521},{"_id":"public/vendors/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1528967685521},{"_id":"public/vendors/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1528967685522},{"_id":"public/css/main.css","hash":"3f7eb1a77c9fad5745faae755d275e423daaba5b","modified":1528967685522},{"_id":"public/vendors/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1528967685522},{"_id":"public/vendors/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1528967685522},{"_id":"public/vendors/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1528967685522},{"_id":"public/vendors/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1528967685522},{"_id":"public/vendors/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1528967685522},{"_id":"public/vendors/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1528967685522},{"_id":"public/vendors/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1528967685522},{"_id":"public/images/background.jpg","hash":"a05982daad69991a53f8f707ed8c8dba3ce858a2","modified":1528967685530},{"_id":"themes/next-gux/source/images/default_avatar.jpg","hash":"930cf7a3be73cd08cbd2ba3f2ae8bee564bad227","modified":1528967825783},{"_id":"public/images/default_avatar.jpg","hash":"930cf7a3be73cd08cbd2ba3f2ae8bee564bad227","modified":1528967831806},{"_id":"source/tags/index.md","hash":"7ccafacf77c140dbb2f677b91362795c96f3ea77","modified":1528968173115},{"_id":"source/categories/index.md","hash":"cccec3d72c25ca05444985af5a716760c86570d6","modified":1528968299623}],"Category":[{"name":"Spark源码阅读计划","_id":"cjiebvrsd0003fanguobzq5da"},{"name":"工具","_id":"cjiebvrsl000bfangjyf0n9jw"},{"name":"笔记","_id":"cjiecd6kr0002k3ngrmu7lqbq"}],"Data":[],"Page":[{"_content":"## hello Im lishion\ngithhub: lishion\nwechat: lishion-me \n","source":"about/index.md","raw":"## hello Im lishion\ngithhub: lishion\nwechat: lishion-me \n","date":"2018-06-13T14:42:19.376Z","updated":"2018-06-13T14:42:19.376Z","path":"about/index.html","title":"","comments":1,"layout":"page","_id":"cjiebvrs90001fang51g14d4q","content":"<h2 id=\"hello-Im-lishion\"><a href=\"#hello-Im-lishion\" class=\"headerlink\" title=\"hello Im lishion\"></a>hello Im lishion</h2><p>githhub: lishion<br>wechat: lishion-me </p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"hello-Im-lishion\"><a href=\"#hello-Im-lishion\" class=\"headerlink\" title=\"hello Im lishion\"></a>hello Im lishion</h2><p>githhub: lishion<br>wechat: lishion-me </p>\n"},{"title":"tags","date":"2018-06-14T09:20:42.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2018-06-14 17:20:42\ntype: \"tags\"\n---\n","updated":"2018-06-14T09:22:53.115Z","path":"tags/index.html","_id":"cjiec3fp70000k3ngone024ps","comments":1,"layout":"page","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"categories","date":"2018-06-14T09:23:48.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2018-06-14 17:23:48\ntype: \"categories\"\n---\n","updated":"2018-06-14T09:24:59.623Z","path":"categories/index.html","_id":"cjiec7fok0001k3ngs81ca3dy","comments":1,"layout":"page","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"Spark 源码阅读计划 - 第三部分 - 图解 shuffle read","date":"2018-06-11T02:42:45.000Z","author":"lishion","toc":true,"_content":"\n你也许注意到了，今天的源码阅读计划后面有一个伪。没错，我们今天不读源码了，主要是因为我~~懒，想玩王者荣耀~~觉得前一章的 shuff write 源码的篇幅太长，害怕大家看完之后摸不着头脑。于是今天制作了 shuffle read 的图解版，顺便解决学习 spark 以来困扰我许久的几个问题。\n\n## 问题\n\n1. key 与如何与分区对应的\n\n   主要取决于分区器\n\n2. 分区发生在什么时候\n\n   建立初始RDD 以及 shuffle的时候\n\n3. key 与分区是一对一映射吗\n\n   不是。一个分区可以包含多个key，但同一个 key 只能存在一个分区中(如果使用hash 分区器)\n\n4. 如果同一个分区会存在多个key，那么在使用聚合函数的时候会不会把多个key的value聚合到一起?\n\n   当然不会，如果这样聚合就出错了。虽然聚合函数的最小处理对象是一个分区，但是实际上还是对 key 进行聚合。\n\n5. combineByKeyWithClassTag 中的 mergeValue 和 mergeCombiners 是否前者作用与分区内，后者作用于分区外(网上大部分都是这么说的，但其实是错误的)\n\n   不是，具体作用下面会讲解\n\n## 图解 shuffle write\n\n### 步骤一、分区 聚合\n\n这里是使用了 hash 表存放了聚合后的数据。与普通的 map 不同，数据的存储在一个 data 数组中。规则为索引为偶数的元素存放 KEY ，而对应的 value 存放在 KEY 的下一个元素，注意存放的 KEY 已经不是原始数据的 key，而是 key 以及其对的分区。由于在插入时使用二次探测法，数组中可能存在空值。用k1,v1表示原始数据键值对;`key=>p(key)`表示分区函数。数据表示流程图如下:\n\n{% asset_img part.png 分区并聚合 %}\n\n\n### 步骤二、排序并写入临时文件\n\n如果判断到 data 数组所占用的内存达到一定的阈值，就会对其中的数据进行排序。排序的方式为先对分区进行排序，分区内按照 key 的 hash 值进行排序。然后将排序好的数据写入临时文件，并产生一个空的 data 数组。这里可能发生多次写入临时文件的操作，也有可能最后仍有数据驻留在内存中而没有写入临时文件。例如，本分区一共有 10M 数据，内存阈值为 3M，则一共会发生 3 次写临时文件的操作以及最后会驻留1M的数据在内存中。需要注意的是这里并没有写入分区信息，只是写入原始的 key 以及对应聚合好的数据。\n\n### 步骤三、对所有的临时文件以及内存中驻留的文件进行排序\n\n由于存在多个临时文件并且内存中还有驻留的数据，因此需要对所有的数据进行排序并写入一个大文件中以减少临时文件的个数。这里的排序算法使用的方法大概是这样的:\n\n{% asset_img sort.png 排序 %}\n\n每次取出第一行的第一个元素就能得到排序结果，图中每一个竖向的数组就可以看做一个排序完成的临时文件。只不过在 shuffle write 并不是取前一行，而是取前一个分区。虽然文件中只存在 k c。但是在写临时文件时记录了每个分区对应的数据条数，因此仍然可以按分区取数据。\n\n这里还需要注意的一个地方是对于同一个 key 可能分布在不同的临时文件中，因此在排序的时候仍然需要对相同的 key 进行聚合。聚合方法其实很简单，例如获取到一条数据 (k1,c1)，如果下一条数据 (key,c) key == k1 就将 mergeCombiners 将 c1 与 c聚合，否则 key == k1 的数据已经全部读取完成，也就是剩下的数据中不会存在 key == k1 的数据。\n\n按照上面的方法不断的读取，聚合数据，写入文件就能将临时文件和内存中的所有数据进行聚合，排序并输出到一个大文件中。随后将每一个分区对应数据条数写入索引文件中，以便于记录大文件中某一条记录属于哪一个分区。完成以上步骤便完成了 shuffle write。\n\n## 总结\n\n读者: 等等，我要先吐个槽。说好的图解呢，怎么就两个图?\n\n作者: 本来有很多图的，但是我~~室友找我开黑~~要去跑步就只画了两个图。\n\n其实搞清楚 shuffle write 的大致步骤还是很简单的，但是具体到每一个细节就需要仔细的阅读源码了。这里推荐一波作者的[spark源码阅读计划](https://lishion.github.io/categories/Spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E8%AE%A1%E5%88%92/)。很~~混乱~~详细的解析了关于 shuffle write 每一部分代码，配合今天的~~图解~~食用更佳。\n\n\n\n\n\n\n\n","source":"_posts/Spark-源码阅读计划-伪-第三部分-图解-shuffle-read.md","raw":"---\ntitle: Spark 源码阅读计划 - 第三部分 - 图解 shuffle read\ndate: 2018-06-11 10:42:45\ntags:\n  - Spark\n  - 编程\ncategories: Spark源码阅读计划\nauthor: lishion\ntoc: true\n---\n\n你也许注意到了，今天的源码阅读计划后面有一个伪。没错，我们今天不读源码了，主要是因为我~~懒，想玩王者荣耀~~觉得前一章的 shuff write 源码的篇幅太长，害怕大家看完之后摸不着头脑。于是今天制作了 shuffle read 的图解版，顺便解决学习 spark 以来困扰我许久的几个问题。\n\n## 问题\n\n1. key 与如何与分区对应的\n\n   主要取决于分区器\n\n2. 分区发生在什么时候\n\n   建立初始RDD 以及 shuffle的时候\n\n3. key 与分区是一对一映射吗\n\n   不是。一个分区可以包含多个key，但同一个 key 只能存在一个分区中(如果使用hash 分区器)\n\n4. 如果同一个分区会存在多个key，那么在使用聚合函数的时候会不会把多个key的value聚合到一起?\n\n   当然不会，如果这样聚合就出错了。虽然聚合函数的最小处理对象是一个分区，但是实际上还是对 key 进行聚合。\n\n5. combineByKeyWithClassTag 中的 mergeValue 和 mergeCombiners 是否前者作用与分区内，后者作用于分区外(网上大部分都是这么说的，但其实是错误的)\n\n   不是，具体作用下面会讲解\n\n## 图解 shuffle write\n\n### 步骤一、分区 聚合\n\n这里是使用了 hash 表存放了聚合后的数据。与普通的 map 不同，数据的存储在一个 data 数组中。规则为索引为偶数的元素存放 KEY ，而对应的 value 存放在 KEY 的下一个元素，注意存放的 KEY 已经不是原始数据的 key，而是 key 以及其对的分区。由于在插入时使用二次探测法，数组中可能存在空值。用k1,v1表示原始数据键值对;`key=>p(key)`表示分区函数。数据表示流程图如下:\n\n{% asset_img part.png 分区并聚合 %}\n\n\n### 步骤二、排序并写入临时文件\n\n如果判断到 data 数组所占用的内存达到一定的阈值，就会对其中的数据进行排序。排序的方式为先对分区进行排序，分区内按照 key 的 hash 值进行排序。然后将排序好的数据写入临时文件，并产生一个空的 data 数组。这里可能发生多次写入临时文件的操作，也有可能最后仍有数据驻留在内存中而没有写入临时文件。例如，本分区一共有 10M 数据，内存阈值为 3M，则一共会发生 3 次写临时文件的操作以及最后会驻留1M的数据在内存中。需要注意的是这里并没有写入分区信息，只是写入原始的 key 以及对应聚合好的数据。\n\n### 步骤三、对所有的临时文件以及内存中驻留的文件进行排序\n\n由于存在多个临时文件并且内存中还有驻留的数据，因此需要对所有的数据进行排序并写入一个大文件中以减少临时文件的个数。这里的排序算法使用的方法大概是这样的:\n\n{% asset_img sort.png 排序 %}\n\n每次取出第一行的第一个元素就能得到排序结果，图中每一个竖向的数组就可以看做一个排序完成的临时文件。只不过在 shuffle write 并不是取前一行，而是取前一个分区。虽然文件中只存在 k c。但是在写临时文件时记录了每个分区对应的数据条数，因此仍然可以按分区取数据。\n\n这里还需要注意的一个地方是对于同一个 key 可能分布在不同的临时文件中，因此在排序的时候仍然需要对相同的 key 进行聚合。聚合方法其实很简单，例如获取到一条数据 (k1,c1)，如果下一条数据 (key,c) key == k1 就将 mergeCombiners 将 c1 与 c聚合，否则 key == k1 的数据已经全部读取完成，也就是剩下的数据中不会存在 key == k1 的数据。\n\n按照上面的方法不断的读取，聚合数据，写入文件就能将临时文件和内存中的所有数据进行聚合，排序并输出到一个大文件中。随后将每一个分区对应数据条数写入索引文件中，以便于记录大文件中某一条记录属于哪一个分区。完成以上步骤便完成了 shuffle write。\n\n## 总结\n\n读者: 等等，我要先吐个槽。说好的图解呢，怎么就两个图?\n\n作者: 本来有很多图的，但是我~~室友找我开黑~~要去跑步就只画了两个图。\n\n其实搞清楚 shuffle write 的大致步骤还是很简单的，但是具体到每一个细节就需要仔细的阅读源码了。这里推荐一波作者的[spark源码阅读计划](https://lishion.github.io/categories/Spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E8%AE%A1%E5%88%92/)。很~~混乱~~详细的解析了关于 shuffle write 每一部分代码，配合今天的~~图解~~食用更佳。\n\n\n\n\n\n\n\n","slug":"Spark-源码阅读计划-伪-第三部分-图解-shuffle-read","published":1,"updated":"2018-06-13T14:42:19.376Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjiebvrs50000fangvlyac8u4","content":"<p>你也许注意到了，今天的源码阅读计划后面有一个伪。没错，我们今天不读源码了，主要是因为我<del>懒，想玩王者荣耀</del>觉得前一章的 shuff write 源码的篇幅太长，害怕大家看完之后摸不着头脑。于是今天制作了 shuffle read 的图解版，顺便解决学习 spark 以来困扰我许久的几个问题。</p>\n<h2 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h2><ol>\n<li><p>key 与如何与分区对应的</p>\n<p>主要取决于分区器</p>\n</li>\n<li><p>分区发生在什么时候</p>\n<p>建立初始RDD 以及 shuffle的时候</p>\n</li>\n<li><p>key 与分区是一对一映射吗</p>\n<p>不是。一个分区可以包含多个key，但同一个 key 只能存在一个分区中(如果使用hash 分区器)</p>\n</li>\n<li><p>如果同一个分区会存在多个key，那么在使用聚合函数的时候会不会把多个key的value聚合到一起?</p>\n<p>当然不会，如果这样聚合就出错了。虽然聚合函数的最小处理对象是一个分区，但是实际上还是对 key 进行聚合。</p>\n</li>\n<li><p>combineByKeyWithClassTag 中的 mergeValue 和 mergeCombiners 是否前者作用与分区内，后者作用于分区外(网上大部分都是这么说的，但其实是错误的)</p>\n<p>不是，具体作用下面会讲解</p>\n</li>\n</ol>\n<h2 id=\"图解-shuffle-write\"><a href=\"#图解-shuffle-write\" class=\"headerlink\" title=\"图解 shuffle write\"></a>图解 shuffle write</h2><h3 id=\"步骤一、分区-聚合\"><a href=\"#步骤一、分区-聚合\" class=\"headerlink\" title=\"步骤一、分区 聚合\"></a>步骤一、分区 聚合</h3><p>这里是使用了 hash 表存放了聚合后的数据。与普通的 map 不同，数据的存储在一个 data 数组中。规则为索引为偶数的元素存放 KEY ，而对应的 value 存放在 KEY 的下一个元素，注意存放的 KEY 已经不是原始数据的 key，而是 key 以及其对的分区。由于在插入时使用二次探测法，数组中可能存在空值。用k1,v1表示原始数据键值对;<code>key=&gt;p(key)</code>表示分区函数。数据表示流程图如下:</p>\n<img src=\"/2018/06/11/Spark-源码阅读计划-伪-第三部分-图解-shuffle-read/part.png\" title=\"分区并聚合\">\n<h3 id=\"步骤二、排序并写入临时文件\"><a href=\"#步骤二、排序并写入临时文件\" class=\"headerlink\" title=\"步骤二、排序并写入临时文件\"></a>步骤二、排序并写入临时文件</h3><p>如果判断到 data 数组所占用的内存达到一定的阈值，就会对其中的数据进行排序。排序的方式为先对分区进行排序，分区内按照 key 的 hash 值进行排序。然后将排序好的数据写入临时文件，并产生一个空的 data 数组。这里可能发生多次写入临时文件的操作，也有可能最后仍有数据驻留在内存中而没有写入临时文件。例如，本分区一共有 10M 数据，内存阈值为 3M，则一共会发生 3 次写临时文件的操作以及最后会驻留1M的数据在内存中。需要注意的是这里并没有写入分区信息，只是写入原始的 key 以及对应聚合好的数据。</p>\n<h3 id=\"步骤三、对所有的临时文件以及内存中驻留的文件进行排序\"><a href=\"#步骤三、对所有的临时文件以及内存中驻留的文件进行排序\" class=\"headerlink\" title=\"步骤三、对所有的临时文件以及内存中驻留的文件进行排序\"></a>步骤三、对所有的临时文件以及内存中驻留的文件进行排序</h3><p>由于存在多个临时文件并且内存中还有驻留的数据，因此需要对所有的数据进行排序并写入一个大文件中以减少临时文件的个数。这里的排序算法使用的方法大概是这样的:</p>\n<img src=\"/2018/06/11/Spark-源码阅读计划-伪-第三部分-图解-shuffle-read/sort.png\" title=\"排序\">\n<p>每次取出第一行的第一个元素就能得到排序结果，图中每一个竖向的数组就可以看做一个排序完成的临时文件。只不过在 shuffle write 并不是取前一行，而是取前一个分区。虽然文件中只存在 k c。但是在写临时文件时记录了每个分区对应的数据条数，因此仍然可以按分区取数据。</p>\n<p>这里还需要注意的一个地方是对于同一个 key 可能分布在不同的临时文件中，因此在排序的时候仍然需要对相同的 key 进行聚合。聚合方法其实很简单，例如获取到一条数据 (k1,c1)，如果下一条数据 (key,c) key == k1 就将 mergeCombiners 将 c1 与 c聚合，否则 key == k1 的数据已经全部读取完成，也就是剩下的数据中不会存在 key == k1 的数据。</p>\n<p>按照上面的方法不断的读取，聚合数据，写入文件就能将临时文件和内存中的所有数据进行聚合，排序并输出到一个大文件中。随后将每一个分区对应数据条数写入索引文件中，以便于记录大文件中某一条记录属于哪一个分区。完成以上步骤便完成了 shuffle write。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>读者: 等等，我要先吐个槽。说好的图解呢，怎么就两个图?</p>\n<p>作者: 本来有很多图的，但是我<del>室友找我开黑</del>要去跑步就只画了两个图。</p>\n<p>其实搞清楚 shuffle write 的大致步骤还是很简单的，但是具体到每一个细节就需要仔细的阅读源码了。这里推荐一波作者的<a href=\"https://lishion.github.io/categories/Spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E8%AE%A1%E5%88%92/\" target=\"_blank\" rel=\"noopener\">spark源码阅读计划</a>。很<del>混乱</del>详细的解析了关于 shuffle write 每一部分代码，配合今天的<del>图解</del>食用更佳。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>你也许注意到了，今天的源码阅读计划后面有一个伪。没错，我们今天不读源码了，主要是因为我<del>懒，想玩王者荣耀</del>觉得前一章的 shuff write 源码的篇幅太长，害怕大家看完之后摸不着头脑。于是今天制作了 shuffle read 的图解版，顺便解决学习 spark 以来困扰我许久的几个问题。</p>\n<h2 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h2><ol>\n<li><p>key 与如何与分区对应的</p>\n<p>主要取决于分区器</p>\n</li>\n<li><p>分区发生在什么时候</p>\n<p>建立初始RDD 以及 shuffle的时候</p>\n</li>\n<li><p>key 与分区是一对一映射吗</p>\n<p>不是。一个分区可以包含多个key，但同一个 key 只能存在一个分区中(如果使用hash 分区器)</p>\n</li>\n<li><p>如果同一个分区会存在多个key，那么在使用聚合函数的时候会不会把多个key的value聚合到一起?</p>\n<p>当然不会，如果这样聚合就出错了。虽然聚合函数的最小处理对象是一个分区，但是实际上还是对 key 进行聚合。</p>\n</li>\n<li><p>combineByKeyWithClassTag 中的 mergeValue 和 mergeCombiners 是否前者作用与分区内，后者作用于分区外(网上大部分都是这么说的，但其实是错误的)</p>\n<p>不是，具体作用下面会讲解</p>\n</li>\n</ol>\n<h2 id=\"图解-shuffle-write\"><a href=\"#图解-shuffle-write\" class=\"headerlink\" title=\"图解 shuffle write\"></a>图解 shuffle write</h2><h3 id=\"步骤一、分区-聚合\"><a href=\"#步骤一、分区-聚合\" class=\"headerlink\" title=\"步骤一、分区 聚合\"></a>步骤一、分区 聚合</h3><p>这里是使用了 hash 表存放了聚合后的数据。与普通的 map 不同，数据的存储在一个 data 数组中。规则为索引为偶数的元素存放 KEY ，而对应的 value 存放在 KEY 的下一个元素，注意存放的 KEY 已经不是原始数据的 key，而是 key 以及其对的分区。由于在插入时使用二次探测法，数组中可能存在空值。用k1,v1表示原始数据键值对;<code>key=&gt;p(key)</code>表示分区函数。数据表示流程图如下:</p>\n<img src=\"/2018/06/11/Spark-源码阅读计划-伪-第三部分-图解-shuffle-read/part.png\" title=\"分区并聚合\">\n<h3 id=\"步骤二、排序并写入临时文件\"><a href=\"#步骤二、排序并写入临时文件\" class=\"headerlink\" title=\"步骤二、排序并写入临时文件\"></a>步骤二、排序并写入临时文件</h3><p>如果判断到 data 数组所占用的内存达到一定的阈值，就会对其中的数据进行排序。排序的方式为先对分区进行排序，分区内按照 key 的 hash 值进行排序。然后将排序好的数据写入临时文件，并产生一个空的 data 数组。这里可能发生多次写入临时文件的操作，也有可能最后仍有数据驻留在内存中而没有写入临时文件。例如，本分区一共有 10M 数据，内存阈值为 3M，则一共会发生 3 次写临时文件的操作以及最后会驻留1M的数据在内存中。需要注意的是这里并没有写入分区信息，只是写入原始的 key 以及对应聚合好的数据。</p>\n<h3 id=\"步骤三、对所有的临时文件以及内存中驻留的文件进行排序\"><a href=\"#步骤三、对所有的临时文件以及内存中驻留的文件进行排序\" class=\"headerlink\" title=\"步骤三、对所有的临时文件以及内存中驻留的文件进行排序\"></a>步骤三、对所有的临时文件以及内存中驻留的文件进行排序</h3><p>由于存在多个临时文件并且内存中还有驻留的数据，因此需要对所有的数据进行排序并写入一个大文件中以减少临时文件的个数。这里的排序算法使用的方法大概是这样的:</p>\n<img src=\"/2018/06/11/Spark-源码阅读计划-伪-第三部分-图解-shuffle-read/sort.png\" title=\"排序\">\n<p>每次取出第一行的第一个元素就能得到排序结果，图中每一个竖向的数组就可以看做一个排序完成的临时文件。只不过在 shuffle write 并不是取前一行，而是取前一个分区。虽然文件中只存在 k c。但是在写临时文件时记录了每个分区对应的数据条数，因此仍然可以按分区取数据。</p>\n<p>这里还需要注意的一个地方是对于同一个 key 可能分布在不同的临时文件中，因此在排序的时候仍然需要对相同的 key 进行聚合。聚合方法其实很简单，例如获取到一条数据 (k1,c1)，如果下一条数据 (key,c) key == k1 就将 mergeCombiners 将 c1 与 c聚合，否则 key == k1 的数据已经全部读取完成，也就是剩下的数据中不会存在 key == k1 的数据。</p>\n<p>按照上面的方法不断的读取，聚合数据，写入文件就能将临时文件和内存中的所有数据进行聚合，排序并输出到一个大文件中。随后将每一个分区对应数据条数写入索引文件中，以便于记录大文件中某一条记录属于哪一个分区。完成以上步骤便完成了 shuffle write。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>读者: 等等，我要先吐个槽。说好的图解呢，怎么就两个图?</p>\n<p>作者: 本来有很多图的，但是我<del>室友找我开黑</del>要去跑步就只画了两个图。</p>\n<p>其实搞清楚 shuffle write 的大致步骤还是很简单的，但是具体到每一个细节就需要仔细的阅读源码了。这里推荐一波作者的<a href=\"https://lishion.github.io/categories/Spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E8%AE%A1%E5%88%92/\" target=\"_blank\" rel=\"noopener\">spark源码阅读计划</a>。很<del>混乱</del>详细的解析了关于 shuffle write 每一部分代码，配合今天的<del>图解</del>食用更佳。</p>\n"},{"title":"Spark 源码阅读计划 - 第一部分 - 迭代计算","date":"2018-06-06T05:46:45.000Z","author":"lishion","toc":true,"_content":"首先立一个flag，这将是一个长期更新的版块。\n\n## 写在最开始\n\n在我使用`spark`进行日志分析的时候感受到了`spark`的便捷与强大。在学习`spark`初期，我阅读了许多与`spark`相关的文档，在这个过程中了解了`RDD`，`分区`，`shuffle`等概念，但是我并没有对这些概念有更多具体的认识。由于不了解`spark`的基本运作方式使得我在编写代码的时候十分困扰。由于这个原因，我准备阅读`spark`的源代码来解决我对基本概念的认识。\n\n## 本部分主要内容\n\n虽然该章节的名字叫做**迭代计算**，但是本章会讨论包括**RDD、分区、Job、迭代计算**等相关的内容，其中会重点讨论分区与迭代计算。因此在阅读本章之前你至少需要对这些提到的概念有一定的了解。\n\n## 准备工作\n\n你需要准备一份 Spark 的源代码以及一个便于全局搜索的编辑器|IDE。\n\n## 一个简单的例子\n\n假设现在我们有这样一个需求 : 寻找从 0 to 100 的偶数并输出其总数。利用 spark 编写代码如下:\n\n```scala\nsc.parallelize(0 to 100).filter(_%2 == 0).count\n//res0: Long = 51\n```\n\n这是一个非常简单的例子，但是里面却蕴涵了许多基础的知识。这一章的内容也是从这个例子展开的。\n\n## 迭代计算\n\n### comput 方法与 itertor \n\n在RDD.scala 定义的类 RDD中，有两个实现迭代计算的核心方法，分别是`compute`以及`itertor`方法。其中`itertor`方法如下:\n\n```scala\n  final def iterator(split: Partition, context: TaskContext): Iterator[T] = {\n    if (storageLevel != StorageLevel.NONE) {//如果有缓存\n      getOrCompute(split, context)\n    } else {\n      computeOrReadCheckpoint(split, context)\n    }\n  }\n```\n\n我们只关心这个方法的第一个参数**分区**，以及返回的迭代器。忽略有缓存的情况，我们继续看`computeOrReadCheckpoint`这个方法:\n\n```scala\n  private[spark] def computeOrReadCheckpoint(split: Partition, context: TaskContext): Iterator[T] =\n  {\n    if (isCheckpointedAndMaterialized) {//如果有checkpoint\n      firstParent[T].iterator(split, context)\n    } else {\n      compute(split, context)\n    }\n  }\n```\n\n可以看到，在没有缓存和 checkpoint 的情况下，itertor 方法直接调用了 compute 方法。而compute方法定义如下:\n\n```scala\n /**\n   * Implemented by subclasses to compute a given partition.\n   */\n  def compute(split: Partition, context: TaskContext): Iterator[T]\n```\n\n从注释可以看出，compute 方法应该由子类实现，用以计算给定分区并生成一个迭代器。如果不考虑缓存和 checkpoint 的情况下，简化一下代码:\n\n```scala\n final def iterator(split: Partition, context: TaskContext): Iterator[T] = {\n    compute(split,context)\n  }\n```\n\n那么实际上 iterator 的功能是: **接受一个分区，对这个分区进行计算，并返回计算结果的迭代器**。而之所以 compute 需要子类来实现，是因为只需要在子类中实现不同的 compute 方法，就能产生不同类型的 RDD。既然不同的RDD 有不同的功能，我们想知道之前的例子是如何进行迭代计算的，就需要知道这个例子中涉及到了哪些 RDD。\n\n首先查看`SparkContext`中与`parallelize`相关的部分代码:\n\n```scala\n  def parallelize[T: ClassTag](\n      seq: Seq[T],\n      numSlices: Int = defaultParallelism): RDD[T] = withScope {\n    assertNotStopped()\n    new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())\n  }\n```\n\n可以看到，`parallelize`实际返回了一个`ParallelCollectionRDD`。在`ParallelCollectionRDD`中并没有对`filter`方法进行重写，因此我们查看`RDD`中的`filter`方法:\n\n```scala\n def filter(f: T => Boolean): RDD[T] = withScope {\n    val cleanF = sc.clean(f)\n    new MapPartitionsRDD[T, T](\n      this,\n      (context, pid, iter) => iter.filter(cleanF),\n      preservesPartitioning = true)\n  }\n```\n\nfilter 方法返回了`MapPartitionsRDD`。为了方便起见，我将 ParallelCollectionRDD 类型的 RDD 称为 a，MapPartitionsRDD 类型的 RDD 成为B。那么先看一下 b 的 compute 方法:\n\n```scala\nprivate[spark] class MapPartitionsRDD[U: ClassTag, T: ClassTag](\n    var prev: RDD[T],\n    f: (TaskContext, Int, Iterator[T]) => Iterator[U],  // (TaskContext, partition index, iterator)\n    preservesPartitioning: Boolean = false)\n  extends RDD[U](prev) {\n  override val partitioner = if (preservesPartitioning) firstParent[T].partitioner else None\n  override def getPartitions: Array[Partition] = firstParent[T].partitions\n  override def compute(split: Partition, context: TaskContext): Iterator[U] =\n    f(context, split.index, firstParent[T].iterator(split, context))\n  override def clearDependencies() {\n    super.clearDependencies()\n    prev = null\n  }\n}\n\n```\n\n这里可以看到 compute 方法实际上调用 f 这个函数。而 f 这个函数是在 filter 方法中传入的`(context, pid, iter) => iter.filter(cleanF)` 。那么实际上 compute 进行的计算为:\n\n```scala\n(context, split.index, firstParent[T].iterator(split, context)) => firstParent[T].iterator(split, context).filter(cleanF)\n```\n\n前两个参数并没有用到，也就是最终的方法可以简化为:\n\n```scala\nfirstParent[T].iterator(split, context) => firstParent[T].iterator(split, context).filter(cleanF)\n```\n\n这里出现了一个`firstParent`，我们知道 RDD 之间存在依赖关系，如果rdd3 依赖 rdd2，rdd2 依赖 rdd1。rdd2 与 rdd3 都是rdd1 的一个父rdd, spark也将其称为血缘关系。而故名意思 rdd2 是 rdd3 的 firstParent。在这个例子中 a 是 b的 firstParent。那么在b 的 compute 中又调用了 a 的 iterator 方法。而 a 的 iterator 方法又会调用a 的 comput 方法。用箭头表示调用关系:\n\n```\nb.iterotr -> b.compute -> a.iterotr -> a.compute ->| 调用\nb.iterotr <- b.compute <- a.iterotr <- a.compute <-| 返回\n```\n\n而这里我们也可以看出来，b 调用 iterotr 返回的数据，是使用 b 中的方法对 a 返回数据进行处理。也就是b 的计算结果依赖于a的计算结果。如果将一个rdd比作一个函数，大概就是 `fb(fa())`，这样的。此时，我们已经得到了关于 spark 迭代的最简单的模型，假如我们有 n 个 rdd。那么之间的调用依赖关系便是:\n\n```\nn.iterotr -> n.compute -> (n-1).iterotr -> (n-1).compute ->...-> 1.iterotr -> 1.compute->| \nn.iterotr <- n.compute <- (n-1).iterotr <- (n-1).compute <-...<- 1.iterotr <- 1.compute<-|\n```\n\n那么细心的同学应该注意到了，所有的数据都是源自于第一个 RDD。这里把它称为源 RDD。例如本例中产生的 RDD 以及 textFile 方法产生的HadoopRDD都属于源RDD。既然属于源 RDD ，那么这个 RDD 一定会与内存或磁盘中的源数据产生交互，否则就没有真实的数据来源。这里的源数据是指内存的数组、本地文件、或者是HDFS上的分布式文件等。\n\n那么我们再继续看 a 的 compute 方法:\n\n```scala\n  override def compute(s: Partition, context: TaskContext): Iterator[T] = {\n    new InterruptibleIterator(context,s.asInstanceOf[ParallelCollectionPartition[T]].iterator)\n  }\n```\n\n可以看到 a 的 compute 根据 ParallelCollectionPartition 类的 iterator直接生成了一个迭代器。再看看ParallelCollectionPartition 的定义:\n\n```scala\nrivate[spark] class ParallelCollectionPartition[T: ClassTag](\n    var rddId: Long,\n    var slice: Int,\n    var values: Seq[T]\n  ) extends Partition with Serializable {\n  def iterator: Iterator[T] = values.iterator\n  ...\n```\n\niterator 是来自于 values 的一个迭代器，显然，values是内存中的数据。这说明 a 的 compute 方法直接返回了一个与源数据相关的迭代器。而这里 ParallelCollectionPartition 的数据是如何传入的，又是在什么时候被传入的呢?在**分区**小节将会提到。\n\n到这里，我们已经基本了解了 spark 的迭代计算的原理以及数据来源。但是仍然还有许多不了解的地方，例如 iterotr 会接受一个  Partition 作为参数。那么这个Partition参数到底起到了什么作用。其次，我们知道 rdd 的 iterotr 方法 是从后开始往前调用，那么又是谁调用的最后一个 rdd 的 itertor 方法呢。接下来我们就探索一下与分区相关的内容.\n\n### 分区\n\n提到分区，大部分都是类似于“RDD 中最小的数据单元”，“只是数据的抽象分区，而不是真正的持有数据”等等这样的描述。\n\n```scala\n/**\n * An identifier for a partition in an RDD.\n */\ntrait Partition extends Serializable {\n  def index: Int\n  override def hashCode(): Int = index\n  override def equals(other: Any): Boolean = super.equals(other)\n}\n```\n\n可以看到分区的类十分的简单，只有一个 index 属性。 当将分区这个参数传入 itertor 时表示我们需要 index 对应的分区的数据经过一系列计算的之后的迭代器。那么显然，在源 RDD 中一定会有更据分区的index获取对应数据的代码部分。而我们又知道，对于`ParallelCollectionPartition`这个分区类是直接持有数据的迭代器，因此我们只需要知道这个类如何被建立，便知道了是如何根据分区获取数据的。我们在整个工程中搜索 ParallelCollectionPartition，发现:\n\n```scala\n override def getPartitions: Array[Partition] = {\n    val slices = ParallelCollectionRDD.slice(data, numSlices).toArray\n    slices.indices.map(i => new ParallelCollectionPartition(id, i, slices(i))).toArray\n  }\n```\n\n中生成了一系列的分区对象。其中 ParallelCollectionRDD.slice() 方法根据总共有多少个分区来将 data  进行切分。而 data 我们在调用 parallelize() 方法时传入的。看到这里就应该明白了，**ParallelCollectionRDD 会将我们输入的数组进行切分，然后将利用分区对象持有数据的迭代器，当调用到 itertor 方法时就返回该分区的迭代器，供后续的RDD**进行计算。虽然这里我们只看了 ParallelCollectionRDD 的原代码，但是其他例如 HadoopRDD 的远离基本相同。 只不过一个是从内存中获取数据进行切分，另一个是从磁盘上获取数据进行切分。\n\n但是直到这里，我们仍然不知道 a 中的 itertor 方法的分区对象是如何传入的，因为这个方法间接被 b 的 itertor 方法调用。 而 b 的  itertor 方法同样也需要在外部被调用 ，因此要解决这个问题只需要找到 b 的 itertor 被调用的地方。不过我们首先可以根据代码猜测一下:\n\n```scala\n  override def compute(s: Partition, context: TaskContext): Iterator[T] = {\n    new InterruptibleIterator(context,s.asInstanceOf[ParallelCollectionPartition[T]].iterator)\n  }\n```\n\na 中的 compute 方法直接将传入的分区对象转为了 ParallelCollectionPartition 并获取了对应的迭代器。根据对象间强转的\n\n规则，传入的分区对象只能是 ParallelCollectionPartition 类型 ，因为其父类 RDD 并没有 iterator 方法。 并且传入的ParallelCollectionPartition 一定是持有源数据迭代器的对象，否则在 a 的 compute 中就无法向后返回迭代器。而且我们知道，spark 的计算是**惰性计算**，在调用 action 算子之后才会真正的开始计算。那么可以猜测，b 的 iterator 也是在调用了 count 方法之后才被调用的。为了证明这一点，我们继续查看代码:\n\n```scala\ndef count(): Long = sc.runJob(this, Utils.getIteratorSize _).sum\n```\n\n可以看到是调用了 SparkContext 中的runJob方法:\n\n```scala\n def runJob[T, U: ClassTag](\n     ...\n    dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)\n    ...\n  }\n```\n\n而 SparkContext 中的 runJob 又继续调用了 DAGScheduler 中的 runJob 方法， 继而调用了 submitJob 方法。之后由经历了 DAGSchedulerEventProcessLoop.post DAGSchedulerEventProcessLoop.doOnReceive 等方法，最后在 submitMissingTasks 看到建立了 ResultTask 对象。由于一个分区的数据会被一个task 处理，因此我们猜测在 ＲesultTask 中会有关于对应 rdd 调用 iterator 的信息，果然，在ResultTask中找到了 runTask 方法:\n\n```scala\noverride def runTask(context: TaskContext): U = {\n    // Deserialize the RDD and the func using the broadcast variables.\n    val threadMXBean = ManagementFactory.getThreadMXBean\n    val deserializeStartTime = System.currentTimeMillis()\n    val deserializeStartCpuTime = if (threadMXBean.isCurrentThreadCpuTimeSupported) {\n      threadMXBean.getCurrentThreadCpuTime\n    } else 0L\n    val ser = SparkEnv.get.closureSerializer.newInstance()\n    val (rdd, func) = ser.deserialize[(RDD[T], (TaskContext, Iterator[T]) => U)](\n      ByteBuffer.wrap(taskBinary.value), Thread.currentThread.getContextClassLoader)\n    _executorDeserializeTime = System.currentTimeMillis() - deserializeStartTime\n    _executorDeserializeCpuTime = if (threadMXBean.isCurrentThreadCpuTimeSupported) {\n      threadMXBean.getCurrentThreadCpuTime - deserializeStartCpuTime\n    } else 0L\n\n    func(context, rdd.iterator(partition, context))//这里就 b 的iterator被真正调用的地方\n  }\n\n\n```\n\n那么我们可以看到最后一行有一个rdd的调用，而这个rdd 是反徐序列化得的，很明显，这个rdd就是 b。此时，b 调用 iterator 方法的地方找到了，但是分区对象partition依然没有找到从哪里传入。由于 partition 是 ResultTask 构造时传入的，我们回到 submitMissingTasks 中，创建ResultTask时分区参数对应的为变量 part 。 \n\n```scala\n      //从需要计算的分区map中获取分区，并生成task\n       partitionsToCompute.map { id =>\n            val p: Int = stage.partitions(id)\n            val part = partitions(p)\n            val locs = taskIdToLocations(id)\n            new ResultTask(stage.id, stage.latestInfo.attemptNumber,\n              taskBinary, part, locs, id, properties, serializedTaskMetrics,\n              Option(jobId), Option(sc.applicationId), sc.applicationAttemptId)\n          }\n```\n\n继续往上看，可以看到 part 是 从 partitions 中获取的。这里我们也可以看出 **一个任务对应一个分区数据**。继续往上看，发现`partitions = stage.rdd.partitions`。实际上来自于 Stage 中的 rdd 中的 partitions。那么看Stage 对 rdd 这个属性的描述:\n\n```scala\n *\n * @param rdd RDD that this stage runs on: for a shuffle map stage, it's the RDD we run map tasks\n *   on, while for a result stage, it's the target RDD that we ran an action on\n *   这里的意思是 rdd 表示调用 action 算子的 rdd 也就是 b \n */\nprivate[scheduler] abstract class Stage(\n    val id: Int,\n    val rdd: RDD[_],\n    val numTasks: Int,\n    val parents: List[Stage],\n    val firstJobId: Int,\n    val callSite: CallSite)\n  extends Logging {\n```\n\n我们发现一个惊人的事实 ，这个 rdd 居然也是 b。也就是说，我们在 runTask函数中实际调用的方法是这样的:\n\n```scala\n  func(context, b.iterator(b.partitions(index), context))//这里就 b 的iterator被真正调用的地方\n```\n\nindex 表示该task对应需要执行的分区标号。随即我们继续看 b 中的 partitions 的定义:\n\n```scala\n override def getPartitions: Array[Partition] = firstParent[T].partitions\n```\n\n说明这个 partitions 来自于 a。 而 a 中的 partitions 实际上又来自于:\n\n```scala\n override def getPartitions: Array[Partition] = {\n    val slices = ParallelCollectionRDD.slice(data, numSlices).toArray\n    slices.indices.map(i => new ParallelCollectionPartition(id, i, slices(i))).toArray\n  }\n```\n\n这样最终 runTask 函数实际调用的方法为:\n\n```scala\n func(context, b.iterator(a.partitions(index), context))//这里就 b 的iterator被真正调用的地方\n```\n\n这和我们猜测的相同，这个分区确实为 ParallelCollectionPartition 对象，并且也持有了源数据的迭代器。这里其实有一个很巧妙的地方，虽然 RDD  的迭代计算是从后往前调用，但是传入源 RDD 的分区对象依然来自于源 RDD 自身。\n\n到了这里，我们也就明白分区对象是如何和数据关联起来的。ParallelCollectionPartition 对象中存在分区编号 index 以及 源数据的迭代器，通过分区编号就能获取到源数据的迭代器。\n\n## 总结\n\n经过如此大篇幅的讲解，终于对 spark 的迭代计算以及分区做了一个简单的分析。虽然还有很多地方不完善，例如还有其它类型的分区对象没有讲解，但是我相信你能完全看懂这篇文章所讲的内容，其他类型的分区对象对于你来说也很简单了。作者水平有限，并且这也是作者的第一篇关于 spark 源码阅读的博客，难免有遗漏和错误。如果想如给作者提建议和意见的话，请联系作者的微信或直接在 github 上提 issue 。\n\n \n\n","source":"_posts/Spark-源码阅读计划-第一部分-迭代计算.md","raw":"---\ntitle: Spark 源码阅读计划 - 第一部分 - 迭代计算\ndate: 2018-06-06 13:46:45\ntags:\n  - Spark\n  - 编程\ncategories: Spark源码阅读计划\nauthor: lishion\ntoc: true\n---\n首先立一个flag，这将是一个长期更新的版块。\n\n## 写在最开始\n\n在我使用`spark`进行日志分析的时候感受到了`spark`的便捷与强大。在学习`spark`初期，我阅读了许多与`spark`相关的文档，在这个过程中了解了`RDD`，`分区`，`shuffle`等概念，但是我并没有对这些概念有更多具体的认识。由于不了解`spark`的基本运作方式使得我在编写代码的时候十分困扰。由于这个原因，我准备阅读`spark`的源代码来解决我对基本概念的认识。\n\n## 本部分主要内容\n\n虽然该章节的名字叫做**迭代计算**，但是本章会讨论包括**RDD、分区、Job、迭代计算**等相关的内容，其中会重点讨论分区与迭代计算。因此在阅读本章之前你至少需要对这些提到的概念有一定的了解。\n\n## 准备工作\n\n你需要准备一份 Spark 的源代码以及一个便于全局搜索的编辑器|IDE。\n\n## 一个简单的例子\n\n假设现在我们有这样一个需求 : 寻找从 0 to 100 的偶数并输出其总数。利用 spark 编写代码如下:\n\n```scala\nsc.parallelize(0 to 100).filter(_%2 == 0).count\n//res0: Long = 51\n```\n\n这是一个非常简单的例子，但是里面却蕴涵了许多基础的知识。这一章的内容也是从这个例子展开的。\n\n## 迭代计算\n\n### comput 方法与 itertor \n\n在RDD.scala 定义的类 RDD中，有两个实现迭代计算的核心方法，分别是`compute`以及`itertor`方法。其中`itertor`方法如下:\n\n```scala\n  final def iterator(split: Partition, context: TaskContext): Iterator[T] = {\n    if (storageLevel != StorageLevel.NONE) {//如果有缓存\n      getOrCompute(split, context)\n    } else {\n      computeOrReadCheckpoint(split, context)\n    }\n  }\n```\n\n我们只关心这个方法的第一个参数**分区**，以及返回的迭代器。忽略有缓存的情况，我们继续看`computeOrReadCheckpoint`这个方法:\n\n```scala\n  private[spark] def computeOrReadCheckpoint(split: Partition, context: TaskContext): Iterator[T] =\n  {\n    if (isCheckpointedAndMaterialized) {//如果有checkpoint\n      firstParent[T].iterator(split, context)\n    } else {\n      compute(split, context)\n    }\n  }\n```\n\n可以看到，在没有缓存和 checkpoint 的情况下，itertor 方法直接调用了 compute 方法。而compute方法定义如下:\n\n```scala\n /**\n   * Implemented by subclasses to compute a given partition.\n   */\n  def compute(split: Partition, context: TaskContext): Iterator[T]\n```\n\n从注释可以看出，compute 方法应该由子类实现，用以计算给定分区并生成一个迭代器。如果不考虑缓存和 checkpoint 的情况下，简化一下代码:\n\n```scala\n final def iterator(split: Partition, context: TaskContext): Iterator[T] = {\n    compute(split,context)\n  }\n```\n\n那么实际上 iterator 的功能是: **接受一个分区，对这个分区进行计算，并返回计算结果的迭代器**。而之所以 compute 需要子类来实现，是因为只需要在子类中实现不同的 compute 方法，就能产生不同类型的 RDD。既然不同的RDD 有不同的功能，我们想知道之前的例子是如何进行迭代计算的，就需要知道这个例子中涉及到了哪些 RDD。\n\n首先查看`SparkContext`中与`parallelize`相关的部分代码:\n\n```scala\n  def parallelize[T: ClassTag](\n      seq: Seq[T],\n      numSlices: Int = defaultParallelism): RDD[T] = withScope {\n    assertNotStopped()\n    new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())\n  }\n```\n\n可以看到，`parallelize`实际返回了一个`ParallelCollectionRDD`。在`ParallelCollectionRDD`中并没有对`filter`方法进行重写，因此我们查看`RDD`中的`filter`方法:\n\n```scala\n def filter(f: T => Boolean): RDD[T] = withScope {\n    val cleanF = sc.clean(f)\n    new MapPartitionsRDD[T, T](\n      this,\n      (context, pid, iter) => iter.filter(cleanF),\n      preservesPartitioning = true)\n  }\n```\n\nfilter 方法返回了`MapPartitionsRDD`。为了方便起见，我将 ParallelCollectionRDD 类型的 RDD 称为 a，MapPartitionsRDD 类型的 RDD 成为B。那么先看一下 b 的 compute 方法:\n\n```scala\nprivate[spark] class MapPartitionsRDD[U: ClassTag, T: ClassTag](\n    var prev: RDD[T],\n    f: (TaskContext, Int, Iterator[T]) => Iterator[U],  // (TaskContext, partition index, iterator)\n    preservesPartitioning: Boolean = false)\n  extends RDD[U](prev) {\n  override val partitioner = if (preservesPartitioning) firstParent[T].partitioner else None\n  override def getPartitions: Array[Partition] = firstParent[T].partitions\n  override def compute(split: Partition, context: TaskContext): Iterator[U] =\n    f(context, split.index, firstParent[T].iterator(split, context))\n  override def clearDependencies() {\n    super.clearDependencies()\n    prev = null\n  }\n}\n\n```\n\n这里可以看到 compute 方法实际上调用 f 这个函数。而 f 这个函数是在 filter 方法中传入的`(context, pid, iter) => iter.filter(cleanF)` 。那么实际上 compute 进行的计算为:\n\n```scala\n(context, split.index, firstParent[T].iterator(split, context)) => firstParent[T].iterator(split, context).filter(cleanF)\n```\n\n前两个参数并没有用到，也就是最终的方法可以简化为:\n\n```scala\nfirstParent[T].iterator(split, context) => firstParent[T].iterator(split, context).filter(cleanF)\n```\n\n这里出现了一个`firstParent`，我们知道 RDD 之间存在依赖关系，如果rdd3 依赖 rdd2，rdd2 依赖 rdd1。rdd2 与 rdd3 都是rdd1 的一个父rdd, spark也将其称为血缘关系。而故名意思 rdd2 是 rdd3 的 firstParent。在这个例子中 a 是 b的 firstParent。那么在b 的 compute 中又调用了 a 的 iterator 方法。而 a 的 iterator 方法又会调用a 的 comput 方法。用箭头表示调用关系:\n\n```\nb.iterotr -> b.compute -> a.iterotr -> a.compute ->| 调用\nb.iterotr <- b.compute <- a.iterotr <- a.compute <-| 返回\n```\n\n而这里我们也可以看出来，b 调用 iterotr 返回的数据，是使用 b 中的方法对 a 返回数据进行处理。也就是b 的计算结果依赖于a的计算结果。如果将一个rdd比作一个函数，大概就是 `fb(fa())`，这样的。此时，我们已经得到了关于 spark 迭代的最简单的模型，假如我们有 n 个 rdd。那么之间的调用依赖关系便是:\n\n```\nn.iterotr -> n.compute -> (n-1).iterotr -> (n-1).compute ->...-> 1.iterotr -> 1.compute->| \nn.iterotr <- n.compute <- (n-1).iterotr <- (n-1).compute <-...<- 1.iterotr <- 1.compute<-|\n```\n\n那么细心的同学应该注意到了，所有的数据都是源自于第一个 RDD。这里把它称为源 RDD。例如本例中产生的 RDD 以及 textFile 方法产生的HadoopRDD都属于源RDD。既然属于源 RDD ，那么这个 RDD 一定会与内存或磁盘中的源数据产生交互，否则就没有真实的数据来源。这里的源数据是指内存的数组、本地文件、或者是HDFS上的分布式文件等。\n\n那么我们再继续看 a 的 compute 方法:\n\n```scala\n  override def compute(s: Partition, context: TaskContext): Iterator[T] = {\n    new InterruptibleIterator(context,s.asInstanceOf[ParallelCollectionPartition[T]].iterator)\n  }\n```\n\n可以看到 a 的 compute 根据 ParallelCollectionPartition 类的 iterator直接生成了一个迭代器。再看看ParallelCollectionPartition 的定义:\n\n```scala\nrivate[spark] class ParallelCollectionPartition[T: ClassTag](\n    var rddId: Long,\n    var slice: Int,\n    var values: Seq[T]\n  ) extends Partition with Serializable {\n  def iterator: Iterator[T] = values.iterator\n  ...\n```\n\niterator 是来自于 values 的一个迭代器，显然，values是内存中的数据。这说明 a 的 compute 方法直接返回了一个与源数据相关的迭代器。而这里 ParallelCollectionPartition 的数据是如何传入的，又是在什么时候被传入的呢?在**分区**小节将会提到。\n\n到这里，我们已经基本了解了 spark 的迭代计算的原理以及数据来源。但是仍然还有许多不了解的地方，例如 iterotr 会接受一个  Partition 作为参数。那么这个Partition参数到底起到了什么作用。其次，我们知道 rdd 的 iterotr 方法 是从后开始往前调用，那么又是谁调用的最后一个 rdd 的 itertor 方法呢。接下来我们就探索一下与分区相关的内容.\n\n### 分区\n\n提到分区，大部分都是类似于“RDD 中最小的数据单元”，“只是数据的抽象分区，而不是真正的持有数据”等等这样的描述。\n\n```scala\n/**\n * An identifier for a partition in an RDD.\n */\ntrait Partition extends Serializable {\n  def index: Int\n  override def hashCode(): Int = index\n  override def equals(other: Any): Boolean = super.equals(other)\n}\n```\n\n可以看到分区的类十分的简单，只有一个 index 属性。 当将分区这个参数传入 itertor 时表示我们需要 index 对应的分区的数据经过一系列计算的之后的迭代器。那么显然，在源 RDD 中一定会有更据分区的index获取对应数据的代码部分。而我们又知道，对于`ParallelCollectionPartition`这个分区类是直接持有数据的迭代器，因此我们只需要知道这个类如何被建立，便知道了是如何根据分区获取数据的。我们在整个工程中搜索 ParallelCollectionPartition，发现:\n\n```scala\n override def getPartitions: Array[Partition] = {\n    val slices = ParallelCollectionRDD.slice(data, numSlices).toArray\n    slices.indices.map(i => new ParallelCollectionPartition(id, i, slices(i))).toArray\n  }\n```\n\n中生成了一系列的分区对象。其中 ParallelCollectionRDD.slice() 方法根据总共有多少个分区来将 data  进行切分。而 data 我们在调用 parallelize() 方法时传入的。看到这里就应该明白了，**ParallelCollectionRDD 会将我们输入的数组进行切分，然后将利用分区对象持有数据的迭代器，当调用到 itertor 方法时就返回该分区的迭代器，供后续的RDD**进行计算。虽然这里我们只看了 ParallelCollectionRDD 的原代码，但是其他例如 HadoopRDD 的远离基本相同。 只不过一个是从内存中获取数据进行切分，另一个是从磁盘上获取数据进行切分。\n\n但是直到这里，我们仍然不知道 a 中的 itertor 方法的分区对象是如何传入的，因为这个方法间接被 b 的 itertor 方法调用。 而 b 的  itertor 方法同样也需要在外部被调用 ，因此要解决这个问题只需要找到 b 的 itertor 被调用的地方。不过我们首先可以根据代码猜测一下:\n\n```scala\n  override def compute(s: Partition, context: TaskContext): Iterator[T] = {\n    new InterruptibleIterator(context,s.asInstanceOf[ParallelCollectionPartition[T]].iterator)\n  }\n```\n\na 中的 compute 方法直接将传入的分区对象转为了 ParallelCollectionPartition 并获取了对应的迭代器。根据对象间强转的\n\n规则，传入的分区对象只能是 ParallelCollectionPartition 类型 ，因为其父类 RDD 并没有 iterator 方法。 并且传入的ParallelCollectionPartition 一定是持有源数据迭代器的对象，否则在 a 的 compute 中就无法向后返回迭代器。而且我们知道，spark 的计算是**惰性计算**，在调用 action 算子之后才会真正的开始计算。那么可以猜测，b 的 iterator 也是在调用了 count 方法之后才被调用的。为了证明这一点，我们继续查看代码:\n\n```scala\ndef count(): Long = sc.runJob(this, Utils.getIteratorSize _).sum\n```\n\n可以看到是调用了 SparkContext 中的runJob方法:\n\n```scala\n def runJob[T, U: ClassTag](\n     ...\n    dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)\n    ...\n  }\n```\n\n而 SparkContext 中的 runJob 又继续调用了 DAGScheduler 中的 runJob 方法， 继而调用了 submitJob 方法。之后由经历了 DAGSchedulerEventProcessLoop.post DAGSchedulerEventProcessLoop.doOnReceive 等方法，最后在 submitMissingTasks 看到建立了 ResultTask 对象。由于一个分区的数据会被一个task 处理，因此我们猜测在 ＲesultTask 中会有关于对应 rdd 调用 iterator 的信息，果然，在ResultTask中找到了 runTask 方法:\n\n```scala\noverride def runTask(context: TaskContext): U = {\n    // Deserialize the RDD and the func using the broadcast variables.\n    val threadMXBean = ManagementFactory.getThreadMXBean\n    val deserializeStartTime = System.currentTimeMillis()\n    val deserializeStartCpuTime = if (threadMXBean.isCurrentThreadCpuTimeSupported) {\n      threadMXBean.getCurrentThreadCpuTime\n    } else 0L\n    val ser = SparkEnv.get.closureSerializer.newInstance()\n    val (rdd, func) = ser.deserialize[(RDD[T], (TaskContext, Iterator[T]) => U)](\n      ByteBuffer.wrap(taskBinary.value), Thread.currentThread.getContextClassLoader)\n    _executorDeserializeTime = System.currentTimeMillis() - deserializeStartTime\n    _executorDeserializeCpuTime = if (threadMXBean.isCurrentThreadCpuTimeSupported) {\n      threadMXBean.getCurrentThreadCpuTime - deserializeStartCpuTime\n    } else 0L\n\n    func(context, rdd.iterator(partition, context))//这里就 b 的iterator被真正调用的地方\n  }\n\n\n```\n\n那么我们可以看到最后一行有一个rdd的调用，而这个rdd 是反徐序列化得的，很明显，这个rdd就是 b。此时，b 调用 iterator 方法的地方找到了，但是分区对象partition依然没有找到从哪里传入。由于 partition 是 ResultTask 构造时传入的，我们回到 submitMissingTasks 中，创建ResultTask时分区参数对应的为变量 part 。 \n\n```scala\n      //从需要计算的分区map中获取分区，并生成task\n       partitionsToCompute.map { id =>\n            val p: Int = stage.partitions(id)\n            val part = partitions(p)\n            val locs = taskIdToLocations(id)\n            new ResultTask(stage.id, stage.latestInfo.attemptNumber,\n              taskBinary, part, locs, id, properties, serializedTaskMetrics,\n              Option(jobId), Option(sc.applicationId), sc.applicationAttemptId)\n          }\n```\n\n继续往上看，可以看到 part 是 从 partitions 中获取的。这里我们也可以看出 **一个任务对应一个分区数据**。继续往上看，发现`partitions = stage.rdd.partitions`。实际上来自于 Stage 中的 rdd 中的 partitions。那么看Stage 对 rdd 这个属性的描述:\n\n```scala\n *\n * @param rdd RDD that this stage runs on: for a shuffle map stage, it's the RDD we run map tasks\n *   on, while for a result stage, it's the target RDD that we ran an action on\n *   这里的意思是 rdd 表示调用 action 算子的 rdd 也就是 b \n */\nprivate[scheduler] abstract class Stage(\n    val id: Int,\n    val rdd: RDD[_],\n    val numTasks: Int,\n    val parents: List[Stage],\n    val firstJobId: Int,\n    val callSite: CallSite)\n  extends Logging {\n```\n\n我们发现一个惊人的事实 ，这个 rdd 居然也是 b。也就是说，我们在 runTask函数中实际调用的方法是这样的:\n\n```scala\n  func(context, b.iterator(b.partitions(index), context))//这里就 b 的iterator被真正调用的地方\n```\n\nindex 表示该task对应需要执行的分区标号。随即我们继续看 b 中的 partitions 的定义:\n\n```scala\n override def getPartitions: Array[Partition] = firstParent[T].partitions\n```\n\n说明这个 partitions 来自于 a。 而 a 中的 partitions 实际上又来自于:\n\n```scala\n override def getPartitions: Array[Partition] = {\n    val slices = ParallelCollectionRDD.slice(data, numSlices).toArray\n    slices.indices.map(i => new ParallelCollectionPartition(id, i, slices(i))).toArray\n  }\n```\n\n这样最终 runTask 函数实际调用的方法为:\n\n```scala\n func(context, b.iterator(a.partitions(index), context))//这里就 b 的iterator被真正调用的地方\n```\n\n这和我们猜测的相同，这个分区确实为 ParallelCollectionPartition 对象，并且也持有了源数据的迭代器。这里其实有一个很巧妙的地方，虽然 RDD  的迭代计算是从后往前调用，但是传入源 RDD 的分区对象依然来自于源 RDD 自身。\n\n到了这里，我们也就明白分区对象是如何和数据关联起来的。ParallelCollectionPartition 对象中存在分区编号 index 以及 源数据的迭代器，通过分区编号就能获取到源数据的迭代器。\n\n## 总结\n\n经过如此大篇幅的讲解，终于对 spark 的迭代计算以及分区做了一个简单的分析。虽然还有很多地方不完善，例如还有其它类型的分区对象没有讲解，但是我相信你能完全看懂这篇文章所讲的内容，其他类型的分区对象对于你来说也很简单了。作者水平有限，并且这也是作者的第一篇关于 spark 源码阅读的博客，难免有遗漏和错误。如果想如给作者提建议和意见的话，请联系作者的微信或直接在 github 上提 issue 。\n\n \n\n","slug":"Spark-源码阅读计划-第一部分-迭代计算","published":1,"updated":"2018-06-13T14:42:19.376Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjiebvrsa0002fang9uvh8h4e","content":"<p>首先立一个flag，这将是一个长期更新的版块。</p>\n<h2 id=\"写在最开始\"><a href=\"#写在最开始\" class=\"headerlink\" title=\"写在最开始\"></a>写在最开始</h2><p>在我使用<code>spark</code>进行日志分析的时候感受到了<code>spark</code>的便捷与强大。在学习<code>spark</code>初期，我阅读了许多与<code>spark</code>相关的文档，在这个过程中了解了<code>RDD</code>，<code>分区</code>，<code>shuffle</code>等概念，但是我并没有对这些概念有更多具体的认识。由于不了解<code>spark</code>的基本运作方式使得我在编写代码的时候十分困扰。由于这个原因，我准备阅读<code>spark</code>的源代码来解决我对基本概念的认识。</p>\n<h2 id=\"本部分主要内容\"><a href=\"#本部分主要内容\" class=\"headerlink\" title=\"本部分主要内容\"></a>本部分主要内容</h2><p>虽然该章节的名字叫做<strong>迭代计算</strong>，但是本章会讨论包括<strong>RDD、分区、Job、迭代计算</strong>等相关的内容，其中会重点讨论分区与迭代计算。因此在阅读本章之前你至少需要对这些提到的概念有一定的了解。</p>\n<h2 id=\"准备工作\"><a href=\"#准备工作\" class=\"headerlink\" title=\"准备工作\"></a>准备工作</h2><p>你需要准备一份 Spark 的源代码以及一个便于全局搜索的编辑器|IDE。</p>\n<h2 id=\"一个简单的例子\"><a href=\"#一个简单的例子\" class=\"headerlink\" title=\"一个简单的例子\"></a>一个简单的例子</h2><p>假设现在我们有这样一个需求 : 寻找从 0 to 100 的偶数并输出其总数。利用 spark 编写代码如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sc.parallelize(<span class=\"number\">0</span> to <span class=\"number\">100</span>).filter(_%<span class=\"number\">2</span> == <span class=\"number\">0</span>).count</span><br><span class=\"line\"><span class=\"comment\">//res0: Long = 51</span></span><br></pre></td></tr></table></figure>\n<p>这是一个非常简单的例子，但是里面却蕴涵了许多基础的知识。这一章的内容也是从这个例子展开的。</p>\n<h2 id=\"迭代计算\"><a href=\"#迭代计算\" class=\"headerlink\" title=\"迭代计算\"></a>迭代计算</h2><h3 id=\"comput-方法与-itertor\"><a href=\"#comput-方法与-itertor\" class=\"headerlink\" title=\"comput 方法与 itertor\"></a>comput 方法与 itertor</h3><p>在RDD.scala 定义的类 RDD中，有两个实现迭代计算的核心方法，分别是<code>compute</code>以及<code>itertor</code>方法。其中<code>itertor</code>方法如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">final</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">iterator</span></span>(split: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>] = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (storageLevel != <span class=\"type\">StorageLevel</span>.<span class=\"type\">NONE</span>) &#123;<span class=\"comment\">//如果有缓存</span></span><br><span class=\"line\">    getOrCompute(split, context)</span><br><span class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">    computeOrReadCheckpoint(split, context)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>我们只关心这个方法的第一个参数<strong>分区</strong>，以及返回的迭代器。忽略有缓存的情况，我们继续看<code>computeOrReadCheckpoint</code>这个方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span>[spark] <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">computeOrReadCheckpoint</span></span>(split: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>] =</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (isCheckpointedAndMaterialized) &#123;<span class=\"comment\">//如果有checkpoint</span></span><br><span class=\"line\">    firstParent[<span class=\"type\">T</span>].iterator(split, context)</span><br><span class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">    compute(split, context)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可以看到，在没有缓存和 checkpoint 的情况下，itertor 方法直接调用了 compute 方法。而compute方法定义如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">  * Implemented by subclasses to compute a given partition.</span></span><br><span class=\"line\"><span class=\"comment\">  */</span></span><br><span class=\"line\"> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span></span>(split: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>]</span><br></pre></td></tr></table></figure>\n<p>从注释可以看出，compute 方法应该由子类实现，用以计算给定分区并生成一个迭代器。如果不考虑缓存和 checkpoint 的情况下，简化一下代码:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">final</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">iterator</span></span>(split: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>] = &#123;</span><br><span class=\"line\">   compute(split,context)</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>那么实际上 iterator 的功能是: <strong>接受一个分区，对这个分区进行计算，并返回计算结果的迭代器</strong>。而之所以 compute 需要子类来实现，是因为只需要在子类中实现不同的 compute 方法，就能产生不同类型的 RDD。既然不同的RDD 有不同的功能，我们想知道之前的例子是如何进行迭代计算的，就需要知道这个例子中涉及到了哪些 RDD。</p>\n<p>首先查看<code>SparkContext</code>中与<code>parallelize</code>相关的部分代码:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">parallelize</span></span>[<span class=\"type\">T</span>: <span class=\"type\">ClassTag</span>](</span><br><span class=\"line\">    seq: <span class=\"type\">Seq</span>[<span class=\"type\">T</span>],</span><br><span class=\"line\">    numSlices: <span class=\"type\">Int</span> = defaultParallelism): <span class=\"type\">RDD</span>[<span class=\"type\">T</span>] = withScope &#123;</span><br><span class=\"line\">  assertNotStopped()</span><br><span class=\"line\">  <span class=\"keyword\">new</span> <span class=\"type\">ParallelCollectionRDD</span>[<span class=\"type\">T</span>](<span class=\"keyword\">this</span>, seq, numSlices, <span class=\"type\">Map</span>[<span class=\"type\">Int</span>, <span class=\"type\">Seq</span>[<span class=\"type\">String</span>]]())</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可以看到，<code>parallelize</code>实际返回了一个<code>ParallelCollectionRDD</code>。在<code>ParallelCollectionRDD</code>中并没有对<code>filter</code>方法进行重写，因此我们查看<code>RDD</code>中的<code>filter</code>方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">filter</span></span>(f: <span class=\"type\">T</span> =&gt; <span class=\"type\">Boolean</span>): <span class=\"type\">RDD</span>[<span class=\"type\">T</span>] = withScope &#123;</span><br><span class=\"line\">   <span class=\"keyword\">val</span> cleanF = sc.clean(f)</span><br><span class=\"line\">   <span class=\"keyword\">new</span> <span class=\"type\">MapPartitionsRDD</span>[<span class=\"type\">T</span>, <span class=\"type\">T</span>](</span><br><span class=\"line\">     <span class=\"keyword\">this</span>,</span><br><span class=\"line\">     (context, pid, iter) =&gt; iter.filter(cleanF),</span><br><span class=\"line\">     preservesPartitioning = <span class=\"literal\">true</span>)</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>filter 方法返回了<code>MapPartitionsRDD</code>。为了方便起见，我将 ParallelCollectionRDD 类型的 RDD 称为 a，MapPartitionsRDD 类型的 RDD 成为B。那么先看一下 b 的 compute 方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span>[spark] <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MapPartitionsRDD</span>[<span class=\"type\">U</span>: <span class=\"type\">ClassTag</span>, <span class=\"type\">T</span>: <span class=\"type\">ClassTag</span>](<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    var prev: <span class=\"type\">RDD</span>[<span class=\"type\">T</span>],</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    f: (<span class=\"type\">TaskContext</span>, <span class=\"type\">Int</span>, <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>]</span>) <span class=\"title\">=&gt;</span> <span class=\"title\">Iterator</span>[<span class=\"type\">U</span>],  <span class=\"title\">//</span> (<span class=\"params\"><span class=\"type\">TaskContext</span>, partition index, iterator</span>)</span></span><br><span class=\"line\"><span class=\"class\">    <span class=\"title\">preservesPartitioning</span></span>: <span class=\"type\">Boolean</span> = <span class=\"literal\">false</span>)</span><br><span class=\"line\">  <span class=\"keyword\">extends</span> <span class=\"type\">RDD</span>[<span class=\"type\">U</span>](prev) &#123;</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"keyword\">val</span> partitioner = <span class=\"keyword\">if</span> (preservesPartitioning) firstParent[<span class=\"type\">T</span>].partitioner <span class=\"keyword\">else</span> <span class=\"type\">None</span></span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPartitions</span></span>: <span class=\"type\">Array</span>[<span class=\"type\">Partition</span>] = firstParent[<span class=\"type\">T</span>].partitions</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span></span>(split: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">U</span>] =</span><br><span class=\"line\">    f(context, split.index, firstParent[<span class=\"type\">T</span>].iterator(split, context))</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">clearDependencies</span></span>() &#123;</span><br><span class=\"line\">    <span class=\"keyword\">super</span>.clearDependencies()</span><br><span class=\"line\">    prev = <span class=\"literal\">null</span></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>这里可以看到 compute 方法实际上调用 f 这个函数。而 f 这个函数是在 filter 方法中传入的<code>(context, pid, iter) =&gt; iter.filter(cleanF)</code> 。那么实际上 compute 进行的计算为:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(context, split.index, firstParent[<span class=\"type\">T</span>].iterator(split, context)) =&gt; firstParent[<span class=\"type\">T</span>].iterator(split, context).filter(cleanF)</span><br></pre></td></tr></table></figure>\n<p>前两个参数并没有用到，也就是最终的方法可以简化为:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">firstParent[<span class=\"type\">T</span>].iterator(split, context) =&gt; firstParent[<span class=\"type\">T</span>].iterator(split, context).filter(cleanF)</span><br></pre></td></tr></table></figure>\n<p>这里出现了一个<code>firstParent</code>，我们知道 RDD 之间存在依赖关系，如果rdd3 依赖 rdd2，rdd2 依赖 rdd1。rdd2 与 rdd3 都是rdd1 的一个父rdd, spark也将其称为血缘关系。而故名意思 rdd2 是 rdd3 的 firstParent。在这个例子中 a 是 b的 firstParent。那么在b 的 compute 中又调用了 a 的 iterator 方法。而 a 的 iterator 方法又会调用a 的 comput 方法。用箭头表示调用关系:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">b.iterotr -&gt; b.compute -&gt; a.iterotr -&gt; a.compute -&gt;| 调用</span><br><span class=\"line\">b.iterotr &lt;- b.compute &lt;- a.iterotr &lt;- a.compute &lt;-| 返回</span><br></pre></td></tr></table></figure>\n<p>而这里我们也可以看出来，b 调用 iterotr 返回的数据，是使用 b 中的方法对 a 返回数据进行处理。也就是b 的计算结果依赖于a的计算结果。如果将一个rdd比作一个函数，大概就是 <code>fb(fa())</code>，这样的。此时，我们已经得到了关于 spark 迭代的最简单的模型，假如我们有 n 个 rdd。那么之间的调用依赖关系便是:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">n.iterotr -&gt; n.compute -&gt; (n-1).iterotr -&gt; (n-1).compute -&gt;...-&gt; 1.iterotr -&gt; 1.compute-&gt;| </span><br><span class=\"line\">n.iterotr &lt;- n.compute &lt;- (n-1).iterotr &lt;- (n-1).compute &lt;-...&lt;- 1.iterotr &lt;- 1.compute&lt;-|</span><br></pre></td></tr></table></figure>\n<p>那么细心的同学应该注意到了，所有的数据都是源自于第一个 RDD。这里把它称为源 RDD。例如本例中产生的 RDD 以及 textFile 方法产生的HadoopRDD都属于源RDD。既然属于源 RDD ，那么这个 RDD 一定会与内存或磁盘中的源数据产生交互，否则就没有真实的数据来源。这里的源数据是指内存的数组、本地文件、或者是HDFS上的分布式文件等。</p>\n<p>那么我们再继续看 a 的 compute 方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span></span>(s: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>] = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">new</span> <span class=\"type\">InterruptibleIterator</span>(context,s.asInstanceOf[<span class=\"type\">ParallelCollectionPartition</span>[<span class=\"type\">T</span>]].iterator)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可以看到 a 的 compute 根据 ParallelCollectionPartition 类的 iterator直接生成了一个迭代器。再看看ParallelCollectionPartition 的定义:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rivate[spark] <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ParallelCollectionPartition</span>[<span class=\"type\">T</span>: <span class=\"type\">ClassTag</span>](<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    var rddId: <span class=\"type\">Long</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    var slice: <span class=\"type\">Int</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    var values: <span class=\"type\">Seq</span>[<span class=\"type\">T</span>]</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">  </span>) <span class=\"keyword\">extends</span> <span class=\"title\">Partition</span> <span class=\"keyword\">with</span> <span class=\"title\">Serializable</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">iterator</span></span>: <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>] = values.iterator</span><br><span class=\"line\">  ...</span><br></pre></td></tr></table></figure>\n<p>iterator 是来自于 values 的一个迭代器，显然，values是内存中的数据。这说明 a 的 compute 方法直接返回了一个与源数据相关的迭代器。而这里 ParallelCollectionPartition 的数据是如何传入的，又是在什么时候被传入的呢?在<strong>分区</strong>小节将会提到。</p>\n<p>到这里，我们已经基本了解了 spark 的迭代计算的原理以及数据来源。但是仍然还有许多不了解的地方，例如 iterotr 会接受一个  Partition 作为参数。那么这个Partition参数到底起到了什么作用。其次，我们知道 rdd 的 iterotr 方法 是从后开始往前调用，那么又是谁调用的最后一个 rdd 的 itertor 方法呢。接下来我们就探索一下与分区相关的内容.</p>\n<h3 id=\"分区\"><a href=\"#分区\" class=\"headerlink\" title=\"分区\"></a>分区</h3><p>提到分区，大部分都是类似于“RDD 中最小的数据单元”，“只是数据的抽象分区，而不是真正的持有数据”等等这样的描述。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * An identifier for a partition in an RDD.</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">trait</span> <span class=\"title\">Partition</span> <span class=\"keyword\">extends</span> <span class=\"title\">Serializable</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">index</span></span>: <span class=\"type\">Int</span></span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hashCode</span></span>(): <span class=\"type\">Int</span> = index</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">equals</span></span>(other: <span class=\"type\">Any</span>): <span class=\"type\">Boolean</span> = <span class=\"keyword\">super</span>.equals(other)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可以看到分区的类十分的简单，只有一个 index 属性。 当将分区这个参数传入 itertor 时表示我们需要 index 对应的分区的数据经过一系列计算的之后的迭代器。那么显然，在源 RDD 中一定会有更据分区的index获取对应数据的代码部分。而我们又知道，对于<code>ParallelCollectionPartition</code>这个分区类是直接持有数据的迭代器，因此我们只需要知道这个类如何被建立，便知道了是如何根据分区获取数据的。我们在整个工程中搜索 ParallelCollectionPartition，发现:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPartitions</span></span>: <span class=\"type\">Array</span>[<span class=\"type\">Partition</span>] = &#123;</span><br><span class=\"line\">   <span class=\"keyword\">val</span> slices = <span class=\"type\">ParallelCollectionRDD</span>.slice(data, numSlices).toArray</span><br><span class=\"line\">   slices.indices.map(i =&gt; <span class=\"keyword\">new</span> <span class=\"type\">ParallelCollectionPartition</span>(id, i, slices(i))).toArray</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>中生成了一系列的分区对象。其中 ParallelCollectionRDD.slice() 方法根据总共有多少个分区来将 data  进行切分。而 data 我们在调用 parallelize() 方法时传入的。看到这里就应该明白了，<strong>ParallelCollectionRDD 会将我们输入的数组进行切分，然后将利用分区对象持有数据的迭代器，当调用到 itertor 方法时就返回该分区的迭代器，供后续的RDD</strong>进行计算。虽然这里我们只看了 ParallelCollectionRDD 的原代码，但是其他例如 HadoopRDD 的远离基本相同。 只不过一个是从内存中获取数据进行切分，另一个是从磁盘上获取数据进行切分。</p>\n<p>但是直到这里，我们仍然不知道 a 中的 itertor 方法的分区对象是如何传入的，因为这个方法间接被 b 的 itertor 方法调用。 而 b 的  itertor 方法同样也需要在外部被调用 ，因此要解决这个问题只需要找到 b 的 itertor 被调用的地方。不过我们首先可以根据代码猜测一下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span></span>(s: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>] = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">new</span> <span class=\"type\">InterruptibleIterator</span>(context,s.asInstanceOf[<span class=\"type\">ParallelCollectionPartition</span>[<span class=\"type\">T</span>]].iterator)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>a 中的 compute 方法直接将传入的分区对象转为了 ParallelCollectionPartition 并获取了对应的迭代器。根据对象间强转的</p>\n<p>规则，传入的分区对象只能是 ParallelCollectionPartition 类型 ，因为其父类 RDD 并没有 iterator 方法。 并且传入的ParallelCollectionPartition 一定是持有源数据迭代器的对象，否则在 a 的 compute 中就无法向后返回迭代器。而且我们知道，spark 的计算是<strong>惰性计算</strong>，在调用 action 算子之后才会真正的开始计算。那么可以猜测，b 的 iterator 也是在调用了 count 方法之后才被调用的。为了证明这一点，我们继续查看代码:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">count</span></span>(): <span class=\"type\">Long</span> = sc.runJob(<span class=\"keyword\">this</span>, <span class=\"type\">Utils</span>.getIteratorSize _).sum</span><br></pre></td></tr></table></figure>\n<p>可以看到是调用了 SparkContext 中的runJob方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">runJob</span></span>[<span class=\"type\">T</span>, <span class=\"type\">U</span>: <span class=\"type\">ClassTag</span>](</span><br><span class=\"line\">    ...</span><br><span class=\"line\">   dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)</span><br><span class=\"line\">   ...</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>而 SparkContext 中的 runJob 又继续调用了 DAGScheduler 中的 runJob 方法， 继而调用了 submitJob 方法。之后由经历了 DAGSchedulerEventProcessLoop.post DAGSchedulerEventProcessLoop.doOnReceive 等方法，最后在 submitMissingTasks 看到建立了 ResultTask 对象。由于一个分区的数据会被一个task 处理，因此我们猜测在 ＲesultTask 中会有关于对应 rdd 调用 iterator 的信息，果然，在ResultTask中找到了 runTask 方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">runTask</span></span>(context: <span class=\"type\">TaskContext</span>): <span class=\"type\">U</span> = &#123;</span><br><span class=\"line\">    <span class=\"comment\">// Deserialize the RDD and the func using the broadcast variables.</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> threadMXBean = <span class=\"type\">ManagementFactory</span>.getThreadMXBean</span><br><span class=\"line\">    <span class=\"keyword\">val</span> deserializeStartTime = <span class=\"type\">System</span>.currentTimeMillis()</span><br><span class=\"line\">    <span class=\"keyword\">val</span> deserializeStartCpuTime = <span class=\"keyword\">if</span> (threadMXBean.isCurrentThreadCpuTimeSupported) &#123;</span><br><span class=\"line\">      threadMXBean.getCurrentThreadCpuTime</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> <span class=\"number\">0</span>L</span><br><span class=\"line\">    <span class=\"keyword\">val</span> ser = <span class=\"type\">SparkEnv</span>.get.closureSerializer.newInstance()</span><br><span class=\"line\">    <span class=\"keyword\">val</span> (rdd, func) = ser.deserialize[(<span class=\"type\">RDD</span>[<span class=\"type\">T</span>], (<span class=\"type\">TaskContext</span>, <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>]) =&gt; <span class=\"type\">U</span>)](</span><br><span class=\"line\">      <span class=\"type\">ByteBuffer</span>.wrap(taskBinary.value), <span class=\"type\">Thread</span>.currentThread.getContextClassLoader)</span><br><span class=\"line\">    _executorDeserializeTime = <span class=\"type\">System</span>.currentTimeMillis() - deserializeStartTime</span><br><span class=\"line\">    _executorDeserializeCpuTime = <span class=\"keyword\">if</span> (threadMXBean.isCurrentThreadCpuTimeSupported) &#123;</span><br><span class=\"line\">      threadMXBean.getCurrentThreadCpuTime - deserializeStartCpuTime</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> <span class=\"number\">0</span>L</span><br><span class=\"line\"></span><br><span class=\"line\">    func(context, rdd.iterator(partition, context))<span class=\"comment\">//这里就 b 的iterator被真正调用的地方</span></span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>那么我们可以看到最后一行有一个rdd的调用，而这个rdd 是反徐序列化得的，很明显，这个rdd就是 b。此时，b 调用 iterator 方法的地方找到了，但是分区对象partition依然没有找到从哪里传入。由于 partition 是 ResultTask 构造时传入的，我们回到 submitMissingTasks 中，创建ResultTask时分区参数对应的为变量 part 。 </p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//从需要计算的分区map中获取分区，并生成task</span></span><br><span class=\"line\"> partitionsToCompute.map &#123; id =&gt;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> p: <span class=\"type\">Int</span> = stage.partitions(id)</span><br><span class=\"line\">      <span class=\"keyword\">val</span> part = partitions(p)</span><br><span class=\"line\">      <span class=\"keyword\">val</span> locs = taskIdToLocations(id)</span><br><span class=\"line\">      <span class=\"keyword\">new</span> <span class=\"type\">ResultTask</span>(stage.id, stage.latestInfo.attemptNumber,</span><br><span class=\"line\">        taskBinary, part, locs, id, properties, serializedTaskMetrics,</span><br><span class=\"line\">        <span class=\"type\">Option</span>(jobId), <span class=\"type\">Option</span>(sc.applicationId), sc.applicationAttemptId)</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>继续往上看，可以看到 part 是 从 partitions 中获取的。这里我们也可以看出 <strong>一个任务对应一个分区数据</strong>。继续往上看，发现<code>partitions = stage.rdd.partitions</code>。实际上来自于 Stage 中的 rdd 中的 partitions。那么看Stage 对 rdd 这个属性的描述:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> *</span><br><span class=\"line\"> * <span class=\"meta\">@param</span> rdd <span class=\"type\">RDD</span> that <span class=\"keyword\">this</span> stage runs on: <span class=\"keyword\">for</span> a shuffle map stage, it<span class=\"symbol\">'s</span> the <span class=\"type\">RDD</span> we run map tasks</span><br><span class=\"line\"> *   on, <span class=\"keyword\">while</span> <span class=\"keyword\">for</span> a result stage, it<span class=\"symbol\">'s</span> the target <span class=\"type\">RDD</span> that we ran an action on</span><br><span class=\"line\"> *   这里的意思是 rdd 表示调用 action 算子的 rdd 也就是 b </span><br><span class=\"line\"> */</span><br><span class=\"line\"><span class=\"keyword\">private</span>[scheduler] <span class=\"keyword\">abstract</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Stage</span>(<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val id: <span class=\"type\">Int</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val rdd: <span class=\"type\">RDD</span>[_],</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val numTasks: <span class=\"type\">Int</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val parents: <span class=\"type\">List</span>[<span class=\"type\">Stage</span>],</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val firstJobId: <span class=\"type\">Int</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val callSite: <span class=\"type\">CallSite</span></span>)</span></span><br><span class=\"line\"><span class=\"class\">  <span class=\"keyword\">extends</span> <span class=\"title\">Logging</span> </span>&#123;</span><br></pre></td></tr></table></figure>\n<p>我们发现一个惊人的事实 ，这个 rdd 居然也是 b。也就是说，我们在 runTask函数中实际调用的方法是这样的:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func(context, b.iterator(b.partitions(index), context))<span class=\"comment\">//这里就 b 的iterator被真正调用的地方</span></span><br></pre></td></tr></table></figure>\n<p>index 表示该task对应需要执行的分区标号。随即我们继续看 b 中的 partitions 的定义:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPartitions</span></span>: <span class=\"type\">Array</span>[<span class=\"type\">Partition</span>] = firstParent[<span class=\"type\">T</span>].partitions</span><br></pre></td></tr></table></figure>\n<p>说明这个 partitions 来自于 a。 而 a 中的 partitions 实际上又来自于:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPartitions</span></span>: <span class=\"type\">Array</span>[<span class=\"type\">Partition</span>] = &#123;</span><br><span class=\"line\">   <span class=\"keyword\">val</span> slices = <span class=\"type\">ParallelCollectionRDD</span>.slice(data, numSlices).toArray</span><br><span class=\"line\">   slices.indices.map(i =&gt; <span class=\"keyword\">new</span> <span class=\"type\">ParallelCollectionPartition</span>(id, i, slices(i))).toArray</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>这样最终 runTask 函数实际调用的方法为:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func(context, b.iterator(a.partitions(index), context))<span class=\"comment\">//这里就 b 的iterator被真正调用的地方</span></span><br></pre></td></tr></table></figure>\n<p>这和我们猜测的相同，这个分区确实为 ParallelCollectionPartition 对象，并且也持有了源数据的迭代器。这里其实有一个很巧妙的地方，虽然 RDD  的迭代计算是从后往前调用，但是传入源 RDD 的分区对象依然来自于源 RDD 自身。</p>\n<p>到了这里，我们也就明白分区对象是如何和数据关联起来的。ParallelCollectionPartition 对象中存在分区编号 index 以及 源数据的迭代器，通过分区编号就能获取到源数据的迭代器。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>经过如此大篇幅的讲解，终于对 spark 的迭代计算以及分区做了一个简单的分析。虽然还有很多地方不完善，例如还有其它类型的分区对象没有讲解，但是我相信你能完全看懂这篇文章所讲的内容，其他类型的分区对象对于你来说也很简单了。作者水平有限，并且这也是作者的第一篇关于 spark 源码阅读的博客，难免有遗漏和错误。如果想如给作者提建议和意见的话，请联系作者的微信或直接在 github 上提 issue 。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>首先立一个flag，这将是一个长期更新的版块。</p>\n<h2 id=\"写在最开始\"><a href=\"#写在最开始\" class=\"headerlink\" title=\"写在最开始\"></a>写在最开始</h2><p>在我使用<code>spark</code>进行日志分析的时候感受到了<code>spark</code>的便捷与强大。在学习<code>spark</code>初期，我阅读了许多与<code>spark</code>相关的文档，在这个过程中了解了<code>RDD</code>，<code>分区</code>，<code>shuffle</code>等概念，但是我并没有对这些概念有更多具体的认识。由于不了解<code>spark</code>的基本运作方式使得我在编写代码的时候十分困扰。由于这个原因，我准备阅读<code>spark</code>的源代码来解决我对基本概念的认识。</p>\n<h2 id=\"本部分主要内容\"><a href=\"#本部分主要内容\" class=\"headerlink\" title=\"本部分主要内容\"></a>本部分主要内容</h2><p>虽然该章节的名字叫做<strong>迭代计算</strong>，但是本章会讨论包括<strong>RDD、分区、Job、迭代计算</strong>等相关的内容，其中会重点讨论分区与迭代计算。因此在阅读本章之前你至少需要对这些提到的概念有一定的了解。</p>\n<h2 id=\"准备工作\"><a href=\"#准备工作\" class=\"headerlink\" title=\"准备工作\"></a>准备工作</h2><p>你需要准备一份 Spark 的源代码以及一个便于全局搜索的编辑器|IDE。</p>\n<h2 id=\"一个简单的例子\"><a href=\"#一个简单的例子\" class=\"headerlink\" title=\"一个简单的例子\"></a>一个简单的例子</h2><p>假设现在我们有这样一个需求 : 寻找从 0 to 100 的偶数并输出其总数。利用 spark 编写代码如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sc.parallelize(<span class=\"number\">0</span> to <span class=\"number\">100</span>).filter(_%<span class=\"number\">2</span> == <span class=\"number\">0</span>).count</span><br><span class=\"line\"><span class=\"comment\">//res0: Long = 51</span></span><br></pre></td></tr></table></figure>\n<p>这是一个非常简单的例子，但是里面却蕴涵了许多基础的知识。这一章的内容也是从这个例子展开的。</p>\n<h2 id=\"迭代计算\"><a href=\"#迭代计算\" class=\"headerlink\" title=\"迭代计算\"></a>迭代计算</h2><h3 id=\"comput-方法与-itertor\"><a href=\"#comput-方法与-itertor\" class=\"headerlink\" title=\"comput 方法与 itertor\"></a>comput 方法与 itertor</h3><p>在RDD.scala 定义的类 RDD中，有两个实现迭代计算的核心方法，分别是<code>compute</code>以及<code>itertor</code>方法。其中<code>itertor</code>方法如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">final</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">iterator</span></span>(split: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>] = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (storageLevel != <span class=\"type\">StorageLevel</span>.<span class=\"type\">NONE</span>) &#123;<span class=\"comment\">//如果有缓存</span></span><br><span class=\"line\">    getOrCompute(split, context)</span><br><span class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">    computeOrReadCheckpoint(split, context)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>我们只关心这个方法的第一个参数<strong>分区</strong>，以及返回的迭代器。忽略有缓存的情况，我们继续看<code>computeOrReadCheckpoint</code>这个方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span>[spark] <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">computeOrReadCheckpoint</span></span>(split: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>] =</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (isCheckpointedAndMaterialized) &#123;<span class=\"comment\">//如果有checkpoint</span></span><br><span class=\"line\">    firstParent[<span class=\"type\">T</span>].iterator(split, context)</span><br><span class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">    compute(split, context)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可以看到，在没有缓存和 checkpoint 的情况下，itertor 方法直接调用了 compute 方法。而compute方法定义如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">  * Implemented by subclasses to compute a given partition.</span></span><br><span class=\"line\"><span class=\"comment\">  */</span></span><br><span class=\"line\"> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span></span>(split: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>]</span><br></pre></td></tr></table></figure>\n<p>从注释可以看出，compute 方法应该由子类实现，用以计算给定分区并生成一个迭代器。如果不考虑缓存和 checkpoint 的情况下，简化一下代码:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">final</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">iterator</span></span>(split: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>] = &#123;</span><br><span class=\"line\">   compute(split,context)</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>那么实际上 iterator 的功能是: <strong>接受一个分区，对这个分区进行计算，并返回计算结果的迭代器</strong>。而之所以 compute 需要子类来实现，是因为只需要在子类中实现不同的 compute 方法，就能产生不同类型的 RDD。既然不同的RDD 有不同的功能，我们想知道之前的例子是如何进行迭代计算的，就需要知道这个例子中涉及到了哪些 RDD。</p>\n<p>首先查看<code>SparkContext</code>中与<code>parallelize</code>相关的部分代码:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">parallelize</span></span>[<span class=\"type\">T</span>: <span class=\"type\">ClassTag</span>](</span><br><span class=\"line\">    seq: <span class=\"type\">Seq</span>[<span class=\"type\">T</span>],</span><br><span class=\"line\">    numSlices: <span class=\"type\">Int</span> = defaultParallelism): <span class=\"type\">RDD</span>[<span class=\"type\">T</span>] = withScope &#123;</span><br><span class=\"line\">  assertNotStopped()</span><br><span class=\"line\">  <span class=\"keyword\">new</span> <span class=\"type\">ParallelCollectionRDD</span>[<span class=\"type\">T</span>](<span class=\"keyword\">this</span>, seq, numSlices, <span class=\"type\">Map</span>[<span class=\"type\">Int</span>, <span class=\"type\">Seq</span>[<span class=\"type\">String</span>]]())</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可以看到，<code>parallelize</code>实际返回了一个<code>ParallelCollectionRDD</code>。在<code>ParallelCollectionRDD</code>中并没有对<code>filter</code>方法进行重写，因此我们查看<code>RDD</code>中的<code>filter</code>方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">filter</span></span>(f: <span class=\"type\">T</span> =&gt; <span class=\"type\">Boolean</span>): <span class=\"type\">RDD</span>[<span class=\"type\">T</span>] = withScope &#123;</span><br><span class=\"line\">   <span class=\"keyword\">val</span> cleanF = sc.clean(f)</span><br><span class=\"line\">   <span class=\"keyword\">new</span> <span class=\"type\">MapPartitionsRDD</span>[<span class=\"type\">T</span>, <span class=\"type\">T</span>](</span><br><span class=\"line\">     <span class=\"keyword\">this</span>,</span><br><span class=\"line\">     (context, pid, iter) =&gt; iter.filter(cleanF),</span><br><span class=\"line\">     preservesPartitioning = <span class=\"literal\">true</span>)</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>filter 方法返回了<code>MapPartitionsRDD</code>。为了方便起见，我将 ParallelCollectionRDD 类型的 RDD 称为 a，MapPartitionsRDD 类型的 RDD 成为B。那么先看一下 b 的 compute 方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span>[spark] <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MapPartitionsRDD</span>[<span class=\"type\">U</span>: <span class=\"type\">ClassTag</span>, <span class=\"type\">T</span>: <span class=\"type\">ClassTag</span>](<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    var prev: <span class=\"type\">RDD</span>[<span class=\"type\">T</span>],</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    f: (<span class=\"type\">TaskContext</span>, <span class=\"type\">Int</span>, <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>]</span>) <span class=\"title\">=&gt;</span> <span class=\"title\">Iterator</span>[<span class=\"type\">U</span>],  <span class=\"title\">//</span> (<span class=\"params\"><span class=\"type\">TaskContext</span>, partition index, iterator</span>)</span></span><br><span class=\"line\"><span class=\"class\">    <span class=\"title\">preservesPartitioning</span></span>: <span class=\"type\">Boolean</span> = <span class=\"literal\">false</span>)</span><br><span class=\"line\">  <span class=\"keyword\">extends</span> <span class=\"type\">RDD</span>[<span class=\"type\">U</span>](prev) &#123;</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"keyword\">val</span> partitioner = <span class=\"keyword\">if</span> (preservesPartitioning) firstParent[<span class=\"type\">T</span>].partitioner <span class=\"keyword\">else</span> <span class=\"type\">None</span></span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPartitions</span></span>: <span class=\"type\">Array</span>[<span class=\"type\">Partition</span>] = firstParent[<span class=\"type\">T</span>].partitions</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span></span>(split: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">U</span>] =</span><br><span class=\"line\">    f(context, split.index, firstParent[<span class=\"type\">T</span>].iterator(split, context))</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">clearDependencies</span></span>() &#123;</span><br><span class=\"line\">    <span class=\"keyword\">super</span>.clearDependencies()</span><br><span class=\"line\">    prev = <span class=\"literal\">null</span></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>这里可以看到 compute 方法实际上调用 f 这个函数。而 f 这个函数是在 filter 方法中传入的<code>(context, pid, iter) =&gt; iter.filter(cleanF)</code> 。那么实际上 compute 进行的计算为:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(context, split.index, firstParent[<span class=\"type\">T</span>].iterator(split, context)) =&gt; firstParent[<span class=\"type\">T</span>].iterator(split, context).filter(cleanF)</span><br></pre></td></tr></table></figure>\n<p>前两个参数并没有用到，也就是最终的方法可以简化为:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">firstParent[<span class=\"type\">T</span>].iterator(split, context) =&gt; firstParent[<span class=\"type\">T</span>].iterator(split, context).filter(cleanF)</span><br></pre></td></tr></table></figure>\n<p>这里出现了一个<code>firstParent</code>，我们知道 RDD 之间存在依赖关系，如果rdd3 依赖 rdd2，rdd2 依赖 rdd1。rdd2 与 rdd3 都是rdd1 的一个父rdd, spark也将其称为血缘关系。而故名意思 rdd2 是 rdd3 的 firstParent。在这个例子中 a 是 b的 firstParent。那么在b 的 compute 中又调用了 a 的 iterator 方法。而 a 的 iterator 方法又会调用a 的 comput 方法。用箭头表示调用关系:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">b.iterotr -&gt; b.compute -&gt; a.iterotr -&gt; a.compute -&gt;| 调用</span><br><span class=\"line\">b.iterotr &lt;- b.compute &lt;- a.iterotr &lt;- a.compute &lt;-| 返回</span><br></pre></td></tr></table></figure>\n<p>而这里我们也可以看出来，b 调用 iterotr 返回的数据，是使用 b 中的方法对 a 返回数据进行处理。也就是b 的计算结果依赖于a的计算结果。如果将一个rdd比作一个函数，大概就是 <code>fb(fa())</code>，这样的。此时，我们已经得到了关于 spark 迭代的最简单的模型，假如我们有 n 个 rdd。那么之间的调用依赖关系便是:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">n.iterotr -&gt; n.compute -&gt; (n-1).iterotr -&gt; (n-1).compute -&gt;...-&gt; 1.iterotr -&gt; 1.compute-&gt;| </span><br><span class=\"line\">n.iterotr &lt;- n.compute &lt;- (n-1).iterotr &lt;- (n-1).compute &lt;-...&lt;- 1.iterotr &lt;- 1.compute&lt;-|</span><br></pre></td></tr></table></figure>\n<p>那么细心的同学应该注意到了，所有的数据都是源自于第一个 RDD。这里把它称为源 RDD。例如本例中产生的 RDD 以及 textFile 方法产生的HadoopRDD都属于源RDD。既然属于源 RDD ，那么这个 RDD 一定会与内存或磁盘中的源数据产生交互，否则就没有真实的数据来源。这里的源数据是指内存的数组、本地文件、或者是HDFS上的分布式文件等。</p>\n<p>那么我们再继续看 a 的 compute 方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span></span>(s: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>] = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">new</span> <span class=\"type\">InterruptibleIterator</span>(context,s.asInstanceOf[<span class=\"type\">ParallelCollectionPartition</span>[<span class=\"type\">T</span>]].iterator)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可以看到 a 的 compute 根据 ParallelCollectionPartition 类的 iterator直接生成了一个迭代器。再看看ParallelCollectionPartition 的定义:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rivate[spark] <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ParallelCollectionPartition</span>[<span class=\"type\">T</span>: <span class=\"type\">ClassTag</span>](<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    var rddId: <span class=\"type\">Long</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    var slice: <span class=\"type\">Int</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    var values: <span class=\"type\">Seq</span>[<span class=\"type\">T</span>]</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">  </span>) <span class=\"keyword\">extends</span> <span class=\"title\">Partition</span> <span class=\"keyword\">with</span> <span class=\"title\">Serializable</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">iterator</span></span>: <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>] = values.iterator</span><br><span class=\"line\">  ...</span><br></pre></td></tr></table></figure>\n<p>iterator 是来自于 values 的一个迭代器，显然，values是内存中的数据。这说明 a 的 compute 方法直接返回了一个与源数据相关的迭代器。而这里 ParallelCollectionPartition 的数据是如何传入的，又是在什么时候被传入的呢?在<strong>分区</strong>小节将会提到。</p>\n<p>到这里，我们已经基本了解了 spark 的迭代计算的原理以及数据来源。但是仍然还有许多不了解的地方，例如 iterotr 会接受一个  Partition 作为参数。那么这个Partition参数到底起到了什么作用。其次，我们知道 rdd 的 iterotr 方法 是从后开始往前调用，那么又是谁调用的最后一个 rdd 的 itertor 方法呢。接下来我们就探索一下与分区相关的内容.</p>\n<h3 id=\"分区\"><a href=\"#分区\" class=\"headerlink\" title=\"分区\"></a>分区</h3><p>提到分区，大部分都是类似于“RDD 中最小的数据单元”，“只是数据的抽象分区，而不是真正的持有数据”等等这样的描述。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * An identifier for a partition in an RDD.</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">trait</span> <span class=\"title\">Partition</span> <span class=\"keyword\">extends</span> <span class=\"title\">Serializable</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">index</span></span>: <span class=\"type\">Int</span></span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hashCode</span></span>(): <span class=\"type\">Int</span> = index</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">equals</span></span>(other: <span class=\"type\">Any</span>): <span class=\"type\">Boolean</span> = <span class=\"keyword\">super</span>.equals(other)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可以看到分区的类十分的简单，只有一个 index 属性。 当将分区这个参数传入 itertor 时表示我们需要 index 对应的分区的数据经过一系列计算的之后的迭代器。那么显然，在源 RDD 中一定会有更据分区的index获取对应数据的代码部分。而我们又知道，对于<code>ParallelCollectionPartition</code>这个分区类是直接持有数据的迭代器，因此我们只需要知道这个类如何被建立，便知道了是如何根据分区获取数据的。我们在整个工程中搜索 ParallelCollectionPartition，发现:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPartitions</span></span>: <span class=\"type\">Array</span>[<span class=\"type\">Partition</span>] = &#123;</span><br><span class=\"line\">   <span class=\"keyword\">val</span> slices = <span class=\"type\">ParallelCollectionRDD</span>.slice(data, numSlices).toArray</span><br><span class=\"line\">   slices.indices.map(i =&gt; <span class=\"keyword\">new</span> <span class=\"type\">ParallelCollectionPartition</span>(id, i, slices(i))).toArray</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>中生成了一系列的分区对象。其中 ParallelCollectionRDD.slice() 方法根据总共有多少个分区来将 data  进行切分。而 data 我们在调用 parallelize() 方法时传入的。看到这里就应该明白了，<strong>ParallelCollectionRDD 会将我们输入的数组进行切分，然后将利用分区对象持有数据的迭代器，当调用到 itertor 方法时就返回该分区的迭代器，供后续的RDD</strong>进行计算。虽然这里我们只看了 ParallelCollectionRDD 的原代码，但是其他例如 HadoopRDD 的远离基本相同。 只不过一个是从内存中获取数据进行切分，另一个是从磁盘上获取数据进行切分。</p>\n<p>但是直到这里，我们仍然不知道 a 中的 itertor 方法的分区对象是如何传入的，因为这个方法间接被 b 的 itertor 方法调用。 而 b 的  itertor 方法同样也需要在外部被调用 ，因此要解决这个问题只需要找到 b 的 itertor 被调用的地方。不过我们首先可以根据代码猜测一下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span></span>(s: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>] = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">new</span> <span class=\"type\">InterruptibleIterator</span>(context,s.asInstanceOf[<span class=\"type\">ParallelCollectionPartition</span>[<span class=\"type\">T</span>]].iterator)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>a 中的 compute 方法直接将传入的分区对象转为了 ParallelCollectionPartition 并获取了对应的迭代器。根据对象间强转的</p>\n<p>规则，传入的分区对象只能是 ParallelCollectionPartition 类型 ，因为其父类 RDD 并没有 iterator 方法。 并且传入的ParallelCollectionPartition 一定是持有源数据迭代器的对象，否则在 a 的 compute 中就无法向后返回迭代器。而且我们知道，spark 的计算是<strong>惰性计算</strong>，在调用 action 算子之后才会真正的开始计算。那么可以猜测，b 的 iterator 也是在调用了 count 方法之后才被调用的。为了证明这一点，我们继续查看代码:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">count</span></span>(): <span class=\"type\">Long</span> = sc.runJob(<span class=\"keyword\">this</span>, <span class=\"type\">Utils</span>.getIteratorSize _).sum</span><br></pre></td></tr></table></figure>\n<p>可以看到是调用了 SparkContext 中的runJob方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">runJob</span></span>[<span class=\"type\">T</span>, <span class=\"type\">U</span>: <span class=\"type\">ClassTag</span>](</span><br><span class=\"line\">    ...</span><br><span class=\"line\">   dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)</span><br><span class=\"line\">   ...</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>而 SparkContext 中的 runJob 又继续调用了 DAGScheduler 中的 runJob 方法， 继而调用了 submitJob 方法。之后由经历了 DAGSchedulerEventProcessLoop.post DAGSchedulerEventProcessLoop.doOnReceive 等方法，最后在 submitMissingTasks 看到建立了 ResultTask 对象。由于一个分区的数据会被一个task 处理，因此我们猜测在 ＲesultTask 中会有关于对应 rdd 调用 iterator 的信息，果然，在ResultTask中找到了 runTask 方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">runTask</span></span>(context: <span class=\"type\">TaskContext</span>): <span class=\"type\">U</span> = &#123;</span><br><span class=\"line\">    <span class=\"comment\">// Deserialize the RDD and the func using the broadcast variables.</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> threadMXBean = <span class=\"type\">ManagementFactory</span>.getThreadMXBean</span><br><span class=\"line\">    <span class=\"keyword\">val</span> deserializeStartTime = <span class=\"type\">System</span>.currentTimeMillis()</span><br><span class=\"line\">    <span class=\"keyword\">val</span> deserializeStartCpuTime = <span class=\"keyword\">if</span> (threadMXBean.isCurrentThreadCpuTimeSupported) &#123;</span><br><span class=\"line\">      threadMXBean.getCurrentThreadCpuTime</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> <span class=\"number\">0</span>L</span><br><span class=\"line\">    <span class=\"keyword\">val</span> ser = <span class=\"type\">SparkEnv</span>.get.closureSerializer.newInstance()</span><br><span class=\"line\">    <span class=\"keyword\">val</span> (rdd, func) = ser.deserialize[(<span class=\"type\">RDD</span>[<span class=\"type\">T</span>], (<span class=\"type\">TaskContext</span>, <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>]) =&gt; <span class=\"type\">U</span>)](</span><br><span class=\"line\">      <span class=\"type\">ByteBuffer</span>.wrap(taskBinary.value), <span class=\"type\">Thread</span>.currentThread.getContextClassLoader)</span><br><span class=\"line\">    _executorDeserializeTime = <span class=\"type\">System</span>.currentTimeMillis() - deserializeStartTime</span><br><span class=\"line\">    _executorDeserializeCpuTime = <span class=\"keyword\">if</span> (threadMXBean.isCurrentThreadCpuTimeSupported) &#123;</span><br><span class=\"line\">      threadMXBean.getCurrentThreadCpuTime - deserializeStartCpuTime</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> <span class=\"number\">0</span>L</span><br><span class=\"line\"></span><br><span class=\"line\">    func(context, rdd.iterator(partition, context))<span class=\"comment\">//这里就 b 的iterator被真正调用的地方</span></span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>那么我们可以看到最后一行有一个rdd的调用，而这个rdd 是反徐序列化得的，很明显，这个rdd就是 b。此时，b 调用 iterator 方法的地方找到了，但是分区对象partition依然没有找到从哪里传入。由于 partition 是 ResultTask 构造时传入的，我们回到 submitMissingTasks 中，创建ResultTask时分区参数对应的为变量 part 。 </p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//从需要计算的分区map中获取分区，并生成task</span></span><br><span class=\"line\"> partitionsToCompute.map &#123; id =&gt;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> p: <span class=\"type\">Int</span> = stage.partitions(id)</span><br><span class=\"line\">      <span class=\"keyword\">val</span> part = partitions(p)</span><br><span class=\"line\">      <span class=\"keyword\">val</span> locs = taskIdToLocations(id)</span><br><span class=\"line\">      <span class=\"keyword\">new</span> <span class=\"type\">ResultTask</span>(stage.id, stage.latestInfo.attemptNumber,</span><br><span class=\"line\">        taskBinary, part, locs, id, properties, serializedTaskMetrics,</span><br><span class=\"line\">        <span class=\"type\">Option</span>(jobId), <span class=\"type\">Option</span>(sc.applicationId), sc.applicationAttemptId)</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>继续往上看，可以看到 part 是 从 partitions 中获取的。这里我们也可以看出 <strong>一个任务对应一个分区数据</strong>。继续往上看，发现<code>partitions = stage.rdd.partitions</code>。实际上来自于 Stage 中的 rdd 中的 partitions。那么看Stage 对 rdd 这个属性的描述:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> *</span><br><span class=\"line\"> * <span class=\"meta\">@param</span> rdd <span class=\"type\">RDD</span> that <span class=\"keyword\">this</span> stage runs on: <span class=\"keyword\">for</span> a shuffle map stage, it<span class=\"symbol\">'s</span> the <span class=\"type\">RDD</span> we run map tasks</span><br><span class=\"line\"> *   on, <span class=\"keyword\">while</span> <span class=\"keyword\">for</span> a result stage, it<span class=\"symbol\">'s</span> the target <span class=\"type\">RDD</span> that we ran an action on</span><br><span class=\"line\"> *   这里的意思是 rdd 表示调用 action 算子的 rdd 也就是 b </span><br><span class=\"line\"> */</span><br><span class=\"line\"><span class=\"keyword\">private</span>[scheduler] <span class=\"keyword\">abstract</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Stage</span>(<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val id: <span class=\"type\">Int</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val rdd: <span class=\"type\">RDD</span>[_],</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val numTasks: <span class=\"type\">Int</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val parents: <span class=\"type\">List</span>[<span class=\"type\">Stage</span>],</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val firstJobId: <span class=\"type\">Int</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val callSite: <span class=\"type\">CallSite</span></span>)</span></span><br><span class=\"line\"><span class=\"class\">  <span class=\"keyword\">extends</span> <span class=\"title\">Logging</span> </span>&#123;</span><br></pre></td></tr></table></figure>\n<p>我们发现一个惊人的事实 ，这个 rdd 居然也是 b。也就是说，我们在 runTask函数中实际调用的方法是这样的:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func(context, b.iterator(b.partitions(index), context))<span class=\"comment\">//这里就 b 的iterator被真正调用的地方</span></span><br></pre></td></tr></table></figure>\n<p>index 表示该task对应需要执行的分区标号。随即我们继续看 b 中的 partitions 的定义:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPartitions</span></span>: <span class=\"type\">Array</span>[<span class=\"type\">Partition</span>] = firstParent[<span class=\"type\">T</span>].partitions</span><br></pre></td></tr></table></figure>\n<p>说明这个 partitions 来自于 a。 而 a 中的 partitions 实际上又来自于:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPartitions</span></span>: <span class=\"type\">Array</span>[<span class=\"type\">Partition</span>] = &#123;</span><br><span class=\"line\">   <span class=\"keyword\">val</span> slices = <span class=\"type\">ParallelCollectionRDD</span>.slice(data, numSlices).toArray</span><br><span class=\"line\">   slices.indices.map(i =&gt; <span class=\"keyword\">new</span> <span class=\"type\">ParallelCollectionPartition</span>(id, i, slices(i))).toArray</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>这样最终 runTask 函数实际调用的方法为:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func(context, b.iterator(a.partitions(index), context))<span class=\"comment\">//这里就 b 的iterator被真正调用的地方</span></span><br></pre></td></tr></table></figure>\n<p>这和我们猜测的相同，这个分区确实为 ParallelCollectionPartition 对象，并且也持有了源数据的迭代器。这里其实有一个很巧妙的地方，虽然 RDD  的迭代计算是从后往前调用，但是传入源 RDD 的分区对象依然来自于源 RDD 自身。</p>\n<p>到了这里，我们也就明白分区对象是如何和数据关联起来的。ParallelCollectionPartition 对象中存在分区编号 index 以及 源数据的迭代器，通过分区编号就能获取到源数据的迭代器。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>经过如此大篇幅的讲解，终于对 spark 的迭代计算以及分区做了一个简单的分析。虽然还有很多地方不完善，例如还有其它类型的分区对象没有讲解，但是我相信你能完全看懂这篇文章所讲的内容，其他类型的分区对象对于你来说也很简单了。作者水平有限，并且这也是作者的第一篇关于 spark 源码阅读的博客，难免有遗漏和错误。如果想如给作者提建议和意见的话，请联系作者的微信或直接在 github 上提 issue 。</p>\n"},{"title":"使用git + hexo 建立个人博客","_content":"\n## 安装 hexo\n\n1. 安装`node`\n2. 安装`npm`\n3. 安装`hexo: npm install -g hexo-cli`\n\n\n## 建立一个新博客\n\n1. `hexo init blog`:使用该命令会在blog目录下建立一个博客并且在`source/_posts/hello-world.md`生成一篇名为`hello world`的文章，随后你可以选择删除它并新建自己的文章。\n2. `cd blog`\n3. `hexo g`:使用该命令将 `source/_posts/hello-world.md` 渲染为`html、css、js`静态资源\n4. `hexo s`:开启服务器。然后http://localhost:4000/\n\n## 关联至github\n\n1. 新建仓库`xxx.github.io`，这里 xxx 可以是你想要取的命令，但是必须以`github.io`结尾\n\n2. 此时可以访问`https://xxx.github.io`，但是没有内容\n\n3. 修改`blog`目录下配置文件`_config.yml`，找到`deploy`选项，修改(新增)为:\n\n   ```yaml\n   deploy:\n     type: git\n     repository: git@github.com:xxx/xxx.github.io.git \n     branch: master\n   ```\n\n4. 安装插件`npm install hexo-deployer-git --save`\n\n5. `hexo d -g` 生成内容后部署\n\n6. 访问`https://xxx.github.io`，应该要延迟一段时间才能看到效果\n\n## 更换主题\n\n由于初始的主题不怎么好看，可以选择更换一下主题。官方主题地址为[hexo-themes](https://hexo.io/themes/)。本教程中采用[maupassant-hexo](https://www.haomwei.com/technology/maupassant-hexo.html#%E6%94%AF%E6%8C%81%E8%AF%AD%E8%A8%80)为主题\n\n1. 由于大部分的主题都托管在`github`上，在`blog`目录下运行:\n\n   `git clone https://github.com/tufu9441/maupassant-hexo.git themes/maupassant` \n\n   `themes/xxx`是`hexo`存放`xxx`主题的目录\n\n2. `npm install hexo-renderer-pug --save`\n\n3. `npm install hexo-renderer-sass --save --registry=https://registry.npm.taobao.org`\n\n4. 修改`_config.yml`中主题为`theme: maupassant`\n\n5. `hexo g`重新生成\n\n6. `hexo s`开启服务器\n\n主题还有许多可用的配置，请参照上面给出的链接进行设置\n\n## 目录、tag\n\n需要`归档`和`tag`只需要在`markdown`上加上一些`YAML`头部信息:\n\n```\n---\ntitle: hello\ncategories: 杂谈\ntag: 杂七杂八\n---\n```\n\n即可\n\n","source":"_posts/使用git+hexo建立个人博客.md","raw":"---\ntitle: 使用git + hexo 建立个人博客\ncategories: 工具\ntag: \n  - 工具\n  - 教程\n---\n\n## 安装 hexo\n\n1. 安装`node`\n2. 安装`npm`\n3. 安装`hexo: npm install -g hexo-cli`\n\n\n## 建立一个新博客\n\n1. `hexo init blog`:使用该命令会在blog目录下建立一个博客并且在`source/_posts/hello-world.md`生成一篇名为`hello world`的文章，随后你可以选择删除它并新建自己的文章。\n2. `cd blog`\n3. `hexo g`:使用该命令将 `source/_posts/hello-world.md` 渲染为`html、css、js`静态资源\n4. `hexo s`:开启服务器。然后http://localhost:4000/\n\n## 关联至github\n\n1. 新建仓库`xxx.github.io`，这里 xxx 可以是你想要取的命令，但是必须以`github.io`结尾\n\n2. 此时可以访问`https://xxx.github.io`，但是没有内容\n\n3. 修改`blog`目录下配置文件`_config.yml`，找到`deploy`选项，修改(新增)为:\n\n   ```yaml\n   deploy:\n     type: git\n     repository: git@github.com:xxx/xxx.github.io.git \n     branch: master\n   ```\n\n4. 安装插件`npm install hexo-deployer-git --save`\n\n5. `hexo d -g` 生成内容后部署\n\n6. 访问`https://xxx.github.io`，应该要延迟一段时间才能看到效果\n\n## 更换主题\n\n由于初始的主题不怎么好看，可以选择更换一下主题。官方主题地址为[hexo-themes](https://hexo.io/themes/)。本教程中采用[maupassant-hexo](https://www.haomwei.com/technology/maupassant-hexo.html#%E6%94%AF%E6%8C%81%E8%AF%AD%E8%A8%80)为主题\n\n1. 由于大部分的主题都托管在`github`上，在`blog`目录下运行:\n\n   `git clone https://github.com/tufu9441/maupassant-hexo.git themes/maupassant` \n\n   `themes/xxx`是`hexo`存放`xxx`主题的目录\n\n2. `npm install hexo-renderer-pug --save`\n\n3. `npm install hexo-renderer-sass --save --registry=https://registry.npm.taobao.org`\n\n4. 修改`_config.yml`中主题为`theme: maupassant`\n\n5. `hexo g`重新生成\n\n6. `hexo s`开启服务器\n\n主题还有许多可用的配置，请参照上面给出的链接进行设置\n\n## 目录、tag\n\n需要`归档`和`tag`只需要在`markdown`上加上一些`YAML`头部信息:\n\n```\n---\ntitle: hello\ncategories: 杂谈\ntag: 杂七杂八\n---\n```\n\n即可\n\n","slug":"使用git+hexo建立个人博客","published":1,"date":"2018-06-13T14:42:19.376Z","updated":"2018-06-13T14:42:19.376Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjiebvrsf0005fangenc7xp65","content":"<h2 id=\"安装-hexo\"><a href=\"#安装-hexo\" class=\"headerlink\" title=\"安装 hexo\"></a>安装 hexo</h2><ol>\n<li>安装<code>node</code></li>\n<li>安装<code>npm</code></li>\n<li>安装<code>hexo: npm install -g hexo-cli</code></li>\n</ol>\n<h2 id=\"建立一个新博客\"><a href=\"#建立一个新博客\" class=\"headerlink\" title=\"建立一个新博客\"></a>建立一个新博客</h2><ol>\n<li><code>hexo init blog</code>:使用该命令会在blog目录下建立一个博客并且在<code>source/_posts/hello-world.md</code>生成一篇名为<code>hello world</code>的文章，随后你可以选择删除它并新建自己的文章。</li>\n<li><code>cd blog</code></li>\n<li><code>hexo g</code>:使用该命令将 <code>source/_posts/hello-world.md</code> 渲染为<code>html、css、js</code>静态资源</li>\n<li><code>hexo s</code>:开启服务器。然后<a href=\"http://localhost:4000/\" target=\"_blank\" rel=\"noopener\">http://localhost:4000/</a></li>\n</ol>\n<h2 id=\"关联至github\"><a href=\"#关联至github\" class=\"headerlink\" title=\"关联至github\"></a>关联至github</h2><ol>\n<li><p>新建仓库<code>xxx.github.io</code>，这里 xxx 可以是你想要取的命令，但是必须以<code>github.io</code>结尾</p>\n</li>\n<li><p>此时可以访问<code>https://xxx.github.io</code>，但是没有内容</p>\n</li>\n<li><p>修改<code>blog</code>目录下配置文件<code>_config.yml</code>，找到<code>deploy</code>选项，修改(新增)为:</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">deploy:</span></span><br><span class=\"line\"><span class=\"attr\">  type:</span> <span class=\"string\">git</span></span><br><span class=\"line\"><span class=\"attr\">  repository:</span> <span class=\"string\">git@github.com:xxx/xxx.github.io.git</span> </span><br><span class=\"line\"><span class=\"attr\">  branch:</span> <span class=\"string\">master</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>安装插件<code>npm install hexo-deployer-git --save</code></p>\n</li>\n<li><p><code>hexo d -g</code> 生成内容后部署</p>\n</li>\n<li><p>访问<code>https://xxx.github.io</code>，应该要延迟一段时间才能看到效果</p>\n</li>\n</ol>\n<h2 id=\"更换主题\"><a href=\"#更换主题\" class=\"headerlink\" title=\"更换主题\"></a>更换主题</h2><p>由于初始的主题不怎么好看，可以选择更换一下主题。官方主题地址为<a href=\"https://hexo.io/themes/\" target=\"_blank\" rel=\"noopener\">hexo-themes</a>。本教程中采用<a href=\"https://www.haomwei.com/technology/maupassant-hexo.html#%E6%94%AF%E6%8C%81%E8%AF%AD%E8%A8%80\" target=\"_blank\" rel=\"noopener\">maupassant-hexo</a>为主题</p>\n<ol>\n<li><p>由于大部分的主题都托管在<code>github</code>上，在<code>blog</code>目录下运行:</p>\n<p><code>git clone https://github.com/tufu9441/maupassant-hexo.git themes/maupassant</code> </p>\n<p><code>themes/xxx</code>是<code>hexo</code>存放<code>xxx</code>主题的目录</p>\n</li>\n<li><p><code>npm install hexo-renderer-pug --save</code></p>\n</li>\n<li><p><code>npm install hexo-renderer-sass --save --registry=https://registry.npm.taobao.org</code></p>\n</li>\n<li><p>修改<code>_config.yml</code>中主题为<code>theme: maupassant</code></p>\n</li>\n<li><p><code>hexo g</code>重新生成</p>\n</li>\n<li><p><code>hexo s</code>开启服务器</p>\n</li>\n</ol>\n<p>主题还有许多可用的配置，请参照上面给出的链接进行设置</p>\n<h2 id=\"目录、tag\"><a href=\"#目录、tag\" class=\"headerlink\" title=\"目录、tag\"></a>目录、tag</h2><p>需要<code>归档</code>和<code>tag</code>只需要在<code>markdown</code>上加上一些<code>YAML</code>头部信息:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">---</span><br><span class=\"line\">title: hello</span><br><span class=\"line\">categories: 杂谈</span><br><span class=\"line\">tag: 杂七杂八</span><br><span class=\"line\">---</span><br></pre></td></tr></table></figure>\n<p>即可</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"安装-hexo\"><a href=\"#安装-hexo\" class=\"headerlink\" title=\"安装 hexo\"></a>安装 hexo</h2><ol>\n<li>安装<code>node</code></li>\n<li>安装<code>npm</code></li>\n<li>安装<code>hexo: npm install -g hexo-cli</code></li>\n</ol>\n<h2 id=\"建立一个新博客\"><a href=\"#建立一个新博客\" class=\"headerlink\" title=\"建立一个新博客\"></a>建立一个新博客</h2><ol>\n<li><code>hexo init blog</code>:使用该命令会在blog目录下建立一个博客并且在<code>source/_posts/hello-world.md</code>生成一篇名为<code>hello world</code>的文章，随后你可以选择删除它并新建自己的文章。</li>\n<li><code>cd blog</code></li>\n<li><code>hexo g</code>:使用该命令将 <code>source/_posts/hello-world.md</code> 渲染为<code>html、css、js</code>静态资源</li>\n<li><code>hexo s</code>:开启服务器。然后<a href=\"http://localhost:4000/\" target=\"_blank\" rel=\"noopener\">http://localhost:4000/</a></li>\n</ol>\n<h2 id=\"关联至github\"><a href=\"#关联至github\" class=\"headerlink\" title=\"关联至github\"></a>关联至github</h2><ol>\n<li><p>新建仓库<code>xxx.github.io</code>，这里 xxx 可以是你想要取的命令，但是必须以<code>github.io</code>结尾</p>\n</li>\n<li><p>此时可以访问<code>https://xxx.github.io</code>，但是没有内容</p>\n</li>\n<li><p>修改<code>blog</code>目录下配置文件<code>_config.yml</code>，找到<code>deploy</code>选项，修改(新增)为:</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">deploy:</span></span><br><span class=\"line\"><span class=\"attr\">  type:</span> <span class=\"string\">git</span></span><br><span class=\"line\"><span class=\"attr\">  repository:</span> <span class=\"string\">git@github.com:xxx/xxx.github.io.git</span> </span><br><span class=\"line\"><span class=\"attr\">  branch:</span> <span class=\"string\">master</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>安装插件<code>npm install hexo-deployer-git --save</code></p>\n</li>\n<li><p><code>hexo d -g</code> 生成内容后部署</p>\n</li>\n<li><p>访问<code>https://xxx.github.io</code>，应该要延迟一段时间才能看到效果</p>\n</li>\n</ol>\n<h2 id=\"更换主题\"><a href=\"#更换主题\" class=\"headerlink\" title=\"更换主题\"></a>更换主题</h2><p>由于初始的主题不怎么好看，可以选择更换一下主题。官方主题地址为<a href=\"https://hexo.io/themes/\" target=\"_blank\" rel=\"noopener\">hexo-themes</a>。本教程中采用<a href=\"https://www.haomwei.com/technology/maupassant-hexo.html#%E6%94%AF%E6%8C%81%E8%AF%AD%E8%A8%80\" target=\"_blank\" rel=\"noopener\">maupassant-hexo</a>为主题</p>\n<ol>\n<li><p>由于大部分的主题都托管在<code>github</code>上，在<code>blog</code>目录下运行:</p>\n<p><code>git clone https://github.com/tufu9441/maupassant-hexo.git themes/maupassant</code> </p>\n<p><code>themes/xxx</code>是<code>hexo</code>存放<code>xxx</code>主题的目录</p>\n</li>\n<li><p><code>npm install hexo-renderer-pug --save</code></p>\n</li>\n<li><p><code>npm install hexo-renderer-sass --save --registry=https://registry.npm.taobao.org</code></p>\n</li>\n<li><p>修改<code>_config.yml</code>中主题为<code>theme: maupassant</code></p>\n</li>\n<li><p><code>hexo g</code>重新生成</p>\n</li>\n<li><p><code>hexo s</code>开启服务器</p>\n</li>\n</ol>\n<p>主题还有许多可用的配置，请参照上面给出的链接进行设置</p>\n<h2 id=\"目录、tag\"><a href=\"#目录、tag\" class=\"headerlink\" title=\"目录、tag\"></a>目录、tag</h2><p>需要<code>归档</code>和<code>tag</code>只需要在<code>markdown</code>上加上一些<code>YAML</code>头部信息:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">---</span><br><span class=\"line\">title: hello</span><br><span class=\"line\">categories: 杂谈</span><br><span class=\"line\">tag: 杂七杂八</span><br><span class=\"line\">---</span><br></pre></td></tr></table></figure>\n<p>即可</p>\n"},{"title":"常用 Linux 命令系列","author":"lishion","toc":true,"_content":"\n记录一下用到的 Linux 命令\n\n## sort\n\nsort 可以用于对文本进行排序，常用的参数有:\n\n> * -t : 指定分隔符，默认为空格\n> * -n : 按照数字规则排序\n> * -k : 按空格分隔后的排序列数\n\n例如:\n\n```bash\n# sort_test\n1:2:3\n3:2:1\n2:3:4\n5:6:11\nsort -nk -t : sort_test # 按数字排序\n3:2:1\n1:2:3\n2:3:4\n5:6:11\nsort -k -t : sort_test # 按首字母进行排序\n3:2:1\n5:6:11\n1:2:3\n2:3:4\n\n```\n\n","source":"_posts/常用-linux-命令系列.md","raw":"---\ntitle: 常用 Linux 命令系列\ncategories: 笔记\nauthor: lishion\ntoc: true\ntags: \n  - Linux\n  - 编程\n---\n\n记录一下用到的 Linux 命令\n\n## sort\n\nsort 可以用于对文本进行排序，常用的参数有:\n\n> * -t : 指定分隔符，默认为空格\n> * -n : 按照数字规则排序\n> * -k : 按空格分隔后的排序列数\n\n例如:\n\n```bash\n# sort_test\n1:2:3\n3:2:1\n2:3:4\n5:6:11\nsort -nk -t : sort_test # 按数字排序\n3:2:1\n1:2:3\n2:3:4\n5:6:11\nsort -k -t : sort_test # 按首字母进行排序\n3:2:1\n5:6:11\n1:2:3\n2:3:4\n\n```\n\n","slug":"常用-linux-命令系列","published":1,"date":"2018-06-14T09:28:16.771Z","updated":"2018-06-14T09:28:16.763Z","_id":"cjiebvrsg0006fangdvr1o7zr","comments":1,"layout":"post","photos":[],"link":"","content":"<p>记录一下用到的 Linux 命令</p>\n<h2 id=\"sort\"><a href=\"#sort\" class=\"headerlink\" title=\"sort\"></a>sort</h2><p>sort 可以用于对文本进行排序，常用的参数有:</p>\n<blockquote>\n<ul>\n<li>-t : 指定分隔符，默认为空格</li>\n<li>-n : 按照数字规则排序</li>\n<li>-k : 按空格分隔后的排序列数</li>\n</ul>\n</blockquote>\n<p>例如:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># sort_test</span></span><br><span class=\"line\">1:2:3</span><br><span class=\"line\">3:2:1</span><br><span class=\"line\">2:3:4</span><br><span class=\"line\">5:6:11</span><br><span class=\"line\">sort -nk -t : sort_test <span class=\"comment\"># 按数字排序</span></span><br><span class=\"line\">3:2:1</span><br><span class=\"line\">1:2:3</span><br><span class=\"line\">2:3:4</span><br><span class=\"line\">5:6:11</span><br><span class=\"line\">sort -k -t : sort_test <span class=\"comment\"># 按首字母进行排序</span></span><br><span class=\"line\">3:2:1</span><br><span class=\"line\">5:6:11</span><br><span class=\"line\">1:2:3</span><br><span class=\"line\">2:3:4</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<p>记录一下用到的 Linux 命令</p>\n<h2 id=\"sort\"><a href=\"#sort\" class=\"headerlink\" title=\"sort\"></a>sort</h2><p>sort 可以用于对文本进行排序，常用的参数有:</p>\n<blockquote>\n<ul>\n<li>-t : 指定分隔符，默认为空格</li>\n<li>-n : 按照数字规则排序</li>\n<li>-k : 按空格分隔后的排序列数</li>\n</ul>\n</blockquote>\n<p>例如:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># sort_test</span></span><br><span class=\"line\">1:2:3</span><br><span class=\"line\">3:2:1</span><br><span class=\"line\">2:3:4</span><br><span class=\"line\">5:6:11</span><br><span class=\"line\">sort -nk -t : sort_test <span class=\"comment\"># 按数字排序</span></span><br><span class=\"line\">3:2:1</span><br><span class=\"line\">1:2:3</span><br><span class=\"line\">2:3:4</span><br><span class=\"line\">5:6:11</span><br><span class=\"line\">sort -k -t : sort_test <span class=\"comment\"># 按首字母进行排序</span></span><br><span class=\"line\">3:2:1</span><br><span class=\"line\">5:6:11</span><br><span class=\"line\">1:2:3</span><br><span class=\"line\">2:3:4</span><br></pre></td></tr></table></figure>\n"},{"title":"spark源码阅读计划 - 第二部分 - shuffle write","date":"2018-06-08T05:46:45.000Z","author":"lishion","toc":true,"_content":"## 写在开始的话\n\nshuffle 作为 spark 最重要的一部分，也是很多初学者感到头痛的一部分。简单来说，shuffle 分为两个部分 : shuffle write 和 shuffle read。shuffle write 在 map 端进行，而shuffle read 在 reduce 端进行。本章就准备就 shuffle write 做一个详细的分析。\n\n## 为什么需要shuffle\n\n在上一章[Spark-源码阅读计划-第一部分-迭代计算](https://lishion.github.io/2018/06/06/Spark-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E8%AE%A1%E5%88%92-%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86-%E8%BF%AD%E4%BB%A3%E8%AE%A1%E7%AE%97/)提到过每一个分区的数据最后都会由一个task来处理，那么当task 执行的类似于 reduceByKey 这种算子的时候一定需要满足相同的 key 在同一个分区这个条件，否则就不能按照 key 进行reduce了。但是当我们从数据源获取数据并进行处理后，相同的 key 不一定在同一个分区。那么在一个 map 端将相同的key 使用某种方式聚集在同一个分区并写入临时文件的过程就称为 shuffle write；而在 reduce 从 map 拉去执行 task 所需要的数据就是 shuffle read；这两个步骤组成了shuffle。\n\n## 一个小例子\n\n本章同样以一个小例子作为本章讲解的中心，例如一个统计词频的代码:\n\n```scala\nsc.parallelize(List(\"hello\",\"world\",\"hello\",\"spark\")).map(x=>(x,1)).reduceByKey(_+_)\n```\n\n相信有一点基础的同学都知道 reduceByKey 这算子会产生 shuffle。但是大部分的同学都不知 shuffle 是什么时候产生的，shuffle 到底做了些什么，这也是这一章的重点。\n\n## shuffle task\n\n在上一章提到过当 job 提交后会生成 result task。而在需要 shuffle 则会生成 ShuffleMapTask。这里涉及到 job 提交以及 stage 生成的的知识，将会在下一章提到。这里我们直接开始看 ShuffleMapTask 中的 runtask :\n\n```scala\n override def runTask(context: TaskContext): MapStatus = {\n    // Deserialize the RDD using the broadcast variable.\n ...\n    var writer: ShuffleWriter[Any, Any] = null\n    try {\n      val manager = SparkEnv.get.shuffleManager\n      writer = manager.getWriter[Any, Any](dep.shuffleHandle, partitionId, context)\n       // 这里依然是rdd调用iterator方法的地方\n      writer.write(rdd.iterator(partition, context).asInstanceOf[Iterator[_ <: Product2[Any, Any]]])\n      writer.stop(success = true).get\n    } catch {\n      case e: Exception =>\n        try {\n          if (writer != null) {\n            writer.stop(success = false)\n          }\n        } catch {\n          case e: Exception =>\n            log.debug(\"Could not stop writer\", e)\n        }\n        throw e\n    }\n  }\n```\n\n可以看到首先获得一个 shuffleWriter。spark 中有许多不同类型的 writer ，默认使用的是 SortShuffleWriter。SortShuffleWriter 中的 write 方法实现了 shuffle write。源代码如下:\n\n```scala\n  override def write(records: Iterator[Product2[K, V]]): Unit = {\n      //如果需要 map 端聚\n    sorter = if (dep.mapSideCombine) {\n      new ExternalSorter[K, V, C](\n        context, dep.aggregator, Some(dep.partitioner), dep.keyOrdering, dep.serializer)\n    } else {\n      new ExternalSorter[K, V, V](\n        context, aggregator = None, Some(dep.partitioner), ordering = None, dep.serializer)\n    }\n    sorter.insertAll(records)\n    val output = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)\n    val tmp = Utils.tempFileWith(output)\n    try { \n      val blockId = ShuffleBlockId(dep.shuffleId, mapId, IndexShuffleBlockResolver.NOOP_REDUCE_ID)\n      val partitionLengths = sorter.writePartitionedFile(blockId, tmp)\n      shuffleBlockResolver.writeIndexFileAndCommit(dep.shuffleId, mapId, partitionLengths, tmp)\n      mapStatus = MapStatus(blockManager.shuffleServerId, partitionLengths)\n    } finally {\n      if (tmp.exists() && !tmp.delete()) {\n        logError(s\"Error while deleting temp file ${tmp.getAbsolutePath}\")\n      }\n    }\n  }\n```\n\n使用了 ExternalSorter 的 insertAll 方法插入了上一个 RDD 生成的数据的迭代器。\n\n#### insertAll\n\n```scala\ndef insertAll(records: Iterator[Product2[K, V]]): Unit = {\n    // TODO: stop combining if we find that the reduction factor isn't high\n    val shouldCombine = aggregator.isDefined\n    // 直接看需要 map 端聚合的情况\n    // 不需要聚合的情况类似\n    if (shouldCombine) {\n      // Combine values in-memory first using our AppendOnlyMap\n       \n      val mergeValue = aggregator.get.mergeValue \n      val createCombiner = aggregator.get.createCombiner\n      var kv: Product2[K, V] = null\n      // 如果 key 不存在则直接创建 Combiner ，否则使用 mergeValue 将 value 添加到创建的 Combiner中\n      val update = (hadValue: Boolean, oldValue: C) => {\n        if (hadValue) mergeValue(oldValue, kv._2) else createCombiner(kv._2)\n      }\n      while (records.hasNext) {\n        addElementsRead()\n        kv = records.next()\n        // 更新值\n        // 这里需要注意的是之前的数据是(k,v)类型，这里转换成了((part,k),v)\n        // 其中 part 是 key 对应的分区\n        map.changeValue((getPartition(kv._1), kv._1), update)\n        maybeSpillCollection(usingMap = true)\n      }\n    } else {\n      // Stick values into our buffer\n      while (records.hasNext) {\n        addElementsRead()\n        val kv = records.next()\n        buffer.insert(getPartition(kv._1), kv._1, kv._2.asInstanceOf[C])\n        maybeSpillCollection(usingMap = false)\n      }\n    }\n  }\n```\n\n在这里使用 getPartition() 进行了数据的**分区**，也就是将不同的 key 与其分区对应起来。默认使用的是 HashPartitioner ，有兴趣的同学可以去阅读源码。再继续看 changeValue 方法。在这里使用非常重要的数据结构来存放分区后的数据，也就是代码中的 map 类型为 PartitionedAppendOnlyMap。其中是使用hask表存放数据，并且存放的方式为表中的偶数索index引存放key, index + 1 存放对应的value。例如:\n\n```\nk1|v1|null|k2|v2|k3|v3|null|....\n```\n\n之所以里面由空值，是因为在插入的时候使用了二次探测法。来看一看 changeValue 方法:\n\nPartitionedAppendOnlyMap 中的  changeValue 方法继承自 \n\n```scala\n  def changeValue(key: K, updateFunc: (Boolean, V) => V): V = {\n    assert(!destroyed, destructionMessage)\n    val k = key.asInstanceOf[AnyRef]\n    if (k.eq(null)) {\n      if (!haveNullValue) {\n        incrementSize()\n      }\n      nullValue = updateFunc(haveNullValue, nullValue)\n      haveNullValue = true\n      return nullValue\n    }\n    var pos = rehash(k.hashCode) & mask\n    var i = 1\n    while (true) {\n      // 偶数位置为 key\n      val curKey = data(2 * pos)\n       // 如果该key不存在，就直接插入\n      if (curKey.eq(null)) {\n        val newValue = updateFunc(false, null.asInstanceOf[V])\n        data(2 * pos) = k\n        data(2 * pos + 1) = newValue.asInstanceOf[AnyRef]\n        incrementSize()\n        return newValue\n      } else if (k.eq(curKey) || k.equals(curKey)) {//如果 key 存在，则进行聚合\n        \n        val newValue = updateFunc(true, data(2 * pos + 1).asInstanceOf[V])\n        data(2 * pos + 1) = newValue.asInstanceOf[AnyRef]\n        return newValue\n      } else { // 否则进行下一次探测\n        val delta = i\n        pos = (pos + delta) & mask\n        i += 1\n      }\n    }\n    null.asInstanceOf[V] // Never reached but needed to keep compiler happy\n  }\n```\n\n再调用 insertAll 进行了数据的聚合|插入之后，调用了 maybeSpillCollection 方法。这个方法的作用是判断 map 占用了内存，如果内存达到一定的值，则将内存中的数据 spill 到临时文件中。\n\n```scala\nprivate def maybeSpillCollection(usingMap: Boolean): Unit = {\n    var estimatedSize = 0L\n    if (usingMap) {\n      estimatedSize = map.estimateSize()\n      if (maybeSpill(map, estimatedSize)) {\n        map = new PartitionedAppendOnlyMap[K, C]\n      }\n    } else {\n      estimatedSize = buffer.estimateSize()\n      if (maybeSpill(buffer, estimatedSize)) {\n        buffer = new PartitionedPairBuffer[K, C]\n      }\n    }\n\n    if (estimatedSize > _peakMemoryUsedBytes) {\n      _peakMemoryUsedBytes = estimatedSize\n    }\n  }\n```\n\nmaybeSpillCollection 继续调用了 maybeSpill 方法，这个方法继承自 Spillable 中\n\n```scala\nprotected def maybeSpill(collection: C, currentMemory: Long): Boolean = {\n    var shouldSpill = false\n    if (elementsRead % 32 == 0 && currentMemory >= myMemoryThreshold) {\n      // Claim up to double our current memory from the shuffle memory pool\n      val amountToRequest = 2 * currentMemory - myMemoryThreshold\n      val granted = acquireMemory(amountToRequest)\n      myMemoryThreshold += granted\n      // If we were granted too little memory to grow further (either tryToAcquire returned 0,\n      // or we already had more memory than myMemoryThreshold), spill the current collection\n      shouldSpill = currentMemory >= myMemoryThreshold\n    }\n    shouldSpill = shouldSpill || _elementsRead > numElementsForceSpillThreshold\n    // Actually spill\n    if (shouldSpill) {\n      _spillCount += 1\n      logSpillage(currentMemory)\n      spill(collection)\n      _elementsRead = 0\n      _memoryBytesSpilled += currentMemory\n      releaseMemory()\n    }\n    shouldSpill\n  }\n```\n\n该方法首先判断了是否需要 spill，如果需要则调用 spill() 方法将内存中的数据写入临时文件中。spill 方法如下:\n\n```scala\n override protected[this] def spill(collection: WritablePartitionedPairCollection[K, C]): Unit = {\n    val inMemoryIterator = collection.destructiveSortedWritablePartitionedIterator(comparator)\n    val spillFile = spillMemoryIteratorToDisk(inMemoryIterator)\n    spills += spillFile\n  }\n```\n\n其中的  comparator 如下:\n\n```scala\n private def comparator: Option[Comparator[K]] = {\n    if (ordering.isDefined || aggregator.isDefined) {\n      Some(keyComparator)\n    } else {\n      None\n    }\n  }\n```\n\n也就是:\n\n```scala\n  private val keyComparator: Comparator[K] = ordering.getOrElse(new Comparator[K] {\n    override def compare(a: K, b: K): Int = {\n      val h1 = if (a == null) 0 else a.hashCode()\n      val h2 = if (b == null) 0 else b.hashCode()\n      if (h1 < h2) -1 else if (h1 == h2) 0 else 1\n    }\n  })\n```\n\ndestructiveSortedWritablePartitionedIterator 位于 WritablePartitionedPairCollection 类中，实现如下:\n\n```scala\ndef destructiveSortedWritablePartitionedIterator(keyComparator: Option[Comparator[K]])\n    : WritablePartitionedIterator = {\n    //这里调用的其实是　PartitionedAppendOnlyMap　中重写的　partitionedDestructiveSortedIterator    \n    val it = partitionedDestructiveSortedIterator(keyComparator)\n    // 这里还实现了一个　writeNext　的方法，后面会用到\n    new WritablePartitionedIterator {\n      private[this] var cur = if (it.hasNext) it.next() else null\n       // 在map 中的数据其实是((partition,k),v)\n       // 这里只写入了(k,v)\n      def writeNext(writer: DiskBlockObjectWriter): Unit = {\n        writer.write(cur._1._2, cur._2)\n        cur = if (it.hasNext) it.next() else null\n      }\n      def hasNext(): Boolean = cur != null\n      def nextPartition(): Int = cur._1._1\n    }\n  }\n```\n\n接下来看看partitionedDestructiveSortedIterator:\n\n```scala\ndef partitionedDestructiveSortedIterator(keyComparator: Option[Comparator[K]])\n    : Iterator[((Int, K), V)] = {\n    val comparator = keyComparator.map(partitionKeyComparator).getOrElse(partitionComparator)\n    destructiveSortedIterator(comparator)\n }\n```\n\nkeyComparator.map(partitionKeyComparator)　等于 partitionKeyComparator(keyComparator)，再看下面的partitionKeyComparator，说明比较器是先对分区进行比较，如果分区相同，再对key的hash值进行比较:\n\n```scala\n def partitionKeyComparator[K](keyComparator: Comparator[K]): Comparator[(Int, K)] = {\n    new Comparator[(Int, K)] {\n      override def compare(a: (Int, K), b: (Int, K)): Int = {\n        val partitionDiff = a._1 - b._1\n        if (partitionDiff != 0) {\n          partitionDiff\n        } else {\n          keyComparator.compare(a._2, b._2)\n        }\n      }\n    }\n  }\n```\n\n然后看destructiveSortedIterator(),这是在 PartitionedAppendOnlyMap 的父类 destructiveSortedIterator 中实现的:\n\n```scala\n def destructiveSortedIterator(keyComparator: Comparator[K]): Iterator[(K, V)] = {\n    destroyed = true\n    // Pack KV pairs into the front of the underlying array\n    // 这里是因为　PartitionedAppendOnlyMap　采用的二次探测法　,kv对之间存在　null值，所以先把非空的kv对全部移动到hash表的前端\n    var keyIndex, newIndex = 0\n    while (keyIndex < capacity) {\n      if (data(2 * keyIndex) != null) {\n        data(2 * newIndex) = data(2 * keyIndex)\n        data(2 * newIndex + 1) = data(2 * keyIndex + 1)\n        newIndex += 1\n      }\n      keyIndex += 1\n    }\n    assert(curSize == newIndex + (if (haveNullValue) 1 else 0))\n    // 这里对给定数据范围为 0 to newIndex，利用　keyComparator　比较器进行排序\n    // 也就是对 map 中的数据根据　(partition,key) 进行排序\n    new Sorter(new KVArraySortDataFormat[K, AnyRef]).sort(data, 0, newIndex, keyComparator)\n\n    // 定义迭代器\n    new Iterator[(K, V)] {\n      var i = 0\n      var nullValueReady = haveNullValue\n      def hasNext: Boolean = (i < newIndex || nullValueReady)\n      def next(): (K, V) = {\n        if (nullValueReady) {\n          nullValueReady = false\n          (null.asInstanceOf[K], nullValue)\n        } else {\n          val item = (data(2 * i).asInstanceOf[K], data(2 * i + 1).asInstanceOf[V])\n          i += 1\n          item\n        }\n      }\n    }\n  }\n```\n\n可以看出整个　`val inMemoryIterator = collection.destructiveSortedWritablePartitionedIterator(comparator)`　这一步其实就是在对　map　或 buffer 进行排序，并且实现了一个 writeNext() 方法供后续调调用。随后调用`val spillFile = spillMemoryIteratorToDisk(inMemoryIterator)`将排序后的缓存写入文件中:\n\n```scala\nprivate[this] def spillMemoryIteratorToDisk(inMemoryIterator: WritablePartitionedIterator)\n      : SpilledFile = {\n   \n    val (blockId, file) = diskBlockManager.createTempShuffleBlock()\n    // These variables are reset after each flush\n    var objectsWritten: Long = 0\n    val spillMetrics: ShuffleWriteMetrics = new ShuffleWriteMetrics\n    val writer: DiskBlockObjectWriter =\n      blockManager.getDiskWriter(blockId, file, serInstance, fileBufferSize, spillMetrics)\n    // List of batch sizes (bytes) in the order they are written to disk\n    val batchSizes = new ArrayBuffer[Long]\n    // How many elements we have in each partition\n    // 用于记录每一个分区有多少条数据\n    // 由于刚才数据写入文件没有写入key对应的分区，因此需要记录每一个分区有多少条数据，然后根据数据的偏移量就可以判断该数据属于哪个分区　\n    val elementsPerPartition = new Array[Long](numPartitions)\n    // Flush the disk writer's contents to disk, and update relevant variables.\n    // The writer is committed at the end of this process.\n    def flush(): Unit = {\n      val segment = writer.commitAndGet()\n      batchSizes += segment.length\n      _diskBytesSpilled += segment.length\n      objectsWritten = 0\n    }\n    var success = false\n    try {\n      while (inMemoryIterator.hasNext) {\n        val partitionId = inMemoryIterator.nextPartition()\n        require(partitionId >= 0 && partitionId < numPartitions,\n          s\"partition Id: ${partitionId} should be in the range [0, ${numPartitions})\")\n        inMemoryIterator.writeNext(writer)\n        elementsPerPartition(partitionId) += 1\n        objectsWritten += 1\n        if (objectsWritten == serializerBatchSize) {　//写入　serializerBatchSize　条数据便刷新一次缓存\n          // batchSize 在类中定义的如下:\n          // 可以看出如果不存在配置默认为　10000　条\n          // private val serializerBatchSize = conf.getLong(\"spark.shuffle.spill.batchSize\", 10000)\n          flush()\n        }\n      }\n      if (objectsWritten > 0) {\n        flush()\n      } else {\n        writer.revertPartialWritesAndClose()\n      }\n      success = true\n    } finally {\n      if (success) {\n        writer.close()\n      } else {\n        // This code path only happens if an exception was thrown above before we set success;\n        // close our stuff and let the exception be thrown further\n        writer.revertPartialWritesAndClose()\n        if (file.exists()) {\n          if (!file.delete()) {\n            logWarning(s\"Error deleting ${file}\")\n          }\n        }\n      }\n    }\n    //最后记录临时文件的信息\n    SpilledFile(file, blockId, batchSizes.toArray, elementsPerPartition)\n\n  }\n```\n\n这就是整个　insertAll()　方法:\n\n merge 达到溢出值后进行排序并写入临时文件，这里要注意的是并非所有的缓存文件都被写入到临时文件中，所以接下来需要将临时文件中的数据加上内存中的数据进行一次排序写入到一个大文件中。\n\n```scala\n//首先获取需要写入的文件:　\n   val output = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)\n　　val tmp = Utils.tempFileWith(output)\n    try { \n      val blockId = ShuffleBlockId(dep.shuffleId, mapId, IndexShuffleBlockResolver.NOOP_REDUCE_ID)\n      val partitionLengths = sorter.writePartitionedFile(blockId, tmp)//这一句是重点 下面会讲解\n      shuffleBlockResolver.writeIndexFileAndCommit(dep.shuffleId, mapId, partitionLengths, tmp)\n      mapStatus = MapStatus(blockManager.shuffleServerId, partitionLengths)\n    } finally {\n      if (tmp.exists() && !tmp.delete()) {\n        logError(s\"Error while deleting temp file ${tmp.getAbsolutePath}\")\n    }\n```\n####　writePartitionedFile\n\n继续看 writePartitionedFile :\n\n```scala\n//  ExternalSorter 中的　writePartitionedFile　方法\n   def writePartitionedFile(\n      blockId: BlockId,\n      outputFile: File): Array[Long] = {\n\n    // Track location of each range in the output file\n    val lengths = new Array[Long](numPartitions)\n    val writer = blockManager.getDiskWriter(blockId, outputFile, serInstance, fileBufferSize,\n      context.taskMetrics().shuffleWriteMetrics)\n\n    // 这里的spills 就是　刚才产生的临时文件的数据　如果为空逻辑就比较简单，直接进行排序并写入文件\n    if (spills.isEmpty) {\n      // Case where we only have in-memory data\n      val collection = if (aggregator.isDefined) map else buffer\n      val it = collection.destructiveSortedWritablePartitionedIterator(comparator)　//这里同样使用　destructiveSortedWritablePartitionedIterator　进行排序\n      while (it.hasNext) {\n        val partitionId = it.nextPartition()\n        while (it.hasNext && it.nextPartition() == partitionId) {\n          it.writeNext(writer)\n        }\n        val segment = writer.commitAndGet()\n        lengths(partitionId) = segment.length\n      }\n    } else {//重点看不为空的时候，这里调用了　partitionedIterator　方法\n      // We must perform merge-sort; get an iterator by partition and write everything directly.\n      for ((id, elements) <- this.partitionedIterator) {\n        if (elements.hasNext) {\n          for (elem <- elements) {\n            writer.write(elem._1, elem._2)\n          }\n          val segment = writer.commitAndGet()\n          lengths(id) = segment.length\n        }\n      }\n    }\n\n    writer.close()\n    context.taskMetrics().incMemoryBytesSpilled(memoryBytesSpilled)\n    context.taskMetrics().incDiskBytesSpilled(diskBytesSpilled)\n    context.taskMetrics().incPeakExecutionMemory(peakMemoryUsedBytes)\n\n    lengths\n  }\n```\n\n然后是 partitionedIterator 方法:\n\n```scala\ndef partitionedIterator: Iterator[(Int, Iterator[Product2[K, C]])] = {\n    val usingMap = aggregator.isDefined\n    val collection: WritablePartitionedPairCollection[K, C] = if (usingMap) map else buffer\n    if (spills.isEmpty) {// 这里又一次判断了是否为空，直接看有临时文件的部分\n      // Special case: if we have only in-memory data, we don't need to merge streams, and perhaps\n      // we don't even need to sort by anything other than partition ID\n      if (!ordering.isDefined) {\n        // The user hasn't requested sorted keys, so only sort by partition ID, not key\n   groupByPartition(destructiveIterator(collection.partitionedDestructiveSortedIterator(None)))\n      } else {\n        // We do need to sort by both partition ID and key\n        groupByPartition(destructiveIterator(\n          collection.partitionedDestructiveSortedIterator(Some(keyComparator))))\n      }\n    } else {\n      // Merge spilled and in-memory data\n      // 这里传入了临时文件　spills　和　排序后的缓存文件\n      merge(spills, destructiveIterator(\n        collection.partitionedDestructiveSortedIterator(comparator)))\n    }\n  }\n```\n\nmerge 方法:\n\n```scala\n // merge方法\n   // inMemory　是根据(partion,hash(k)) 排序后的内存数据\n   private def merge(spills: Seq[SpilledFile], inMemory: Iterator[((Int, K), C)])\n      : Iterator[(Int, Iterator[Product2[K, C]])] = {\n    //　将所有缓存文件转化为 SpillReader \n    val readers = spills.map(new SpillReader(_))\n    // buffered方法只是比普通的　itertor 方法多提供一个　head　方法，例如:\n    // val a = List(1,2,3,4,5)\n    // val b = a.iterator.buffered\n    // b.head : 1\n    // b.next : 1\n    // b.head : 2\n    // b.next : 2\n    val inMemBuffered = inMemory.buffered\n    (0 until numPartitions).iterator.map { p =>\n      // 获得分区对应数据的迭代器 \n      // 这里的　IteratorForPartition　就是得到分区 p 对应的数据的迭代器，逻辑比较简单，就不单独拿出来分析\n      val inMemIterator = new IteratorForPartition(p, inMemBuffered)\n      \n      //这里获取所有文件的第　p　个分区　并与内存中的第　p 个分区进行合并\n      val iterators = readers.map(_.readNextPartition()) ++ Seq(inMemIterator)\n　　　// 这里的　iterators　是由　多个 iterator 组成的，其中每一个都是关于 p 分区数据的 iterator\n      if (aggregator.isDefined) {\n        // Perform partial aggregation across partitions\n        (p, mergeWithAggregation(\n          iterators, aggregator.get.mergeCombiners, keyComparator, ordering.isDefined))\n      } else if (ordering.isDefined) {\n        // No aggregator given, but we have an ordering (e.g. used by reduce tasks in sortByKey);\n        // sort the elements without trying to merge them\n        (p, mergeSort(iterators, ordering.get))\n      } else {\n        (p, iterators.iterator.flatten)\n      }\n    }\n```\nmergeWithAggregation\n```scala\n\n      private def mergeWithAggregation(\n      iterators: Seq[Iterator[Product2[K, C]]],\n      mergeCombiners: (C, C) => C,\n      comparator: Comparator[K],\n      totalOrder: Boolean)\n      : Iterator[Product2[K, C]] =\n  {\n    if (!totalOrder) {\n      new Iterator[Iterator[Product2[K, C]]] {\n        val sorted = mergeSort(iterators, comparator).buffered\n        val keys = new ArrayBuffer[K]\n        val combiners = new ArrayBuffer[C]\n        override def hasNext: Boolean = sorted.hasNext\n        override def next(): Iterator[Product2[K, C]] = {\n          if (!hasNext) {\n            throw new NoSuchElementException\n          }\n          keys.clear()\n          combiners.clear()\n          val firstPair = sorted.next()\n          keys += firstPair._1\n          combiners += firstPair._2\n          val key = firstPair._1\n          while (sorted.hasNext && comparator.compare(sorted.head._1, key) == 0) {\n            val pair = sorted.next()\n            var i = 0\n            var foundKey = false\n            while (i < keys.size && !foundKey) {\n              if (keys(i) == pair._1) {\n                combiners(i) = mergeCombiners(combiners(i), pair._2)\n                foundKey = true\n              }\n              i += 1\n            }\n            if (!foundKey) {\n              keys += pair._1\n              combiners += pair._2\n            }\n          }\n          keys.iterator.zip(combiners.iterator)\n        }\n      }.flatMap(i => i)\n    } else {\n      // We have a total ordering, so the objects with the same key are sequential.\n      new Iterator[Product2[K, C]] {\n        val sorted = mergeSort(iterators, comparator).buffered\n        override def hasNext: Boolean = sorted.hasNext\n        override def next(): Product2[K, C] = {\n          if (!hasNext) {\n            throw new NoSuchElementException\n          }\n          val elem = sorted.next()\n          val k = elem._1\n          var c = elem._2\n          // 这里的意思是可能存在多个相同的key 分布在不同临时文件或内存中\n          // 所以还需要将不同的 key 对应的值进行合并 \n          while (sorted.hasNext && sorted.head._1 == k) {\n            val pair = sorted.next()\n            c = mergeCombiners(c, pair._2)\n          }\n          (k, c)//返回\n        }\n      }\n    }\n  }\n```\n\n继续看 mergeSort 方法，首先选除头元素最小对应那个分区，然后选出这个分区的第一个元素，那么这个元素一定是最小的，然后将剩余的元素放回去:\n\n```scala\n\n  private def mergeSort(iterators: Seq[Iterator[Product2[K, C]]], comparator: Comparator[K])\n      : Iterator[Product2[K, C]] =\n  {\n    val bufferedIters = iterators.filter(_.hasNext).map(_.buffered)\n    type Iter = BufferedIterator[Product2[K, C]]\n    //选取头元素最小的分区\n    val heap = new mutable.PriorityQueue[Iter]()(new Ordering[Iter] {\n      // Use the reverse of comparator.compare because PriorityQueue dequeues the max\n      override def compare(x: Iter, y: Iter): Int = -comparator.compare(x.head._1, y.head._1)\n    })\n    heap.enqueue(bufferedIters: _*) \n    new Iterator[Product2[K, C]] {\n      override def hasNext: Boolean = !heap.isEmpty\n      override def next(): Product2[K, C] = {\n        if (!hasNext) {\n          throw new NoSuchElementException\n        }\n        val firstBuf = heap.dequeue()\n        val firstPair = firstBuf.next()\n        if (firstBuf.hasNext) {\n          heap.enqueue(firstBuf)\n        }\n        firstPair\n      }\n    }\n  }\n```\n\n接下来就使用 writeIndexFileAndCommit 将每一个分区的数据条数写入索引文件，便于 reduce 端获取。\n\n## 总结\n\nshuflle write 相比起 shuffe read 要复杂很多，因此本文的篇幅也较长。其实在 shuffle write 有两个较为重要的方法，其中一个是 insertAll。这个方法的作用就是将数据缓存到一个可以添加的 hash 表中。如果表占用的内存达到的一定值，就对其进行排序。排序方式先按照分区进行排序，如果分区相同则按key的hash进行排序，随后将排序后的数据写入临时文件中，这个过程可能会生成多个临时文件。\n\n然后 writePartitionedFile 的作用是最后将内存中的数据和临时文件中的数据进行全部排序，写入到一个大文件中。\n\n最后将每一个分区对应的索引数据写入文件。整个shuffle write 阶段就完成了\n","source":"_posts/源码阅读计划-第二部分-shuffle-write.md","raw":"---\ntitle: spark源码阅读计划 - 第二部分 - shuffle write\ndate: 2018-06-08 13:46:45\ntags:\n  - Spark\n  - 编程\ncategories: Spark源码阅读计划\nauthor: lishion\ntoc: true\n---\n## 写在开始的话\n\nshuffle 作为 spark 最重要的一部分，也是很多初学者感到头痛的一部分。简单来说，shuffle 分为两个部分 : shuffle write 和 shuffle read。shuffle write 在 map 端进行，而shuffle read 在 reduce 端进行。本章就准备就 shuffle write 做一个详细的分析。\n\n## 为什么需要shuffle\n\n在上一章[Spark-源码阅读计划-第一部分-迭代计算](https://lishion.github.io/2018/06/06/Spark-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E8%AE%A1%E5%88%92-%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86-%E8%BF%AD%E4%BB%A3%E8%AE%A1%E7%AE%97/)提到过每一个分区的数据最后都会由一个task来处理，那么当task 执行的类似于 reduceByKey 这种算子的时候一定需要满足相同的 key 在同一个分区这个条件，否则就不能按照 key 进行reduce了。但是当我们从数据源获取数据并进行处理后，相同的 key 不一定在同一个分区。那么在一个 map 端将相同的key 使用某种方式聚集在同一个分区并写入临时文件的过程就称为 shuffle write；而在 reduce 从 map 拉去执行 task 所需要的数据就是 shuffle read；这两个步骤组成了shuffle。\n\n## 一个小例子\n\n本章同样以一个小例子作为本章讲解的中心，例如一个统计词频的代码:\n\n```scala\nsc.parallelize(List(\"hello\",\"world\",\"hello\",\"spark\")).map(x=>(x,1)).reduceByKey(_+_)\n```\n\n相信有一点基础的同学都知道 reduceByKey 这算子会产生 shuffle。但是大部分的同学都不知 shuffle 是什么时候产生的，shuffle 到底做了些什么，这也是这一章的重点。\n\n## shuffle task\n\n在上一章提到过当 job 提交后会生成 result task。而在需要 shuffle 则会生成 ShuffleMapTask。这里涉及到 job 提交以及 stage 生成的的知识，将会在下一章提到。这里我们直接开始看 ShuffleMapTask 中的 runtask :\n\n```scala\n override def runTask(context: TaskContext): MapStatus = {\n    // Deserialize the RDD using the broadcast variable.\n ...\n    var writer: ShuffleWriter[Any, Any] = null\n    try {\n      val manager = SparkEnv.get.shuffleManager\n      writer = manager.getWriter[Any, Any](dep.shuffleHandle, partitionId, context)\n       // 这里依然是rdd调用iterator方法的地方\n      writer.write(rdd.iterator(partition, context).asInstanceOf[Iterator[_ <: Product2[Any, Any]]])\n      writer.stop(success = true).get\n    } catch {\n      case e: Exception =>\n        try {\n          if (writer != null) {\n            writer.stop(success = false)\n          }\n        } catch {\n          case e: Exception =>\n            log.debug(\"Could not stop writer\", e)\n        }\n        throw e\n    }\n  }\n```\n\n可以看到首先获得一个 shuffleWriter。spark 中有许多不同类型的 writer ，默认使用的是 SortShuffleWriter。SortShuffleWriter 中的 write 方法实现了 shuffle write。源代码如下:\n\n```scala\n  override def write(records: Iterator[Product2[K, V]]): Unit = {\n      //如果需要 map 端聚\n    sorter = if (dep.mapSideCombine) {\n      new ExternalSorter[K, V, C](\n        context, dep.aggregator, Some(dep.partitioner), dep.keyOrdering, dep.serializer)\n    } else {\n      new ExternalSorter[K, V, V](\n        context, aggregator = None, Some(dep.partitioner), ordering = None, dep.serializer)\n    }\n    sorter.insertAll(records)\n    val output = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)\n    val tmp = Utils.tempFileWith(output)\n    try { \n      val blockId = ShuffleBlockId(dep.shuffleId, mapId, IndexShuffleBlockResolver.NOOP_REDUCE_ID)\n      val partitionLengths = sorter.writePartitionedFile(blockId, tmp)\n      shuffleBlockResolver.writeIndexFileAndCommit(dep.shuffleId, mapId, partitionLengths, tmp)\n      mapStatus = MapStatus(blockManager.shuffleServerId, partitionLengths)\n    } finally {\n      if (tmp.exists() && !tmp.delete()) {\n        logError(s\"Error while deleting temp file ${tmp.getAbsolutePath}\")\n      }\n    }\n  }\n```\n\n使用了 ExternalSorter 的 insertAll 方法插入了上一个 RDD 生成的数据的迭代器。\n\n#### insertAll\n\n```scala\ndef insertAll(records: Iterator[Product2[K, V]]): Unit = {\n    // TODO: stop combining if we find that the reduction factor isn't high\n    val shouldCombine = aggregator.isDefined\n    // 直接看需要 map 端聚合的情况\n    // 不需要聚合的情况类似\n    if (shouldCombine) {\n      // Combine values in-memory first using our AppendOnlyMap\n       \n      val mergeValue = aggregator.get.mergeValue \n      val createCombiner = aggregator.get.createCombiner\n      var kv: Product2[K, V] = null\n      // 如果 key 不存在则直接创建 Combiner ，否则使用 mergeValue 将 value 添加到创建的 Combiner中\n      val update = (hadValue: Boolean, oldValue: C) => {\n        if (hadValue) mergeValue(oldValue, kv._2) else createCombiner(kv._2)\n      }\n      while (records.hasNext) {\n        addElementsRead()\n        kv = records.next()\n        // 更新值\n        // 这里需要注意的是之前的数据是(k,v)类型，这里转换成了((part,k),v)\n        // 其中 part 是 key 对应的分区\n        map.changeValue((getPartition(kv._1), kv._1), update)\n        maybeSpillCollection(usingMap = true)\n      }\n    } else {\n      // Stick values into our buffer\n      while (records.hasNext) {\n        addElementsRead()\n        val kv = records.next()\n        buffer.insert(getPartition(kv._1), kv._1, kv._2.asInstanceOf[C])\n        maybeSpillCollection(usingMap = false)\n      }\n    }\n  }\n```\n\n在这里使用 getPartition() 进行了数据的**分区**，也就是将不同的 key 与其分区对应起来。默认使用的是 HashPartitioner ，有兴趣的同学可以去阅读源码。再继续看 changeValue 方法。在这里使用非常重要的数据结构来存放分区后的数据，也就是代码中的 map 类型为 PartitionedAppendOnlyMap。其中是使用hask表存放数据，并且存放的方式为表中的偶数索index引存放key, index + 1 存放对应的value。例如:\n\n```\nk1|v1|null|k2|v2|k3|v3|null|....\n```\n\n之所以里面由空值，是因为在插入的时候使用了二次探测法。来看一看 changeValue 方法:\n\nPartitionedAppendOnlyMap 中的  changeValue 方法继承自 \n\n```scala\n  def changeValue(key: K, updateFunc: (Boolean, V) => V): V = {\n    assert(!destroyed, destructionMessage)\n    val k = key.asInstanceOf[AnyRef]\n    if (k.eq(null)) {\n      if (!haveNullValue) {\n        incrementSize()\n      }\n      nullValue = updateFunc(haveNullValue, nullValue)\n      haveNullValue = true\n      return nullValue\n    }\n    var pos = rehash(k.hashCode) & mask\n    var i = 1\n    while (true) {\n      // 偶数位置为 key\n      val curKey = data(2 * pos)\n       // 如果该key不存在，就直接插入\n      if (curKey.eq(null)) {\n        val newValue = updateFunc(false, null.asInstanceOf[V])\n        data(2 * pos) = k\n        data(2 * pos + 1) = newValue.asInstanceOf[AnyRef]\n        incrementSize()\n        return newValue\n      } else if (k.eq(curKey) || k.equals(curKey)) {//如果 key 存在，则进行聚合\n        \n        val newValue = updateFunc(true, data(2 * pos + 1).asInstanceOf[V])\n        data(2 * pos + 1) = newValue.asInstanceOf[AnyRef]\n        return newValue\n      } else { // 否则进行下一次探测\n        val delta = i\n        pos = (pos + delta) & mask\n        i += 1\n      }\n    }\n    null.asInstanceOf[V] // Never reached but needed to keep compiler happy\n  }\n```\n\n再调用 insertAll 进行了数据的聚合|插入之后，调用了 maybeSpillCollection 方法。这个方法的作用是判断 map 占用了内存，如果内存达到一定的值，则将内存中的数据 spill 到临时文件中。\n\n```scala\nprivate def maybeSpillCollection(usingMap: Boolean): Unit = {\n    var estimatedSize = 0L\n    if (usingMap) {\n      estimatedSize = map.estimateSize()\n      if (maybeSpill(map, estimatedSize)) {\n        map = new PartitionedAppendOnlyMap[K, C]\n      }\n    } else {\n      estimatedSize = buffer.estimateSize()\n      if (maybeSpill(buffer, estimatedSize)) {\n        buffer = new PartitionedPairBuffer[K, C]\n      }\n    }\n\n    if (estimatedSize > _peakMemoryUsedBytes) {\n      _peakMemoryUsedBytes = estimatedSize\n    }\n  }\n```\n\nmaybeSpillCollection 继续调用了 maybeSpill 方法，这个方法继承自 Spillable 中\n\n```scala\nprotected def maybeSpill(collection: C, currentMemory: Long): Boolean = {\n    var shouldSpill = false\n    if (elementsRead % 32 == 0 && currentMemory >= myMemoryThreshold) {\n      // Claim up to double our current memory from the shuffle memory pool\n      val amountToRequest = 2 * currentMemory - myMemoryThreshold\n      val granted = acquireMemory(amountToRequest)\n      myMemoryThreshold += granted\n      // If we were granted too little memory to grow further (either tryToAcquire returned 0,\n      // or we already had more memory than myMemoryThreshold), spill the current collection\n      shouldSpill = currentMemory >= myMemoryThreshold\n    }\n    shouldSpill = shouldSpill || _elementsRead > numElementsForceSpillThreshold\n    // Actually spill\n    if (shouldSpill) {\n      _spillCount += 1\n      logSpillage(currentMemory)\n      spill(collection)\n      _elementsRead = 0\n      _memoryBytesSpilled += currentMemory\n      releaseMemory()\n    }\n    shouldSpill\n  }\n```\n\n该方法首先判断了是否需要 spill，如果需要则调用 spill() 方法将内存中的数据写入临时文件中。spill 方法如下:\n\n```scala\n override protected[this] def spill(collection: WritablePartitionedPairCollection[K, C]): Unit = {\n    val inMemoryIterator = collection.destructiveSortedWritablePartitionedIterator(comparator)\n    val spillFile = spillMemoryIteratorToDisk(inMemoryIterator)\n    spills += spillFile\n  }\n```\n\n其中的  comparator 如下:\n\n```scala\n private def comparator: Option[Comparator[K]] = {\n    if (ordering.isDefined || aggregator.isDefined) {\n      Some(keyComparator)\n    } else {\n      None\n    }\n  }\n```\n\n也就是:\n\n```scala\n  private val keyComparator: Comparator[K] = ordering.getOrElse(new Comparator[K] {\n    override def compare(a: K, b: K): Int = {\n      val h1 = if (a == null) 0 else a.hashCode()\n      val h2 = if (b == null) 0 else b.hashCode()\n      if (h1 < h2) -1 else if (h1 == h2) 0 else 1\n    }\n  })\n```\n\ndestructiveSortedWritablePartitionedIterator 位于 WritablePartitionedPairCollection 类中，实现如下:\n\n```scala\ndef destructiveSortedWritablePartitionedIterator(keyComparator: Option[Comparator[K]])\n    : WritablePartitionedIterator = {\n    //这里调用的其实是　PartitionedAppendOnlyMap　中重写的　partitionedDestructiveSortedIterator    \n    val it = partitionedDestructiveSortedIterator(keyComparator)\n    // 这里还实现了一个　writeNext　的方法，后面会用到\n    new WritablePartitionedIterator {\n      private[this] var cur = if (it.hasNext) it.next() else null\n       // 在map 中的数据其实是((partition,k),v)\n       // 这里只写入了(k,v)\n      def writeNext(writer: DiskBlockObjectWriter): Unit = {\n        writer.write(cur._1._2, cur._2)\n        cur = if (it.hasNext) it.next() else null\n      }\n      def hasNext(): Boolean = cur != null\n      def nextPartition(): Int = cur._1._1\n    }\n  }\n```\n\n接下来看看partitionedDestructiveSortedIterator:\n\n```scala\ndef partitionedDestructiveSortedIterator(keyComparator: Option[Comparator[K]])\n    : Iterator[((Int, K), V)] = {\n    val comparator = keyComparator.map(partitionKeyComparator).getOrElse(partitionComparator)\n    destructiveSortedIterator(comparator)\n }\n```\n\nkeyComparator.map(partitionKeyComparator)　等于 partitionKeyComparator(keyComparator)，再看下面的partitionKeyComparator，说明比较器是先对分区进行比较，如果分区相同，再对key的hash值进行比较:\n\n```scala\n def partitionKeyComparator[K](keyComparator: Comparator[K]): Comparator[(Int, K)] = {\n    new Comparator[(Int, K)] {\n      override def compare(a: (Int, K), b: (Int, K)): Int = {\n        val partitionDiff = a._1 - b._1\n        if (partitionDiff != 0) {\n          partitionDiff\n        } else {\n          keyComparator.compare(a._2, b._2)\n        }\n      }\n    }\n  }\n```\n\n然后看destructiveSortedIterator(),这是在 PartitionedAppendOnlyMap 的父类 destructiveSortedIterator 中实现的:\n\n```scala\n def destructiveSortedIterator(keyComparator: Comparator[K]): Iterator[(K, V)] = {\n    destroyed = true\n    // Pack KV pairs into the front of the underlying array\n    // 这里是因为　PartitionedAppendOnlyMap　采用的二次探测法　,kv对之间存在　null值，所以先把非空的kv对全部移动到hash表的前端\n    var keyIndex, newIndex = 0\n    while (keyIndex < capacity) {\n      if (data(2 * keyIndex) != null) {\n        data(2 * newIndex) = data(2 * keyIndex)\n        data(2 * newIndex + 1) = data(2 * keyIndex + 1)\n        newIndex += 1\n      }\n      keyIndex += 1\n    }\n    assert(curSize == newIndex + (if (haveNullValue) 1 else 0))\n    // 这里对给定数据范围为 0 to newIndex，利用　keyComparator　比较器进行排序\n    // 也就是对 map 中的数据根据　(partition,key) 进行排序\n    new Sorter(new KVArraySortDataFormat[K, AnyRef]).sort(data, 0, newIndex, keyComparator)\n\n    // 定义迭代器\n    new Iterator[(K, V)] {\n      var i = 0\n      var nullValueReady = haveNullValue\n      def hasNext: Boolean = (i < newIndex || nullValueReady)\n      def next(): (K, V) = {\n        if (nullValueReady) {\n          nullValueReady = false\n          (null.asInstanceOf[K], nullValue)\n        } else {\n          val item = (data(2 * i).asInstanceOf[K], data(2 * i + 1).asInstanceOf[V])\n          i += 1\n          item\n        }\n      }\n    }\n  }\n```\n\n可以看出整个　`val inMemoryIterator = collection.destructiveSortedWritablePartitionedIterator(comparator)`　这一步其实就是在对　map　或 buffer 进行排序，并且实现了一个 writeNext() 方法供后续调调用。随后调用`val spillFile = spillMemoryIteratorToDisk(inMemoryIterator)`将排序后的缓存写入文件中:\n\n```scala\nprivate[this] def spillMemoryIteratorToDisk(inMemoryIterator: WritablePartitionedIterator)\n      : SpilledFile = {\n   \n    val (blockId, file) = diskBlockManager.createTempShuffleBlock()\n    // These variables are reset after each flush\n    var objectsWritten: Long = 0\n    val spillMetrics: ShuffleWriteMetrics = new ShuffleWriteMetrics\n    val writer: DiskBlockObjectWriter =\n      blockManager.getDiskWriter(blockId, file, serInstance, fileBufferSize, spillMetrics)\n    // List of batch sizes (bytes) in the order they are written to disk\n    val batchSizes = new ArrayBuffer[Long]\n    // How many elements we have in each partition\n    // 用于记录每一个分区有多少条数据\n    // 由于刚才数据写入文件没有写入key对应的分区，因此需要记录每一个分区有多少条数据，然后根据数据的偏移量就可以判断该数据属于哪个分区　\n    val elementsPerPartition = new Array[Long](numPartitions)\n    // Flush the disk writer's contents to disk, and update relevant variables.\n    // The writer is committed at the end of this process.\n    def flush(): Unit = {\n      val segment = writer.commitAndGet()\n      batchSizes += segment.length\n      _diskBytesSpilled += segment.length\n      objectsWritten = 0\n    }\n    var success = false\n    try {\n      while (inMemoryIterator.hasNext) {\n        val partitionId = inMemoryIterator.nextPartition()\n        require(partitionId >= 0 && partitionId < numPartitions,\n          s\"partition Id: ${partitionId} should be in the range [0, ${numPartitions})\")\n        inMemoryIterator.writeNext(writer)\n        elementsPerPartition(partitionId) += 1\n        objectsWritten += 1\n        if (objectsWritten == serializerBatchSize) {　//写入　serializerBatchSize　条数据便刷新一次缓存\n          // batchSize 在类中定义的如下:\n          // 可以看出如果不存在配置默认为　10000　条\n          // private val serializerBatchSize = conf.getLong(\"spark.shuffle.spill.batchSize\", 10000)\n          flush()\n        }\n      }\n      if (objectsWritten > 0) {\n        flush()\n      } else {\n        writer.revertPartialWritesAndClose()\n      }\n      success = true\n    } finally {\n      if (success) {\n        writer.close()\n      } else {\n        // This code path only happens if an exception was thrown above before we set success;\n        // close our stuff and let the exception be thrown further\n        writer.revertPartialWritesAndClose()\n        if (file.exists()) {\n          if (!file.delete()) {\n            logWarning(s\"Error deleting ${file}\")\n          }\n        }\n      }\n    }\n    //最后记录临时文件的信息\n    SpilledFile(file, blockId, batchSizes.toArray, elementsPerPartition)\n\n  }\n```\n\n这就是整个　insertAll()　方法:\n\n merge 达到溢出值后进行排序并写入临时文件，这里要注意的是并非所有的缓存文件都被写入到临时文件中，所以接下来需要将临时文件中的数据加上内存中的数据进行一次排序写入到一个大文件中。\n\n```scala\n//首先获取需要写入的文件:　\n   val output = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)\n　　val tmp = Utils.tempFileWith(output)\n    try { \n      val blockId = ShuffleBlockId(dep.shuffleId, mapId, IndexShuffleBlockResolver.NOOP_REDUCE_ID)\n      val partitionLengths = sorter.writePartitionedFile(blockId, tmp)//这一句是重点 下面会讲解\n      shuffleBlockResolver.writeIndexFileAndCommit(dep.shuffleId, mapId, partitionLengths, tmp)\n      mapStatus = MapStatus(blockManager.shuffleServerId, partitionLengths)\n    } finally {\n      if (tmp.exists() && !tmp.delete()) {\n        logError(s\"Error while deleting temp file ${tmp.getAbsolutePath}\")\n    }\n```\n####　writePartitionedFile\n\n继续看 writePartitionedFile :\n\n```scala\n//  ExternalSorter 中的　writePartitionedFile　方法\n   def writePartitionedFile(\n      blockId: BlockId,\n      outputFile: File): Array[Long] = {\n\n    // Track location of each range in the output file\n    val lengths = new Array[Long](numPartitions)\n    val writer = blockManager.getDiskWriter(blockId, outputFile, serInstance, fileBufferSize,\n      context.taskMetrics().shuffleWriteMetrics)\n\n    // 这里的spills 就是　刚才产生的临时文件的数据　如果为空逻辑就比较简单，直接进行排序并写入文件\n    if (spills.isEmpty) {\n      // Case where we only have in-memory data\n      val collection = if (aggregator.isDefined) map else buffer\n      val it = collection.destructiveSortedWritablePartitionedIterator(comparator)　//这里同样使用　destructiveSortedWritablePartitionedIterator　进行排序\n      while (it.hasNext) {\n        val partitionId = it.nextPartition()\n        while (it.hasNext && it.nextPartition() == partitionId) {\n          it.writeNext(writer)\n        }\n        val segment = writer.commitAndGet()\n        lengths(partitionId) = segment.length\n      }\n    } else {//重点看不为空的时候，这里调用了　partitionedIterator　方法\n      // We must perform merge-sort; get an iterator by partition and write everything directly.\n      for ((id, elements) <- this.partitionedIterator) {\n        if (elements.hasNext) {\n          for (elem <- elements) {\n            writer.write(elem._1, elem._2)\n          }\n          val segment = writer.commitAndGet()\n          lengths(id) = segment.length\n        }\n      }\n    }\n\n    writer.close()\n    context.taskMetrics().incMemoryBytesSpilled(memoryBytesSpilled)\n    context.taskMetrics().incDiskBytesSpilled(diskBytesSpilled)\n    context.taskMetrics().incPeakExecutionMemory(peakMemoryUsedBytes)\n\n    lengths\n  }\n```\n\n然后是 partitionedIterator 方法:\n\n```scala\ndef partitionedIterator: Iterator[(Int, Iterator[Product2[K, C]])] = {\n    val usingMap = aggregator.isDefined\n    val collection: WritablePartitionedPairCollection[K, C] = if (usingMap) map else buffer\n    if (spills.isEmpty) {// 这里又一次判断了是否为空，直接看有临时文件的部分\n      // Special case: if we have only in-memory data, we don't need to merge streams, and perhaps\n      // we don't even need to sort by anything other than partition ID\n      if (!ordering.isDefined) {\n        // The user hasn't requested sorted keys, so only sort by partition ID, not key\n   groupByPartition(destructiveIterator(collection.partitionedDestructiveSortedIterator(None)))\n      } else {\n        // We do need to sort by both partition ID and key\n        groupByPartition(destructiveIterator(\n          collection.partitionedDestructiveSortedIterator(Some(keyComparator))))\n      }\n    } else {\n      // Merge spilled and in-memory data\n      // 这里传入了临时文件　spills　和　排序后的缓存文件\n      merge(spills, destructiveIterator(\n        collection.partitionedDestructiveSortedIterator(comparator)))\n    }\n  }\n```\n\nmerge 方法:\n\n```scala\n // merge方法\n   // inMemory　是根据(partion,hash(k)) 排序后的内存数据\n   private def merge(spills: Seq[SpilledFile], inMemory: Iterator[((Int, K), C)])\n      : Iterator[(Int, Iterator[Product2[K, C]])] = {\n    //　将所有缓存文件转化为 SpillReader \n    val readers = spills.map(new SpillReader(_))\n    // buffered方法只是比普通的　itertor 方法多提供一个　head　方法，例如:\n    // val a = List(1,2,3,4,5)\n    // val b = a.iterator.buffered\n    // b.head : 1\n    // b.next : 1\n    // b.head : 2\n    // b.next : 2\n    val inMemBuffered = inMemory.buffered\n    (0 until numPartitions).iterator.map { p =>\n      // 获得分区对应数据的迭代器 \n      // 这里的　IteratorForPartition　就是得到分区 p 对应的数据的迭代器，逻辑比较简单，就不单独拿出来分析\n      val inMemIterator = new IteratorForPartition(p, inMemBuffered)\n      \n      //这里获取所有文件的第　p　个分区　并与内存中的第　p 个分区进行合并\n      val iterators = readers.map(_.readNextPartition()) ++ Seq(inMemIterator)\n　　　// 这里的　iterators　是由　多个 iterator 组成的，其中每一个都是关于 p 分区数据的 iterator\n      if (aggregator.isDefined) {\n        // Perform partial aggregation across partitions\n        (p, mergeWithAggregation(\n          iterators, aggregator.get.mergeCombiners, keyComparator, ordering.isDefined))\n      } else if (ordering.isDefined) {\n        // No aggregator given, but we have an ordering (e.g. used by reduce tasks in sortByKey);\n        // sort the elements without trying to merge them\n        (p, mergeSort(iterators, ordering.get))\n      } else {\n        (p, iterators.iterator.flatten)\n      }\n    }\n```\nmergeWithAggregation\n```scala\n\n      private def mergeWithAggregation(\n      iterators: Seq[Iterator[Product2[K, C]]],\n      mergeCombiners: (C, C) => C,\n      comparator: Comparator[K],\n      totalOrder: Boolean)\n      : Iterator[Product2[K, C]] =\n  {\n    if (!totalOrder) {\n      new Iterator[Iterator[Product2[K, C]]] {\n        val sorted = mergeSort(iterators, comparator).buffered\n        val keys = new ArrayBuffer[K]\n        val combiners = new ArrayBuffer[C]\n        override def hasNext: Boolean = sorted.hasNext\n        override def next(): Iterator[Product2[K, C]] = {\n          if (!hasNext) {\n            throw new NoSuchElementException\n          }\n          keys.clear()\n          combiners.clear()\n          val firstPair = sorted.next()\n          keys += firstPair._1\n          combiners += firstPair._2\n          val key = firstPair._1\n          while (sorted.hasNext && comparator.compare(sorted.head._1, key) == 0) {\n            val pair = sorted.next()\n            var i = 0\n            var foundKey = false\n            while (i < keys.size && !foundKey) {\n              if (keys(i) == pair._1) {\n                combiners(i) = mergeCombiners(combiners(i), pair._2)\n                foundKey = true\n              }\n              i += 1\n            }\n            if (!foundKey) {\n              keys += pair._1\n              combiners += pair._2\n            }\n          }\n          keys.iterator.zip(combiners.iterator)\n        }\n      }.flatMap(i => i)\n    } else {\n      // We have a total ordering, so the objects with the same key are sequential.\n      new Iterator[Product2[K, C]] {\n        val sorted = mergeSort(iterators, comparator).buffered\n        override def hasNext: Boolean = sorted.hasNext\n        override def next(): Product2[K, C] = {\n          if (!hasNext) {\n            throw new NoSuchElementException\n          }\n          val elem = sorted.next()\n          val k = elem._1\n          var c = elem._2\n          // 这里的意思是可能存在多个相同的key 分布在不同临时文件或内存中\n          // 所以还需要将不同的 key 对应的值进行合并 \n          while (sorted.hasNext && sorted.head._1 == k) {\n            val pair = sorted.next()\n            c = mergeCombiners(c, pair._2)\n          }\n          (k, c)//返回\n        }\n      }\n    }\n  }\n```\n\n继续看 mergeSort 方法，首先选除头元素最小对应那个分区，然后选出这个分区的第一个元素，那么这个元素一定是最小的，然后将剩余的元素放回去:\n\n```scala\n\n  private def mergeSort(iterators: Seq[Iterator[Product2[K, C]]], comparator: Comparator[K])\n      : Iterator[Product2[K, C]] =\n  {\n    val bufferedIters = iterators.filter(_.hasNext).map(_.buffered)\n    type Iter = BufferedIterator[Product2[K, C]]\n    //选取头元素最小的分区\n    val heap = new mutable.PriorityQueue[Iter]()(new Ordering[Iter] {\n      // Use the reverse of comparator.compare because PriorityQueue dequeues the max\n      override def compare(x: Iter, y: Iter): Int = -comparator.compare(x.head._1, y.head._1)\n    })\n    heap.enqueue(bufferedIters: _*) \n    new Iterator[Product2[K, C]] {\n      override def hasNext: Boolean = !heap.isEmpty\n      override def next(): Product2[K, C] = {\n        if (!hasNext) {\n          throw new NoSuchElementException\n        }\n        val firstBuf = heap.dequeue()\n        val firstPair = firstBuf.next()\n        if (firstBuf.hasNext) {\n          heap.enqueue(firstBuf)\n        }\n        firstPair\n      }\n    }\n  }\n```\n\n接下来就使用 writeIndexFileAndCommit 将每一个分区的数据条数写入索引文件，便于 reduce 端获取。\n\n## 总结\n\nshuflle write 相比起 shuffe read 要复杂很多，因此本文的篇幅也较长。其实在 shuffle write 有两个较为重要的方法，其中一个是 insertAll。这个方法的作用就是将数据缓存到一个可以添加的 hash 表中。如果表占用的内存达到的一定值，就对其进行排序。排序方式先按照分区进行排序，如果分区相同则按key的hash进行排序，随后将排序后的数据写入临时文件中，这个过程可能会生成多个临时文件。\n\n然后 writePartitionedFile 的作用是最后将内存中的数据和临时文件中的数据进行全部排序，写入到一个大文件中。\n\n最后将每一个分区对应的索引数据写入文件。整个shuffle write 阶段就完成了\n","slug":"源码阅读计划-第二部分-shuffle-write","published":1,"updated":"2018-06-13T14:42:19.376Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjiebvrsi0007fangedvhz17o","content":"<h2 id=\"写在开始的话\"><a href=\"#写在开始的话\" class=\"headerlink\" title=\"写在开始的话\"></a>写在开始的话</h2><p>shuffle 作为 spark 最重要的一部分，也是很多初学者感到头痛的一部分。简单来说，shuffle 分为两个部分 : shuffle write 和 shuffle read。shuffle write 在 map 端进行，而shuffle read 在 reduce 端进行。本章就准备就 shuffle write 做一个详细的分析。</p>\n<h2 id=\"为什么需要shuffle\"><a href=\"#为什么需要shuffle\" class=\"headerlink\" title=\"为什么需要shuffle\"></a>为什么需要shuffle</h2><p>在上一章<a href=\"https://lishion.github.io/2018/06/06/Spark-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E8%AE%A1%E5%88%92-%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86-%E8%BF%AD%E4%BB%A3%E8%AE%A1%E7%AE%97/\" target=\"_blank\" rel=\"noopener\">Spark-源码阅读计划-第一部分-迭代计算</a>提到过每一个分区的数据最后都会由一个task来处理，那么当task 执行的类似于 reduceByKey 这种算子的时候一定需要满足相同的 key 在同一个分区这个条件，否则就不能按照 key 进行reduce了。但是当我们从数据源获取数据并进行处理后，相同的 key 不一定在同一个分区。那么在一个 map 端将相同的key 使用某种方式聚集在同一个分区并写入临时文件的过程就称为 shuffle write；而在 reduce 从 map 拉去执行 task 所需要的数据就是 shuffle read；这两个步骤组成了shuffle。</p>\n<h2 id=\"一个小例子\"><a href=\"#一个小例子\" class=\"headerlink\" title=\"一个小例子\"></a>一个小例子</h2><p>本章同样以一个小例子作为本章讲解的中心，例如一个统计词频的代码:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sc.parallelize(<span class=\"type\">List</span>(<span class=\"string\">\"hello\"</span>,<span class=\"string\">\"world\"</span>,<span class=\"string\">\"hello\"</span>,<span class=\"string\">\"spark\"</span>)).map(x=&gt;(x,<span class=\"number\">1</span>)).reduceByKey(_+_)</span><br></pre></td></tr></table></figure>\n<p>相信有一点基础的同学都知道 reduceByKey 这算子会产生 shuffle。但是大部分的同学都不知 shuffle 是什么时候产生的，shuffle 到底做了些什么，这也是这一章的重点。</p>\n<h2 id=\"shuffle-task\"><a href=\"#shuffle-task\" class=\"headerlink\" title=\"shuffle task\"></a>shuffle task</h2><p>在上一章提到过当 job 提交后会生成 result task。而在需要 shuffle 则会生成 ShuffleMapTask。这里涉及到 job 提交以及 stage 生成的的知识，将会在下一章提到。这里我们直接开始看 ShuffleMapTask 中的 runtask :</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">runTask</span></span>(context: <span class=\"type\">TaskContext</span>): <span class=\"type\">MapStatus</span> = &#123;</span><br><span class=\"line\">   <span class=\"comment\">// Deserialize the RDD using the broadcast variable.</span></span><br><span class=\"line\">...</span><br><span class=\"line\">   <span class=\"keyword\">var</span> writer: <span class=\"type\">ShuffleWriter</span>[<span class=\"type\">Any</span>, <span class=\"type\">Any</span>] = <span class=\"literal\">null</span></span><br><span class=\"line\">   <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">     <span class=\"keyword\">val</span> manager = <span class=\"type\">SparkEnv</span>.get.shuffleManager</span><br><span class=\"line\">     writer = manager.getWriter[<span class=\"type\">Any</span>, <span class=\"type\">Any</span>](dep.shuffleHandle, partitionId, context)</span><br><span class=\"line\">      <span class=\"comment\">// 这里依然是rdd调用iterator方法的地方</span></span><br><span class=\"line\">     writer.write(rdd.iterator(partition, context).asInstanceOf[<span class=\"type\">Iterator</span>[_ &lt;: <span class=\"type\">Product2</span>[<span class=\"type\">Any</span>, <span class=\"type\">Any</span>]]])</span><br><span class=\"line\">     writer.stop(success = <span class=\"literal\">true</span>).get</span><br><span class=\"line\">   &#125; <span class=\"keyword\">catch</span> &#123;</span><br><span class=\"line\">     <span class=\"keyword\">case</span> e: <span class=\"type\">Exception</span> =&gt;</span><br><span class=\"line\">       <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">         <span class=\"keyword\">if</span> (writer != <span class=\"literal\">null</span>) &#123;</span><br><span class=\"line\">           writer.stop(success = <span class=\"literal\">false</span>)</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">       &#125; <span class=\"keyword\">catch</span> &#123;</span><br><span class=\"line\">         <span class=\"keyword\">case</span> e: <span class=\"type\">Exception</span> =&gt;</span><br><span class=\"line\">           log.debug(<span class=\"string\">\"Could not stop writer\"</span>, e)</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">       <span class=\"keyword\">throw</span> e</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>可以看到首先获得一个 shuffleWriter。spark 中有许多不同类型的 writer ，默认使用的是 SortShuffleWriter。SortShuffleWriter 中的 write 方法实现了 shuffle write。源代码如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">write</span></span>(records: <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>]]): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">    <span class=\"comment\">//如果需要 map 端聚</span></span><br><span class=\"line\">  sorter = <span class=\"keyword\">if</span> (dep.mapSideCombine) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">ExternalSorter</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>, <span class=\"type\">C</span>](</span><br><span class=\"line\">      context, dep.aggregator, <span class=\"type\">Some</span>(dep.partitioner), dep.keyOrdering, dep.serializer)</span><br><span class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">ExternalSorter</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>, <span class=\"type\">V</span>](</span><br><span class=\"line\">      context, aggregator = <span class=\"type\">None</span>, <span class=\"type\">Some</span>(dep.partitioner), ordering = <span class=\"type\">None</span>, dep.serializer)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  sorter.insertAll(records)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> output = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> tmp = <span class=\"type\">Utils</span>.tempFileWith(output)</span><br><span class=\"line\">  <span class=\"keyword\">try</span> &#123; </span><br><span class=\"line\">    <span class=\"keyword\">val</span> blockId = <span class=\"type\">ShuffleBlockId</span>(dep.shuffleId, mapId, <span class=\"type\">IndexShuffleBlockResolver</span>.<span class=\"type\">NOOP_REDUCE_ID</span>)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> partitionLengths = sorter.writePartitionedFile(blockId, tmp)</span><br><span class=\"line\">    shuffleBlockResolver.writeIndexFileAndCommit(dep.shuffleId, mapId, partitionLengths, tmp)</span><br><span class=\"line\">    mapStatus = <span class=\"type\">MapStatus</span>(blockManager.shuffleServerId, partitionLengths)</span><br><span class=\"line\">  &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (tmp.exists() &amp;&amp; !tmp.delete()) &#123;</span><br><span class=\"line\">      logError(<span class=\"string\">s\"Error while deleting temp file <span class=\"subst\">$&#123;tmp.getAbsolutePath&#125;</span>\"</span>)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>使用了 ExternalSorter 的 insertAll 方法插入了上一个 RDD 生成的数据的迭代器。</p>\n<h4 id=\"insertAll\"><a href=\"#insertAll\" class=\"headerlink\" title=\"insertAll\"></a>insertAll</h4><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">insertAll</span></span>(records: <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>]]): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">    <span class=\"comment\">// <span class=\"doctag\">TODO:</span> stop combining if we find that the reduction factor isn't high</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> shouldCombine = aggregator.isDefined</span><br><span class=\"line\">    <span class=\"comment\">// 直接看需要 map 端聚合的情况</span></span><br><span class=\"line\">    <span class=\"comment\">// 不需要聚合的情况类似</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (shouldCombine) &#123;</span><br><span class=\"line\">      <span class=\"comment\">// Combine values in-memory first using our AppendOnlyMap</span></span><br><span class=\"line\">       </span><br><span class=\"line\">      <span class=\"keyword\">val</span> mergeValue = aggregator.get.mergeValue </span><br><span class=\"line\">      <span class=\"keyword\">val</span> createCombiner = aggregator.get.createCombiner</span><br><span class=\"line\">      <span class=\"keyword\">var</span> kv: <span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>] = <span class=\"literal\">null</span></span><br><span class=\"line\">      <span class=\"comment\">// 如果 key 不存在则直接创建 Combiner ，否则使用 mergeValue 将 value 添加到创建的 Combiner中</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> update = (hadValue: <span class=\"type\">Boolean</span>, oldValue: <span class=\"type\">C</span>) =&gt; &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (hadValue) mergeValue(oldValue, kv._2) <span class=\"keyword\">else</span> createCombiner(kv._2)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      <span class=\"keyword\">while</span> (records.hasNext) &#123;</span><br><span class=\"line\">        addElementsRead()</span><br><span class=\"line\">        kv = records.next()</span><br><span class=\"line\">        <span class=\"comment\">// 更新值</span></span><br><span class=\"line\">        <span class=\"comment\">// 这里需要注意的是之前的数据是(k,v)类型，这里转换成了((part,k),v)</span></span><br><span class=\"line\">        <span class=\"comment\">// 其中 part 是 key 对应的分区</span></span><br><span class=\"line\">        map.changeValue((getPartition(kv._1), kv._1), update)</span><br><span class=\"line\">        maybeSpillCollection(usingMap = <span class=\"literal\">true</span>)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">      <span class=\"comment\">// Stick values into our buffer</span></span><br><span class=\"line\">      <span class=\"keyword\">while</span> (records.hasNext) &#123;</span><br><span class=\"line\">        addElementsRead()</span><br><span class=\"line\">        <span class=\"keyword\">val</span> kv = records.next()</span><br><span class=\"line\">        buffer.insert(getPartition(kv._1), kv._1, kv._2.asInstanceOf[<span class=\"type\">C</span>])</span><br><span class=\"line\">        maybeSpillCollection(usingMap = <span class=\"literal\">false</span>)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>在这里使用 getPartition() 进行了数据的<strong>分区</strong>，也就是将不同的 key 与其分区对应起来。默认使用的是 HashPartitioner ，有兴趣的同学可以去阅读源码。再继续看 changeValue 方法。在这里使用非常重要的数据结构来存放分区后的数据，也就是代码中的 map 类型为 PartitionedAppendOnlyMap。其中是使用hask表存放数据，并且存放的方式为表中的偶数索index引存放key, index + 1 存放对应的value。例如:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">k1|v1|null|k2|v2|k3|v3|null|....</span><br></pre></td></tr></table></figure>\n<p>之所以里面由空值，是因为在插入的时候使用了二次探测法。来看一看 changeValue 方法:</p>\n<p>PartitionedAppendOnlyMap 中的  changeValue 方法继承自 </p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">changeValue</span></span>(key: <span class=\"type\">K</span>, updateFunc: (<span class=\"type\">Boolean</span>, <span class=\"type\">V</span>) =&gt; <span class=\"type\">V</span>): <span class=\"type\">V</span> = &#123;</span><br><span class=\"line\">  assert(!destroyed, destructionMessage)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> k = key.asInstanceOf[<span class=\"type\">AnyRef</span>]</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (k.eq(<span class=\"literal\">null</span>)) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (!haveNullValue) &#123;</span><br><span class=\"line\">      incrementSize()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    nullValue = updateFunc(haveNullValue, nullValue)</span><br><span class=\"line\">    haveNullValue = <span class=\"literal\">true</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> nullValue</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"keyword\">var</span> pos = rehash(k.hashCode) &amp; mask</span><br><span class=\"line\">  <span class=\"keyword\">var</span> i = <span class=\"number\">1</span></span><br><span class=\"line\">  <span class=\"keyword\">while</span> (<span class=\"literal\">true</span>) &#123;</span><br><span class=\"line\">    <span class=\"comment\">// 偶数位置为 key</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> curKey = data(<span class=\"number\">2</span> * pos)</span><br><span class=\"line\">     <span class=\"comment\">// 如果该key不存在，就直接插入</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (curKey.eq(<span class=\"literal\">null</span>)) &#123;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> newValue = updateFunc(<span class=\"literal\">false</span>, <span class=\"literal\">null</span>.asInstanceOf[<span class=\"type\">V</span>])</span><br><span class=\"line\">      data(<span class=\"number\">2</span> * pos) = k</span><br><span class=\"line\">      data(<span class=\"number\">2</span> * pos + <span class=\"number\">1</span>) = newValue.asInstanceOf[<span class=\"type\">AnyRef</span>]</span><br><span class=\"line\">      incrementSize()</span><br><span class=\"line\">      <span class=\"keyword\">return</span> newValue</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (k.eq(curKey) || k.equals(curKey)) &#123;<span class=\"comment\">//如果 key 存在，则进行聚合</span></span><br><span class=\"line\">      </span><br><span class=\"line\">      <span class=\"keyword\">val</span> newValue = updateFunc(<span class=\"literal\">true</span>, data(<span class=\"number\">2</span> * pos + <span class=\"number\">1</span>).asInstanceOf[<span class=\"type\">V</span>])</span><br><span class=\"line\">      data(<span class=\"number\">2</span> * pos + <span class=\"number\">1</span>) = newValue.asInstanceOf[<span class=\"type\">AnyRef</span>]</span><br><span class=\"line\">      <span class=\"keyword\">return</span> newValue</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123; <span class=\"comment\">// 否则进行下一次探测</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> delta = i</span><br><span class=\"line\">      pos = (pos + delta) &amp; mask</span><br><span class=\"line\">      i += <span class=\"number\">1</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"literal\">null</span>.asInstanceOf[<span class=\"type\">V</span>] <span class=\"comment\">// Never reached but needed to keep compiler happy</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>再调用 insertAll 进行了数据的聚合|插入之后，调用了 maybeSpillCollection 方法。这个方法的作用是判断 map 占用了内存，如果内存达到一定的值，则将内存中的数据 spill 到临时文件中。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">maybeSpillCollection</span></span>(usingMap: <span class=\"type\">Boolean</span>): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">var</span> estimatedSize = <span class=\"number\">0</span>L</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (usingMap) &#123;</span><br><span class=\"line\">      estimatedSize = map.estimateSize()</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (maybeSpill(map, estimatedSize)) &#123;</span><br><span class=\"line\">        map = <span class=\"keyword\">new</span> <span class=\"type\">PartitionedAppendOnlyMap</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">      estimatedSize = buffer.estimateSize()</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (maybeSpill(buffer, estimatedSize)) &#123;</span><br><span class=\"line\">        buffer = <span class=\"keyword\">new</span> <span class=\"type\">PartitionedPairBuffer</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (estimatedSize &gt; _peakMemoryUsedBytes) &#123;</span><br><span class=\"line\">      _peakMemoryUsedBytes = estimatedSize</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>maybeSpillCollection 继续调用了 maybeSpill 方法，这个方法继承自 Spillable 中</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">protected</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">maybeSpill</span></span>(collection: <span class=\"type\">C</span>, currentMemory: <span class=\"type\">Long</span>): <span class=\"type\">Boolean</span> = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">var</span> shouldSpill = <span class=\"literal\">false</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (elementsRead % <span class=\"number\">32</span> == <span class=\"number\">0</span> &amp;&amp; currentMemory &gt;= myMemoryThreshold) &#123;</span><br><span class=\"line\">      <span class=\"comment\">// Claim up to double our current memory from the shuffle memory pool</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> amountToRequest = <span class=\"number\">2</span> * currentMemory - myMemoryThreshold</span><br><span class=\"line\">      <span class=\"keyword\">val</span> granted = acquireMemory(amountToRequest)</span><br><span class=\"line\">      myMemoryThreshold += granted</span><br><span class=\"line\">      <span class=\"comment\">// If we were granted too little memory to grow further (either tryToAcquire returned 0,</span></span><br><span class=\"line\">      <span class=\"comment\">// or we already had more memory than myMemoryThreshold), spill the current collection</span></span><br><span class=\"line\">      shouldSpill = currentMemory &gt;= myMemoryThreshold</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    shouldSpill = shouldSpill || _elementsRead &gt; numElementsForceSpillThreshold</span><br><span class=\"line\">    <span class=\"comment\">// Actually spill</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (shouldSpill) &#123;</span><br><span class=\"line\">      _spillCount += <span class=\"number\">1</span></span><br><span class=\"line\">      logSpillage(currentMemory)</span><br><span class=\"line\">      spill(collection)</span><br><span class=\"line\">      _elementsRead = <span class=\"number\">0</span></span><br><span class=\"line\">      _memoryBytesSpilled += currentMemory</span><br><span class=\"line\">      releaseMemory()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    shouldSpill</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>该方法首先判断了是否需要 spill，如果需要则调用 spill() 方法将内存中的数据写入临时文件中。spill 方法如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"keyword\">protected</span>[<span class=\"keyword\">this</span>] <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">spill</span></span>(collection: <span class=\"type\">WritablePartitionedPairCollection</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">   <span class=\"keyword\">val</span> inMemoryIterator = collection.destructiveSortedWritablePartitionedIterator(comparator)</span><br><span class=\"line\">   <span class=\"keyword\">val</span> spillFile = spillMemoryIteratorToDisk(inMemoryIterator)</span><br><span class=\"line\">   spills += spillFile</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>其中的  comparator 如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">comparator</span></span>: <span class=\"type\">Option</span>[<span class=\"type\">Comparator</span>[<span class=\"type\">K</span>]] = &#123;</span><br><span class=\"line\">   <span class=\"keyword\">if</span> (ordering.isDefined || aggregator.isDefined) &#123;</span><br><span class=\"line\">     <span class=\"type\">Some</span>(keyComparator)</span><br><span class=\"line\">   &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">     <span class=\"type\">None</span></span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>也就是:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"keyword\">val</span> keyComparator: <span class=\"type\">Comparator</span>[<span class=\"type\">K</span>] = ordering.getOrElse(<span class=\"keyword\">new</span> <span class=\"type\">Comparator</span>[<span class=\"type\">K</span>] &#123;</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compare</span></span>(a: <span class=\"type\">K</span>, b: <span class=\"type\">K</span>): <span class=\"type\">Int</span> = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> h1 = <span class=\"keyword\">if</span> (a == <span class=\"literal\">null</span>) <span class=\"number\">0</span> <span class=\"keyword\">else</span> a.hashCode()</span><br><span class=\"line\">    <span class=\"keyword\">val</span> h2 = <span class=\"keyword\">if</span> (b == <span class=\"literal\">null</span>) <span class=\"number\">0</span> <span class=\"keyword\">else</span> b.hashCode()</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (h1 &lt; h2) <span class=\"number\">-1</span> <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (h1 == h2) <span class=\"number\">0</span> <span class=\"keyword\">else</span> <span class=\"number\">1</span></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;)</span><br></pre></td></tr></table></figure>\n<p>destructiveSortedWritablePartitionedIterator 位于 WritablePartitionedPairCollection 类中，实现如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">destructiveSortedWritablePartitionedIterator</span></span>(keyComparator: <span class=\"type\">Option</span>[<span class=\"type\">Comparator</span>[<span class=\"type\">K</span>]])</span><br><span class=\"line\">    : <span class=\"type\">WritablePartitionedIterator</span> = &#123;</span><br><span class=\"line\">    <span class=\"comment\">//这里调用的其实是　PartitionedAppendOnlyMap　中重写的　partitionedDestructiveSortedIterator    </span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> it = partitionedDestructiveSortedIterator(keyComparator)</span><br><span class=\"line\">    <span class=\"comment\">// 这里还实现了一个　writeNext　的方法，后面会用到</span></span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">WritablePartitionedIterator</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">private</span>[<span class=\"keyword\">this</span>] <span class=\"keyword\">var</span> cur = <span class=\"keyword\">if</span> (it.hasNext) it.next() <span class=\"keyword\">else</span> <span class=\"literal\">null</span></span><br><span class=\"line\">       <span class=\"comment\">// 在map 中的数据其实是((partition,k),v)</span></span><br><span class=\"line\">       <span class=\"comment\">// 这里只写入了(k,v)</span></span><br><span class=\"line\">      <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">writeNext</span></span>(writer: <span class=\"type\">DiskBlockObjectWriter</span>): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">        writer.write(cur._1._2, cur._2)</span><br><span class=\"line\">        cur = <span class=\"keyword\">if</span> (it.hasNext) it.next() <span class=\"keyword\">else</span> <span class=\"literal\">null</span></span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hasNext</span></span>(): <span class=\"type\">Boolean</span> = cur != <span class=\"literal\">null</span></span><br><span class=\"line\">      <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">nextPartition</span></span>(): <span class=\"type\">Int</span> = cur._1._1</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>接下来看看partitionedDestructiveSortedIterator:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">partitionedDestructiveSortedIterator</span></span>(keyComparator: <span class=\"type\">Option</span>[<span class=\"type\">Comparator</span>[<span class=\"type\">K</span>]])</span><br><span class=\"line\">    : <span class=\"type\">Iterator</span>[((<span class=\"type\">Int</span>, <span class=\"type\">K</span>), <span class=\"type\">V</span>)] = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> comparator = keyComparator.map(partitionKeyComparator).getOrElse(partitionComparator)</span><br><span class=\"line\">    destructiveSortedIterator(comparator)</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>keyComparator.map(partitionKeyComparator)　等于 partitionKeyComparator(keyComparator)，再看下面的partitionKeyComparator，说明比较器是先对分区进行比较，如果分区相同，再对key的hash值进行比较:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">partitionKeyComparator</span></span>[<span class=\"type\">K</span>](keyComparator: <span class=\"type\">Comparator</span>[<span class=\"type\">K</span>]): <span class=\"type\">Comparator</span>[(<span class=\"type\">Int</span>, <span class=\"type\">K</span>)] = &#123;</span><br><span class=\"line\">   <span class=\"keyword\">new</span> <span class=\"type\">Comparator</span>[(<span class=\"type\">Int</span>, <span class=\"type\">K</span>)] &#123;</span><br><span class=\"line\">     <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compare</span></span>(a: (<span class=\"type\">Int</span>, <span class=\"type\">K</span>), b: (<span class=\"type\">Int</span>, <span class=\"type\">K</span>)): <span class=\"type\">Int</span> = &#123;</span><br><span class=\"line\">       <span class=\"keyword\">val</span> partitionDiff = a._1 - b._1</span><br><span class=\"line\">       <span class=\"keyword\">if</span> (partitionDiff != <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">         partitionDiff</span><br><span class=\"line\">       &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">         keyComparator.compare(a._2, b._2)</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>然后看destructiveSortedIterator(),这是在 PartitionedAppendOnlyMap 的父类 destructiveSortedIterator 中实现的:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">destructiveSortedIterator</span></span>(keyComparator: <span class=\"type\">Comparator</span>[<span class=\"type\">K</span>]): <span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">V</span>)] = &#123;</span><br><span class=\"line\">   destroyed = <span class=\"literal\">true</span></span><br><span class=\"line\">   <span class=\"comment\">// Pack KV pairs into the front of the underlying array</span></span><br><span class=\"line\">   <span class=\"comment\">// 这里是因为　PartitionedAppendOnlyMap　采用的二次探测法　,kv对之间存在　null值，所以先把非空的kv对全部移动到hash表的前端</span></span><br><span class=\"line\">   <span class=\"keyword\">var</span> keyIndex, newIndex = <span class=\"number\">0</span></span><br><span class=\"line\">   <span class=\"keyword\">while</span> (keyIndex &lt; capacity) &#123;</span><br><span class=\"line\">     <span class=\"keyword\">if</span> (data(<span class=\"number\">2</span> * keyIndex) != <span class=\"literal\">null</span>) &#123;</span><br><span class=\"line\">       data(<span class=\"number\">2</span> * newIndex) = data(<span class=\"number\">2</span> * keyIndex)</span><br><span class=\"line\">       data(<span class=\"number\">2</span> * newIndex + <span class=\"number\">1</span>) = data(<span class=\"number\">2</span> * keyIndex + <span class=\"number\">1</span>)</span><br><span class=\"line\">       newIndex += <span class=\"number\">1</span></span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">     keyIndex += <span class=\"number\">1</span></span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   assert(curSize == newIndex + (<span class=\"keyword\">if</span> (haveNullValue) <span class=\"number\">1</span> <span class=\"keyword\">else</span> <span class=\"number\">0</span>))</span><br><span class=\"line\">   <span class=\"comment\">// 这里对给定数据范围为 0 to newIndex，利用　keyComparator　比较器进行排序</span></span><br><span class=\"line\">   <span class=\"comment\">// 也就是对 map 中的数据根据　(partition,key) 进行排序</span></span><br><span class=\"line\">   <span class=\"keyword\">new</span> <span class=\"type\">Sorter</span>(<span class=\"keyword\">new</span> <span class=\"type\">KVArraySortDataFormat</span>[<span class=\"type\">K</span>, <span class=\"type\">AnyRef</span>]).sort(data, <span class=\"number\">0</span>, newIndex, keyComparator)</span><br><span class=\"line\"></span><br><span class=\"line\">   <span class=\"comment\">// 定义迭代器</span></span><br><span class=\"line\">   <span class=\"keyword\">new</span> <span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">V</span>)] &#123;</span><br><span class=\"line\">     <span class=\"keyword\">var</span> i = <span class=\"number\">0</span></span><br><span class=\"line\">     <span class=\"keyword\">var</span> nullValueReady = haveNullValue</span><br><span class=\"line\">     <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hasNext</span></span>: <span class=\"type\">Boolean</span> = (i &lt; newIndex || nullValueReady)</span><br><span class=\"line\">     <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">next</span></span>(): (<span class=\"type\">K</span>, <span class=\"type\">V</span>) = &#123;</span><br><span class=\"line\">       <span class=\"keyword\">if</span> (nullValueReady) &#123;</span><br><span class=\"line\">         nullValueReady = <span class=\"literal\">false</span></span><br><span class=\"line\">         (<span class=\"literal\">null</span>.asInstanceOf[<span class=\"type\">K</span>], nullValue)</span><br><span class=\"line\">       &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">         <span class=\"keyword\">val</span> item = (data(<span class=\"number\">2</span> * i).asInstanceOf[<span class=\"type\">K</span>], data(<span class=\"number\">2</span> * i + <span class=\"number\">1</span>).asInstanceOf[<span class=\"type\">V</span>])</span><br><span class=\"line\">         i += <span class=\"number\">1</span></span><br><span class=\"line\">         item</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>可以看出整个　<code>val inMemoryIterator = collection.destructiveSortedWritablePartitionedIterator(comparator)</code>　这一步其实就是在对　map　或 buffer 进行排序，并且实现了一个 writeNext() 方法供后续调调用。随后调用<code>val spillFile = spillMemoryIteratorToDisk(inMemoryIterator)</code>将排序后的缓存写入文件中:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span>[<span class=\"keyword\">this</span>] <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">spillMemoryIteratorToDisk</span></span>(inMemoryIterator: <span class=\"type\">WritablePartitionedIterator</span>)</span><br><span class=\"line\">      : <span class=\"type\">SpilledFile</span> = &#123;</span><br><span class=\"line\">   </span><br><span class=\"line\">    <span class=\"keyword\">val</span> (blockId, file) = diskBlockManager.createTempShuffleBlock()</span><br><span class=\"line\">    <span class=\"comment\">// These variables are reset after each flush</span></span><br><span class=\"line\">    <span class=\"keyword\">var</span> objectsWritten: <span class=\"type\">Long</span> = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> spillMetrics: <span class=\"type\">ShuffleWriteMetrics</span> = <span class=\"keyword\">new</span> <span class=\"type\">ShuffleWriteMetrics</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> writer: <span class=\"type\">DiskBlockObjectWriter</span> =</span><br><span class=\"line\">      blockManager.getDiskWriter(blockId, file, serInstance, fileBufferSize, spillMetrics)</span><br><span class=\"line\">    <span class=\"comment\">// List of batch sizes (bytes) in the order they are written to disk</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> batchSizes = <span class=\"keyword\">new</span> <span class=\"type\">ArrayBuffer</span>[<span class=\"type\">Long</span>]</span><br><span class=\"line\">    <span class=\"comment\">// How many elements we have in each partition</span></span><br><span class=\"line\">    <span class=\"comment\">// 用于记录每一个分区有多少条数据</span></span><br><span class=\"line\">    <span class=\"comment\">// 由于刚才数据写入文件没有写入key对应的分区，因此需要记录每一个分区有多少条数据，然后根据数据的偏移量就可以判断该数据属于哪个分区　</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> elementsPerPartition = <span class=\"keyword\">new</span> <span class=\"type\">Array</span>[<span class=\"type\">Long</span>](numPartitions)</span><br><span class=\"line\">    <span class=\"comment\">// Flush the disk writer's contents to disk, and update relevant variables.</span></span><br><span class=\"line\">    <span class=\"comment\">// The writer is committed at the end of this process.</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">flush</span></span>(): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> segment = writer.commitAndGet()</span><br><span class=\"line\">      batchSizes += segment.length</span><br><span class=\"line\">      _diskBytesSpilled += segment.length</span><br><span class=\"line\">      objectsWritten = <span class=\"number\">0</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">var</span> success = <span class=\"literal\">false</span></span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">while</span> (inMemoryIterator.hasNext) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">val</span> partitionId = inMemoryIterator.nextPartition()</span><br><span class=\"line\">        require(partitionId &gt;= <span class=\"number\">0</span> &amp;&amp; partitionId &lt; numPartitions,</span><br><span class=\"line\">          <span class=\"string\">s\"partition Id: <span class=\"subst\">$&#123;partitionId&#125;</span> should be in the range [0, <span class=\"subst\">$&#123;numPartitions&#125;</span>)\"</span>)</span><br><span class=\"line\">        inMemoryIterator.writeNext(writer)</span><br><span class=\"line\">        elementsPerPartition(partitionId) += <span class=\"number\">1</span></span><br><span class=\"line\">        objectsWritten += <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (objectsWritten == serializerBatchSize) &#123;　<span class=\"comment\">//写入　serializerBatchSize　条数据便刷新一次缓存</span></span><br><span class=\"line\">          <span class=\"comment\">// batchSize 在类中定义的如下:</span></span><br><span class=\"line\">          <span class=\"comment\">// 可以看出如果不存在配置默认为　10000　条</span></span><br><span class=\"line\">          <span class=\"comment\">// private val serializerBatchSize = conf.getLong(\"spark.shuffle.spill.batchSize\", 10000)</span></span><br><span class=\"line\">          flush()</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (objectsWritten &gt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">        flush()</span><br><span class=\"line\">      &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        writer.revertPartialWritesAndClose()</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      success = <span class=\"literal\">true</span></span><br><span class=\"line\">    &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (success) &#123;</span><br><span class=\"line\">        writer.close()</span><br><span class=\"line\">      &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// This code path only happens if an exception was thrown above before we set success;</span></span><br><span class=\"line\">        <span class=\"comment\">// close our stuff and let the exception be thrown further</span></span><br><span class=\"line\">        writer.revertPartialWritesAndClose()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (file.exists()) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">if</span> (!file.delete()) &#123;</span><br><span class=\"line\">            logWarning(<span class=\"string\">s\"Error deleting <span class=\"subst\">$&#123;file&#125;</span>\"</span>)</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">//最后记录临时文件的信息</span></span><br><span class=\"line\">    <span class=\"type\">SpilledFile</span>(file, blockId, batchSizes.toArray, elementsPerPartition)</span><br><span class=\"line\"></span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>这就是整个　insertAll()　方法:</p>\n<p> merge 达到溢出值后进行排序并写入临时文件，这里要注意的是并非所有的缓存文件都被写入到临时文件中，所以接下来需要将临时文件中的数据加上内存中的数据进行一次排序写入到一个大文件中。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//首先获取需要写入的文件:　</span></span><br><span class=\"line\">   <span class=\"keyword\">val</span> output = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)</span><br><span class=\"line\">　　<span class=\"keyword\">val</span> tmp = <span class=\"type\">Utils</span>.tempFileWith(output)</span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123; </span><br><span class=\"line\">      <span class=\"keyword\">val</span> blockId = <span class=\"type\">ShuffleBlockId</span>(dep.shuffleId, mapId, <span class=\"type\">IndexShuffleBlockResolver</span>.<span class=\"type\">NOOP_REDUCE_ID</span>)</span><br><span class=\"line\">      <span class=\"keyword\">val</span> partitionLengths = sorter.writePartitionedFile(blockId, tmp)<span class=\"comment\">//这一句是重点 下面会讲解</span></span><br><span class=\"line\">      shuffleBlockResolver.writeIndexFileAndCommit(dep.shuffleId, mapId, partitionLengths, tmp)</span><br><span class=\"line\">      mapStatus = <span class=\"type\">MapStatus</span>(blockManager.shuffleServerId, partitionLengths)</span><br><span class=\"line\">    &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (tmp.exists() &amp;&amp; !tmp.delete()) &#123;</span><br><span class=\"line\">        logError(<span class=\"string\">s\"Error while deleting temp file <span class=\"subst\">$&#123;tmp.getAbsolutePath&#125;</span>\"</span>)</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>####　writePartitionedFile</p>\n<p>继续看 writePartitionedFile :</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//  ExternalSorter 中的　writePartitionedFile　方法</span></span><br><span class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">writePartitionedFile</span></span>(</span><br><span class=\"line\">      blockId: <span class=\"type\">BlockId</span>,</span><br><span class=\"line\">      outputFile: <span class=\"type\">File</span>): <span class=\"type\">Array</span>[<span class=\"type\">Long</span>] = &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// Track location of each range in the output file</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> lengths = <span class=\"keyword\">new</span> <span class=\"type\">Array</span>[<span class=\"type\">Long</span>](numPartitions)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> writer = blockManager.getDiskWriter(blockId, outputFile, serInstance, fileBufferSize,</span><br><span class=\"line\">      context.taskMetrics().shuffleWriteMetrics)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 这里的spills 就是　刚才产生的临时文件的数据　如果为空逻辑就比较简单，直接进行排序并写入文件</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (spills.isEmpty) &#123;</span><br><span class=\"line\">      <span class=\"comment\">// Case where we only have in-memory data</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> collection = <span class=\"keyword\">if</span> (aggregator.isDefined) map <span class=\"keyword\">else</span> buffer</span><br><span class=\"line\">      <span class=\"keyword\">val</span> it = collection.destructiveSortedWritablePartitionedIterator(comparator)　<span class=\"comment\">//这里同样使用　destructiveSortedWritablePartitionedIterator　进行排序</span></span><br><span class=\"line\">      <span class=\"keyword\">while</span> (it.hasNext) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">val</span> partitionId = it.nextPartition()</span><br><span class=\"line\">        <span class=\"keyword\">while</span> (it.hasNext &amp;&amp; it.nextPartition() == partitionId) &#123;</span><br><span class=\"line\">          it.writeNext(writer)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">val</span> segment = writer.commitAndGet()</span><br><span class=\"line\">        lengths(partitionId) = segment.length</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;<span class=\"comment\">//重点看不为空的时候，这里调用了　partitionedIterator　方法</span></span><br><span class=\"line\">      <span class=\"comment\">// We must perform merge-sort; get an iterator by partition and write everything directly.</span></span><br><span class=\"line\">      <span class=\"keyword\">for</span> ((id, elements) &lt;- <span class=\"keyword\">this</span>.partitionedIterator) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (elements.hasNext) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">for</span> (elem &lt;- elements) &#123;</span><br><span class=\"line\">            writer.write(elem._1, elem._2)</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">          <span class=\"keyword\">val</span> segment = writer.commitAndGet()</span><br><span class=\"line\">          lengths(id) = segment.length</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    writer.close()</span><br><span class=\"line\">    context.taskMetrics().incMemoryBytesSpilled(memoryBytesSpilled)</span><br><span class=\"line\">    context.taskMetrics().incDiskBytesSpilled(diskBytesSpilled)</span><br><span class=\"line\">    context.taskMetrics().incPeakExecutionMemory(peakMemoryUsedBytes)</span><br><span class=\"line\"></span><br><span class=\"line\">    lengths</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>然后是 partitionedIterator 方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">partitionedIterator</span></span>: <span class=\"type\">Iterator</span>[(<span class=\"type\">Int</span>, <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]])] = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> usingMap = aggregator.isDefined</span><br><span class=\"line\">    <span class=\"keyword\">val</span> collection: <span class=\"type\">WritablePartitionedPairCollection</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>] = <span class=\"keyword\">if</span> (usingMap) map <span class=\"keyword\">else</span> buffer</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (spills.isEmpty) &#123;<span class=\"comment\">// 这里又一次判断了是否为空，直接看有临时文件的部分</span></span><br><span class=\"line\">      <span class=\"comment\">// Special case: if we have only in-memory data, we don't need to merge streams, and perhaps</span></span><br><span class=\"line\">      <span class=\"comment\">// we don't even need to sort by anything other than partition ID</span></span><br><span class=\"line\">      <span class=\"keyword\">if</span> (!ordering.isDefined) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// The user hasn't requested sorted keys, so only sort by partition ID, not key</span></span><br><span class=\"line\">   groupByPartition(destructiveIterator(collection.partitionedDestructiveSortedIterator(<span class=\"type\">None</span>)))</span><br><span class=\"line\">      &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// We do need to sort by both partition ID and key</span></span><br><span class=\"line\">        groupByPartition(destructiveIterator(</span><br><span class=\"line\">          collection.partitionedDestructiveSortedIterator(<span class=\"type\">Some</span>(keyComparator))))</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">      <span class=\"comment\">// Merge spilled and in-memory data</span></span><br><span class=\"line\">      <span class=\"comment\">// 这里传入了临时文件　spills　和　排序后的缓存文件</span></span><br><span class=\"line\">      merge(spills, destructiveIterator(</span><br><span class=\"line\">        collection.partitionedDestructiveSortedIterator(comparator)))</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>merge 方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// merge方法</span></span><br><span class=\"line\">  <span class=\"comment\">// inMemory　是根据(partion,hash(k)) 排序后的内存数据</span></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">merge</span></span>(spills: <span class=\"type\">Seq</span>[<span class=\"type\">SpilledFile</span>], inMemory: <span class=\"type\">Iterator</span>[((<span class=\"type\">Int</span>, <span class=\"type\">K</span>), <span class=\"type\">C</span>)])</span><br><span class=\"line\">     : <span class=\"type\">Iterator</span>[(<span class=\"type\">Int</span>, <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]])] = &#123;</span><br><span class=\"line\">   <span class=\"comment\">//　将所有缓存文件转化为 SpillReader </span></span><br><span class=\"line\">   <span class=\"keyword\">val</span> readers = spills.map(<span class=\"keyword\">new</span> <span class=\"type\">SpillReader</span>(_))</span><br><span class=\"line\">   <span class=\"comment\">// buffered方法只是比普通的　itertor 方法多提供一个　head　方法，例如:</span></span><br><span class=\"line\">   <span class=\"comment\">// val a = List(1,2,3,4,5)</span></span><br><span class=\"line\">   <span class=\"comment\">// val b = a.iterator.buffered</span></span><br><span class=\"line\">   <span class=\"comment\">// b.head : 1</span></span><br><span class=\"line\">   <span class=\"comment\">// b.next : 1</span></span><br><span class=\"line\">   <span class=\"comment\">// b.head : 2</span></span><br><span class=\"line\">   <span class=\"comment\">// b.next : 2</span></span><br><span class=\"line\">   <span class=\"keyword\">val</span> inMemBuffered = inMemory.buffered</span><br><span class=\"line\">   (<span class=\"number\">0</span> until numPartitions).iterator.map &#123; p =&gt;</span><br><span class=\"line\">     <span class=\"comment\">// 获得分区对应数据的迭代器 </span></span><br><span class=\"line\">     <span class=\"comment\">// 这里的　IteratorForPartition　就是得到分区 p 对应的数据的迭代器，逻辑比较简单，就不单独拿出来分析</span></span><br><span class=\"line\">     <span class=\"keyword\">val</span> inMemIterator = <span class=\"keyword\">new</span> <span class=\"type\">IteratorForPartition</span>(p, inMemBuffered)</span><br><span class=\"line\">     </span><br><span class=\"line\">     <span class=\"comment\">//这里获取所有文件的第　p　个分区　并与内存中的第　p 个分区进行合并</span></span><br><span class=\"line\">     <span class=\"keyword\">val</span> iterators = readers.map(_.readNextPartition()) ++ <span class=\"type\">Seq</span>(inMemIterator)</span><br><span class=\"line\">　　　<span class=\"comment\">// 这里的　iterators　是由　多个 iterator 组成的，其中每一个都是关于 p 分区数据的 iterator</span></span><br><span class=\"line\">     <span class=\"keyword\">if</span> (aggregator.isDefined) &#123;</span><br><span class=\"line\">       <span class=\"comment\">// Perform partial aggregation across partitions</span></span><br><span class=\"line\">       (p, mergeWithAggregation(</span><br><span class=\"line\">         iterators, aggregator.get.mergeCombiners, keyComparator, ordering.isDefined))</span><br><span class=\"line\">     &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (ordering.isDefined) &#123;</span><br><span class=\"line\">       <span class=\"comment\">// No aggregator given, but we have an ordering (e.g. used by reduce tasks in sortByKey);</span></span><br><span class=\"line\">       <span class=\"comment\">// sort the elements without trying to merge them</span></span><br><span class=\"line\">       (p, mergeSort(iterators, ordering.get))</span><br><span class=\"line\">     &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">       (p, iterators.iterator.flatten)</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">   &#125;</span><br></pre></td></tr></table></figure>\n<p>mergeWithAggregation<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">mergeWithAggregation</span></span>(</span><br><span class=\"line\">    iterators: <span class=\"type\">Seq</span>[<span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]]],</span><br><span class=\"line\">    mergeCombiners: (<span class=\"type\">C</span>, <span class=\"type\">C</span>) =&gt; <span class=\"type\">C</span>,</span><br><span class=\"line\">    comparator: <span class=\"type\">Comparator</span>[<span class=\"type\">K</span>],</span><br><span class=\"line\">    totalOrder: <span class=\"type\">Boolean</span>)</span><br><span class=\"line\">    : <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] =</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (!totalOrder) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">Iterator</span>[<span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]]] &#123;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> sorted = mergeSort(iterators, comparator).buffered</span><br><span class=\"line\">      <span class=\"keyword\">val</span> keys = <span class=\"keyword\">new</span> <span class=\"type\">ArrayBuffer</span>[<span class=\"type\">K</span>]</span><br><span class=\"line\">      <span class=\"keyword\">val</span> combiners = <span class=\"keyword\">new</span> <span class=\"type\">ArrayBuffer</span>[<span class=\"type\">C</span>]</span><br><span class=\"line\">      <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hasNext</span></span>: <span class=\"type\">Boolean</span> = sorted.hasNext</span><br><span class=\"line\">      <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">next</span></span>(): <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] = &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!hasNext) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">NoSuchElementException</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        keys.clear()</span><br><span class=\"line\">        combiners.clear()</span><br><span class=\"line\">        <span class=\"keyword\">val</span> firstPair = sorted.next()</span><br><span class=\"line\">        keys += firstPair._1</span><br><span class=\"line\">        combiners += firstPair._2</span><br><span class=\"line\">        <span class=\"keyword\">val</span> key = firstPair._1</span><br><span class=\"line\">        <span class=\"keyword\">while</span> (sorted.hasNext &amp;&amp; comparator.compare(sorted.head._1, key) == <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">val</span> pair = sorted.next()</span><br><span class=\"line\">          <span class=\"keyword\">var</span> i = <span class=\"number\">0</span></span><br><span class=\"line\">          <span class=\"keyword\">var</span> foundKey = <span class=\"literal\">false</span></span><br><span class=\"line\">          <span class=\"keyword\">while</span> (i &lt; keys.size &amp;&amp; !foundKey) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (keys(i) == pair._1) &#123;</span><br><span class=\"line\">              combiners(i) = mergeCombiners(combiners(i), pair._2)</span><br><span class=\"line\">              foundKey = <span class=\"literal\">true</span></span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            i += <span class=\"number\">1</span></span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">          <span class=\"keyword\">if</span> (!foundKey) &#123;</span><br><span class=\"line\">            keys += pair._1</span><br><span class=\"line\">            combiners += pair._2</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        keys.iterator.zip(combiners.iterator)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;.flatMap(i =&gt; i)</span><br><span class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">// We have a total ordering, so the objects with the same key are sequential.</span></span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] &#123;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> sorted = mergeSort(iterators, comparator).buffered</span><br><span class=\"line\">      <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hasNext</span></span>: <span class=\"type\">Boolean</span> = sorted.hasNext</span><br><span class=\"line\">      <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">next</span></span>(): <span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>] = &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!hasNext) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">NoSuchElementException</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">val</span> elem = sorted.next()</span><br><span class=\"line\">        <span class=\"keyword\">val</span> k = elem._1</span><br><span class=\"line\">        <span class=\"keyword\">var</span> c = elem._2</span><br><span class=\"line\">        <span class=\"comment\">// 这里的意思是可能存在多个相同的key 分布在不同临时文件或内存中</span></span><br><span class=\"line\">        <span class=\"comment\">// 所以还需要将不同的 key 对应的值进行合并 </span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> (sorted.hasNext &amp;&amp; sorted.head._1 == k) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">val</span> pair = sorted.next()</span><br><span class=\"line\">          c = mergeCombiners(c, pair._2)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        (k, c)<span class=\"comment\">//返回</span></span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>继续看 mergeSort 方法，首先选除头元素最小对应那个分区，然后选出这个分区的第一个元素，那么这个元素一定是最小的，然后将剩余的元素放回去:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">mergeSort</span></span>(iterators: <span class=\"type\">Seq</span>[<span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]]], comparator: <span class=\"type\">Comparator</span>[<span class=\"type\">K</span>])</span><br><span class=\"line\">    : <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] =</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  <span class=\"keyword\">val</span> bufferedIters = iterators.filter(_.hasNext).map(_.buffered)</span><br><span class=\"line\">  <span class=\"class\"><span class=\"keyword\">type</span> <span class=\"title\">Iter</span> </span>= <span class=\"type\">BufferedIterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]]</span><br><span class=\"line\">  <span class=\"comment\">//选取头元素最小的分区</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> heap = <span class=\"keyword\">new</span> mutable.<span class=\"type\">PriorityQueue</span>[<span class=\"type\">Iter</span>]()(<span class=\"keyword\">new</span> <span class=\"type\">Ordering</span>[<span class=\"type\">Iter</span>] &#123;</span><br><span class=\"line\">    <span class=\"comment\">// Use the reverse of comparator.compare because PriorityQueue dequeues the max</span></span><br><span class=\"line\">    <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compare</span></span>(x: <span class=\"type\">Iter</span>, y: <span class=\"type\">Iter</span>): <span class=\"type\">Int</span> = -comparator.compare(x.head._1, y.head._1)</span><br><span class=\"line\">  &#125;)</span><br><span class=\"line\">  heap.enqueue(bufferedIters: _*) </span><br><span class=\"line\">  <span class=\"keyword\">new</span> <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] &#123;</span><br><span class=\"line\">    <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hasNext</span></span>: <span class=\"type\">Boolean</span> = !heap.isEmpty</span><br><span class=\"line\">    <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">next</span></span>(): <span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>] = &#123;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (!hasNext) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">NoSuchElementException</span></span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> firstBuf = heap.dequeue()</span><br><span class=\"line\">      <span class=\"keyword\">val</span> firstPair = firstBuf.next()</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (firstBuf.hasNext) &#123;</span><br><span class=\"line\">        heap.enqueue(firstBuf)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      firstPair</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>接下来就使用 writeIndexFileAndCommit 将每一个分区的数据条数写入索引文件，便于 reduce 端获取。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>shuflle write 相比起 shuffe read 要复杂很多，因此本文的篇幅也较长。其实在 shuffle write 有两个较为重要的方法，其中一个是 insertAll。这个方法的作用就是将数据缓存到一个可以添加的 hash 表中。如果表占用的内存达到的一定值，就对其进行排序。排序方式先按照分区进行排序，如果分区相同则按key的hash进行排序，随后将排序后的数据写入临时文件中，这个过程可能会生成多个临时文件。</p>\n<p>然后 writePartitionedFile 的作用是最后将内存中的数据和临时文件中的数据进行全部排序，写入到一个大文件中。</p>\n<p>最后将每一个分区对应的索引数据写入文件。整个shuffle write 阶段就完成了</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"写在开始的话\"><a href=\"#写在开始的话\" class=\"headerlink\" title=\"写在开始的话\"></a>写在开始的话</h2><p>shuffle 作为 spark 最重要的一部分，也是很多初学者感到头痛的一部分。简单来说，shuffle 分为两个部分 : shuffle write 和 shuffle read。shuffle write 在 map 端进行，而shuffle read 在 reduce 端进行。本章就准备就 shuffle write 做一个详细的分析。</p>\n<h2 id=\"为什么需要shuffle\"><a href=\"#为什么需要shuffle\" class=\"headerlink\" title=\"为什么需要shuffle\"></a>为什么需要shuffle</h2><p>在上一章<a href=\"https://lishion.github.io/2018/06/06/Spark-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E8%AE%A1%E5%88%92-%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86-%E8%BF%AD%E4%BB%A3%E8%AE%A1%E7%AE%97/\" target=\"_blank\" rel=\"noopener\">Spark-源码阅读计划-第一部分-迭代计算</a>提到过每一个分区的数据最后都会由一个task来处理，那么当task 执行的类似于 reduceByKey 这种算子的时候一定需要满足相同的 key 在同一个分区这个条件，否则就不能按照 key 进行reduce了。但是当我们从数据源获取数据并进行处理后，相同的 key 不一定在同一个分区。那么在一个 map 端将相同的key 使用某种方式聚集在同一个分区并写入临时文件的过程就称为 shuffle write；而在 reduce 从 map 拉去执行 task 所需要的数据就是 shuffle read；这两个步骤组成了shuffle。</p>\n<h2 id=\"一个小例子\"><a href=\"#一个小例子\" class=\"headerlink\" title=\"一个小例子\"></a>一个小例子</h2><p>本章同样以一个小例子作为本章讲解的中心，例如一个统计词频的代码:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sc.parallelize(<span class=\"type\">List</span>(<span class=\"string\">\"hello\"</span>,<span class=\"string\">\"world\"</span>,<span class=\"string\">\"hello\"</span>,<span class=\"string\">\"spark\"</span>)).map(x=&gt;(x,<span class=\"number\">1</span>)).reduceByKey(_+_)</span><br></pre></td></tr></table></figure>\n<p>相信有一点基础的同学都知道 reduceByKey 这算子会产生 shuffle。但是大部分的同学都不知 shuffle 是什么时候产生的，shuffle 到底做了些什么，这也是这一章的重点。</p>\n<h2 id=\"shuffle-task\"><a href=\"#shuffle-task\" class=\"headerlink\" title=\"shuffle task\"></a>shuffle task</h2><p>在上一章提到过当 job 提交后会生成 result task。而在需要 shuffle 则会生成 ShuffleMapTask。这里涉及到 job 提交以及 stage 生成的的知识，将会在下一章提到。这里我们直接开始看 ShuffleMapTask 中的 runtask :</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">runTask</span></span>(context: <span class=\"type\">TaskContext</span>): <span class=\"type\">MapStatus</span> = &#123;</span><br><span class=\"line\">   <span class=\"comment\">// Deserialize the RDD using the broadcast variable.</span></span><br><span class=\"line\">...</span><br><span class=\"line\">   <span class=\"keyword\">var</span> writer: <span class=\"type\">ShuffleWriter</span>[<span class=\"type\">Any</span>, <span class=\"type\">Any</span>] = <span class=\"literal\">null</span></span><br><span class=\"line\">   <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">     <span class=\"keyword\">val</span> manager = <span class=\"type\">SparkEnv</span>.get.shuffleManager</span><br><span class=\"line\">     writer = manager.getWriter[<span class=\"type\">Any</span>, <span class=\"type\">Any</span>](dep.shuffleHandle, partitionId, context)</span><br><span class=\"line\">      <span class=\"comment\">// 这里依然是rdd调用iterator方法的地方</span></span><br><span class=\"line\">     writer.write(rdd.iterator(partition, context).asInstanceOf[<span class=\"type\">Iterator</span>[_ &lt;: <span class=\"type\">Product2</span>[<span class=\"type\">Any</span>, <span class=\"type\">Any</span>]]])</span><br><span class=\"line\">     writer.stop(success = <span class=\"literal\">true</span>).get</span><br><span class=\"line\">   &#125; <span class=\"keyword\">catch</span> &#123;</span><br><span class=\"line\">     <span class=\"keyword\">case</span> e: <span class=\"type\">Exception</span> =&gt;</span><br><span class=\"line\">       <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">         <span class=\"keyword\">if</span> (writer != <span class=\"literal\">null</span>) &#123;</span><br><span class=\"line\">           writer.stop(success = <span class=\"literal\">false</span>)</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">       &#125; <span class=\"keyword\">catch</span> &#123;</span><br><span class=\"line\">         <span class=\"keyword\">case</span> e: <span class=\"type\">Exception</span> =&gt;</span><br><span class=\"line\">           log.debug(<span class=\"string\">\"Could not stop writer\"</span>, e)</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">       <span class=\"keyword\">throw</span> e</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>可以看到首先获得一个 shuffleWriter。spark 中有许多不同类型的 writer ，默认使用的是 SortShuffleWriter。SortShuffleWriter 中的 write 方法实现了 shuffle write。源代码如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">write</span></span>(records: <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>]]): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">    <span class=\"comment\">//如果需要 map 端聚</span></span><br><span class=\"line\">  sorter = <span class=\"keyword\">if</span> (dep.mapSideCombine) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">ExternalSorter</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>, <span class=\"type\">C</span>](</span><br><span class=\"line\">      context, dep.aggregator, <span class=\"type\">Some</span>(dep.partitioner), dep.keyOrdering, dep.serializer)</span><br><span class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">ExternalSorter</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>, <span class=\"type\">V</span>](</span><br><span class=\"line\">      context, aggregator = <span class=\"type\">None</span>, <span class=\"type\">Some</span>(dep.partitioner), ordering = <span class=\"type\">None</span>, dep.serializer)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  sorter.insertAll(records)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> output = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> tmp = <span class=\"type\">Utils</span>.tempFileWith(output)</span><br><span class=\"line\">  <span class=\"keyword\">try</span> &#123; </span><br><span class=\"line\">    <span class=\"keyword\">val</span> blockId = <span class=\"type\">ShuffleBlockId</span>(dep.shuffleId, mapId, <span class=\"type\">IndexShuffleBlockResolver</span>.<span class=\"type\">NOOP_REDUCE_ID</span>)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> partitionLengths = sorter.writePartitionedFile(blockId, tmp)</span><br><span class=\"line\">    shuffleBlockResolver.writeIndexFileAndCommit(dep.shuffleId, mapId, partitionLengths, tmp)</span><br><span class=\"line\">    mapStatus = <span class=\"type\">MapStatus</span>(blockManager.shuffleServerId, partitionLengths)</span><br><span class=\"line\">  &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (tmp.exists() &amp;&amp; !tmp.delete()) &#123;</span><br><span class=\"line\">      logError(<span class=\"string\">s\"Error while deleting temp file <span class=\"subst\">$&#123;tmp.getAbsolutePath&#125;</span>\"</span>)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>使用了 ExternalSorter 的 insertAll 方法插入了上一个 RDD 生成的数据的迭代器。</p>\n<h4 id=\"insertAll\"><a href=\"#insertAll\" class=\"headerlink\" title=\"insertAll\"></a>insertAll</h4><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">insertAll</span></span>(records: <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>]]): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">    <span class=\"comment\">// <span class=\"doctag\">TODO:</span> stop combining if we find that the reduction factor isn't high</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> shouldCombine = aggregator.isDefined</span><br><span class=\"line\">    <span class=\"comment\">// 直接看需要 map 端聚合的情况</span></span><br><span class=\"line\">    <span class=\"comment\">// 不需要聚合的情况类似</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (shouldCombine) &#123;</span><br><span class=\"line\">      <span class=\"comment\">// Combine values in-memory first using our AppendOnlyMap</span></span><br><span class=\"line\">       </span><br><span class=\"line\">      <span class=\"keyword\">val</span> mergeValue = aggregator.get.mergeValue </span><br><span class=\"line\">      <span class=\"keyword\">val</span> createCombiner = aggregator.get.createCombiner</span><br><span class=\"line\">      <span class=\"keyword\">var</span> kv: <span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>] = <span class=\"literal\">null</span></span><br><span class=\"line\">      <span class=\"comment\">// 如果 key 不存在则直接创建 Combiner ，否则使用 mergeValue 将 value 添加到创建的 Combiner中</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> update = (hadValue: <span class=\"type\">Boolean</span>, oldValue: <span class=\"type\">C</span>) =&gt; &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (hadValue) mergeValue(oldValue, kv._2) <span class=\"keyword\">else</span> createCombiner(kv._2)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      <span class=\"keyword\">while</span> (records.hasNext) &#123;</span><br><span class=\"line\">        addElementsRead()</span><br><span class=\"line\">        kv = records.next()</span><br><span class=\"line\">        <span class=\"comment\">// 更新值</span></span><br><span class=\"line\">        <span class=\"comment\">// 这里需要注意的是之前的数据是(k,v)类型，这里转换成了((part,k),v)</span></span><br><span class=\"line\">        <span class=\"comment\">// 其中 part 是 key 对应的分区</span></span><br><span class=\"line\">        map.changeValue((getPartition(kv._1), kv._1), update)</span><br><span class=\"line\">        maybeSpillCollection(usingMap = <span class=\"literal\">true</span>)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">      <span class=\"comment\">// Stick values into our buffer</span></span><br><span class=\"line\">      <span class=\"keyword\">while</span> (records.hasNext) &#123;</span><br><span class=\"line\">        addElementsRead()</span><br><span class=\"line\">        <span class=\"keyword\">val</span> kv = records.next()</span><br><span class=\"line\">        buffer.insert(getPartition(kv._1), kv._1, kv._2.asInstanceOf[<span class=\"type\">C</span>])</span><br><span class=\"line\">        maybeSpillCollection(usingMap = <span class=\"literal\">false</span>)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>在这里使用 getPartition() 进行了数据的<strong>分区</strong>，也就是将不同的 key 与其分区对应起来。默认使用的是 HashPartitioner ，有兴趣的同学可以去阅读源码。再继续看 changeValue 方法。在这里使用非常重要的数据结构来存放分区后的数据，也就是代码中的 map 类型为 PartitionedAppendOnlyMap。其中是使用hask表存放数据，并且存放的方式为表中的偶数索index引存放key, index + 1 存放对应的value。例如:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">k1|v1|null|k2|v2|k3|v3|null|....</span><br></pre></td></tr></table></figure>\n<p>之所以里面由空值，是因为在插入的时候使用了二次探测法。来看一看 changeValue 方法:</p>\n<p>PartitionedAppendOnlyMap 中的  changeValue 方法继承自 </p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">changeValue</span></span>(key: <span class=\"type\">K</span>, updateFunc: (<span class=\"type\">Boolean</span>, <span class=\"type\">V</span>) =&gt; <span class=\"type\">V</span>): <span class=\"type\">V</span> = &#123;</span><br><span class=\"line\">  assert(!destroyed, destructionMessage)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> k = key.asInstanceOf[<span class=\"type\">AnyRef</span>]</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (k.eq(<span class=\"literal\">null</span>)) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (!haveNullValue) &#123;</span><br><span class=\"line\">      incrementSize()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    nullValue = updateFunc(haveNullValue, nullValue)</span><br><span class=\"line\">    haveNullValue = <span class=\"literal\">true</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> nullValue</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"keyword\">var</span> pos = rehash(k.hashCode) &amp; mask</span><br><span class=\"line\">  <span class=\"keyword\">var</span> i = <span class=\"number\">1</span></span><br><span class=\"line\">  <span class=\"keyword\">while</span> (<span class=\"literal\">true</span>) &#123;</span><br><span class=\"line\">    <span class=\"comment\">// 偶数位置为 key</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> curKey = data(<span class=\"number\">2</span> * pos)</span><br><span class=\"line\">     <span class=\"comment\">// 如果该key不存在，就直接插入</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (curKey.eq(<span class=\"literal\">null</span>)) &#123;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> newValue = updateFunc(<span class=\"literal\">false</span>, <span class=\"literal\">null</span>.asInstanceOf[<span class=\"type\">V</span>])</span><br><span class=\"line\">      data(<span class=\"number\">2</span> * pos) = k</span><br><span class=\"line\">      data(<span class=\"number\">2</span> * pos + <span class=\"number\">1</span>) = newValue.asInstanceOf[<span class=\"type\">AnyRef</span>]</span><br><span class=\"line\">      incrementSize()</span><br><span class=\"line\">      <span class=\"keyword\">return</span> newValue</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (k.eq(curKey) || k.equals(curKey)) &#123;<span class=\"comment\">//如果 key 存在，则进行聚合</span></span><br><span class=\"line\">      </span><br><span class=\"line\">      <span class=\"keyword\">val</span> newValue = updateFunc(<span class=\"literal\">true</span>, data(<span class=\"number\">2</span> * pos + <span class=\"number\">1</span>).asInstanceOf[<span class=\"type\">V</span>])</span><br><span class=\"line\">      data(<span class=\"number\">2</span> * pos + <span class=\"number\">1</span>) = newValue.asInstanceOf[<span class=\"type\">AnyRef</span>]</span><br><span class=\"line\">      <span class=\"keyword\">return</span> newValue</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123; <span class=\"comment\">// 否则进行下一次探测</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> delta = i</span><br><span class=\"line\">      pos = (pos + delta) &amp; mask</span><br><span class=\"line\">      i += <span class=\"number\">1</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"literal\">null</span>.asInstanceOf[<span class=\"type\">V</span>] <span class=\"comment\">// Never reached but needed to keep compiler happy</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>再调用 insertAll 进行了数据的聚合|插入之后，调用了 maybeSpillCollection 方法。这个方法的作用是判断 map 占用了内存，如果内存达到一定的值，则将内存中的数据 spill 到临时文件中。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">maybeSpillCollection</span></span>(usingMap: <span class=\"type\">Boolean</span>): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">var</span> estimatedSize = <span class=\"number\">0</span>L</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (usingMap) &#123;</span><br><span class=\"line\">      estimatedSize = map.estimateSize()</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (maybeSpill(map, estimatedSize)) &#123;</span><br><span class=\"line\">        map = <span class=\"keyword\">new</span> <span class=\"type\">PartitionedAppendOnlyMap</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">      estimatedSize = buffer.estimateSize()</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (maybeSpill(buffer, estimatedSize)) &#123;</span><br><span class=\"line\">        buffer = <span class=\"keyword\">new</span> <span class=\"type\">PartitionedPairBuffer</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (estimatedSize &gt; _peakMemoryUsedBytes) &#123;</span><br><span class=\"line\">      _peakMemoryUsedBytes = estimatedSize</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>maybeSpillCollection 继续调用了 maybeSpill 方法，这个方法继承自 Spillable 中</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">protected</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">maybeSpill</span></span>(collection: <span class=\"type\">C</span>, currentMemory: <span class=\"type\">Long</span>): <span class=\"type\">Boolean</span> = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">var</span> shouldSpill = <span class=\"literal\">false</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (elementsRead % <span class=\"number\">32</span> == <span class=\"number\">0</span> &amp;&amp; currentMemory &gt;= myMemoryThreshold) &#123;</span><br><span class=\"line\">      <span class=\"comment\">// Claim up to double our current memory from the shuffle memory pool</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> amountToRequest = <span class=\"number\">2</span> * currentMemory - myMemoryThreshold</span><br><span class=\"line\">      <span class=\"keyword\">val</span> granted = acquireMemory(amountToRequest)</span><br><span class=\"line\">      myMemoryThreshold += granted</span><br><span class=\"line\">      <span class=\"comment\">// If we were granted too little memory to grow further (either tryToAcquire returned 0,</span></span><br><span class=\"line\">      <span class=\"comment\">// or we already had more memory than myMemoryThreshold), spill the current collection</span></span><br><span class=\"line\">      shouldSpill = currentMemory &gt;= myMemoryThreshold</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    shouldSpill = shouldSpill || _elementsRead &gt; numElementsForceSpillThreshold</span><br><span class=\"line\">    <span class=\"comment\">// Actually spill</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (shouldSpill) &#123;</span><br><span class=\"line\">      _spillCount += <span class=\"number\">1</span></span><br><span class=\"line\">      logSpillage(currentMemory)</span><br><span class=\"line\">      spill(collection)</span><br><span class=\"line\">      _elementsRead = <span class=\"number\">0</span></span><br><span class=\"line\">      _memoryBytesSpilled += currentMemory</span><br><span class=\"line\">      releaseMemory()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    shouldSpill</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>该方法首先判断了是否需要 spill，如果需要则调用 spill() 方法将内存中的数据写入临时文件中。spill 方法如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"keyword\">protected</span>[<span class=\"keyword\">this</span>] <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">spill</span></span>(collection: <span class=\"type\">WritablePartitionedPairCollection</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">   <span class=\"keyword\">val</span> inMemoryIterator = collection.destructiveSortedWritablePartitionedIterator(comparator)</span><br><span class=\"line\">   <span class=\"keyword\">val</span> spillFile = spillMemoryIteratorToDisk(inMemoryIterator)</span><br><span class=\"line\">   spills += spillFile</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>其中的  comparator 如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">comparator</span></span>: <span class=\"type\">Option</span>[<span class=\"type\">Comparator</span>[<span class=\"type\">K</span>]] = &#123;</span><br><span class=\"line\">   <span class=\"keyword\">if</span> (ordering.isDefined || aggregator.isDefined) &#123;</span><br><span class=\"line\">     <span class=\"type\">Some</span>(keyComparator)</span><br><span class=\"line\">   &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">     <span class=\"type\">None</span></span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>也就是:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"keyword\">val</span> keyComparator: <span class=\"type\">Comparator</span>[<span class=\"type\">K</span>] = ordering.getOrElse(<span class=\"keyword\">new</span> <span class=\"type\">Comparator</span>[<span class=\"type\">K</span>] &#123;</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compare</span></span>(a: <span class=\"type\">K</span>, b: <span class=\"type\">K</span>): <span class=\"type\">Int</span> = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> h1 = <span class=\"keyword\">if</span> (a == <span class=\"literal\">null</span>) <span class=\"number\">0</span> <span class=\"keyword\">else</span> a.hashCode()</span><br><span class=\"line\">    <span class=\"keyword\">val</span> h2 = <span class=\"keyword\">if</span> (b == <span class=\"literal\">null</span>) <span class=\"number\">0</span> <span class=\"keyword\">else</span> b.hashCode()</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (h1 &lt; h2) <span class=\"number\">-1</span> <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (h1 == h2) <span class=\"number\">0</span> <span class=\"keyword\">else</span> <span class=\"number\">1</span></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;)</span><br></pre></td></tr></table></figure>\n<p>destructiveSortedWritablePartitionedIterator 位于 WritablePartitionedPairCollection 类中，实现如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">destructiveSortedWritablePartitionedIterator</span></span>(keyComparator: <span class=\"type\">Option</span>[<span class=\"type\">Comparator</span>[<span class=\"type\">K</span>]])</span><br><span class=\"line\">    : <span class=\"type\">WritablePartitionedIterator</span> = &#123;</span><br><span class=\"line\">    <span class=\"comment\">//这里调用的其实是　PartitionedAppendOnlyMap　中重写的　partitionedDestructiveSortedIterator    </span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> it = partitionedDestructiveSortedIterator(keyComparator)</span><br><span class=\"line\">    <span class=\"comment\">// 这里还实现了一个　writeNext　的方法，后面会用到</span></span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">WritablePartitionedIterator</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">private</span>[<span class=\"keyword\">this</span>] <span class=\"keyword\">var</span> cur = <span class=\"keyword\">if</span> (it.hasNext) it.next() <span class=\"keyword\">else</span> <span class=\"literal\">null</span></span><br><span class=\"line\">       <span class=\"comment\">// 在map 中的数据其实是((partition,k),v)</span></span><br><span class=\"line\">       <span class=\"comment\">// 这里只写入了(k,v)</span></span><br><span class=\"line\">      <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">writeNext</span></span>(writer: <span class=\"type\">DiskBlockObjectWriter</span>): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">        writer.write(cur._1._2, cur._2)</span><br><span class=\"line\">        cur = <span class=\"keyword\">if</span> (it.hasNext) it.next() <span class=\"keyword\">else</span> <span class=\"literal\">null</span></span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hasNext</span></span>(): <span class=\"type\">Boolean</span> = cur != <span class=\"literal\">null</span></span><br><span class=\"line\">      <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">nextPartition</span></span>(): <span class=\"type\">Int</span> = cur._1._1</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>接下来看看partitionedDestructiveSortedIterator:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">partitionedDestructiveSortedIterator</span></span>(keyComparator: <span class=\"type\">Option</span>[<span class=\"type\">Comparator</span>[<span class=\"type\">K</span>]])</span><br><span class=\"line\">    : <span class=\"type\">Iterator</span>[((<span class=\"type\">Int</span>, <span class=\"type\">K</span>), <span class=\"type\">V</span>)] = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> comparator = keyComparator.map(partitionKeyComparator).getOrElse(partitionComparator)</span><br><span class=\"line\">    destructiveSortedIterator(comparator)</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>keyComparator.map(partitionKeyComparator)　等于 partitionKeyComparator(keyComparator)，再看下面的partitionKeyComparator，说明比较器是先对分区进行比较，如果分区相同，再对key的hash值进行比较:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">partitionKeyComparator</span></span>[<span class=\"type\">K</span>](keyComparator: <span class=\"type\">Comparator</span>[<span class=\"type\">K</span>]): <span class=\"type\">Comparator</span>[(<span class=\"type\">Int</span>, <span class=\"type\">K</span>)] = &#123;</span><br><span class=\"line\">   <span class=\"keyword\">new</span> <span class=\"type\">Comparator</span>[(<span class=\"type\">Int</span>, <span class=\"type\">K</span>)] &#123;</span><br><span class=\"line\">     <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compare</span></span>(a: (<span class=\"type\">Int</span>, <span class=\"type\">K</span>), b: (<span class=\"type\">Int</span>, <span class=\"type\">K</span>)): <span class=\"type\">Int</span> = &#123;</span><br><span class=\"line\">       <span class=\"keyword\">val</span> partitionDiff = a._1 - b._1</span><br><span class=\"line\">       <span class=\"keyword\">if</span> (partitionDiff != <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">         partitionDiff</span><br><span class=\"line\">       &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">         keyComparator.compare(a._2, b._2)</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>然后看destructiveSortedIterator(),这是在 PartitionedAppendOnlyMap 的父类 destructiveSortedIterator 中实现的:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">destructiveSortedIterator</span></span>(keyComparator: <span class=\"type\">Comparator</span>[<span class=\"type\">K</span>]): <span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">V</span>)] = &#123;</span><br><span class=\"line\">   destroyed = <span class=\"literal\">true</span></span><br><span class=\"line\">   <span class=\"comment\">// Pack KV pairs into the front of the underlying array</span></span><br><span class=\"line\">   <span class=\"comment\">// 这里是因为　PartitionedAppendOnlyMap　采用的二次探测法　,kv对之间存在　null值，所以先把非空的kv对全部移动到hash表的前端</span></span><br><span class=\"line\">   <span class=\"keyword\">var</span> keyIndex, newIndex = <span class=\"number\">0</span></span><br><span class=\"line\">   <span class=\"keyword\">while</span> (keyIndex &lt; capacity) &#123;</span><br><span class=\"line\">     <span class=\"keyword\">if</span> (data(<span class=\"number\">2</span> * keyIndex) != <span class=\"literal\">null</span>) &#123;</span><br><span class=\"line\">       data(<span class=\"number\">2</span> * newIndex) = data(<span class=\"number\">2</span> * keyIndex)</span><br><span class=\"line\">       data(<span class=\"number\">2</span> * newIndex + <span class=\"number\">1</span>) = data(<span class=\"number\">2</span> * keyIndex + <span class=\"number\">1</span>)</span><br><span class=\"line\">       newIndex += <span class=\"number\">1</span></span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">     keyIndex += <span class=\"number\">1</span></span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   assert(curSize == newIndex + (<span class=\"keyword\">if</span> (haveNullValue) <span class=\"number\">1</span> <span class=\"keyword\">else</span> <span class=\"number\">0</span>))</span><br><span class=\"line\">   <span class=\"comment\">// 这里对给定数据范围为 0 to newIndex，利用　keyComparator　比较器进行排序</span></span><br><span class=\"line\">   <span class=\"comment\">// 也就是对 map 中的数据根据　(partition,key) 进行排序</span></span><br><span class=\"line\">   <span class=\"keyword\">new</span> <span class=\"type\">Sorter</span>(<span class=\"keyword\">new</span> <span class=\"type\">KVArraySortDataFormat</span>[<span class=\"type\">K</span>, <span class=\"type\">AnyRef</span>]).sort(data, <span class=\"number\">0</span>, newIndex, keyComparator)</span><br><span class=\"line\"></span><br><span class=\"line\">   <span class=\"comment\">// 定义迭代器</span></span><br><span class=\"line\">   <span class=\"keyword\">new</span> <span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">V</span>)] &#123;</span><br><span class=\"line\">     <span class=\"keyword\">var</span> i = <span class=\"number\">0</span></span><br><span class=\"line\">     <span class=\"keyword\">var</span> nullValueReady = haveNullValue</span><br><span class=\"line\">     <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hasNext</span></span>: <span class=\"type\">Boolean</span> = (i &lt; newIndex || nullValueReady)</span><br><span class=\"line\">     <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">next</span></span>(): (<span class=\"type\">K</span>, <span class=\"type\">V</span>) = &#123;</span><br><span class=\"line\">       <span class=\"keyword\">if</span> (nullValueReady) &#123;</span><br><span class=\"line\">         nullValueReady = <span class=\"literal\">false</span></span><br><span class=\"line\">         (<span class=\"literal\">null</span>.asInstanceOf[<span class=\"type\">K</span>], nullValue)</span><br><span class=\"line\">       &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">         <span class=\"keyword\">val</span> item = (data(<span class=\"number\">2</span> * i).asInstanceOf[<span class=\"type\">K</span>], data(<span class=\"number\">2</span> * i + <span class=\"number\">1</span>).asInstanceOf[<span class=\"type\">V</span>])</span><br><span class=\"line\">         i += <span class=\"number\">1</span></span><br><span class=\"line\">         item</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>可以看出整个　<code>val inMemoryIterator = collection.destructiveSortedWritablePartitionedIterator(comparator)</code>　这一步其实就是在对　map　或 buffer 进行排序，并且实现了一个 writeNext() 方法供后续调调用。随后调用<code>val spillFile = spillMemoryIteratorToDisk(inMemoryIterator)</code>将排序后的缓存写入文件中:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span>[<span class=\"keyword\">this</span>] <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">spillMemoryIteratorToDisk</span></span>(inMemoryIterator: <span class=\"type\">WritablePartitionedIterator</span>)</span><br><span class=\"line\">      : <span class=\"type\">SpilledFile</span> = &#123;</span><br><span class=\"line\">   </span><br><span class=\"line\">    <span class=\"keyword\">val</span> (blockId, file) = diskBlockManager.createTempShuffleBlock()</span><br><span class=\"line\">    <span class=\"comment\">// These variables are reset after each flush</span></span><br><span class=\"line\">    <span class=\"keyword\">var</span> objectsWritten: <span class=\"type\">Long</span> = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> spillMetrics: <span class=\"type\">ShuffleWriteMetrics</span> = <span class=\"keyword\">new</span> <span class=\"type\">ShuffleWriteMetrics</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> writer: <span class=\"type\">DiskBlockObjectWriter</span> =</span><br><span class=\"line\">      blockManager.getDiskWriter(blockId, file, serInstance, fileBufferSize, spillMetrics)</span><br><span class=\"line\">    <span class=\"comment\">// List of batch sizes (bytes) in the order they are written to disk</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> batchSizes = <span class=\"keyword\">new</span> <span class=\"type\">ArrayBuffer</span>[<span class=\"type\">Long</span>]</span><br><span class=\"line\">    <span class=\"comment\">// How many elements we have in each partition</span></span><br><span class=\"line\">    <span class=\"comment\">// 用于记录每一个分区有多少条数据</span></span><br><span class=\"line\">    <span class=\"comment\">// 由于刚才数据写入文件没有写入key对应的分区，因此需要记录每一个分区有多少条数据，然后根据数据的偏移量就可以判断该数据属于哪个分区　</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> elementsPerPartition = <span class=\"keyword\">new</span> <span class=\"type\">Array</span>[<span class=\"type\">Long</span>](numPartitions)</span><br><span class=\"line\">    <span class=\"comment\">// Flush the disk writer's contents to disk, and update relevant variables.</span></span><br><span class=\"line\">    <span class=\"comment\">// The writer is committed at the end of this process.</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">flush</span></span>(): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> segment = writer.commitAndGet()</span><br><span class=\"line\">      batchSizes += segment.length</span><br><span class=\"line\">      _diskBytesSpilled += segment.length</span><br><span class=\"line\">      objectsWritten = <span class=\"number\">0</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">var</span> success = <span class=\"literal\">false</span></span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">while</span> (inMemoryIterator.hasNext) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">val</span> partitionId = inMemoryIterator.nextPartition()</span><br><span class=\"line\">        require(partitionId &gt;= <span class=\"number\">0</span> &amp;&amp; partitionId &lt; numPartitions,</span><br><span class=\"line\">          <span class=\"string\">s\"partition Id: <span class=\"subst\">$&#123;partitionId&#125;</span> should be in the range [0, <span class=\"subst\">$&#123;numPartitions&#125;</span>)\"</span>)</span><br><span class=\"line\">        inMemoryIterator.writeNext(writer)</span><br><span class=\"line\">        elementsPerPartition(partitionId) += <span class=\"number\">1</span></span><br><span class=\"line\">        objectsWritten += <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (objectsWritten == serializerBatchSize) &#123;　<span class=\"comment\">//写入　serializerBatchSize　条数据便刷新一次缓存</span></span><br><span class=\"line\">          <span class=\"comment\">// batchSize 在类中定义的如下:</span></span><br><span class=\"line\">          <span class=\"comment\">// 可以看出如果不存在配置默认为　10000　条</span></span><br><span class=\"line\">          <span class=\"comment\">// private val serializerBatchSize = conf.getLong(\"spark.shuffle.spill.batchSize\", 10000)</span></span><br><span class=\"line\">          flush()</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (objectsWritten &gt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">        flush()</span><br><span class=\"line\">      &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        writer.revertPartialWritesAndClose()</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      success = <span class=\"literal\">true</span></span><br><span class=\"line\">    &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (success) &#123;</span><br><span class=\"line\">        writer.close()</span><br><span class=\"line\">      &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// This code path only happens if an exception was thrown above before we set success;</span></span><br><span class=\"line\">        <span class=\"comment\">// close our stuff and let the exception be thrown further</span></span><br><span class=\"line\">        writer.revertPartialWritesAndClose()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (file.exists()) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">if</span> (!file.delete()) &#123;</span><br><span class=\"line\">            logWarning(<span class=\"string\">s\"Error deleting <span class=\"subst\">$&#123;file&#125;</span>\"</span>)</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">//最后记录临时文件的信息</span></span><br><span class=\"line\">    <span class=\"type\">SpilledFile</span>(file, blockId, batchSizes.toArray, elementsPerPartition)</span><br><span class=\"line\"></span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>这就是整个　insertAll()　方法:</p>\n<p> merge 达到溢出值后进行排序并写入临时文件，这里要注意的是并非所有的缓存文件都被写入到临时文件中，所以接下来需要将临时文件中的数据加上内存中的数据进行一次排序写入到一个大文件中。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//首先获取需要写入的文件:　</span></span><br><span class=\"line\">   <span class=\"keyword\">val</span> output = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)</span><br><span class=\"line\">　　<span class=\"keyword\">val</span> tmp = <span class=\"type\">Utils</span>.tempFileWith(output)</span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123; </span><br><span class=\"line\">      <span class=\"keyword\">val</span> blockId = <span class=\"type\">ShuffleBlockId</span>(dep.shuffleId, mapId, <span class=\"type\">IndexShuffleBlockResolver</span>.<span class=\"type\">NOOP_REDUCE_ID</span>)</span><br><span class=\"line\">      <span class=\"keyword\">val</span> partitionLengths = sorter.writePartitionedFile(blockId, tmp)<span class=\"comment\">//这一句是重点 下面会讲解</span></span><br><span class=\"line\">      shuffleBlockResolver.writeIndexFileAndCommit(dep.shuffleId, mapId, partitionLengths, tmp)</span><br><span class=\"line\">      mapStatus = <span class=\"type\">MapStatus</span>(blockManager.shuffleServerId, partitionLengths)</span><br><span class=\"line\">    &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (tmp.exists() &amp;&amp; !tmp.delete()) &#123;</span><br><span class=\"line\">        logError(<span class=\"string\">s\"Error while deleting temp file <span class=\"subst\">$&#123;tmp.getAbsolutePath&#125;</span>\"</span>)</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>####　writePartitionedFile</p>\n<p>继续看 writePartitionedFile :</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//  ExternalSorter 中的　writePartitionedFile　方法</span></span><br><span class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">writePartitionedFile</span></span>(</span><br><span class=\"line\">      blockId: <span class=\"type\">BlockId</span>,</span><br><span class=\"line\">      outputFile: <span class=\"type\">File</span>): <span class=\"type\">Array</span>[<span class=\"type\">Long</span>] = &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// Track location of each range in the output file</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> lengths = <span class=\"keyword\">new</span> <span class=\"type\">Array</span>[<span class=\"type\">Long</span>](numPartitions)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> writer = blockManager.getDiskWriter(blockId, outputFile, serInstance, fileBufferSize,</span><br><span class=\"line\">      context.taskMetrics().shuffleWriteMetrics)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 这里的spills 就是　刚才产生的临时文件的数据　如果为空逻辑就比较简单，直接进行排序并写入文件</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (spills.isEmpty) &#123;</span><br><span class=\"line\">      <span class=\"comment\">// Case where we only have in-memory data</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> collection = <span class=\"keyword\">if</span> (aggregator.isDefined) map <span class=\"keyword\">else</span> buffer</span><br><span class=\"line\">      <span class=\"keyword\">val</span> it = collection.destructiveSortedWritablePartitionedIterator(comparator)　<span class=\"comment\">//这里同样使用　destructiveSortedWritablePartitionedIterator　进行排序</span></span><br><span class=\"line\">      <span class=\"keyword\">while</span> (it.hasNext) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">val</span> partitionId = it.nextPartition()</span><br><span class=\"line\">        <span class=\"keyword\">while</span> (it.hasNext &amp;&amp; it.nextPartition() == partitionId) &#123;</span><br><span class=\"line\">          it.writeNext(writer)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">val</span> segment = writer.commitAndGet()</span><br><span class=\"line\">        lengths(partitionId) = segment.length</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;<span class=\"comment\">//重点看不为空的时候，这里调用了　partitionedIterator　方法</span></span><br><span class=\"line\">      <span class=\"comment\">// We must perform merge-sort; get an iterator by partition and write everything directly.</span></span><br><span class=\"line\">      <span class=\"keyword\">for</span> ((id, elements) &lt;- <span class=\"keyword\">this</span>.partitionedIterator) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (elements.hasNext) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">for</span> (elem &lt;- elements) &#123;</span><br><span class=\"line\">            writer.write(elem._1, elem._2)</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">          <span class=\"keyword\">val</span> segment = writer.commitAndGet()</span><br><span class=\"line\">          lengths(id) = segment.length</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    writer.close()</span><br><span class=\"line\">    context.taskMetrics().incMemoryBytesSpilled(memoryBytesSpilled)</span><br><span class=\"line\">    context.taskMetrics().incDiskBytesSpilled(diskBytesSpilled)</span><br><span class=\"line\">    context.taskMetrics().incPeakExecutionMemory(peakMemoryUsedBytes)</span><br><span class=\"line\"></span><br><span class=\"line\">    lengths</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>然后是 partitionedIterator 方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">partitionedIterator</span></span>: <span class=\"type\">Iterator</span>[(<span class=\"type\">Int</span>, <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]])] = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> usingMap = aggregator.isDefined</span><br><span class=\"line\">    <span class=\"keyword\">val</span> collection: <span class=\"type\">WritablePartitionedPairCollection</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>] = <span class=\"keyword\">if</span> (usingMap) map <span class=\"keyword\">else</span> buffer</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (spills.isEmpty) &#123;<span class=\"comment\">// 这里又一次判断了是否为空，直接看有临时文件的部分</span></span><br><span class=\"line\">      <span class=\"comment\">// Special case: if we have only in-memory data, we don't need to merge streams, and perhaps</span></span><br><span class=\"line\">      <span class=\"comment\">// we don't even need to sort by anything other than partition ID</span></span><br><span class=\"line\">      <span class=\"keyword\">if</span> (!ordering.isDefined) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// The user hasn't requested sorted keys, so only sort by partition ID, not key</span></span><br><span class=\"line\">   groupByPartition(destructiveIterator(collection.partitionedDestructiveSortedIterator(<span class=\"type\">None</span>)))</span><br><span class=\"line\">      &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// We do need to sort by both partition ID and key</span></span><br><span class=\"line\">        groupByPartition(destructiveIterator(</span><br><span class=\"line\">          collection.partitionedDestructiveSortedIterator(<span class=\"type\">Some</span>(keyComparator))))</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">      <span class=\"comment\">// Merge spilled and in-memory data</span></span><br><span class=\"line\">      <span class=\"comment\">// 这里传入了临时文件　spills　和　排序后的缓存文件</span></span><br><span class=\"line\">      merge(spills, destructiveIterator(</span><br><span class=\"line\">        collection.partitionedDestructiveSortedIterator(comparator)))</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>merge 方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// merge方法</span></span><br><span class=\"line\">  <span class=\"comment\">// inMemory　是根据(partion,hash(k)) 排序后的内存数据</span></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">merge</span></span>(spills: <span class=\"type\">Seq</span>[<span class=\"type\">SpilledFile</span>], inMemory: <span class=\"type\">Iterator</span>[((<span class=\"type\">Int</span>, <span class=\"type\">K</span>), <span class=\"type\">C</span>)])</span><br><span class=\"line\">     : <span class=\"type\">Iterator</span>[(<span class=\"type\">Int</span>, <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]])] = &#123;</span><br><span class=\"line\">   <span class=\"comment\">//　将所有缓存文件转化为 SpillReader </span></span><br><span class=\"line\">   <span class=\"keyword\">val</span> readers = spills.map(<span class=\"keyword\">new</span> <span class=\"type\">SpillReader</span>(_))</span><br><span class=\"line\">   <span class=\"comment\">// buffered方法只是比普通的　itertor 方法多提供一个　head　方法，例如:</span></span><br><span class=\"line\">   <span class=\"comment\">// val a = List(1,2,3,4,5)</span></span><br><span class=\"line\">   <span class=\"comment\">// val b = a.iterator.buffered</span></span><br><span class=\"line\">   <span class=\"comment\">// b.head : 1</span></span><br><span class=\"line\">   <span class=\"comment\">// b.next : 1</span></span><br><span class=\"line\">   <span class=\"comment\">// b.head : 2</span></span><br><span class=\"line\">   <span class=\"comment\">// b.next : 2</span></span><br><span class=\"line\">   <span class=\"keyword\">val</span> inMemBuffered = inMemory.buffered</span><br><span class=\"line\">   (<span class=\"number\">0</span> until numPartitions).iterator.map &#123; p =&gt;</span><br><span class=\"line\">     <span class=\"comment\">// 获得分区对应数据的迭代器 </span></span><br><span class=\"line\">     <span class=\"comment\">// 这里的　IteratorForPartition　就是得到分区 p 对应的数据的迭代器，逻辑比较简单，就不单独拿出来分析</span></span><br><span class=\"line\">     <span class=\"keyword\">val</span> inMemIterator = <span class=\"keyword\">new</span> <span class=\"type\">IteratorForPartition</span>(p, inMemBuffered)</span><br><span class=\"line\">     </span><br><span class=\"line\">     <span class=\"comment\">//这里获取所有文件的第　p　个分区　并与内存中的第　p 个分区进行合并</span></span><br><span class=\"line\">     <span class=\"keyword\">val</span> iterators = readers.map(_.readNextPartition()) ++ <span class=\"type\">Seq</span>(inMemIterator)</span><br><span class=\"line\">　　　<span class=\"comment\">// 这里的　iterators　是由　多个 iterator 组成的，其中每一个都是关于 p 分区数据的 iterator</span></span><br><span class=\"line\">     <span class=\"keyword\">if</span> (aggregator.isDefined) &#123;</span><br><span class=\"line\">       <span class=\"comment\">// Perform partial aggregation across partitions</span></span><br><span class=\"line\">       (p, mergeWithAggregation(</span><br><span class=\"line\">         iterators, aggregator.get.mergeCombiners, keyComparator, ordering.isDefined))</span><br><span class=\"line\">     &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (ordering.isDefined) &#123;</span><br><span class=\"line\">       <span class=\"comment\">// No aggregator given, but we have an ordering (e.g. used by reduce tasks in sortByKey);</span></span><br><span class=\"line\">       <span class=\"comment\">// sort the elements without trying to merge them</span></span><br><span class=\"line\">       (p, mergeSort(iterators, ordering.get))</span><br><span class=\"line\">     &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">       (p, iterators.iterator.flatten)</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">   &#125;</span><br></pre></td></tr></table></figure>\n<p>mergeWithAggregation<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">mergeWithAggregation</span></span>(</span><br><span class=\"line\">    iterators: <span class=\"type\">Seq</span>[<span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]]],</span><br><span class=\"line\">    mergeCombiners: (<span class=\"type\">C</span>, <span class=\"type\">C</span>) =&gt; <span class=\"type\">C</span>,</span><br><span class=\"line\">    comparator: <span class=\"type\">Comparator</span>[<span class=\"type\">K</span>],</span><br><span class=\"line\">    totalOrder: <span class=\"type\">Boolean</span>)</span><br><span class=\"line\">    : <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] =</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (!totalOrder) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">Iterator</span>[<span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]]] &#123;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> sorted = mergeSort(iterators, comparator).buffered</span><br><span class=\"line\">      <span class=\"keyword\">val</span> keys = <span class=\"keyword\">new</span> <span class=\"type\">ArrayBuffer</span>[<span class=\"type\">K</span>]</span><br><span class=\"line\">      <span class=\"keyword\">val</span> combiners = <span class=\"keyword\">new</span> <span class=\"type\">ArrayBuffer</span>[<span class=\"type\">C</span>]</span><br><span class=\"line\">      <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hasNext</span></span>: <span class=\"type\">Boolean</span> = sorted.hasNext</span><br><span class=\"line\">      <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">next</span></span>(): <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] = &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!hasNext) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">NoSuchElementException</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        keys.clear()</span><br><span class=\"line\">        combiners.clear()</span><br><span class=\"line\">        <span class=\"keyword\">val</span> firstPair = sorted.next()</span><br><span class=\"line\">        keys += firstPair._1</span><br><span class=\"line\">        combiners += firstPair._2</span><br><span class=\"line\">        <span class=\"keyword\">val</span> key = firstPair._1</span><br><span class=\"line\">        <span class=\"keyword\">while</span> (sorted.hasNext &amp;&amp; comparator.compare(sorted.head._1, key) == <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">val</span> pair = sorted.next()</span><br><span class=\"line\">          <span class=\"keyword\">var</span> i = <span class=\"number\">0</span></span><br><span class=\"line\">          <span class=\"keyword\">var</span> foundKey = <span class=\"literal\">false</span></span><br><span class=\"line\">          <span class=\"keyword\">while</span> (i &lt; keys.size &amp;&amp; !foundKey) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (keys(i) == pair._1) &#123;</span><br><span class=\"line\">              combiners(i) = mergeCombiners(combiners(i), pair._2)</span><br><span class=\"line\">              foundKey = <span class=\"literal\">true</span></span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            i += <span class=\"number\">1</span></span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">          <span class=\"keyword\">if</span> (!foundKey) &#123;</span><br><span class=\"line\">            keys += pair._1</span><br><span class=\"line\">            combiners += pair._2</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        keys.iterator.zip(combiners.iterator)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;.flatMap(i =&gt; i)</span><br><span class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">// We have a total ordering, so the objects with the same key are sequential.</span></span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] &#123;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> sorted = mergeSort(iterators, comparator).buffered</span><br><span class=\"line\">      <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hasNext</span></span>: <span class=\"type\">Boolean</span> = sorted.hasNext</span><br><span class=\"line\">      <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">next</span></span>(): <span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>] = &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!hasNext) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">NoSuchElementException</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">val</span> elem = sorted.next()</span><br><span class=\"line\">        <span class=\"keyword\">val</span> k = elem._1</span><br><span class=\"line\">        <span class=\"keyword\">var</span> c = elem._2</span><br><span class=\"line\">        <span class=\"comment\">// 这里的意思是可能存在多个相同的key 分布在不同临时文件或内存中</span></span><br><span class=\"line\">        <span class=\"comment\">// 所以还需要将不同的 key 对应的值进行合并 </span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> (sorted.hasNext &amp;&amp; sorted.head._1 == k) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">val</span> pair = sorted.next()</span><br><span class=\"line\">          c = mergeCombiners(c, pair._2)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        (k, c)<span class=\"comment\">//返回</span></span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>继续看 mergeSort 方法，首先选除头元素最小对应那个分区，然后选出这个分区的第一个元素，那么这个元素一定是最小的，然后将剩余的元素放回去:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">mergeSort</span></span>(iterators: <span class=\"type\">Seq</span>[<span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]]], comparator: <span class=\"type\">Comparator</span>[<span class=\"type\">K</span>])</span><br><span class=\"line\">    : <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] =</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  <span class=\"keyword\">val</span> bufferedIters = iterators.filter(_.hasNext).map(_.buffered)</span><br><span class=\"line\">  <span class=\"class\"><span class=\"keyword\">type</span> <span class=\"title\">Iter</span> </span>= <span class=\"type\">BufferedIterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]]</span><br><span class=\"line\">  <span class=\"comment\">//选取头元素最小的分区</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> heap = <span class=\"keyword\">new</span> mutable.<span class=\"type\">PriorityQueue</span>[<span class=\"type\">Iter</span>]()(<span class=\"keyword\">new</span> <span class=\"type\">Ordering</span>[<span class=\"type\">Iter</span>] &#123;</span><br><span class=\"line\">    <span class=\"comment\">// Use the reverse of comparator.compare because PriorityQueue dequeues the max</span></span><br><span class=\"line\">    <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compare</span></span>(x: <span class=\"type\">Iter</span>, y: <span class=\"type\">Iter</span>): <span class=\"type\">Int</span> = -comparator.compare(x.head._1, y.head._1)</span><br><span class=\"line\">  &#125;)</span><br><span class=\"line\">  heap.enqueue(bufferedIters: _*) </span><br><span class=\"line\">  <span class=\"keyword\">new</span> <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] &#123;</span><br><span class=\"line\">    <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hasNext</span></span>: <span class=\"type\">Boolean</span> = !heap.isEmpty</span><br><span class=\"line\">    <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">next</span></span>(): <span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>] = &#123;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (!hasNext) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">NoSuchElementException</span></span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> firstBuf = heap.dequeue()</span><br><span class=\"line\">      <span class=\"keyword\">val</span> firstPair = firstBuf.next()</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (firstBuf.hasNext) &#123;</span><br><span class=\"line\">        heap.enqueue(firstBuf)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      firstPair</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>接下来就使用 writeIndexFileAndCommit 将每一个分区的数据条数写入索引文件，便于 reduce 端获取。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>shuflle write 相比起 shuffe read 要复杂很多，因此本文的篇幅也较长。其实在 shuffle write 有两个较为重要的方法，其中一个是 insertAll。这个方法的作用就是将数据缓存到一个可以添加的 hash 表中。如果表占用的内存达到的一定值，就对其进行排序。排序方式先按照分区进行排序，如果分区相同则按key的hash进行排序，随后将排序后的数据写入临时文件中，这个过程可能会生成多个临时文件。</p>\n<p>然后 writePartitionedFile 的作用是最后将内存中的数据和临时文件中的数据进行全部排序，写入到一个大文件中。</p>\n<p>最后将每一个分区对应的索引数据写入文件。整个shuffle write 阶段就完成了</p>\n"}],"PostAsset":[{"_id":"source/_posts/Spark-源码阅读计划-伪-第三部分-图解-shuffle-read/part.png","slug":"part.png","post":"cjiebvrs50000fangvlyac8u4","modified":0,"renderable":0},{"_id":"source/_posts/Spark-源码阅读计划-伪-第三部分-图解-shuffle-read/sort.png","slug":"sort.png","post":"cjiebvrs50000fangvlyac8u4","modified":0,"renderable":0}],"PostCategory":[{"post_id":"cjiebvrs50000fangvlyac8u4","category_id":"cjiebvrsd0003fanguobzq5da","_id":"cjiebvrsm000dfang456varvc"},{"post_id":"cjiebvrsi0007fangedvhz17o","category_id":"cjiebvrsd0003fanguobzq5da","_id":"cjiebvrsn000ffang9r4rcq4t"},{"post_id":"cjiebvrsa0002fang9uvh8h4e","category_id":"cjiebvrsd0003fanguobzq5da","_id":"cjiebvrsn000ifangffm2tlhe"},{"post_id":"cjiebvrsf0005fangenc7xp65","category_id":"cjiebvrsl000bfangjyf0n9jw","_id":"cjiebvrso000jfangfywp18nf"},{"post_id":"cjiebvrsg0006fangdvr1o7zr","category_id":"cjiecd6kr0002k3ngrmu7lqbq","_id":"cjiecd6kr0003k3ngazubxq05"}],"PostTag":[{"post_id":"cjiebvrs50000fangvlyac8u4","tag_id":"cjiebvrse0004fange7k0815y","_id":"cjiebvrsm000efangqplbxh7o"},{"post_id":"cjiebvrs50000fangvlyac8u4","tag_id":"cjiebvrsi0009fang2znqrf4p","_id":"cjiebvrsn000gfang7anuo45g"},{"post_id":"cjiebvrsa0002fang9uvh8h4e","tag_id":"cjiebvrse0004fange7k0815y","_id":"cjiebvrso000lfanglm2gmnp2"},{"post_id":"cjiebvrsa0002fang9uvh8h4e","tag_id":"cjiebvrsi0009fang2znqrf4p","_id":"cjiebvrsp000mfang5f4p5o5m"},{"post_id":"cjiebvrsf0005fangenc7xp65","tag_id":"cjiebvrso000kfangvzxoqvxr","_id":"cjiebvrsq000pfangfyjy1jmq"},{"post_id":"cjiebvrsf0005fangenc7xp65","tag_id":"cjiebvrsp000nfangm22je3zs","_id":"cjiebvrsq000qfang6yyll6qr"},{"post_id":"cjiebvrsg0006fangdvr1o7zr","tag_id":"cjiebvrsp000ofangdppy8s5l","_id":"cjiebvrsr000tfangjob7ro1h"},{"post_id":"cjiebvrsg0006fangdvr1o7zr","tag_id":"cjiebvrsi0009fang2znqrf4p","_id":"cjiebvrsr000ufangwwypbypw"},{"post_id":"cjiebvrsi0007fangedvhz17o","tag_id":"cjiebvrse0004fange7k0815y","_id":"cjiebvrss000vfangawt8mlh4"},{"post_id":"cjiebvrsi0007fangedvhz17o","tag_id":"cjiebvrsi0009fang2znqrf4p","_id":"cjiebvrss000wfangbq1n645e"}],"Tag":[{"name":"Spark","_id":"cjiebvrse0004fange7k0815y"},{"name":"编程","_id":"cjiebvrsi0009fang2znqrf4p"},{"name":"工具","_id":"cjiebvrso000kfangvzxoqvxr"},{"name":"教程","_id":"cjiebvrsp000nfangm22je3zs"},{"name":"Linux","_id":"cjiebvrsp000ofangdppy8s5l"}]}}