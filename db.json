{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/next-gux/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/next-gux/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":1,"renderable":1},{"_id":"themes/next-gux/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":1,"renderable":1},{"_id":"themes/next-gux/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":1,"renderable":1},{"_id":"themes/next-gux/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":1,"renderable":1},{"_id":"themes/next-gux/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":1,"renderable":1},{"_id":"themes/next-gux/source/images/cc-by.svg","path":"images/cc-by.svg","modified":1,"renderable":1},{"_id":"themes/next-gux/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":1,"renderable":1},{"_id":"themes/next-gux/source/images/default_avatar.jpg","path":"images/default_avatar.jpg","modified":1,"renderable":1},{"_id":"themes/next-gux/source/images/loading.gif","path":"images/loading.gif","modified":1,"renderable":1},{"_id":"themes/next-gux/source/images/placeholder.gif","path":"images/placeholder.gif","modified":1,"renderable":1},{"_id":"themes/next-gux/source/images/searchicon.png","path":"images/searchicon.png","modified":1,"renderable":1},{"_id":"themes/next-gux/source/js/bootstrap.scrollspy.js","path":"js/bootstrap.scrollspy.js","modified":1,"renderable":1},{"_id":"themes/next-gux/source/js/fancy-box.js","path":"js/fancy-box.js","modified":1,"renderable":1},{"_id":"themes/next-gux/source/js/helpers.js","path":"js/helpers.js","modified":1,"renderable":1},{"_id":"themes/next-gux/source/js/hook-duoshuo.js","path":"js/hook-duoshuo.js","modified":1,"renderable":1},{"_id":"themes/next-gux/source/js/lazyload.js","path":"js/lazyload.js","modified":1,"renderable":1},{"_id":"themes/next-gux/source/js/motion_fallback.js","path":"js/motion_fallback.js","modified":1,"renderable":1},{"_id":"themes/next-gux/source/js/motion_global.js","path":"js/motion_global.js","modified":1,"renderable":1},{"_id":"themes/next-gux/source/js/nav-toggle.js","path":"js/nav-toggle.js","modified":1,"renderable":1},{"_id":"themes/next-gux/source/js/ua-parser.min.js","path":"js/ua-parser.min.js","modified":1,"renderable":1},{"_id":"themes/next-gux/source/images/background.jpg","path":"images/background.jpg","modified":1,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-default/icomoon.eot","path":"fonts/icon-default/icomoon.eot","modified":1,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-default/icomoon.svg","path":"fonts/icon-default/icomoon.svg","modified":1,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-default/icomoon.ttf","path":"fonts/icon-default/icomoon.ttf","modified":1,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-default/icomoon.woff","path":"fonts/icon-default/icomoon.woff","modified":1,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-default/selection.json","path":"fonts/icon-default/selection.json","modified":1,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-feather/icomoon.eot","path":"fonts/icon-feather/icomoon.eot","modified":1,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-feather/icomoon.svg","path":"fonts/icon-feather/icomoon.svg","modified":1,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-feather/icomoon.ttf","path":"fonts/icon-feather/icomoon.ttf","modified":1,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-feather/icomoon.woff","path":"fonts/icon-feather/icomoon.woff","modified":1,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-feather/selection.json","path":"fonts/icon-feather/selection.json","modified":1,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-fifty-shades/icomoon.eot","path":"fonts/icon-fifty-shades/icomoon.eot","modified":1,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-fifty-shades/icomoon.svg","path":"fonts/icon-fifty-shades/icomoon.svg","modified":1,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-fifty-shades/icomoon.ttf","path":"fonts/icon-fifty-shades/icomoon.ttf","modified":1,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-fifty-shades/icomoon.woff","path":"fonts/icon-fifty-shades/icomoon.woff","modified":1,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-fifty-shades/selection.json","path":"fonts/icon-fifty-shades/selection.json","modified":1,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-icomoon/icomoon.eot","path":"fonts/icon-icomoon/icomoon.eot","modified":1,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-icomoon/icomoon.svg","path":"fonts/icon-icomoon/icomoon.svg","modified":1,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-icomoon/icomoon.ttf","path":"fonts/icon-icomoon/icomoon.ttf","modified":1,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-icomoon/icomoon.woff","path":"fonts/icon-icomoon/icomoon.woff","modified":1,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-linecons/icomoon.eot","path":"fonts/icon-linecons/icomoon.eot","modified":1,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-linecons/icomoon.svg","path":"fonts/icon-linecons/icomoon.svg","modified":1,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-linecons/icomoon.ttf","path":"fonts/icon-linecons/icomoon.ttf","modified":1,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-linecons/icomoon.woff","path":"fonts/icon-linecons/icomoon.woff","modified":1,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-linecons/selection.json","path":"fonts/icon-linecons/selection.json","modified":1,"renderable":1},{"_id":"themes/next-gux/source/vendors/fastclick/LICENSE","path":"vendors/fastclick/LICENSE","modified":1,"renderable":1},{"_id":"themes/next-gux/source/vendors/fastclick/README.md","path":"vendors/fastclick/README.md","modified":1,"renderable":1},{"_id":"themes/next-gux/source/vendors/fastclick/bower.json","path":"vendors/fastclick/bower.json","modified":1,"renderable":1},{"_id":"themes/next-gux/source/vendors/velocity/bower.json","path":"vendors/velocity/bower.json","modified":1,"renderable":1},{"_id":"themes/next-gux/source/vendors/velocity/velocity.min.js","path":"vendors/velocity/velocity.min.js","modified":1,"renderable":1},{"_id":"themes/next-gux/source/vendors/velocity/velocity.ui.js","path":"vendors/velocity/velocity.ui.js","modified":1,"renderable":1},{"_id":"themes/next-gux/source/vendors/velocity/velocity.ui.min.js","path":"vendors/velocity/velocity.ui.min.js","modified":1,"renderable":1},{"_id":"themes/next-gux/source/vendors/jquery/index.js","path":"vendors/jquery/index.js","modified":1,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/blank.gif","path":"vendors/fancybox/source/blank.gif","modified":1,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/fancybox_loading.gif","path":"vendors/fancybox/source/fancybox_loading.gif","modified":1,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/fancybox_loading@2x.gif","path":"vendors/fancybox/source/fancybox_loading@2x.gif","modified":1,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/fancybox_overlay.png","path":"vendors/fancybox/source/fancybox_overlay.png","modified":1,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/fancybox_sprite.png","path":"vendors/fancybox/source/fancybox_sprite.png","modified":1,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/fancybox_sprite@2x.png","path":"vendors/fancybox/source/fancybox_sprite@2x.png","modified":1,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/jquery.fancybox.css","path":"vendors/fancybox/source/jquery.fancybox.css","modified":1,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/jquery.fancybox.js","path":"vendors/fancybox/source/jquery.fancybox.js","modified":1,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/jquery.fancybox.pack.js","path":"vendors/fancybox/source/jquery.fancybox.pack.js","modified":1,"renderable":1},{"_id":"themes/next-gux/source/vendors/fastclick/lib/fastclick.js","path":"vendors/fastclick/lib/fastclick.js","modified":1,"renderable":1},{"_id":"themes/next-gux/source/vendors/fastclick/lib/fastclick.min.js","path":"vendors/fastclick/lib/fastclick.min.js","modified":1,"renderable":1},{"_id":"themes/next-gux/source/vendors/velocity/velocity.js","path":"vendors/velocity/velocity.js","modified":1,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/helpers/fancybox_buttons.png","path":"vendors/fancybox/source/helpers/fancybox_buttons.png","modified":1,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"vendors/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":1,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"vendors/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":1,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/helpers/jquery.fancybox-media.js","path":"vendors/fancybox/source/helpers/jquery.fancybox-media.js","modified":1,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"vendors/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":1,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"vendors/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":1,"renderable":1}],"Cache":[{"_id":"themes/next-gux/.bowerrc","hash":"80e096fdc1cf912ee85dd9f7e6e77fd40cf60f10","modified":1530291968907},{"_id":"themes/next-gux/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1530291968907},{"_id":"themes/next-gux/.gitignore","hash":"689a812947ccb5de9ed3ecdc5f927030adfd1d7a","modified":1530291968907},{"_id":"themes/next-gux/.jshintrc","hash":"12c5e37da3432bee2219ed1c667076d54f1639c0","modified":1530291968911},{"_id":"themes/next-gux/README.md","hash":"57c54b159185e346012669a5595b72a5b31cc91f","modified":1530291968911},{"_id":"themes/next-gux/_config.yml","hash":"eebb0388f030d33d8e25e7e3426cb5342d7a0fa0","modified":1530291968911},{"_id":"themes/next-gux/_config.yml.bak","hash":"f18ce03aaee919abef64fd6df1772935b577c3e0","modified":1530291968911},{"_id":"themes/next-gux/bower.json","hash":"057ec1580f78a7adae66bd26fe9e9f924621174f","modified":1530291968911},{"_id":"source/_posts/Spark-源码阅读计划-伪-第三部分-图解-shuffle-write.md","hash":"9fba0917b9c65919a4b28035998a9bdaa9ec8c15","modified":1530291968923},{"_id":"source/_posts/Spark-源码阅读计划-第一部分-迭代计算.md","hash":"1e2c475b21077ccad1516bcc4fcb343ab6cdbbe7","modified":1530291968923},{"_id":"source/_posts/Spark优化-减少分区.md","hash":"154052f1180f8854af49bbb6c001132efb0c76f0","modified":1530291968923},{"_id":"source/_posts/Spark源码阅读计划-第四部分-shuffle-read.md","hash":"23c5f9a5f2d2b50ddd4467be9846a4b493210765","modified":1530291968923},{"_id":"source/_posts/使用git+hexo建立个人博客.md","hash":"8ea88d51903561b09c5dc4a6048d16151603c5f7","modified":1530291968923},{"_id":"source/_posts/使用travis自动部署hexo到github.md","hash":"7d231be537f78b74a5494a3a3f85bd208d4705ce","modified":1530291968923},{"_id":"source/_posts/利用idea搭建spark-shell开发环境.md","hash":"26aaa1855aab7713c18d80f996a34efc048f599f","modified":1530291968923},{"_id":"source/_posts/在hexo中书写latex公式.md","hash":"94e3c231361bff3f02b3652bbf1e080734fce5b1","modified":1530291968923},{"_id":"source/_posts/常用-linux-命令系列.md","hash":"ada7baa32728dfd75eb82e481cb859d28436f0d8","modified":1530291968923},{"_id":"source/_posts/数据挖掘基础-最大似然估计与最大后验估计.md","hash":"87f1413853d8fd5296779690481d30db572c2589","modified":1530291968923},{"_id":"source/_posts/数据挖掘基础-熵.md","hash":"9f99409afeb79249c940f4216c4c245525340fc5","modified":1530291968923},{"_id":"source/_posts/源码阅读计划-第二部分-shuffle-write.md","hash":"a0fa36faf8445b4d86c549890ddf15aa15e28f11","modified":1530291968923},{"_id":"source/about/index.md","hash":"e9ca55c635160ccd3b41404fc77c6d4595573222","modified":1530291968923},{"_id":"source/categories/index.md","hash":"cccec3d72c25ca05444985af5a716760c86570d6","modified":1530291968923},{"_id":"source/tags/index.md","hash":"7ccafacf77c140dbb2f677b91362795c96f3ea77","modified":1530291968923},{"_id":"themes/next-gux/.idea/compiler.xml","hash":"bff5196ea91a033d64bb5c4d6647a2e3b71bb548","modified":1530291968907},{"_id":"themes/next-gux/.idea/misc.xml","hash":"318a7276f979d7822e16d305da5f7c939a00852d","modified":1530291968907},{"_id":"themes/next-gux/.idea/modules.xml","hash":"64ff052d1dae024fe94ea8e2b1d7f704609f22a6","modified":1530291968907},{"_id":"themes/next-gux/.idea/next-guxiangfly.iml","hash":"980957b57c4f1eae5e85d664d8375f83d47d3e5a","modified":1530291968907},{"_id":"themes/next-gux/.idea/vcs.xml","hash":"6f94fc1df9e8721673d47588ac444667dc9ded06","modified":1530291968907},{"_id":"themes/next-gux/.idea/workspace.xml","hash":"a73aef09b7d8a062a38f2a2f6f0242f91844846b","modified":1530291968907},{"_id":"themes/next-gux/languages/de.yml","hash":"7a8de0e5665c52a1bf168c1e7dd222c8a74fb0ab","modified":1530291968911},{"_id":"themes/next-gux/languages/default.yml","hash":"7e65ef918f16d0189055deb5f1616b9dedcb1920","modified":1530291968911},{"_id":"themes/next-gux/languages/en.yml","hash":"7e65ef918f16d0189055deb5f1616b9dedcb1920","modified":1530291968911},{"_id":"themes/next-gux/languages/fr-FR.yml","hash":"6d097445342a9fb5235afea35d65bf5271b772f0","modified":1530291968911},{"_id":"themes/next-gux/languages/ru.yml","hash":"b4a827b9ddac9d5f6dca096fe513aeafb46a3e93","modified":1530291968911},{"_id":"themes/next-gux/languages/zh-Hans.yml","hash":"8af76df5557561050a950bdd7091d3bb3939c5c0","modified":1530291968911},{"_id":"themes/next-gux/languages/zh-hk.yml","hash":"3fc38103c9efa6f6c37149adbddb014ff85ec849","modified":1530291968911},{"_id":"themes/next-gux/languages/zh-tw.yml","hash":"8897a06e521b36c7a1226c72057c8357611eded8","modified":1530291968911},{"_id":"themes/next-gux/layout/_layout.swig","hash":"cbf2a45d0aca11965f083a732e482141f3f0b1c0","modified":1530291968911},{"_id":"themes/next-gux/layout/archive.swig","hash":"0c3ce594759f347ea90a4ce592a7a18e2ae4cc5c","modified":1530291968911},{"_id":"themes/next-gux/layout/category.swig","hash":"d6b3e1dc5e0b8deade9a084c463126e70188ee9b","modified":1530291968911},{"_id":"themes/next-gux/layout/index.swig","hash":"fdc801f0da71a2eb205ce9c0b12f156b219fdc9c","modified":1530291968911},{"_id":"themes/next-gux/layout/page.swig","hash":"beb1fc9a4e35b602a18b59f895544c6a838a67f2","modified":1530291968911},{"_id":"themes/next-gux/layout/post.swig","hash":"d1fe5e273d5852bbc5009cc1df629248fee54df1","modified":1530291968911},{"_id":"themes/next-gux/layout/tag.swig","hash":"aab44af54fcbc66fea4ad12b2767ffca3eadd451","modified":1530291968911},{"_id":"themes/next-gux/scripts/merge-configs.js","hash":"dfd147d1317e56d283f5e779f00608e913603b51","modified":1530291968911},{"_id":"themes/next-gux/test/.jshintrc","hash":"096ed6df627373edd820f24d46b8baf528dee61d","modified":1530291968923},{"_id":"themes/next-gux/test/helpers.js","hash":"7c8b0c7213ae06ec4e7948971f9b12842207b5c7","modified":1530291968923},{"_id":"themes/next-gux/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1530291968923},{"_id":"source/_posts/Spark-源码阅读计划-伪-第三部分-图解-shuffle-read/part.png","hash":"b2f702f44036d20b43eedec75b1cbbe0a06d5877","modified":1530291968923},{"_id":"source/_posts/Spark-源码阅读计划-伪-第三部分-图解-shuffle-read/sort.png","hash":"2fc1a5957fc0bfc7cf3027e38759fa3c65077768","modified":1530291968923},{"_id":"source/_posts/使用travis自动部署hexo到github/author-code.png","hash":"ca07f759bb39a3fe8ec9a5f78209c7043881051f","modified":1530291968923},{"_id":"source/_posts/使用travis自动部署hexo到github/author.png","hash":"ae2cf78724ebf56138d98e26c609905206634f81","modified":1530291968923},{"_id":"source/_posts/数据挖掘基础-最大似然估计与最大后验估计/func_img.png","hash":"374f722fb3fde7054d7e6264e2dfd8a62fbff710","modified":1530291968923},{"_id":"themes/next-gux/.idea/inspectionProfiles/Project_Default.xml","hash":"e1a86ce90b80bedfa05ec86db802b187d973f133","modified":1530291968907},{"_id":"themes/next-gux/layout/_macro/post-collapse.swig","hash":"42927bdde998cefd3cf4f19b0476d69bd9e5116a","modified":1530291968911},{"_id":"themes/next-gux/layout/_macro/post.swig","hash":"770c6ed5562205ed2b64d4c02f5418893144a9ff","modified":1530291968911},{"_id":"themes/next-gux/layout/_macro/sidebar.swig","hash":"7af60c855c060c5318df7264eb6860a7fbb7c3ce","modified":1530291968911},{"_id":"themes/next-gux/layout/_partials/footer.swig","hash":"f25e080de405dd5db5b85366eb9257eee5997dce","modified":1530291968911},{"_id":"themes/next-gux/layout/_partials/footer.swig.bak","hash":"5cbf9456977c2087238bfcc47623327fb0e57f20","modified":1530291968911},{"_id":"themes/next-gux/layout/_partials/head.swig","hash":"0187e02567897c42027803f80f54fe3b92b949d3","modified":1530291968911},{"_id":"themes/next-gux/layout/_partials/header.swig","hash":"e66b8fca801d5daba31496d4b00bac3018b7c29b","modified":1530291968911},{"_id":"themes/next-gux/layout/_partials/old-browsers.swig","hash":"dbbfea810bf3a2ed9c83b9a6683037175aacfc67","modified":1530291968911},{"_id":"themes/next-gux/layout/_partials/pagination.swig","hash":"d6c7f04eee4388d8f133eb5526b7c0875c321a30","modified":1530291968911},{"_id":"themes/next-gux/layout/_partials/search.swig","hash":"64f14da26792a17bc27836c4e9d83190175f36e6","modified":1530291968911},{"_id":"themes/next-gux/layout/_scripts/analytics.swig","hash":"0ebbf76c2317faa8ba31365adba59331c2e0262c","modified":1530291968911},{"_id":"themes/next-gux/layout/_scripts/bootstrap.scrollspy.swig","hash":"85295f126836b95f0837d03e58228bb3cf8c4490","modified":1530291968911},{"_id":"themes/next-gux/layout/_scripts/fancy-box.swig","hash":"41b4ff1446060c88c33bf666a32277dcf12129f0","modified":1530291968911},{"_id":"themes/next-gux/layout/_scripts/helpers.swig","hash":"4d2cbfca0aaf546a2b5813288073e824c1498fdf","modified":1530291968911},{"_id":"themes/next-gux/layout/_scripts/mathjax.swig","hash":"abc52fefb276c52cbb19de5c214521dfcf2a10fd","modified":1530291968911},{"_id":"themes/next-gux/layout/_scripts/motion.swig","hash":"817705bfd1a1282cb6bf59094afe507e11455aa0","modified":1530291968911},{"_id":"themes/next-gux/scripts/tags/center-quote.js","hash":"37274f743c2054244dcbbde56fba9ff353414281","modified":1530291968911},{"_id":"themes/next-gux/scripts/tags/full-image.js","hash":"0d69739d1bad5861a4a6ff2db511c3669783e438","modified":1530291968911},{"_id":"themes/next-gux/source/css/main.styl","hash":"151dccbe683e6a858d8a6ea09df913a2344b417f","modified":1530291968915},{"_id":"themes/next-gux/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1530291968919},{"_id":"themes/next-gux/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1530291968919},{"_id":"themes/next-gux/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1530291968919},{"_id":"themes/next-gux/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1530291968919},{"_id":"themes/next-gux/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1530291968919},{"_id":"themes/next-gux/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1530291968919},{"_id":"themes/next-gux/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1530291968919},{"_id":"themes/next-gux/source/images/default_avatar.jpg","hash":"930cf7a3be73cd08cbd2ba3f2ae8bee564bad227","modified":1530291968919},{"_id":"themes/next-gux/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1530291968919},{"_id":"themes/next-gux/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1530291968919},{"_id":"themes/next-gux/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1530291968919},{"_id":"themes/next-gux/source/js/bootstrap.scrollspy.js","hash":"ae7bdce88b515aade4eea8bf7407eec458bcd625","modified":1530291968919},{"_id":"themes/next-gux/source/js/fancy-box.js","hash":"b382ba746f4566682948ce92f2588ee940cd1755","modified":1530291968919},{"_id":"themes/next-gux/source/js/helpers.js","hash":"7499b413242a2e75a9308444aade5b72a12cce7d","modified":1530291968919},{"_id":"themes/next-gux/source/js/hook-duoshuo.js","hash":"9881b19132ad90dffd82c53947e4c356e30353e7","modified":1530291968919},{"_id":"themes/next-gux/source/js/lazyload.js","hash":"b92e9acdc7afc15468314c03f4a643b0c93944cf","modified":1530291968919},{"_id":"themes/next-gux/source/js/motion_fallback.js","hash":"a767d522c65a8b2fbad49135c9332135c6785c3e","modified":1530291968919},{"_id":"themes/next-gux/source/js/motion_global.js","hash":"fea8cbb854601b7aee14e51079b3e3f80a1de261","modified":1530291968919},{"_id":"themes/next-gux/source/js/nav-toggle.js","hash":"78b59f1beb12adea0d7f9bcf4377cb699963f220","modified":1530291968919},{"_id":"themes/next-gux/source/js/ua-parser.min.js","hash":"acf0ee6a47ffb7231472b56e43996e3f947c258a","modified":1530291968919},{"_id":"source/_posts/使用travis自动部署hexo到github/token-add.png","hash":"cf6aa24ed57ef160636765d5cc04a39b64e56154","modified":1530291968923},{"_id":"source/_posts/使用travis自动部署hexo到github/travis-add.png","hash":"790573634ebe578cd737ae099a4c93e06deee237","modified":1530291968923},{"_id":"source/_posts/使用travis自动部署hexo到github/travis-mainpage.png","hash":"667243094bd3f95e95285e58a26dd881ed7a0b6c","modified":1530291968923},{"_id":"themes/next-gux/source/css/_mixins/Mala.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1530291968915},{"_id":"themes/next-gux/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1530291968915},{"_id":"themes/next-gux/source/css/_mixins/default.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1530291968915},{"_id":"themes/next-gux/source/css/_variables/default.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1530291968915},{"_id":"themes/next-gux/source/images/background.jpg","hash":"6d3eac15433a762c0dd07b58fe81d555ceb62422","modified":1530291968919},{"_id":"themes/next-gux/layout/_partials/search/swiftype.swig","hash":"00c2b49f6289198b0b2b4e157e4ee783277f32a7","modified":1530291968911},{"_id":"themes/next-gux/layout/_partials/search/tinysou.swig","hash":"2f92046e0b50ebd65abb7045b1cbbfc50abbb034","modified":1530291968911},{"_id":"themes/next-gux/layout/_partials/share/baidu_share.swig","hash":"b4506174e385ee5fb1c94122b45732e3413a0ba2","modified":1530291968911},{"_id":"themes/next-gux/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1530291968911},{"_id":"themes/next-gux/layout/_partials/share/jiathis.swig","hash":"63315fcf210799f894208c9f512737096df84962","modified":1530291968911},{"_id":"themes/next-gux/layout/_partials/suprise/assist.swig","hash":"6b8a25353dbfe9f92e0d48388a6f46996e03b7cb","modified":1530291968911},{"_id":"themes/next-gux/layout/_partials/suprise/donate.swig","hash":"25f196afc193a7b192a49cb7d84db7d727a9e8c2","modified":1530291968911},{"_id":"themes/next-gux/layout/_scripts/analytics/baidu-analytics.swig","hash":"7c43d66da93cde65b473a7d6db2a86f9a42647d6","modified":1530291968911},{"_id":"themes/next-gux/layout/_scripts/analytics/busuanzi.swig","hash":"dee5f8ce80fc34fa2b0c914a45465c79da80612b","modified":1530291968911},{"_id":"themes/next-gux/layout/_scripts/analytics/google-analytics.swig","hash":"30a23fa7e816496fdec0e932aa42e2d13098a9c2","modified":1530291968911},{"_id":"themes/next-gux/layout/_scripts/comments/disqus.swig","hash":"3491d3cebabc8a28857200db28a1be65aad6adc2","modified":1530291968911},{"_id":"themes/next-gux/layout/_scripts/comments/duoshuo.swig","hash":"3351ea62225933f8045d036a79654e19e84d19a7","modified":1530291968911},{"_id":"themes/next-gux/layout/_scripts/pages/post-details.swig","hash":"b63ef233886538f30ced60344ac15d25e5f3e0af","modified":1530291968911},{"_id":"themes/next-gux/source/css/_custom/Mala.styl","hash":"ec81ad093b68bbb121e45d054a646f32397e137d","modified":1530291968915},{"_id":"themes/next-gux/source/css/_custom/Mala.styl.bak","hash":"4ec83e3e4ef02e67dca86615aa013c0b8a4f4b18","modified":1530291968915},{"_id":"themes/next-gux/source/css/_mixins/base.styl","hash":"66985fe77bd323f7f8f634908e17166f51e96e95","modified":1530291968915},{"_id":"themes/next-gux/source/css/_variables/Mala.styl","hash":"360aaa1746bc4032079493ff6027f8431f65b6df","modified":1530291968915},{"_id":"themes/next-gux/source/css/_variables/base.styl","hash":"fb85e5d8e37661c4c333b8aa08a7619cd7b3d046","modified":1530291968915},{"_id":"themes/next-gux/source/css/_variables/custom.styl","hash":"1a3e002602b0dff287b2463d2cd25c22f349a145","modified":1530291968915},{"_id":"themes/next-gux/source/fonts/icon-default/icomoon.eot","hash":"90763e97be18be78e65749075225cceeddc6fa8a","modified":1530291968915},{"_id":"themes/next-gux/source/fonts/icon-default/icomoon.svg","hash":"f92ad8cddc250f0bb5ca466fca95d321987e127e","modified":1530291968915},{"_id":"themes/next-gux/source/fonts/icon-default/icomoon.ttf","hash":"c093408e6030221cafc1f79d897f1fb5283c1178","modified":1530291968915},{"_id":"themes/next-gux/source/fonts/icon-default/icomoon.woff","hash":"dbe0368f2a65d87b13234cfea29d9783892fc7a8","modified":1530291968915},{"_id":"themes/next-gux/source/fonts/icon-default/selection.json","hash":"dc07c29f687315f9458f6b251c214768af865fb2","modified":1530291968915},{"_id":"themes/next-gux/source/fonts/icon-feather/icomoon.eot","hash":"11554b9e9d5b9f535ba96cbb27d45d8c8f1689fd","modified":1530291968915},{"_id":"themes/next-gux/source/fonts/icon-feather/icomoon.svg","hash":"d5eb756eefda9b454dcb23c2b1cefd4051d18d41","modified":1530291968915},{"_id":"themes/next-gux/source/fonts/icon-feather/icomoon.ttf","hash":"b2bbae4b613403cf61ad25037913378da1c07b8f","modified":1530291968915},{"_id":"themes/next-gux/source/fonts/icon-feather/icomoon.woff","hash":"2ea1c59c17422798e64ee6f4e9ce1f7aff1a06a5","modified":1530291968915},{"_id":"themes/next-gux/source/fonts/icon-feather/selection.json","hash":"06ea91e3f98ebe1080087acad4356802bc5b6ebf","modified":1530291968915},{"_id":"themes/next-gux/source/fonts/icon-fifty-shades/icomoon.eot","hash":"da86ba5df72d1288de9e9633e5f528062dd427d5","modified":1530291968915},{"_id":"themes/next-gux/source/fonts/icon-fifty-shades/icomoon.svg","hash":"1a4afd739e1f8eb8d430dbdd29e36a9999802e8d","modified":1530291968915},{"_id":"themes/next-gux/source/fonts/icon-fifty-shades/icomoon.ttf","hash":"72fe82e1f3db52414eed706952d385af241cb196","modified":1530291968915},{"_id":"themes/next-gux/source/fonts/icon-fifty-shades/icomoon.woff","hash":"4de6a74f523dee33d95dde61caae5809f9a5d448","modified":1530291968915},{"_id":"themes/next-gux/source/fonts/icon-fifty-shades/selection.json","hash":"fdd09098d1c3688e2c88cf33fd51e76b383b6d7f","modified":1530291968915},{"_id":"themes/next-gux/source/fonts/icon-icomoon/icomoon.eot","hash":"301fcf00c24750dddf1c529f944ca62c7f1a217d","modified":1530291968915},{"_id":"themes/next-gux/source/fonts/icon-icomoon/icomoon.svg","hash":"e316347805eb93425faa678611c5e42a7152da8f","modified":1530291968915},{"_id":"themes/next-gux/source/fonts/icon-icomoon/icomoon.ttf","hash":"f399713d1c9400d4d3373e38991a7e362a754a94","modified":1530291968915},{"_id":"themes/next-gux/source/fonts/icon-icomoon/icomoon.woff","hash":"05f1ec0bd307da5e731a86eb4961589a6042aebb","modified":1530291968915},{"_id":"themes/next-gux/source/fonts/icon-linecons/icomoon.eot","hash":"e2d7f040428a632f3c50bfa94083b759936effc2","modified":1530291968915},{"_id":"themes/next-gux/source/fonts/icon-linecons/icomoon.svg","hash":"808eaf7d61f7e67c76976265c885e79c36920f0b","modified":1530291968915},{"_id":"themes/next-gux/source/fonts/icon-linecons/icomoon.ttf","hash":"078068206684e4f185b0187ad3cee16f54a287d7","modified":1530291968915},{"_id":"themes/next-gux/source/fonts/icon-linecons/icomoon.woff","hash":"0b07ee6ceda3b1bceb40c1e7379b3aa48dcc15a8","modified":1530291968915},{"_id":"themes/next-gux/source/fonts/icon-linecons/selection.json","hash":"db4ce25d31449ecc6685b32e145252103967bb5c","modified":1530291968915},{"_id":"themes/next-gux/source/vendors/fastclick/.bower.json","hash":"93ebd5b35e632f714dcf1753e1f6db77ec74449b","modified":1530291968919},{"_id":"themes/next-gux/source/vendors/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1530291968919},{"_id":"themes/next-gux/source/vendors/fastclick/README.md","hash":"1decd8e1adad2cd6db0ab50cf56de6035156f4ea","modified":1530291968919},{"_id":"themes/next-gux/source/vendors/fastclick/bower.json","hash":"13379463c7463b4b96d13556b46faa4cc38d81e6","modified":1530291968919},{"_id":"themes/next-gux/source/vendors/jquery/.bower.json","hash":"91745c2cc6c946c7275f952b2b0760b880cea69e","modified":1530291968923},{"_id":"themes/next-gux/source/vendors/velocity/.bower.json","hash":"05f960846f1c7a93dab1d3f9a1121e86812e8c88","modified":1530291968923},{"_id":"themes/next-gux/source/vendors/velocity/bower.json","hash":"2ec99573e84c7117368beccb9e94b6bf35d2db03","modified":1530291968923},{"_id":"themes/next-gux/source/vendors/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1530291968923},{"_id":"themes/next-gux/source/vendors/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1530291968923},{"_id":"themes/next-gux/source/vendors/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1530291968923},{"_id":"themes/next-gux/source/css/_common/_page/home.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1530291968915},{"_id":"themes/next-gux/source/vendors/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1530291968923},{"_id":"themes/next-gux/source/css/_common/_component/back-to-top.styl","hash":"88cd66910260006aa8e9e795df4948d4b67bfa11","modified":1530291968911},{"_id":"themes/next-gux/source/css/_common/_component/buttons.styl","hash":"81063e0979f04a0f9af37f321d7321dda9abf593","modified":1530291968911},{"_id":"themes/next-gux/source/css/_common/_component/comments.styl","hash":"b468e452f29df359957731ee8846e165aef13b3d","modified":1530291968911},{"_id":"themes/next-gux/source/css/_common/_component/duoshuo.styl","hash":"c307f1e4827d7cb82816a5f9de109ae14ed4199c","modified":1530291968911},{"_id":"themes/next-gux/source/css/_common/_component/gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1530291968911},{"_id":"themes/next-gux/source/css/_common/_component/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1530291968911},{"_id":"themes/next-gux/source/css/_common/_component/pagination.styl","hash":"711c8830886619d4f4a0598b0cde5499dce50c62","modified":1530291968911},{"_id":"themes/next-gux/source/css/_common/_component/posts-collapse.styl","hash":"8f9e8f5f65956ccf1d52ff8526392803dff579d3","modified":1530291968911},{"_id":"themes/next-gux/source/css/_common/_component/posts-expand.styl","hash":"e5d24cc3b5486d1c24080161f2ea1d44e6bbcbb9","modified":1530291968911},{"_id":"themes/next-gux/source/css/_common/_component/posts-type.styl","hash":"40b593134bf96d1d6095b3439d47820659d7f10b","modified":1530291968911},{"_id":"themes/next-gux/source/css/_common/_component/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1530291968911},{"_id":"themes/next-gux/source/css/_common/_core/base.styl","hash":"e79a08484b191dca14ccfc005053eb95786dafae","modified":1530291968911},{"_id":"themes/next-gux/source/css/_common/_core/helpers.styl","hash":"41a31d651b60b4f458fc56a1d191dfbbdcb6d794","modified":1530291968911},{"_id":"themes/next-gux/source/css/_common/_core/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1530291968911},{"_id":"themes/next-gux/source/css/_common/_core/scaffolding.styl","hash":"584c636707e0c8bfd6efc936c1b3a0d35d14f29d","modified":1530291968911},{"_id":"themes/next-gux/source/css/_common/_core/tables.styl","hash":"f142a185fda68bc579e89ead9a31bc8fa0f3ca8c","modified":1530291968911},{"_id":"themes/next-gux/source/css/_common/_fonts/icon-default.styl","hash":"8b809aef383bebaeb3f282b47675f3a364ce3569","modified":1530291968915},{"_id":"themes/next-gux/source/css/_common/_fonts/icon-feather.styl","hash":"80413afacfa656322100ce1900fed1ebcd8f8f44","modified":1530291968915},{"_id":"themes/next-gux/source/css/_common/_fonts/icon-fifty-shades.styl","hash":"249f75bafa26b99d272352c0646e7497ea680b39","modified":1530291968915},{"_id":"themes/next-gux/source/css/_common/_fonts/icon-font.styl","hash":"ec3f86739bede393cafcd3e31052c01115ae20d6","modified":1530291968915},{"_id":"themes/next-gux/source/css/_common/_fonts/icon-linecons.styl","hash":"9cdbedb3627ac941cfb063b152abe5a75c3c699a","modified":1530291968915},{"_id":"themes/next-gux/source/css/_common/_page/archive.styl","hash":"dff879f55ca65fa79c07e9098719e53eeea7ac88","modified":1530291968915},{"_id":"themes/next-gux/source/css/_common/_page/categories.styl","hash":"4f696a2eaeee2f214adcf273eab25c62a398077a","modified":1530291968915},{"_id":"themes/next-gux/source/css/_common/_page/post-detail.styl","hash":"73796f6f13caa7151a2ee8e55755627e0d189f55","modified":1530291968915},{"_id":"themes/next-gux/source/css/_common/_section/body.styl","hash":"ca1a4766cbe25baac757c6b47a4858d221afdc40","modified":1530291968915},{"_id":"themes/next-gux/source/css/_common/_section/footer.styl","hash":"8994ffcce84deac0471532f270f97c44fea54dc0","modified":1530291968915},{"_id":"themes/next-gux/source/css/_common/_section/header.styl","hash":"ba501332fb111bd72dc0777f2e1c8a29ad538ff9","modified":1530291968915},{"_id":"themes/next-gux/source/css/_common/_section/layout.styl","hash":"4daaadd156ece64ae05908ad6bb0159c8a27c071","modified":1530291968915},{"_id":"themes/next-gux/source/css/_common/_section/media.styl","hash":"fa9809d2ecc753cf32f70803c1d0821c405211f4","modified":1530291968915},{"_id":"themes/next-gux/source/css/_common/_section/sidebar.styl","hash":"19ba3653e45187c064bbaf8142c2596a83ae7b08","modified":1530291968915},{"_id":"themes/next-gux/source/css/_schemes/Mala/index.styl","hash":"b2c5e70968c381ba9af79247aac4ef2891b0015c","modified":1530291968915},{"_id":"themes/next-gux/source/css/_schemes/default/_menu.styl","hash":"4bba29cece65ffc5122f4e052063dea4439fe4ae","modified":1530291968915},{"_id":"themes/next-gux/source/css/_schemes/default/_search.styl","hash":"c524bccdc554349106d1c8be9c3f275d4c0d4281","modified":1530291968915},{"_id":"themes/next-gux/source/css/_schemes/default/index.styl","hash":"2588e55132e10d82c0608f03c2c72a2bace8fa4e","modified":1530291968915},{"_id":"themes/next-gux/source/vendors/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1530291968919},{"_id":"themes/next-gux/source/vendors/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1530291968919},{"_id":"themes/next-gux/source/vendors/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1530291968919},{"_id":"themes/next-gux/source/vendors/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1530291968919},{"_id":"themes/next-gux/source/vendors/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1530291968919},{"_id":"themes/next-gux/source/vendors/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1530291968919},{"_id":"themes/next-gux/source/vendors/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1530291968919},{"_id":"themes/next-gux/source/vendors/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1530291968919},{"_id":"themes/next-gux/source/vendors/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1530291968919},{"_id":"themes/next-gux/source/vendors/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1530291968919},{"_id":"themes/next-gux/source/vendors/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1530291968919},{"_id":"themes/next-gux/layout/_partials/suprise/ball.swig","hash":"2c18d2cb89a054068fd04a9cf81c28fe3ac48120","modified":1530291968911},{"_id":"themes/next-gux/source/vendors/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1530291968923},{"_id":"themes/next-gux/source/css/_common/_vendor/highlight/highlight.styl","hash":"f3529b7da284c4b859429573c9b1004d32937e40","modified":1530291968915},{"_id":"themes/next-gux/source/css/_common/_vendor/highlight/theme.styl","hash":"ae19721ceee5ba460e131cb2427dae3c1ff39d6f","modified":1530291968915},{"_id":"themes/next-gux/source/vendors/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1530291968919},{"_id":"themes/next-gux/source/vendors/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1530291968919},{"_id":"themes/next-gux/source/vendors/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1530291968919},{"_id":"themes/next-gux/source/vendors/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1530291968919},{"_id":"themes/next-gux/source/vendors/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1530291968919},{"_id":"themes/next-gux/source/vendors/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1530291968919}],"Category":[{"name":"Spark源码阅读计划","_id":"cjj08c9nf0004ksng27uy7ekv"},{"name":"Spark调优","_id":"cjj08c9nl000aksngz0eu27px"},{"name":"工具","_id":"cjj08c9ny000mksngqb5zttc3"},{"name":"笔记","_id":"cjj08c9o60014ksng20vxucxz"},{"name":"数据挖掘基础","_id":"cjj08c9o7001bksngpzzdqv93"}],"Data":[],"Page":[{"_content":"## hello Im lishion\ngithhub: lishion\nwechat: lishion-me \n","source":"about/index.md","raw":"## hello Im lishion\ngithhub: lishion\nwechat: lishion-me \n","date":"2018-06-29T17:06:08.923Z","updated":"2018-06-29T17:06:08.923Z","path":"about/index.html","title":"","comments":1,"layout":"page","_id":"cjj08c9nb0001ksngdpwcvptv","content":"<h2 id=\"hello-Im-lishion\"><a href=\"#hello-Im-lishion\" class=\"headerlink\" title=\"hello Im lishion\"></a>hello Im lishion</h2><p>githhub: lishion<br>wechat: lishion-me </p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"hello-Im-lishion\"><a href=\"#hello-Im-lishion\" class=\"headerlink\" title=\"hello Im lishion\"></a>hello Im lishion</h2><p>githhub: lishion<br>wechat: lishion-me </p>\n"},{"title":"categories","date":"2018-06-14T09:23:48.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2018-06-14 17:23:48\ntype: \"categories\"\n---\n","updated":"2018-06-29T17:06:08.923Z","path":"categories/index.html","comments":1,"layout":"page","_id":"cjj08c9ne0003ksng2f8i5kqn","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"tags","date":"2018-06-14T09:20:42.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2018-06-14 17:20:42\ntype: \"tags\"\n---\n","updated":"2018-06-29T17:06:08.923Z","path":"tags/index.html","comments":1,"layout":"page","_id":"cjj08c9ni0007ksnglwiw9lo0","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"Spark 源码阅读计划 - 第三部分 - 图解 shuffle read","date":"2018-06-11T02:42:45.000Z","author":"lishion","toc":true,"_content":"\n你也许注意到了，今天的源码阅读计划后面有一个伪。没错，我们今天不读源码了，主要是因为我~~懒，想玩王者荣耀~~觉得前一章的 shuff write 源码的篇幅太长，害怕大家看完之后摸不着头脑。于是今天制作了 shuffle read 的图解版，顺便解决学习 spark 以来困扰我许久的几个问题。\n\n## 问题\n\n1. key 与如何与分区对应的\n\n   主要取决于分区器\n\n2. 分区发生在什么时候\n\n   建立初始RDD 以及 shuffle的时候\n\n3. key 与分区是一对一映射吗\n\n   不是。一个分区可以包含多个key，但同一个 key 只能存在一个分区中(如果使用hash 分区器)\n\n4. 如果同一个分区会存在多个key，那么在使用聚合函数的时候会不会把多个key的value聚合到一起?\n\n   当然不会，如果这样聚合就出错了。虽然聚合函数的最小处理对象是一个分区，但是实际上还是对 key 进行聚合。\n\n5. combineByKeyWithClassTag 中的 mergeValue 和 mergeCombiners 是否前者作用与分区内，后者作用于分区外(网上大部分都是这么说的，但其实是错误的)\n\n   不是，具体作用下面会讲解\n\n## 图解 shuffle write\n\n### 步骤一、分区 聚合\n\n这里是使用了 hash 表存放了聚合后的数据。与普通的 map 不同，数据的存储在一个 data 数组中。规则为索引为偶数的元素存放 KEY ，而对应的 value 存放在 KEY 的下一个元素，注意存放的 KEY 已经不是原始数据的 key，而是 key 以及其对的分区。由于在插入时使用二次探测法，数组中可能存在空值。用k1,v1表示原始数据键值对;`key=>p(key)`表示分区函数。数据表示流程图如下:\n\n{% asset_img part.png 分区并聚合 %}\n\n\n### 步骤二、排序并写入临时文件\n\n如果判断到 data 数组所占用的内存达到一定的阈值，就会对其中的数据进行排序。排序的方式为先对分区进行排序，分区内按照 key 的 hash 值进行排序。然后将排序好的数据写入临时文件，并产生一个空的 data 数组。这里可能发生多次写入临时文件的操作，也有可能最后仍有数据驻留在内存中而没有写入临时文件。例如，本分区一共有 10M 数据，内存阈值为 3M，则一共会发生 3 次写临时文件的操作以及最后会驻留1M的数据在内存中。需要注意的是这里并没有写入分区信息，只是写入原始的 key 以及对应聚合好的数据。\n\n### 步骤三、对所有的临时文件以及内存中驻留的文件进行排序\n\n由于存在多个临时文件并且内存中还有驻留的数据，因此需要对所有的数据进行排序并写入一个大文件中以减少临时文件的个数。这里的排序算法使用的方法大概是这样的:\n\n{% asset_img sort.png 排序 %}\n\n每次取出第一行的第一个元素就能得到排序结果，图中每一个竖向的数组就可以看做一个排序完成的临时文件。只不过在 shuffle write 并不是取前一行，而是取前一个分区。虽然文件中只存在 k c。但是在写临时文件时记录了每个分区对应的数据条数，因此仍然可以按分区取数据。\n\n这里还需要注意的一个地方是对于同一个 key 可能分布在不同的临时文件中，因此在排序的时候仍然需要对相同的 key 进行聚合。聚合方法其实很简单，例如获取到一条数据 (k1,c1)，如果下一条数据 (key,c) key == k1 就将 mergeCombiners 将 c1 与 c聚合，否则 key == k1 的数据已经全部读取完成，也就是剩下的数据中不会存在 key == k1 的数据。\n\n按照上面的方法不断的读取，聚合数据，写入文件就能将临时文件和内存中的所有数据进行聚合，排序并输出到一个大文件中。随后将每一个分区对应数据条数写入索引文件中，以便于记录大文件中某一条记录属于哪一个分区。完成以上步骤便完成了 shuffle write。\n\n## 总结\n\n读者: 等等，我要先吐个槽。说好的图解呢，怎么就两个图?\n\n作者: 本来有很多图的，但是我~~室友找我开黑~~要去跑步就只画了两个图。\n\n其实搞清楚 shuffle write 的大致步骤还是很简单的，但是具体到每一个细节就需要仔细的阅读源码了。这里推荐一波作者的[spark源码阅读计划](https://lishion.github.io/categories/Spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E8%AE%A1%E5%88%92/)。很~~混乱~~详细的解析了关于 shuffle write 每一部分代码，配合今天的~~图解~~食用更佳。\n\n\n\n\n\n\n\n","source":"_posts/Spark-源码阅读计划-伪-第三部分-图解-shuffle-write.md","raw":"---\ntitle: Spark 源码阅读计划 - 第三部分 - 图解 shuffle read\ndate: 2018-06-11 10:42:45\ntags:\n  - Spark\n  - 编程\ncategories: Spark源码阅读计划\nauthor: lishion\ntoc: true\n---\n\n你也许注意到了，今天的源码阅读计划后面有一个伪。没错，我们今天不读源码了，主要是因为我~~懒，想玩王者荣耀~~觉得前一章的 shuff write 源码的篇幅太长，害怕大家看完之后摸不着头脑。于是今天制作了 shuffle read 的图解版，顺便解决学习 spark 以来困扰我许久的几个问题。\n\n## 问题\n\n1. key 与如何与分区对应的\n\n   主要取决于分区器\n\n2. 分区发生在什么时候\n\n   建立初始RDD 以及 shuffle的时候\n\n3. key 与分区是一对一映射吗\n\n   不是。一个分区可以包含多个key，但同一个 key 只能存在一个分区中(如果使用hash 分区器)\n\n4. 如果同一个分区会存在多个key，那么在使用聚合函数的时候会不会把多个key的value聚合到一起?\n\n   当然不会，如果这样聚合就出错了。虽然聚合函数的最小处理对象是一个分区，但是实际上还是对 key 进行聚合。\n\n5. combineByKeyWithClassTag 中的 mergeValue 和 mergeCombiners 是否前者作用与分区内，后者作用于分区外(网上大部分都是这么说的，但其实是错误的)\n\n   不是，具体作用下面会讲解\n\n## 图解 shuffle write\n\n### 步骤一、分区 聚合\n\n这里是使用了 hash 表存放了聚合后的数据。与普通的 map 不同，数据的存储在一个 data 数组中。规则为索引为偶数的元素存放 KEY ，而对应的 value 存放在 KEY 的下一个元素，注意存放的 KEY 已经不是原始数据的 key，而是 key 以及其对的分区。由于在插入时使用二次探测法，数组中可能存在空值。用k1,v1表示原始数据键值对;`key=>p(key)`表示分区函数。数据表示流程图如下:\n\n{% asset_img part.png 分区并聚合 %}\n\n\n### 步骤二、排序并写入临时文件\n\n如果判断到 data 数组所占用的内存达到一定的阈值，就会对其中的数据进行排序。排序的方式为先对分区进行排序，分区内按照 key 的 hash 值进行排序。然后将排序好的数据写入临时文件，并产生一个空的 data 数组。这里可能发生多次写入临时文件的操作，也有可能最后仍有数据驻留在内存中而没有写入临时文件。例如，本分区一共有 10M 数据，内存阈值为 3M，则一共会发生 3 次写临时文件的操作以及最后会驻留1M的数据在内存中。需要注意的是这里并没有写入分区信息，只是写入原始的 key 以及对应聚合好的数据。\n\n### 步骤三、对所有的临时文件以及内存中驻留的文件进行排序\n\n由于存在多个临时文件并且内存中还有驻留的数据，因此需要对所有的数据进行排序并写入一个大文件中以减少临时文件的个数。这里的排序算法使用的方法大概是这样的:\n\n{% asset_img sort.png 排序 %}\n\n每次取出第一行的第一个元素就能得到排序结果，图中每一个竖向的数组就可以看做一个排序完成的临时文件。只不过在 shuffle write 并不是取前一行，而是取前一个分区。虽然文件中只存在 k c。但是在写临时文件时记录了每个分区对应的数据条数，因此仍然可以按分区取数据。\n\n这里还需要注意的一个地方是对于同一个 key 可能分布在不同的临时文件中，因此在排序的时候仍然需要对相同的 key 进行聚合。聚合方法其实很简单，例如获取到一条数据 (k1,c1)，如果下一条数据 (key,c) key == k1 就将 mergeCombiners 将 c1 与 c聚合，否则 key == k1 的数据已经全部读取完成，也就是剩下的数据中不会存在 key == k1 的数据。\n\n按照上面的方法不断的读取，聚合数据，写入文件就能将临时文件和内存中的所有数据进行聚合，排序并输出到一个大文件中。随后将每一个分区对应数据条数写入索引文件中，以便于记录大文件中某一条记录属于哪一个分区。完成以上步骤便完成了 shuffle write。\n\n## 总结\n\n读者: 等等，我要先吐个槽。说好的图解呢，怎么就两个图?\n\n作者: 本来有很多图的，但是我~~室友找我开黑~~要去跑步就只画了两个图。\n\n其实搞清楚 shuffle write 的大致步骤还是很简单的，但是具体到每一个细节就需要仔细的阅读源码了。这里推荐一波作者的[spark源码阅读计划](https://lishion.github.io/categories/Spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E8%AE%A1%E5%88%92/)。很~~混乱~~详细的解析了关于 shuffle write 每一部分代码，配合今天的~~图解~~食用更佳。\n\n\n\n\n\n\n\n","slug":"Spark-源码阅读计划-伪-第三部分-图解-shuffle-write","published":1,"updated":"2018-06-29T17:06:08.923Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjj08c9n70000ksngd8b0qh3f","content":"<p>你也许注意到了，今天的源码阅读计划后面有一个伪。没错，我们今天不读源码了，主要是因为我<del>懒，想玩王者荣耀</del>觉得前一章的 shuff write 源码的篇幅太长，害怕大家看完之后摸不着头脑。于是今天制作了 shuffle read 的图解版，顺便解决学习 spark 以来困扰我许久的几个问题。</p>\n<h2 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h2><ol>\n<li><p>key 与如何与分区对应的</p>\n<p>主要取决于分区器</p>\n</li>\n<li><p>分区发生在什么时候</p>\n<p>建立初始RDD 以及 shuffle的时候</p>\n</li>\n<li><p>key 与分区是一对一映射吗</p>\n<p>不是。一个分区可以包含多个key，但同一个 key 只能存在一个分区中(如果使用hash 分区器)</p>\n</li>\n<li><p>如果同一个分区会存在多个key，那么在使用聚合函数的时候会不会把多个key的value聚合到一起?</p>\n<p>当然不会，如果这样聚合就出错了。虽然聚合函数的最小处理对象是一个分区，但是实际上还是对 key 进行聚合。</p>\n</li>\n<li><p>combineByKeyWithClassTag 中的 mergeValue 和 mergeCombiners 是否前者作用与分区内，后者作用于分区外(网上大部分都是这么说的，但其实是错误的)</p>\n<p>不是，具体作用下面会讲解</p>\n</li>\n</ol>\n<h2 id=\"图解-shuffle-write\"><a href=\"#图解-shuffle-write\" class=\"headerlink\" title=\"图解 shuffle write\"></a>图解 shuffle write</h2><h3 id=\"步骤一、分区-聚合\"><a href=\"#步骤一、分区-聚合\" class=\"headerlink\" title=\"步骤一、分区 聚合\"></a>步骤一、分区 聚合</h3><p>这里是使用了 hash 表存放了聚合后的数据。与普通的 map 不同，数据的存储在一个 data 数组中。规则为索引为偶数的元素存放 KEY ，而对应的 value 存放在 KEY 的下一个元素，注意存放的 KEY 已经不是原始数据的 key，而是 key 以及其对的分区。由于在插入时使用二次探测法，数组中可能存在空值。用k1,v1表示原始数据键值对;<code>key=&gt;p(key)</code>表示分区函数。数据表示流程图如下:</p>\n\n<h3 id=\"步骤二、排序并写入临时文件\"><a href=\"#步骤二、排序并写入临时文件\" class=\"headerlink\" title=\"步骤二、排序并写入临时文件\"></a>步骤二、排序并写入临时文件</h3><p>如果判断到 data 数组所占用的内存达到一定的阈值，就会对其中的数据进行排序。排序的方式为先对分区进行排序，分区内按照 key 的 hash 值进行排序。然后将排序好的数据写入临时文件，并产生一个空的 data 数组。这里可能发生多次写入临时文件的操作，也有可能最后仍有数据驻留在内存中而没有写入临时文件。例如，本分区一共有 10M 数据，内存阈值为 3M，则一共会发生 3 次写临时文件的操作以及最后会驻留1M的数据在内存中。需要注意的是这里并没有写入分区信息，只是写入原始的 key 以及对应聚合好的数据。</p>\n<h3 id=\"步骤三、对所有的临时文件以及内存中驻留的文件进行排序\"><a href=\"#步骤三、对所有的临时文件以及内存中驻留的文件进行排序\" class=\"headerlink\" title=\"步骤三、对所有的临时文件以及内存中驻留的文件进行排序\"></a>步骤三、对所有的临时文件以及内存中驻留的文件进行排序</h3><p>由于存在多个临时文件并且内存中还有驻留的数据，因此需要对所有的数据进行排序并写入一个大文件中以减少临时文件的个数。这里的排序算法使用的方法大概是这样的:</p>\n\n<p>每次取出第一行的第一个元素就能得到排序结果，图中每一个竖向的数组就可以看做一个排序完成的临时文件。只不过在 shuffle write 并不是取前一行，而是取前一个分区。虽然文件中只存在 k c。但是在写临时文件时记录了每个分区对应的数据条数，因此仍然可以按分区取数据。</p>\n<p>这里还需要注意的一个地方是对于同一个 key 可能分布在不同的临时文件中，因此在排序的时候仍然需要对相同的 key 进行聚合。聚合方法其实很简单，例如获取到一条数据 (k1,c1)，如果下一条数据 (key,c) key == k1 就将 mergeCombiners 将 c1 与 c聚合，否则 key == k1 的数据已经全部读取完成，也就是剩下的数据中不会存在 key == k1 的数据。</p>\n<p>按照上面的方法不断的读取，聚合数据，写入文件就能将临时文件和内存中的所有数据进行聚合，排序并输出到一个大文件中。随后将每一个分区对应数据条数写入索引文件中，以便于记录大文件中某一条记录属于哪一个分区。完成以上步骤便完成了 shuffle write。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>读者: 等等，我要先吐个槽。说好的图解呢，怎么就两个图?</p>\n<p>作者: 本来有很多图的，但是我<del>室友找我开黑</del>要去跑步就只画了两个图。</p>\n<p>其实搞清楚 shuffle write 的大致步骤还是很简单的，但是具体到每一个细节就需要仔细的阅读源码了。这里推荐一波作者的<a href=\"https://lishion.github.io/categories/Spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E8%AE%A1%E5%88%92/\" target=\"_blank\" rel=\"noopener\">spark源码阅读计划</a>。很<del>混乱</del>详细的解析了关于 shuffle write 每一部分代码，配合今天的<del>图解</del>食用更佳。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>你也许注意到了，今天的源码阅读计划后面有一个伪。没错，我们今天不读源码了，主要是因为我<del>懒，想玩王者荣耀</del>觉得前一章的 shuff write 源码的篇幅太长，害怕大家看完之后摸不着头脑。于是今天制作了 shuffle read 的图解版，顺便解决学习 spark 以来困扰我许久的几个问题。</p>\n<h2 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h2><ol>\n<li><p>key 与如何与分区对应的</p>\n<p>主要取决于分区器</p>\n</li>\n<li><p>分区发生在什么时候</p>\n<p>建立初始RDD 以及 shuffle的时候</p>\n</li>\n<li><p>key 与分区是一对一映射吗</p>\n<p>不是。一个分区可以包含多个key，但同一个 key 只能存在一个分区中(如果使用hash 分区器)</p>\n</li>\n<li><p>如果同一个分区会存在多个key，那么在使用聚合函数的时候会不会把多个key的value聚合到一起?</p>\n<p>当然不会，如果这样聚合就出错了。虽然聚合函数的最小处理对象是一个分区，但是实际上还是对 key 进行聚合。</p>\n</li>\n<li><p>combineByKeyWithClassTag 中的 mergeValue 和 mergeCombiners 是否前者作用与分区内，后者作用于分区外(网上大部分都是这么说的，但其实是错误的)</p>\n<p>不是，具体作用下面会讲解</p>\n</li>\n</ol>\n<h2 id=\"图解-shuffle-write\"><a href=\"#图解-shuffle-write\" class=\"headerlink\" title=\"图解 shuffle write\"></a>图解 shuffle write</h2><h3 id=\"步骤一、分区-聚合\"><a href=\"#步骤一、分区-聚合\" class=\"headerlink\" title=\"步骤一、分区 聚合\"></a>步骤一、分区 聚合</h3><p>这里是使用了 hash 表存放了聚合后的数据。与普通的 map 不同，数据的存储在一个 data 数组中。规则为索引为偶数的元素存放 KEY ，而对应的 value 存放在 KEY 的下一个元素，注意存放的 KEY 已经不是原始数据的 key，而是 key 以及其对的分区。由于在插入时使用二次探测法，数组中可能存在空值。用k1,v1表示原始数据键值对;<code>key=&gt;p(key)</code>表示分区函数。数据表示流程图如下:</p>\n\n<h3 id=\"步骤二、排序并写入临时文件\"><a href=\"#步骤二、排序并写入临时文件\" class=\"headerlink\" title=\"步骤二、排序并写入临时文件\"></a>步骤二、排序并写入临时文件</h3><p>如果判断到 data 数组所占用的内存达到一定的阈值，就会对其中的数据进行排序。排序的方式为先对分区进行排序，分区内按照 key 的 hash 值进行排序。然后将排序好的数据写入临时文件，并产生一个空的 data 数组。这里可能发生多次写入临时文件的操作，也有可能最后仍有数据驻留在内存中而没有写入临时文件。例如，本分区一共有 10M 数据，内存阈值为 3M，则一共会发生 3 次写临时文件的操作以及最后会驻留1M的数据在内存中。需要注意的是这里并没有写入分区信息，只是写入原始的 key 以及对应聚合好的数据。</p>\n<h3 id=\"步骤三、对所有的临时文件以及内存中驻留的文件进行排序\"><a href=\"#步骤三、对所有的临时文件以及内存中驻留的文件进行排序\" class=\"headerlink\" title=\"步骤三、对所有的临时文件以及内存中驻留的文件进行排序\"></a>步骤三、对所有的临时文件以及内存中驻留的文件进行排序</h3><p>由于存在多个临时文件并且内存中还有驻留的数据，因此需要对所有的数据进行排序并写入一个大文件中以减少临时文件的个数。这里的排序算法使用的方法大概是这样的:</p>\n\n<p>每次取出第一行的第一个元素就能得到排序结果，图中每一个竖向的数组就可以看做一个排序完成的临时文件。只不过在 shuffle write 并不是取前一行，而是取前一个分区。虽然文件中只存在 k c。但是在写临时文件时记录了每个分区对应的数据条数，因此仍然可以按分区取数据。</p>\n<p>这里还需要注意的一个地方是对于同一个 key 可能分布在不同的临时文件中，因此在排序的时候仍然需要对相同的 key 进行聚合。聚合方法其实很简单，例如获取到一条数据 (k1,c1)，如果下一条数据 (key,c) key == k1 就将 mergeCombiners 将 c1 与 c聚合，否则 key == k1 的数据已经全部读取完成，也就是剩下的数据中不会存在 key == k1 的数据。</p>\n<p>按照上面的方法不断的读取，聚合数据，写入文件就能将临时文件和内存中的所有数据进行聚合，排序并输出到一个大文件中。随后将每一个分区对应数据条数写入索引文件中，以便于记录大文件中某一条记录属于哪一个分区。完成以上步骤便完成了 shuffle write。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>读者: 等等，我要先吐个槽。说好的图解呢，怎么就两个图?</p>\n<p>作者: 本来有很多图的，但是我<del>室友找我开黑</del>要去跑步就只画了两个图。</p>\n<p>其实搞清楚 shuffle write 的大致步骤还是很简单的，但是具体到每一个细节就需要仔细的阅读源码了。这里推荐一波作者的<a href=\"https://lishion.github.io/categories/Spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E8%AE%A1%E5%88%92/\" target=\"_blank\" rel=\"noopener\">spark源码阅读计划</a>。很<del>混乱</del>详细的解析了关于 shuffle write 每一部分代码，配合今天的<del>图解</del>食用更佳。</p>\n"},{"title":"Spark调优-选择合适的分区","date":"2018-06-15T03:48:57.000Z","_content":"\n昨天在利用 Spark 进行统计分析的时候大佬提醒我可以适当的减少分区数量来加快作业的执行速度。于是今天测试了一下，利用`sc.textFile(\"xxx\")`读取数据，采用默认分区(6657个)时执行时间为 190  秒。当手动使用 coalesce 进行重新分至 120 个分区时执行时间减少到 112 秒。当然这里的 120 并不是随便设置的，而是正好等于作业时候的并行度。\n\n## 原因\n\n我们知道分区的个数是等于 task 的个数的。如果分区数量远大于并行度，那么就会造成大量的时间浪费在 task 的切换中。而如果分区数量小于并行度，那么就会存在一些核心无法分配到需要的数据而闲置，造成资源浪费。因此在设置分区的时候尽量等于并行度。\n\n## 方法\n\n可以使用 coalesce 和 repartition 可以对 RDD 重新设置分区的个数。具体区别会在[Spark源码阅读计划](https://lishion.github.io/categories/Spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E8%AE%A1%E5%88%92/)中讲解。\n\n","source":"_posts/Spark优化-减少分区.md","raw":"---\ntitle: Spark调优-选择合适的分区\ndate: 2018-06-15 11:48:57\ntags: Spark\ncategories: Spark调优\n---\n\n昨天在利用 Spark 进行统计分析的时候大佬提醒我可以适当的减少分区数量来加快作业的执行速度。于是今天测试了一下，利用`sc.textFile(\"xxx\")`读取数据，采用默认分区(6657个)时执行时间为 190  秒。当手动使用 coalesce 进行重新分至 120 个分区时执行时间减少到 112 秒。当然这里的 120 并不是随便设置的，而是正好等于作业时候的并行度。\n\n## 原因\n\n我们知道分区的个数是等于 task 的个数的。如果分区数量远大于并行度，那么就会造成大量的时间浪费在 task 的切换中。而如果分区数量小于并行度，那么就会存在一些核心无法分配到需要的数据而闲置，造成资源浪费。因此在设置分区的时候尽量等于并行度。\n\n## 方法\n\n可以使用 coalesce 和 repartition 可以对 RDD 重新设置分区的个数。具体区别会在[Spark源码阅读计划](https://lishion.github.io/categories/Spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E8%AE%A1%E5%88%92/)中讲解。\n\n","slug":"Spark优化-减少分区","published":1,"updated":"2018-06-29T17:06:08.923Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjj08c9nc0002ksng6ebukqyc","content":"<p>昨天在利用 Spark 进行统计分析的时候大佬提醒我可以适当的减少分区数量来加快作业的执行速度。于是今天测试了一下，利用<code>sc.textFile(&quot;xxx&quot;)</code>读取数据，采用默认分区(6657个)时执行时间为 190  秒。当手动使用 coalesce 进行重新分至 120 个分区时执行时间减少到 112 秒。当然这里的 120 并不是随便设置的，而是正好等于作业时候的并行度。</p>\n<h2 id=\"原因\"><a href=\"#原因\" class=\"headerlink\" title=\"原因\"></a>原因</h2><p>我们知道分区的个数是等于 task 的个数的。如果分区数量远大于并行度，那么就会造成大量的时间浪费在 task 的切换中。而如果分区数量小于并行度，那么就会存在一些核心无法分配到需要的数据而闲置，造成资源浪费。因此在设置分区的时候尽量等于并行度。</p>\n<h2 id=\"方法\"><a href=\"#方法\" class=\"headerlink\" title=\"方法\"></a>方法</h2><p>可以使用 coalesce 和 repartition 可以对 RDD 重新设置分区的个数。具体区别会在<a href=\"https://lishion.github.io/categories/Spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E8%AE%A1%E5%88%92/\" target=\"_blank\" rel=\"noopener\">Spark源码阅读计划</a>中讲解。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>昨天在利用 Spark 进行统计分析的时候大佬提醒我可以适当的减少分区数量来加快作业的执行速度。于是今天测试了一下，利用<code>sc.textFile(&quot;xxx&quot;)</code>读取数据，采用默认分区(6657个)时执行时间为 190  秒。当手动使用 coalesce 进行重新分至 120 个分区时执行时间减少到 112 秒。当然这里的 120 并不是随便设置的，而是正好等于作业时候的并行度。</p>\n<h2 id=\"原因\"><a href=\"#原因\" class=\"headerlink\" title=\"原因\"></a>原因</h2><p>我们知道分区的个数是等于 task 的个数的。如果分区数量远大于并行度，那么就会造成大量的时间浪费在 task 的切换中。而如果分区数量小于并行度，那么就会存在一些核心无法分配到需要的数据而闲置，造成资源浪费。因此在设置分区的时候尽量等于并行度。</p>\n<h2 id=\"方法\"><a href=\"#方法\" class=\"headerlink\" title=\"方法\"></a>方法</h2><p>可以使用 coalesce 和 repartition 可以对 RDD 重新设置分区的个数。具体区别会在<a href=\"https://lishion.github.io/categories/Spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E8%AE%A1%E5%88%92/\" target=\"_blank\" rel=\"noopener\">Spark源码阅读计划</a>中讲解。</p>\n"},{"title":"Spark 源码阅读计划 - 第一部分 - 迭代计算","date":"2018-06-06T05:46:45.000Z","author":"lishion","toc":true,"_content":"首先立一个flag，这将是一个长期更新的版块。\n\n## 写在最开始\n\n在我使用`spark`进行日志分析的时候感受到了`spark`的便捷与强大。在学习`spark`初期，我阅读了许多与`spark`相关的文档，在这个过程中了解了`RDD`，`分区`，`shuffle`等概念，但是我并没有对这些概念有更多具体的认识。由于不了解`spark`的基本运作方式使得我在编写代码的时候十分困扰。由于这个原因，我准备阅读`spark`的源代码来解决我对基本概念的认识。\n\n## 本部分主要内容\n\n虽然该章节的名字叫做**迭代计算**，但是本章会讨论包括**RDD、分区、Job、迭代计算**等相关的内容，其中会重点讨论分区与迭代计算。因此在阅读本章之前你至少需要对这些提到的概念有一定的了解。\n\n## 准备工作\n\n你需要准备一份 Spark 的源代码以及一个便于全局搜索的编辑器|IDE。\n\n## 一个简单的例子\n\n假设现在我们有这样一个需求 : 寻找从 0 to 100 的偶数并输出其总数。利用 spark 编写代码如下:\n\n```scala\nsc.parallelize(0 to 100).filter(_%2 == 0).count\n//res0: Long = 51\n```\n\n这是一个非常简单的例子，但是里面却蕴涵了许多基础的知识。这一章的内容也是从这个例子展开的。\n\n## 迭代计算\n\n### comput 方法与 itertor \n\n在RDD.scala 定义的类 RDD中，有两个实现迭代计算的核心方法，分别是`compute`以及`itertor`方法。其中`itertor`方法如下:\n\n```scala\n  final def iterator(split: Partition, context: TaskContext): Iterator[T] = {\n    if (storageLevel != StorageLevel.NONE) {//如果有缓存\n      getOrCompute(split, context)\n    } else {\n      computeOrReadCheckpoint(split, context)\n    }\n  }\n```\n\n我们只关心这个方法的第一个参数**分区**，以及返回的迭代器。忽略有缓存的情况，我们继续看`computeOrReadCheckpoint`这个方法:\n\n```scala\n  private[spark] def computeOrReadCheckpoint(split: Partition, context: TaskContext): Iterator[T] =\n  {\n    if (isCheckpointedAndMaterialized) {//如果有checkpoint\n      firstParent[T].iterator(split, context)\n    } else {\n      compute(split, context)\n    }\n  }\n```\n\n可以看到，在没有缓存和 checkpoint 的情况下，itertor 方法直接调用了 compute 方法。而compute方法定义如下:\n\n```scala\n /**\n   * Implemented by subclasses to compute a given partition.\n   */\n  def compute(split: Partition, context: TaskContext): Iterator[T]\n```\n\n从注释可以看出，compute 方法应该由子类实现，用以计算给定分区并生成一个迭代器。如果不考虑缓存和 checkpoint 的情况下，简化一下代码:\n\n```scala\n final def iterator(split: Partition, context: TaskContext): Iterator[T] = {\n    compute(split,context)\n  }\n```\n\n那么实际上 iterator 的功能是: **接受一个分区，对这个分区进行计算，并返回计算结果的迭代器**。而之所以 compute 需要子类来实现，是因为只需要在子类中实现不同的 compute 方法，就能产生不同类型的 RDD。既然不同的RDD 有不同的功能，我们想知道之前的例子是如何进行迭代计算的，就需要知道这个例子中涉及到了哪些 RDD。\n\n首先查看`SparkContext`中与`parallelize`相关的部分代码:\n\n```scala\n  def parallelize[T: ClassTag](\n      seq: Seq[T],\n      numSlices: Int = defaultParallelism): RDD[T] = withScope {\n    assertNotStopped()\n    new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())\n  }\n```\n\n可以看到，`parallelize`实际返回了一个`ParallelCollectionRDD`。在`ParallelCollectionRDD`中并没有对`filter`方法进行重写，因此我们查看`RDD`中的`filter`方法:\n\n```scala\n def filter(f: T => Boolean): RDD[T] = withScope {\n    val cleanF = sc.clean(f)\n    new MapPartitionsRDD[T, T](\n      this,\n      (context, pid, iter) => iter.filter(cleanF),\n      preservesPartitioning = true)\n  }\n```\n\nfilter 方法返回了`MapPartitionsRDD`。为了方便起见，我将 ParallelCollectionRDD 类型的 RDD 称为 a，MapPartitionsRDD 类型的 RDD 成为B。那么先看一下 b 的 compute 方法:\n\n```scala\nprivate[spark] class MapPartitionsRDD[U: ClassTag, T: ClassTag](\n    var prev: RDD[T],\n    f: (TaskContext, Int, Iterator[T]) => Iterator[U],  // (TaskContext, partition index, iterator)\n    preservesPartitioning: Boolean = false)\n  extends RDD[U](prev) {\n  override val partitioner = if (preservesPartitioning) firstParent[T].partitioner else None\n  override def getPartitions: Array[Partition] = firstParent[T].partitions\n  override def compute(split: Partition, context: TaskContext): Iterator[U] =\n    f(context, split.index, firstParent[T].iterator(split, context))\n  override def clearDependencies() {\n    super.clearDependencies()\n    prev = null\n  }\n}\n\n```\n\n这里可以看到 compute 方法实际上调用 f 这个函数。而 f 这个函数是在 filter 方法中传入的`(context, pid, iter) => iter.filter(cleanF)` 。那么实际上 compute 进行的计算为:\n\n```scala\n(context, split.index, firstParent[T].iterator(split, context)) => firstParent[T].iterator(split, context).filter(cleanF)\n```\n\n前两个参数并没有用到，也就是最终的方法可以简化为:\n\n```scala\nfirstParent[T].iterator(split, context) => firstParent[T].iterator(split, context).filter(cleanF)\n```\n\n这里出现了一个`firstParent`，我们知道 RDD 之间存在依赖关系，如果rdd3 依赖 rdd2，rdd2 依赖 rdd1。rdd2 与 rdd3 都是rdd1 的一个父rdd, spark也将其称为血缘关系。而故名意思 rdd2 是 rdd3 的 firstParent。在这个例子中 a 是 b的 firstParent。那么在b 的 compute 中又调用了 a 的 iterator 方法。而 a 的 iterator 方法又会调用a 的 comput 方法。用箭头表示调用关系:\n\n```\nb.iterotr -> b.compute -> a.iterotr -> a.compute ->| 调用\nb.iterotr <- b.compute <- a.iterotr <- a.compute <-| 返回\n```\n\n而这里我们也可以看出来，b 调用 iterotr 返回的数据，是使用 b 中的方法对 a 返回数据进行处理。也就是b 的计算结果依赖于a的计算结果。如果将一个rdd比作一个函数，大概就是 `fb(fa())`，这样的。此时，我们已经得到了关于 spark 迭代的最简单的模型，假如我们有 n 个 rdd。那么之间的调用依赖关系便是:\n\n```\nn.iterotr -> n.compute -> (n-1).iterotr -> (n-1).compute ->...-> 1.iterotr -> 1.compute->| \nn.iterotr <- n.compute <- (n-1).iterotr <- (n-1).compute <-...<- 1.iterotr <- 1.compute<-|\n```\n\n那么细心的同学应该注意到了，所有的数据都是源自于第一个 RDD。这里把它称为源 RDD。例如本例中产生的 RDD 以及 textFile 方法产生的HadoopRDD都属于源RDD。既然属于源 RDD ，那么这个 RDD 一定会与内存或磁盘中的源数据产生交互，否则就没有真实的数据来源。这里的源数据是指内存的数组、本地文件、或者是HDFS上的分布式文件等。\n\n那么我们再继续看 a 的 compute 方法:\n\n```scala\n  override def compute(s: Partition, context: TaskContext): Iterator[T] = {\n    new InterruptibleIterator(context,s.asInstanceOf[ParallelCollectionPartition[T]].iterator)\n  }\n```\n\n可以看到 a 的 compute 根据 ParallelCollectionPartition 类的 iterator直接生成了一个迭代器。再看看ParallelCollectionPartition 的定义:\n\n```scala\nrivate[spark] class ParallelCollectionPartition[T: ClassTag](\n    var rddId: Long,\n    var slice: Int,\n    var values: Seq[T]\n  ) extends Partition with Serializable {\n  def iterator: Iterator[T] = values.iterator\n  ...\n```\n\niterator 是来自于 values 的一个迭代器，显然，values是内存中的数据。这说明 a 的 compute 方法直接返回了一个与源数据相关的迭代器。而这里 ParallelCollectionPartition 的数据是如何传入的，又是在什么时候被传入的呢?在**分区**小节将会提到。\n\n到这里，我们已经基本了解了 spark 的迭代计算的原理以及数据来源。但是仍然还有许多不了解的地方，例如 iterotr 会接受一个  Partition 作为参数。那么这个Partition参数到底起到了什么作用。其次，我们知道 rdd 的 iterotr 方法 是从后开始往前调用，那么又是谁调用的最后一个 rdd 的 itertor 方法呢。接下来我们就探索一下与分区相关的内容.\n\n### 分区\n\n提到分区，大部分都是类似于“RDD 中最小的数据单元”，“只是数据的抽象分区，而不是真正的持有数据”等等这样的描述。\n\n```scala\n/**\n * An identifier for a partition in an RDD.\n */\ntrait Partition extends Serializable {\n  def index: Int\n  override def hashCode(): Int = index\n  override def equals(other: Any): Boolean = super.equals(other)\n}\n```\n\n可以看到分区的类十分的简单，只有一个 index 属性。 当将分区这个参数传入 itertor 时表示我们需要 index 对应的分区的数据经过一系列计算的之后的迭代器。那么显然，在源 RDD 中一定会有更据分区的index获取对应数据的代码部分。而我们又知道，对于`ParallelCollectionPartition`这个分区类是直接持有数据的迭代器，因此我们只需要知道这个类如何被建立，便知道了是如何根据分区获取数据的。我们在整个工程中搜索 ParallelCollectionPartition，发现:\n\n```scala\n override def getPartitions: Array[Partition] = {\n    val slices = ParallelCollectionRDD.slice(data, numSlices).toArray\n    slices.indices.map(i => new ParallelCollectionPartition(id, i, slices(i))).toArray\n  }\n```\n\n中生成了一系列的分区对象。其中 ParallelCollectionRDD.slice() 方法根据总共有多少个分区来将 data  进行切分。而 data 我们在调用 parallelize() 方法时传入的。看到这里就应该明白了，**ParallelCollectionRDD 会将我们输入的数组进行切分，然后将利用分区对象持有数据的迭代器，当调用到 itertor 方法时就返回该分区的迭代器，供后续的RDD**进行计算。虽然这里我们只看了 ParallelCollectionRDD 的原代码，但是其他例如 HadoopRDD 的远离基本相同。 只不过一个是从内存中获取数据进行切分，另一个是从磁盘上获取数据进行切分。\n\n但是直到这里，我们仍然不知道 a 中的 itertor 方法的分区对象是如何传入的，因为这个方法间接被 b 的 itertor 方法调用。 而 b 的  itertor 方法同样也需要在外部被调用 ，因此要解决这个问题只需要找到 b 的 itertor 被调用的地方。不过我们首先可以根据代码猜测一下:\n\n```scala\n  override def compute(s: Partition, context: TaskContext): Iterator[T] = {\n    new InterruptibleIterator(context,s.asInstanceOf[ParallelCollectionPartition[T]].iterator)\n  }\n```\n\na 中的 compute 方法直接将传入的分区对象转为了 ParallelCollectionPartition 并获取了对应的迭代器。根据对象间强转的\n\n规则，传入的分区对象只能是 ParallelCollectionPartition 类型 ，因为其父类 RDD 并没有 iterator 方法。 并且传入的ParallelCollectionPartition 一定是持有源数据迭代器的对象，否则在 a 的 compute 中就无法向后返回迭代器。而且我们知道，spark 的计算是**惰性计算**，在调用 action 算子之后才会真正的开始计算。那么可以猜测，b 的 iterator 也是在调用了 count 方法之后才被调用的。为了证明这一点，我们继续查看代码:\n\n```scala\ndef count(): Long = sc.runJob(this, Utils.getIteratorSize _).sum\n```\n\n可以看到是调用了 SparkContext 中的runJob方法:\n\n```scala\n def runJob[T, U: ClassTag](\n     ...\n    dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)\n    ...\n  }\n```\n\n而 SparkContext 中的 runJob 又继续调用了 DAGScheduler 中的 runJob 方法， 继而调用了 submitJob 方法。之后由经历了 DAGSchedulerEventProcessLoop.post DAGSchedulerEventProcessLoop.doOnReceive 等方法，最后在 submitMissingTasks 看到建立了 ResultTask 对象。由于一个分区的数据会被一个task 处理，因此我们猜测在 ＲesultTask 中会有关于对应 rdd 调用 iterator 的信息，果然，在ResultTask中找到了 runTask 方法:\n\n```scala\noverride def runTask(context: TaskContext): U = {\n    // Deserialize the RDD and the func using the broadcast variables.\n    val threadMXBean = ManagementFactory.getThreadMXBean\n    val deserializeStartTime = System.currentTimeMillis()\n    val deserializeStartCpuTime = if (threadMXBean.isCurrentThreadCpuTimeSupported) {\n      threadMXBean.getCurrentThreadCpuTime\n    } else 0L\n    val ser = SparkEnv.get.closureSerializer.newInstance()\n    val (rdd, func) = ser.deserialize[(RDD[T], (TaskContext, Iterator[T]) => U)](\n      ByteBuffer.wrap(taskBinary.value), Thread.currentThread.getContextClassLoader)\n    _executorDeserializeTime = System.currentTimeMillis() - deserializeStartTime\n    _executorDeserializeCpuTime = if (threadMXBean.isCurrentThreadCpuTimeSupported) {\n      threadMXBean.getCurrentThreadCpuTime - deserializeStartCpuTime\n    } else 0L\n\n    func(context, rdd.iterator(partition, context))//这里就 b 的iterator被真正调用的地方\n  }\n\n\n```\n\n那么我们可以看到最后一行有一个rdd的调用，而这个rdd 是反徐序列化得的，很明显，这个rdd就是 b。此时，b 调用 iterator 方法的地方找到了，但是分区对象partition依然没有找到从哪里传入。由于 partition 是 ResultTask 构造时传入的，我们回到 submitMissingTasks 中，创建ResultTask时分区参数对应的为变量 part 。 \n\n```scala\n      //从需要计算的分区map中获取分区，并生成task\n       partitionsToCompute.map { id =>\n            val p: Int = stage.partitions(id)\n            val part = partitions(p)\n            val locs = taskIdToLocations(id)\n            new ResultTask(stage.id, stage.latestInfo.attemptNumber,\n              taskBinary, part, locs, id, properties, serializedTaskMetrics,\n              Option(jobId), Option(sc.applicationId), sc.applicationAttemptId)\n          }\n```\n\n继续往上看，可以看到 part 是 从 partitions 中获取的。这里我们也可以看出 **一个任务对应一个分区数据**。继续往上看，发现`partitions = stage.rdd.partitions`。实际上来自于 Stage 中的 rdd 中的 partitions。那么看Stage 对 rdd 这个属性的描述:\n\n```scala\n *\n * @param rdd RDD that this stage runs on: for a shuffle map stage, it's the RDD we run map tasks\n *   on, while for a result stage, it's the target RDD that we ran an action on\n *   这里的意思是 rdd 表示调用 action 算子的 rdd 也就是 b \n */\nprivate[scheduler] abstract class Stage(\n    val id: Int,\n    val rdd: RDD[_],\n    val numTasks: Int,\n    val parents: List[Stage],\n    val firstJobId: Int,\n    val callSite: CallSite)\n  extends Logging {\n```\n\n我们发现一个惊人的事实 ，这个 rdd 居然也是 b。也就是说，我们在 runTask函数中实际调用的方法是这样的:\n\n```scala\n  func(context, b.iterator(b.partitions(index), context))//这里就 b 的iterator被真正调用的地方\n```\n\nindex 表示该task对应需要执行的分区标号。随即我们继续看 b 中的 partitions 的定义:\n\n```scala\n override def getPartitions: Array[Partition] = firstParent[T].partitions\n```\n\n说明这个 partitions 来自于 a。 而 a 中的 partitions 实际上又来自于:\n\n```scala\n override def getPartitions: Array[Partition] = {\n    val slices = ParallelCollectionRDD.slice(data, numSlices).toArray\n    slices.indices.map(i => new ParallelCollectionPartition(id, i, slices(i))).toArray\n  }\n```\n\n这样最终 runTask 函数实际调用的方法为:\n\n```scala\n func(context, b.iterator(a.partitions(index), context))//这里就 b 的iterator被真正调用的地方\n```\n\n这和我们猜测的相同，这个分区确实为 ParallelCollectionPartition 对象，并且也持有了源数据的迭代器。这里其实有一个很巧妙的地方，虽然 RDD  的迭代计算是从后往前调用，但是传入源 RDD 的分区对象依然来自于源 RDD 自身。\n\n到了这里，我们也就明白分区对象是如何和数据关联起来的。ParallelCollectionPartition 对象中存在分区编号 index 以及 源数据的迭代器，通过分区编号就能获取到源数据的迭代器。\n\n## 总结\n\n经过如此大篇幅的讲解，终于对 spark 的迭代计算以及分区做了一个简单的分析。虽然还有很多地方不完善，例如还有其它类型的分区对象没有讲解，但是我相信你能完全看懂这篇文章所讲的内容，其他类型的分区对象对于你来说也很简单了。作者水平有限，并且这也是作者的第一篇关于 spark 源码阅读的博客，难免有遗漏和错误。如果想如给作者提建议和意见的话，请联系作者的微信或直接在 github 上提 issue 。\n\n \n\n","source":"_posts/Spark-源码阅读计划-第一部分-迭代计算.md","raw":"---\ntitle: Spark 源码阅读计划 - 第一部分 - 迭代计算\ndate: 2018-06-06 13:46:45\ntags:\n  - Spark\n  - 编程\ncategories: Spark源码阅读计划\nauthor: lishion\ntoc: true\n---\n首先立一个flag，这将是一个长期更新的版块。\n\n## 写在最开始\n\n在我使用`spark`进行日志分析的时候感受到了`spark`的便捷与强大。在学习`spark`初期，我阅读了许多与`spark`相关的文档，在这个过程中了解了`RDD`，`分区`，`shuffle`等概念，但是我并没有对这些概念有更多具体的认识。由于不了解`spark`的基本运作方式使得我在编写代码的时候十分困扰。由于这个原因，我准备阅读`spark`的源代码来解决我对基本概念的认识。\n\n## 本部分主要内容\n\n虽然该章节的名字叫做**迭代计算**，但是本章会讨论包括**RDD、分区、Job、迭代计算**等相关的内容，其中会重点讨论分区与迭代计算。因此在阅读本章之前你至少需要对这些提到的概念有一定的了解。\n\n## 准备工作\n\n你需要准备一份 Spark 的源代码以及一个便于全局搜索的编辑器|IDE。\n\n## 一个简单的例子\n\n假设现在我们有这样一个需求 : 寻找从 0 to 100 的偶数并输出其总数。利用 spark 编写代码如下:\n\n```scala\nsc.parallelize(0 to 100).filter(_%2 == 0).count\n//res0: Long = 51\n```\n\n这是一个非常简单的例子，但是里面却蕴涵了许多基础的知识。这一章的内容也是从这个例子展开的。\n\n## 迭代计算\n\n### comput 方法与 itertor \n\n在RDD.scala 定义的类 RDD中，有两个实现迭代计算的核心方法，分别是`compute`以及`itertor`方法。其中`itertor`方法如下:\n\n```scala\n  final def iterator(split: Partition, context: TaskContext): Iterator[T] = {\n    if (storageLevel != StorageLevel.NONE) {//如果有缓存\n      getOrCompute(split, context)\n    } else {\n      computeOrReadCheckpoint(split, context)\n    }\n  }\n```\n\n我们只关心这个方法的第一个参数**分区**，以及返回的迭代器。忽略有缓存的情况，我们继续看`computeOrReadCheckpoint`这个方法:\n\n```scala\n  private[spark] def computeOrReadCheckpoint(split: Partition, context: TaskContext): Iterator[T] =\n  {\n    if (isCheckpointedAndMaterialized) {//如果有checkpoint\n      firstParent[T].iterator(split, context)\n    } else {\n      compute(split, context)\n    }\n  }\n```\n\n可以看到，在没有缓存和 checkpoint 的情况下，itertor 方法直接调用了 compute 方法。而compute方法定义如下:\n\n```scala\n /**\n   * Implemented by subclasses to compute a given partition.\n   */\n  def compute(split: Partition, context: TaskContext): Iterator[T]\n```\n\n从注释可以看出，compute 方法应该由子类实现，用以计算给定分区并生成一个迭代器。如果不考虑缓存和 checkpoint 的情况下，简化一下代码:\n\n```scala\n final def iterator(split: Partition, context: TaskContext): Iterator[T] = {\n    compute(split,context)\n  }\n```\n\n那么实际上 iterator 的功能是: **接受一个分区，对这个分区进行计算，并返回计算结果的迭代器**。而之所以 compute 需要子类来实现，是因为只需要在子类中实现不同的 compute 方法，就能产生不同类型的 RDD。既然不同的RDD 有不同的功能，我们想知道之前的例子是如何进行迭代计算的，就需要知道这个例子中涉及到了哪些 RDD。\n\n首先查看`SparkContext`中与`parallelize`相关的部分代码:\n\n```scala\n  def parallelize[T: ClassTag](\n      seq: Seq[T],\n      numSlices: Int = defaultParallelism): RDD[T] = withScope {\n    assertNotStopped()\n    new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())\n  }\n```\n\n可以看到，`parallelize`实际返回了一个`ParallelCollectionRDD`。在`ParallelCollectionRDD`中并没有对`filter`方法进行重写，因此我们查看`RDD`中的`filter`方法:\n\n```scala\n def filter(f: T => Boolean): RDD[T] = withScope {\n    val cleanF = sc.clean(f)\n    new MapPartitionsRDD[T, T](\n      this,\n      (context, pid, iter) => iter.filter(cleanF),\n      preservesPartitioning = true)\n  }\n```\n\nfilter 方法返回了`MapPartitionsRDD`。为了方便起见，我将 ParallelCollectionRDD 类型的 RDD 称为 a，MapPartitionsRDD 类型的 RDD 成为B。那么先看一下 b 的 compute 方法:\n\n```scala\nprivate[spark] class MapPartitionsRDD[U: ClassTag, T: ClassTag](\n    var prev: RDD[T],\n    f: (TaskContext, Int, Iterator[T]) => Iterator[U],  // (TaskContext, partition index, iterator)\n    preservesPartitioning: Boolean = false)\n  extends RDD[U](prev) {\n  override val partitioner = if (preservesPartitioning) firstParent[T].partitioner else None\n  override def getPartitions: Array[Partition] = firstParent[T].partitions\n  override def compute(split: Partition, context: TaskContext): Iterator[U] =\n    f(context, split.index, firstParent[T].iterator(split, context))\n  override def clearDependencies() {\n    super.clearDependencies()\n    prev = null\n  }\n}\n\n```\n\n这里可以看到 compute 方法实际上调用 f 这个函数。而 f 这个函数是在 filter 方法中传入的`(context, pid, iter) => iter.filter(cleanF)` 。那么实际上 compute 进行的计算为:\n\n```scala\n(context, split.index, firstParent[T].iterator(split, context)) => firstParent[T].iterator(split, context).filter(cleanF)\n```\n\n前两个参数并没有用到，也就是最终的方法可以简化为:\n\n```scala\nfirstParent[T].iterator(split, context) => firstParent[T].iterator(split, context).filter(cleanF)\n```\n\n这里出现了一个`firstParent`，我们知道 RDD 之间存在依赖关系，如果rdd3 依赖 rdd2，rdd2 依赖 rdd1。rdd2 与 rdd3 都是rdd1 的一个父rdd, spark也将其称为血缘关系。而故名意思 rdd2 是 rdd3 的 firstParent。在这个例子中 a 是 b的 firstParent。那么在b 的 compute 中又调用了 a 的 iterator 方法。而 a 的 iterator 方法又会调用a 的 comput 方法。用箭头表示调用关系:\n\n```\nb.iterotr -> b.compute -> a.iterotr -> a.compute ->| 调用\nb.iterotr <- b.compute <- a.iterotr <- a.compute <-| 返回\n```\n\n而这里我们也可以看出来，b 调用 iterotr 返回的数据，是使用 b 中的方法对 a 返回数据进行处理。也就是b 的计算结果依赖于a的计算结果。如果将一个rdd比作一个函数，大概就是 `fb(fa())`，这样的。此时，我们已经得到了关于 spark 迭代的最简单的模型，假如我们有 n 个 rdd。那么之间的调用依赖关系便是:\n\n```\nn.iterotr -> n.compute -> (n-1).iterotr -> (n-1).compute ->...-> 1.iterotr -> 1.compute->| \nn.iterotr <- n.compute <- (n-1).iterotr <- (n-1).compute <-...<- 1.iterotr <- 1.compute<-|\n```\n\n那么细心的同学应该注意到了，所有的数据都是源自于第一个 RDD。这里把它称为源 RDD。例如本例中产生的 RDD 以及 textFile 方法产生的HadoopRDD都属于源RDD。既然属于源 RDD ，那么这个 RDD 一定会与内存或磁盘中的源数据产生交互，否则就没有真实的数据来源。这里的源数据是指内存的数组、本地文件、或者是HDFS上的分布式文件等。\n\n那么我们再继续看 a 的 compute 方法:\n\n```scala\n  override def compute(s: Partition, context: TaskContext): Iterator[T] = {\n    new InterruptibleIterator(context,s.asInstanceOf[ParallelCollectionPartition[T]].iterator)\n  }\n```\n\n可以看到 a 的 compute 根据 ParallelCollectionPartition 类的 iterator直接生成了一个迭代器。再看看ParallelCollectionPartition 的定义:\n\n```scala\nrivate[spark] class ParallelCollectionPartition[T: ClassTag](\n    var rddId: Long,\n    var slice: Int,\n    var values: Seq[T]\n  ) extends Partition with Serializable {\n  def iterator: Iterator[T] = values.iterator\n  ...\n```\n\niterator 是来自于 values 的一个迭代器，显然，values是内存中的数据。这说明 a 的 compute 方法直接返回了一个与源数据相关的迭代器。而这里 ParallelCollectionPartition 的数据是如何传入的，又是在什么时候被传入的呢?在**分区**小节将会提到。\n\n到这里，我们已经基本了解了 spark 的迭代计算的原理以及数据来源。但是仍然还有许多不了解的地方，例如 iterotr 会接受一个  Partition 作为参数。那么这个Partition参数到底起到了什么作用。其次，我们知道 rdd 的 iterotr 方法 是从后开始往前调用，那么又是谁调用的最后一个 rdd 的 itertor 方法呢。接下来我们就探索一下与分区相关的内容.\n\n### 分区\n\n提到分区，大部分都是类似于“RDD 中最小的数据单元”，“只是数据的抽象分区，而不是真正的持有数据”等等这样的描述。\n\n```scala\n/**\n * An identifier for a partition in an RDD.\n */\ntrait Partition extends Serializable {\n  def index: Int\n  override def hashCode(): Int = index\n  override def equals(other: Any): Boolean = super.equals(other)\n}\n```\n\n可以看到分区的类十分的简单，只有一个 index 属性。 当将分区这个参数传入 itertor 时表示我们需要 index 对应的分区的数据经过一系列计算的之后的迭代器。那么显然，在源 RDD 中一定会有更据分区的index获取对应数据的代码部分。而我们又知道，对于`ParallelCollectionPartition`这个分区类是直接持有数据的迭代器，因此我们只需要知道这个类如何被建立，便知道了是如何根据分区获取数据的。我们在整个工程中搜索 ParallelCollectionPartition，发现:\n\n```scala\n override def getPartitions: Array[Partition] = {\n    val slices = ParallelCollectionRDD.slice(data, numSlices).toArray\n    slices.indices.map(i => new ParallelCollectionPartition(id, i, slices(i))).toArray\n  }\n```\n\n中生成了一系列的分区对象。其中 ParallelCollectionRDD.slice() 方法根据总共有多少个分区来将 data  进行切分。而 data 我们在调用 parallelize() 方法时传入的。看到这里就应该明白了，**ParallelCollectionRDD 会将我们输入的数组进行切分，然后将利用分区对象持有数据的迭代器，当调用到 itertor 方法时就返回该分区的迭代器，供后续的RDD**进行计算。虽然这里我们只看了 ParallelCollectionRDD 的原代码，但是其他例如 HadoopRDD 的远离基本相同。 只不过一个是从内存中获取数据进行切分，另一个是从磁盘上获取数据进行切分。\n\n但是直到这里，我们仍然不知道 a 中的 itertor 方法的分区对象是如何传入的，因为这个方法间接被 b 的 itertor 方法调用。 而 b 的  itertor 方法同样也需要在外部被调用 ，因此要解决这个问题只需要找到 b 的 itertor 被调用的地方。不过我们首先可以根据代码猜测一下:\n\n```scala\n  override def compute(s: Partition, context: TaskContext): Iterator[T] = {\n    new InterruptibleIterator(context,s.asInstanceOf[ParallelCollectionPartition[T]].iterator)\n  }\n```\n\na 中的 compute 方法直接将传入的分区对象转为了 ParallelCollectionPartition 并获取了对应的迭代器。根据对象间强转的\n\n规则，传入的分区对象只能是 ParallelCollectionPartition 类型 ，因为其父类 RDD 并没有 iterator 方法。 并且传入的ParallelCollectionPartition 一定是持有源数据迭代器的对象，否则在 a 的 compute 中就无法向后返回迭代器。而且我们知道，spark 的计算是**惰性计算**，在调用 action 算子之后才会真正的开始计算。那么可以猜测，b 的 iterator 也是在调用了 count 方法之后才被调用的。为了证明这一点，我们继续查看代码:\n\n```scala\ndef count(): Long = sc.runJob(this, Utils.getIteratorSize _).sum\n```\n\n可以看到是调用了 SparkContext 中的runJob方法:\n\n```scala\n def runJob[T, U: ClassTag](\n     ...\n    dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)\n    ...\n  }\n```\n\n而 SparkContext 中的 runJob 又继续调用了 DAGScheduler 中的 runJob 方法， 继而调用了 submitJob 方法。之后由经历了 DAGSchedulerEventProcessLoop.post DAGSchedulerEventProcessLoop.doOnReceive 等方法，最后在 submitMissingTasks 看到建立了 ResultTask 对象。由于一个分区的数据会被一个task 处理，因此我们猜测在 ＲesultTask 中会有关于对应 rdd 调用 iterator 的信息，果然，在ResultTask中找到了 runTask 方法:\n\n```scala\noverride def runTask(context: TaskContext): U = {\n    // Deserialize the RDD and the func using the broadcast variables.\n    val threadMXBean = ManagementFactory.getThreadMXBean\n    val deserializeStartTime = System.currentTimeMillis()\n    val deserializeStartCpuTime = if (threadMXBean.isCurrentThreadCpuTimeSupported) {\n      threadMXBean.getCurrentThreadCpuTime\n    } else 0L\n    val ser = SparkEnv.get.closureSerializer.newInstance()\n    val (rdd, func) = ser.deserialize[(RDD[T], (TaskContext, Iterator[T]) => U)](\n      ByteBuffer.wrap(taskBinary.value), Thread.currentThread.getContextClassLoader)\n    _executorDeserializeTime = System.currentTimeMillis() - deserializeStartTime\n    _executorDeserializeCpuTime = if (threadMXBean.isCurrentThreadCpuTimeSupported) {\n      threadMXBean.getCurrentThreadCpuTime - deserializeStartCpuTime\n    } else 0L\n\n    func(context, rdd.iterator(partition, context))//这里就 b 的iterator被真正调用的地方\n  }\n\n\n```\n\n那么我们可以看到最后一行有一个rdd的调用，而这个rdd 是反徐序列化得的，很明显，这个rdd就是 b。此时，b 调用 iterator 方法的地方找到了，但是分区对象partition依然没有找到从哪里传入。由于 partition 是 ResultTask 构造时传入的，我们回到 submitMissingTasks 中，创建ResultTask时分区参数对应的为变量 part 。 \n\n```scala\n      //从需要计算的分区map中获取分区，并生成task\n       partitionsToCompute.map { id =>\n            val p: Int = stage.partitions(id)\n            val part = partitions(p)\n            val locs = taskIdToLocations(id)\n            new ResultTask(stage.id, stage.latestInfo.attemptNumber,\n              taskBinary, part, locs, id, properties, serializedTaskMetrics,\n              Option(jobId), Option(sc.applicationId), sc.applicationAttemptId)\n          }\n```\n\n继续往上看，可以看到 part 是 从 partitions 中获取的。这里我们也可以看出 **一个任务对应一个分区数据**。继续往上看，发现`partitions = stage.rdd.partitions`。实际上来自于 Stage 中的 rdd 中的 partitions。那么看Stage 对 rdd 这个属性的描述:\n\n```scala\n *\n * @param rdd RDD that this stage runs on: for a shuffle map stage, it's the RDD we run map tasks\n *   on, while for a result stage, it's the target RDD that we ran an action on\n *   这里的意思是 rdd 表示调用 action 算子的 rdd 也就是 b \n */\nprivate[scheduler] abstract class Stage(\n    val id: Int,\n    val rdd: RDD[_],\n    val numTasks: Int,\n    val parents: List[Stage],\n    val firstJobId: Int,\n    val callSite: CallSite)\n  extends Logging {\n```\n\n我们发现一个惊人的事实 ，这个 rdd 居然也是 b。也就是说，我们在 runTask函数中实际调用的方法是这样的:\n\n```scala\n  func(context, b.iterator(b.partitions(index), context))//这里就 b 的iterator被真正调用的地方\n```\n\nindex 表示该task对应需要执行的分区标号。随即我们继续看 b 中的 partitions 的定义:\n\n```scala\n override def getPartitions: Array[Partition] = firstParent[T].partitions\n```\n\n说明这个 partitions 来自于 a。 而 a 中的 partitions 实际上又来自于:\n\n```scala\n override def getPartitions: Array[Partition] = {\n    val slices = ParallelCollectionRDD.slice(data, numSlices).toArray\n    slices.indices.map(i => new ParallelCollectionPartition(id, i, slices(i))).toArray\n  }\n```\n\n这样最终 runTask 函数实际调用的方法为:\n\n```scala\n func(context, b.iterator(a.partitions(index), context))//这里就 b 的iterator被真正调用的地方\n```\n\n这和我们猜测的相同，这个分区确实为 ParallelCollectionPartition 对象，并且也持有了源数据的迭代器。这里其实有一个很巧妙的地方，虽然 RDD  的迭代计算是从后往前调用，但是传入源 RDD 的分区对象依然来自于源 RDD 自身。\n\n到了这里，我们也就明白分区对象是如何和数据关联起来的。ParallelCollectionPartition 对象中存在分区编号 index 以及 源数据的迭代器，通过分区编号就能获取到源数据的迭代器。\n\n## 总结\n\n经过如此大篇幅的讲解，终于对 spark 的迭代计算以及分区做了一个简单的分析。虽然还有很多地方不完善，例如还有其它类型的分区对象没有讲解，但是我相信你能完全看懂这篇文章所讲的内容，其他类型的分区对象对于你来说也很简单了。作者水平有限，并且这也是作者的第一篇关于 spark 源码阅读的博客，难免有遗漏和错误。如果想如给作者提建议和意见的话，请联系作者的微信或直接在 github 上提 issue 。\n\n \n\n","slug":"Spark-源码阅读计划-第一部分-迭代计算","published":1,"updated":"2018-06-29T17:06:08.923Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjj08c9nh0006ksngu4ndif7o","content":"<p>首先立一个flag，这将是一个长期更新的版块。</p>\n<h2 id=\"写在最开始\"><a href=\"#写在最开始\" class=\"headerlink\" title=\"写在最开始\"></a>写在最开始</h2><p>在我使用<code>spark</code>进行日志分析的时候感受到了<code>spark</code>的便捷与强大。在学习<code>spark</code>初期，我阅读了许多与<code>spark</code>相关的文档，在这个过程中了解了<code>RDD</code>，<code>分区</code>，<code>shuffle</code>等概念，但是我并没有对这些概念有更多具体的认识。由于不了解<code>spark</code>的基本运作方式使得我在编写代码的时候十分困扰。由于这个原因，我准备阅读<code>spark</code>的源代码来解决我对基本概念的认识。</p>\n<h2 id=\"本部分主要内容\"><a href=\"#本部分主要内容\" class=\"headerlink\" title=\"本部分主要内容\"></a>本部分主要内容</h2><p>虽然该章节的名字叫做<strong>迭代计算</strong>，但是本章会讨论包括<strong>RDD、分区、Job、迭代计算</strong>等相关的内容，其中会重点讨论分区与迭代计算。因此在阅读本章之前你至少需要对这些提到的概念有一定的了解。</p>\n<h2 id=\"准备工作\"><a href=\"#准备工作\" class=\"headerlink\" title=\"准备工作\"></a>准备工作</h2><p>你需要准备一份 Spark 的源代码以及一个便于全局搜索的编辑器|IDE。</p>\n<h2 id=\"一个简单的例子\"><a href=\"#一个简单的例子\" class=\"headerlink\" title=\"一个简单的例子\"></a>一个简单的例子</h2><p>假设现在我们有这样一个需求 : 寻找从 0 to 100 的偶数并输出其总数。利用 spark 编写代码如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sc.parallelize(<span class=\"number\">0</span> to <span class=\"number\">100</span>).filter(_%<span class=\"number\">2</span> == <span class=\"number\">0</span>).count</span><br><span class=\"line\"><span class=\"comment\">//res0: Long = 51</span></span><br></pre></td></tr></table></figure>\n<p>这是一个非常简单的例子，但是里面却蕴涵了许多基础的知识。这一章的内容也是从这个例子展开的。</p>\n<h2 id=\"迭代计算\"><a href=\"#迭代计算\" class=\"headerlink\" title=\"迭代计算\"></a>迭代计算</h2><h3 id=\"comput-方法与-itertor\"><a href=\"#comput-方法与-itertor\" class=\"headerlink\" title=\"comput 方法与 itertor\"></a>comput 方法与 itertor</h3><p>在RDD.scala 定义的类 RDD中，有两个实现迭代计算的核心方法，分别是<code>compute</code>以及<code>itertor</code>方法。其中<code>itertor</code>方法如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">final</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">iterator</span></span>(split: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>] = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (storageLevel != <span class=\"type\">StorageLevel</span>.<span class=\"type\">NONE</span>) &#123;<span class=\"comment\">//如果有缓存</span></span><br><span class=\"line\">    getOrCompute(split, context)</span><br><span class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">    computeOrReadCheckpoint(split, context)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>我们只关心这个方法的第一个参数<strong>分区</strong>，以及返回的迭代器。忽略有缓存的情况，我们继续看<code>computeOrReadCheckpoint</code>这个方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span>[spark] <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">computeOrReadCheckpoint</span></span>(split: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>] =</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (isCheckpointedAndMaterialized) &#123;<span class=\"comment\">//如果有checkpoint</span></span><br><span class=\"line\">    firstParent[<span class=\"type\">T</span>].iterator(split, context)</span><br><span class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">    compute(split, context)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可以看到，在没有缓存和 checkpoint 的情况下，itertor 方法直接调用了 compute 方法。而compute方法定义如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">  * Implemented by subclasses to compute a given partition.</span></span><br><span class=\"line\"><span class=\"comment\">  */</span></span><br><span class=\"line\"> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span></span>(split: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>]</span><br></pre></td></tr></table></figure>\n<p>从注释可以看出，compute 方法应该由子类实现，用以计算给定分区并生成一个迭代器。如果不考虑缓存和 checkpoint 的情况下，简化一下代码:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">final</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">iterator</span></span>(split: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>] = &#123;</span><br><span class=\"line\">   compute(split,context)</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>那么实际上 iterator 的功能是: <strong>接受一个分区，对这个分区进行计算，并返回计算结果的迭代器</strong>。而之所以 compute 需要子类来实现，是因为只需要在子类中实现不同的 compute 方法，就能产生不同类型的 RDD。既然不同的RDD 有不同的功能，我们想知道之前的例子是如何进行迭代计算的，就需要知道这个例子中涉及到了哪些 RDD。</p>\n<p>首先查看<code>SparkContext</code>中与<code>parallelize</code>相关的部分代码:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">parallelize</span></span>[<span class=\"type\">T</span>: <span class=\"type\">ClassTag</span>](</span><br><span class=\"line\">    seq: <span class=\"type\">Seq</span>[<span class=\"type\">T</span>],</span><br><span class=\"line\">    numSlices: <span class=\"type\">Int</span> = defaultParallelism): <span class=\"type\">RDD</span>[<span class=\"type\">T</span>] = withScope &#123;</span><br><span class=\"line\">  assertNotStopped()</span><br><span class=\"line\">  <span class=\"keyword\">new</span> <span class=\"type\">ParallelCollectionRDD</span>[<span class=\"type\">T</span>](<span class=\"keyword\">this</span>, seq, numSlices, <span class=\"type\">Map</span>[<span class=\"type\">Int</span>, <span class=\"type\">Seq</span>[<span class=\"type\">String</span>]]())</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可以看到，<code>parallelize</code>实际返回了一个<code>ParallelCollectionRDD</code>。在<code>ParallelCollectionRDD</code>中并没有对<code>filter</code>方法进行重写，因此我们查看<code>RDD</code>中的<code>filter</code>方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">filter</span></span>(f: <span class=\"type\">T</span> =&gt; <span class=\"type\">Boolean</span>): <span class=\"type\">RDD</span>[<span class=\"type\">T</span>] = withScope &#123;</span><br><span class=\"line\">   <span class=\"keyword\">val</span> cleanF = sc.clean(f)</span><br><span class=\"line\">   <span class=\"keyword\">new</span> <span class=\"type\">MapPartitionsRDD</span>[<span class=\"type\">T</span>, <span class=\"type\">T</span>](</span><br><span class=\"line\">     <span class=\"keyword\">this</span>,</span><br><span class=\"line\">     (context, pid, iter) =&gt; iter.filter(cleanF),</span><br><span class=\"line\">     preservesPartitioning = <span class=\"literal\">true</span>)</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>filter 方法返回了<code>MapPartitionsRDD</code>。为了方便起见，我将 ParallelCollectionRDD 类型的 RDD 称为 a，MapPartitionsRDD 类型的 RDD 成为B。那么先看一下 b 的 compute 方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span>[spark] <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MapPartitionsRDD</span>[<span class=\"type\">U</span>: <span class=\"type\">ClassTag</span>, <span class=\"type\">T</span>: <span class=\"type\">ClassTag</span>](<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    var prev: <span class=\"type\">RDD</span>[<span class=\"type\">T</span>],</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    f: (<span class=\"type\">TaskContext</span>, <span class=\"type\">Int</span>, <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>]</span>) <span class=\"title\">=&gt;</span> <span class=\"title\">Iterator</span>[<span class=\"type\">U</span>],  <span class=\"title\">//</span> (<span class=\"params\"><span class=\"type\">TaskContext</span>, partition index, iterator</span>)</span></span><br><span class=\"line\"><span class=\"class\">    <span class=\"title\">preservesPartitioning</span></span>: <span class=\"type\">Boolean</span> = <span class=\"literal\">false</span>)</span><br><span class=\"line\">  <span class=\"keyword\">extends</span> <span class=\"type\">RDD</span>[<span class=\"type\">U</span>](prev) &#123;</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"keyword\">val</span> partitioner = <span class=\"keyword\">if</span> (preservesPartitioning) firstParent[<span class=\"type\">T</span>].partitioner <span class=\"keyword\">else</span> <span class=\"type\">None</span></span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPartitions</span></span>: <span class=\"type\">Array</span>[<span class=\"type\">Partition</span>] = firstParent[<span class=\"type\">T</span>].partitions</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span></span>(split: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">U</span>] =</span><br><span class=\"line\">    f(context, split.index, firstParent[<span class=\"type\">T</span>].iterator(split, context))</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">clearDependencies</span></span>() &#123;</span><br><span class=\"line\">    <span class=\"keyword\">super</span>.clearDependencies()</span><br><span class=\"line\">    prev = <span class=\"literal\">null</span></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>这里可以看到 compute 方法实际上调用 f 这个函数。而 f 这个函数是在 filter 方法中传入的<code>(context, pid, iter) =&gt; iter.filter(cleanF)</code> 。那么实际上 compute 进行的计算为:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(context, split.index, firstParent[<span class=\"type\">T</span>].iterator(split, context)) =&gt; firstParent[<span class=\"type\">T</span>].iterator(split, context).filter(cleanF)</span><br></pre></td></tr></table></figure>\n<p>前两个参数并没有用到，也就是最终的方法可以简化为:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">firstParent[<span class=\"type\">T</span>].iterator(split, context) =&gt; firstParent[<span class=\"type\">T</span>].iterator(split, context).filter(cleanF)</span><br></pre></td></tr></table></figure>\n<p>这里出现了一个<code>firstParent</code>，我们知道 RDD 之间存在依赖关系，如果rdd3 依赖 rdd2，rdd2 依赖 rdd1。rdd2 与 rdd3 都是rdd1 的一个父rdd, spark也将其称为血缘关系。而故名意思 rdd2 是 rdd3 的 firstParent。在这个例子中 a 是 b的 firstParent。那么在b 的 compute 中又调用了 a 的 iterator 方法。而 a 的 iterator 方法又会调用a 的 comput 方法。用箭头表示调用关系:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">b.iterotr -&gt; b.compute -&gt; a.iterotr -&gt; a.compute -&gt;| 调用</span><br><span class=\"line\">b.iterotr &lt;- b.compute &lt;- a.iterotr &lt;- a.compute &lt;-| 返回</span><br></pre></td></tr></table></figure>\n<p>而这里我们也可以看出来，b 调用 iterotr 返回的数据，是使用 b 中的方法对 a 返回数据进行处理。也就是b 的计算结果依赖于a的计算结果。如果将一个rdd比作一个函数，大概就是 <code>fb(fa())</code>，这样的。此时，我们已经得到了关于 spark 迭代的最简单的模型，假如我们有 n 个 rdd。那么之间的调用依赖关系便是:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">n.iterotr -&gt; n.compute -&gt; (n-1).iterotr -&gt; (n-1).compute -&gt;...-&gt; 1.iterotr -&gt; 1.compute-&gt;| </span><br><span class=\"line\">n.iterotr &lt;- n.compute &lt;- (n-1).iterotr &lt;- (n-1).compute &lt;-...&lt;- 1.iterotr &lt;- 1.compute&lt;-|</span><br></pre></td></tr></table></figure>\n<p>那么细心的同学应该注意到了，所有的数据都是源自于第一个 RDD。这里把它称为源 RDD。例如本例中产生的 RDD 以及 textFile 方法产生的HadoopRDD都属于源RDD。既然属于源 RDD ，那么这个 RDD 一定会与内存或磁盘中的源数据产生交互，否则就没有真实的数据来源。这里的源数据是指内存的数组、本地文件、或者是HDFS上的分布式文件等。</p>\n<p>那么我们再继续看 a 的 compute 方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span></span>(s: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>] = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">new</span> <span class=\"type\">InterruptibleIterator</span>(context,s.asInstanceOf[<span class=\"type\">ParallelCollectionPartition</span>[<span class=\"type\">T</span>]].iterator)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可以看到 a 的 compute 根据 ParallelCollectionPartition 类的 iterator直接生成了一个迭代器。再看看ParallelCollectionPartition 的定义:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rivate[spark] <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ParallelCollectionPartition</span>[<span class=\"type\">T</span>: <span class=\"type\">ClassTag</span>](<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    var rddId: <span class=\"type\">Long</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    var slice: <span class=\"type\">Int</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    var values: <span class=\"type\">Seq</span>[<span class=\"type\">T</span>]</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">  </span>) <span class=\"keyword\">extends</span> <span class=\"title\">Partition</span> <span class=\"keyword\">with</span> <span class=\"title\">Serializable</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">iterator</span></span>: <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>] = values.iterator</span><br><span class=\"line\">  ...</span><br></pre></td></tr></table></figure>\n<p>iterator 是来自于 values 的一个迭代器，显然，values是内存中的数据。这说明 a 的 compute 方法直接返回了一个与源数据相关的迭代器。而这里 ParallelCollectionPartition 的数据是如何传入的，又是在什么时候被传入的呢?在<strong>分区</strong>小节将会提到。</p>\n<p>到这里，我们已经基本了解了 spark 的迭代计算的原理以及数据来源。但是仍然还有许多不了解的地方，例如 iterotr 会接受一个  Partition 作为参数。那么这个Partition参数到底起到了什么作用。其次，我们知道 rdd 的 iterotr 方法 是从后开始往前调用，那么又是谁调用的最后一个 rdd 的 itertor 方法呢。接下来我们就探索一下与分区相关的内容.</p>\n<h3 id=\"分区\"><a href=\"#分区\" class=\"headerlink\" title=\"分区\"></a>分区</h3><p>提到分区，大部分都是类似于“RDD 中最小的数据单元”，“只是数据的抽象分区，而不是真正的持有数据”等等这样的描述。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * An identifier for a partition in an RDD.</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">trait</span> <span class=\"title\">Partition</span> <span class=\"keyword\">extends</span> <span class=\"title\">Serializable</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">index</span></span>: <span class=\"type\">Int</span></span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hashCode</span></span>(): <span class=\"type\">Int</span> = index</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">equals</span></span>(other: <span class=\"type\">Any</span>): <span class=\"type\">Boolean</span> = <span class=\"keyword\">super</span>.equals(other)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可以看到分区的类十分的简单，只有一个 index 属性。 当将分区这个参数传入 itertor 时表示我们需要 index 对应的分区的数据经过一系列计算的之后的迭代器。那么显然，在源 RDD 中一定会有更据分区的index获取对应数据的代码部分。而我们又知道，对于<code>ParallelCollectionPartition</code>这个分区类是直接持有数据的迭代器，因此我们只需要知道这个类如何被建立，便知道了是如何根据分区获取数据的。我们在整个工程中搜索 ParallelCollectionPartition，发现:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPartitions</span></span>: <span class=\"type\">Array</span>[<span class=\"type\">Partition</span>] = &#123;</span><br><span class=\"line\">   <span class=\"keyword\">val</span> slices = <span class=\"type\">ParallelCollectionRDD</span>.slice(data, numSlices).toArray</span><br><span class=\"line\">   slices.indices.map(i =&gt; <span class=\"keyword\">new</span> <span class=\"type\">ParallelCollectionPartition</span>(id, i, slices(i))).toArray</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>中生成了一系列的分区对象。其中 ParallelCollectionRDD.slice() 方法根据总共有多少个分区来将 data  进行切分。而 data 我们在调用 parallelize() 方法时传入的。看到这里就应该明白了，<strong>ParallelCollectionRDD 会将我们输入的数组进行切分，然后将利用分区对象持有数据的迭代器，当调用到 itertor 方法时就返回该分区的迭代器，供后续的RDD</strong>进行计算。虽然这里我们只看了 ParallelCollectionRDD 的原代码，但是其他例如 HadoopRDD 的远离基本相同。 只不过一个是从内存中获取数据进行切分，另一个是从磁盘上获取数据进行切分。</p>\n<p>但是直到这里，我们仍然不知道 a 中的 itertor 方法的分区对象是如何传入的，因为这个方法间接被 b 的 itertor 方法调用。 而 b 的  itertor 方法同样也需要在外部被调用 ，因此要解决这个问题只需要找到 b 的 itertor 被调用的地方。不过我们首先可以根据代码猜测一下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span></span>(s: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>] = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">new</span> <span class=\"type\">InterruptibleIterator</span>(context,s.asInstanceOf[<span class=\"type\">ParallelCollectionPartition</span>[<span class=\"type\">T</span>]].iterator)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>a 中的 compute 方法直接将传入的分区对象转为了 ParallelCollectionPartition 并获取了对应的迭代器。根据对象间强转的</p>\n<p>规则，传入的分区对象只能是 ParallelCollectionPartition 类型 ，因为其父类 RDD 并没有 iterator 方法。 并且传入的ParallelCollectionPartition 一定是持有源数据迭代器的对象，否则在 a 的 compute 中就无法向后返回迭代器。而且我们知道，spark 的计算是<strong>惰性计算</strong>，在调用 action 算子之后才会真正的开始计算。那么可以猜测，b 的 iterator 也是在调用了 count 方法之后才被调用的。为了证明这一点，我们继续查看代码:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">count</span></span>(): <span class=\"type\">Long</span> = sc.runJob(<span class=\"keyword\">this</span>, <span class=\"type\">Utils</span>.getIteratorSize _).sum</span><br></pre></td></tr></table></figure>\n<p>可以看到是调用了 SparkContext 中的runJob方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">runJob</span></span>[<span class=\"type\">T</span>, <span class=\"type\">U</span>: <span class=\"type\">ClassTag</span>](</span><br><span class=\"line\">    ...</span><br><span class=\"line\">   dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)</span><br><span class=\"line\">   ...</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>而 SparkContext 中的 runJob 又继续调用了 DAGScheduler 中的 runJob 方法， 继而调用了 submitJob 方法。之后由经历了 DAGSchedulerEventProcessLoop.post DAGSchedulerEventProcessLoop.doOnReceive 等方法，最后在 submitMissingTasks 看到建立了 ResultTask 对象。由于一个分区的数据会被一个task 处理，因此我们猜测在 ＲesultTask 中会有关于对应 rdd 调用 iterator 的信息，果然，在ResultTask中找到了 runTask 方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">runTask</span></span>(context: <span class=\"type\">TaskContext</span>): <span class=\"type\">U</span> = &#123;</span><br><span class=\"line\">    <span class=\"comment\">// Deserialize the RDD and the func using the broadcast variables.</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> threadMXBean = <span class=\"type\">ManagementFactory</span>.getThreadMXBean</span><br><span class=\"line\">    <span class=\"keyword\">val</span> deserializeStartTime = <span class=\"type\">System</span>.currentTimeMillis()</span><br><span class=\"line\">    <span class=\"keyword\">val</span> deserializeStartCpuTime = <span class=\"keyword\">if</span> (threadMXBean.isCurrentThreadCpuTimeSupported) &#123;</span><br><span class=\"line\">      threadMXBean.getCurrentThreadCpuTime</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> <span class=\"number\">0</span>L</span><br><span class=\"line\">    <span class=\"keyword\">val</span> ser = <span class=\"type\">SparkEnv</span>.get.closureSerializer.newInstance()</span><br><span class=\"line\">    <span class=\"keyword\">val</span> (rdd, func) = ser.deserialize[(<span class=\"type\">RDD</span>[<span class=\"type\">T</span>], (<span class=\"type\">TaskContext</span>, <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>]) =&gt; <span class=\"type\">U</span>)](</span><br><span class=\"line\">      <span class=\"type\">ByteBuffer</span>.wrap(taskBinary.value), <span class=\"type\">Thread</span>.currentThread.getContextClassLoader)</span><br><span class=\"line\">    _executorDeserializeTime = <span class=\"type\">System</span>.currentTimeMillis() - deserializeStartTime</span><br><span class=\"line\">    _executorDeserializeCpuTime = <span class=\"keyword\">if</span> (threadMXBean.isCurrentThreadCpuTimeSupported) &#123;</span><br><span class=\"line\">      threadMXBean.getCurrentThreadCpuTime - deserializeStartCpuTime</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> <span class=\"number\">0</span>L</span><br><span class=\"line\"></span><br><span class=\"line\">    func(context, rdd.iterator(partition, context))<span class=\"comment\">//这里就 b 的iterator被真正调用的地方</span></span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>那么我们可以看到最后一行有一个rdd的调用，而这个rdd 是反徐序列化得的，很明显，这个rdd就是 b。此时，b 调用 iterator 方法的地方找到了，但是分区对象partition依然没有找到从哪里传入。由于 partition 是 ResultTask 构造时传入的，我们回到 submitMissingTasks 中，创建ResultTask时分区参数对应的为变量 part 。 </p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//从需要计算的分区map中获取分区，并生成task</span></span><br><span class=\"line\"> partitionsToCompute.map &#123; id =&gt;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> p: <span class=\"type\">Int</span> = stage.partitions(id)</span><br><span class=\"line\">      <span class=\"keyword\">val</span> part = partitions(p)</span><br><span class=\"line\">      <span class=\"keyword\">val</span> locs = taskIdToLocations(id)</span><br><span class=\"line\">      <span class=\"keyword\">new</span> <span class=\"type\">ResultTask</span>(stage.id, stage.latestInfo.attemptNumber,</span><br><span class=\"line\">        taskBinary, part, locs, id, properties, serializedTaskMetrics,</span><br><span class=\"line\">        <span class=\"type\">Option</span>(jobId), <span class=\"type\">Option</span>(sc.applicationId), sc.applicationAttemptId)</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>继续往上看，可以看到 part 是 从 partitions 中获取的。这里我们也可以看出 <strong>一个任务对应一个分区数据</strong>。继续往上看，发现<code>partitions = stage.rdd.partitions</code>。实际上来自于 Stage 中的 rdd 中的 partitions。那么看Stage 对 rdd 这个属性的描述:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> *</span><br><span class=\"line\"> * <span class=\"meta\">@param</span> rdd <span class=\"type\">RDD</span> that <span class=\"keyword\">this</span> stage runs on: <span class=\"keyword\">for</span> a shuffle map stage, it<span class=\"symbol\">'s</span> the <span class=\"type\">RDD</span> we run map tasks</span><br><span class=\"line\"> *   on, <span class=\"keyword\">while</span> <span class=\"keyword\">for</span> a result stage, it<span class=\"symbol\">'s</span> the target <span class=\"type\">RDD</span> that we ran an action on</span><br><span class=\"line\"> *   这里的意思是 rdd 表示调用 action 算子的 rdd 也就是 b </span><br><span class=\"line\"> */</span><br><span class=\"line\"><span class=\"keyword\">private</span>[scheduler] <span class=\"keyword\">abstract</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Stage</span>(<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val id: <span class=\"type\">Int</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val rdd: <span class=\"type\">RDD</span>[_],</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val numTasks: <span class=\"type\">Int</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val parents: <span class=\"type\">List</span>[<span class=\"type\">Stage</span>],</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val firstJobId: <span class=\"type\">Int</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val callSite: <span class=\"type\">CallSite</span></span>)</span></span><br><span class=\"line\"><span class=\"class\">  <span class=\"keyword\">extends</span> <span class=\"title\">Logging</span> </span>&#123;</span><br></pre></td></tr></table></figure>\n<p>我们发现一个惊人的事实 ，这个 rdd 居然也是 b。也就是说，我们在 runTask函数中实际调用的方法是这样的:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func(context, b.iterator(b.partitions(index), context))<span class=\"comment\">//这里就 b 的iterator被真正调用的地方</span></span><br></pre></td></tr></table></figure>\n<p>index 表示该task对应需要执行的分区标号。随即我们继续看 b 中的 partitions 的定义:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPartitions</span></span>: <span class=\"type\">Array</span>[<span class=\"type\">Partition</span>] = firstParent[<span class=\"type\">T</span>].partitions</span><br></pre></td></tr></table></figure>\n<p>说明这个 partitions 来自于 a。 而 a 中的 partitions 实际上又来自于:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPartitions</span></span>: <span class=\"type\">Array</span>[<span class=\"type\">Partition</span>] = &#123;</span><br><span class=\"line\">   <span class=\"keyword\">val</span> slices = <span class=\"type\">ParallelCollectionRDD</span>.slice(data, numSlices).toArray</span><br><span class=\"line\">   slices.indices.map(i =&gt; <span class=\"keyword\">new</span> <span class=\"type\">ParallelCollectionPartition</span>(id, i, slices(i))).toArray</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>这样最终 runTask 函数实际调用的方法为:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func(context, b.iterator(a.partitions(index), context))<span class=\"comment\">//这里就 b 的iterator被真正调用的地方</span></span><br></pre></td></tr></table></figure>\n<p>这和我们猜测的相同，这个分区确实为 ParallelCollectionPartition 对象，并且也持有了源数据的迭代器。这里其实有一个很巧妙的地方，虽然 RDD  的迭代计算是从后往前调用，但是传入源 RDD 的分区对象依然来自于源 RDD 自身。</p>\n<p>到了这里，我们也就明白分区对象是如何和数据关联起来的。ParallelCollectionPartition 对象中存在分区编号 index 以及 源数据的迭代器，通过分区编号就能获取到源数据的迭代器。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>经过如此大篇幅的讲解，终于对 spark 的迭代计算以及分区做了一个简单的分析。虽然还有很多地方不完善，例如还有其它类型的分区对象没有讲解，但是我相信你能完全看懂这篇文章所讲的内容，其他类型的分区对象对于你来说也很简单了。作者水平有限，并且这也是作者的第一篇关于 spark 源码阅读的博客，难免有遗漏和错误。如果想如给作者提建议和意见的话，请联系作者的微信或直接在 github 上提 issue 。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>首先立一个flag，这将是一个长期更新的版块。</p>\n<h2 id=\"写在最开始\"><a href=\"#写在最开始\" class=\"headerlink\" title=\"写在最开始\"></a>写在最开始</h2><p>在我使用<code>spark</code>进行日志分析的时候感受到了<code>spark</code>的便捷与强大。在学习<code>spark</code>初期，我阅读了许多与<code>spark</code>相关的文档，在这个过程中了解了<code>RDD</code>，<code>分区</code>，<code>shuffle</code>等概念，但是我并没有对这些概念有更多具体的认识。由于不了解<code>spark</code>的基本运作方式使得我在编写代码的时候十分困扰。由于这个原因，我准备阅读<code>spark</code>的源代码来解决我对基本概念的认识。</p>\n<h2 id=\"本部分主要内容\"><a href=\"#本部分主要内容\" class=\"headerlink\" title=\"本部分主要内容\"></a>本部分主要内容</h2><p>虽然该章节的名字叫做<strong>迭代计算</strong>，但是本章会讨论包括<strong>RDD、分区、Job、迭代计算</strong>等相关的内容，其中会重点讨论分区与迭代计算。因此在阅读本章之前你至少需要对这些提到的概念有一定的了解。</p>\n<h2 id=\"准备工作\"><a href=\"#准备工作\" class=\"headerlink\" title=\"准备工作\"></a>准备工作</h2><p>你需要准备一份 Spark 的源代码以及一个便于全局搜索的编辑器|IDE。</p>\n<h2 id=\"一个简单的例子\"><a href=\"#一个简单的例子\" class=\"headerlink\" title=\"一个简单的例子\"></a>一个简单的例子</h2><p>假设现在我们有这样一个需求 : 寻找从 0 to 100 的偶数并输出其总数。利用 spark 编写代码如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sc.parallelize(<span class=\"number\">0</span> to <span class=\"number\">100</span>).filter(_%<span class=\"number\">2</span> == <span class=\"number\">0</span>).count</span><br><span class=\"line\"><span class=\"comment\">//res0: Long = 51</span></span><br></pre></td></tr></table></figure>\n<p>这是一个非常简单的例子，但是里面却蕴涵了许多基础的知识。这一章的内容也是从这个例子展开的。</p>\n<h2 id=\"迭代计算\"><a href=\"#迭代计算\" class=\"headerlink\" title=\"迭代计算\"></a>迭代计算</h2><h3 id=\"comput-方法与-itertor\"><a href=\"#comput-方法与-itertor\" class=\"headerlink\" title=\"comput 方法与 itertor\"></a>comput 方法与 itertor</h3><p>在RDD.scala 定义的类 RDD中，有两个实现迭代计算的核心方法，分别是<code>compute</code>以及<code>itertor</code>方法。其中<code>itertor</code>方法如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">final</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">iterator</span></span>(split: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>] = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (storageLevel != <span class=\"type\">StorageLevel</span>.<span class=\"type\">NONE</span>) &#123;<span class=\"comment\">//如果有缓存</span></span><br><span class=\"line\">    getOrCompute(split, context)</span><br><span class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">    computeOrReadCheckpoint(split, context)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>我们只关心这个方法的第一个参数<strong>分区</strong>，以及返回的迭代器。忽略有缓存的情况，我们继续看<code>computeOrReadCheckpoint</code>这个方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span>[spark] <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">computeOrReadCheckpoint</span></span>(split: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>] =</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (isCheckpointedAndMaterialized) &#123;<span class=\"comment\">//如果有checkpoint</span></span><br><span class=\"line\">    firstParent[<span class=\"type\">T</span>].iterator(split, context)</span><br><span class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">    compute(split, context)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可以看到，在没有缓存和 checkpoint 的情况下，itertor 方法直接调用了 compute 方法。而compute方法定义如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">  * Implemented by subclasses to compute a given partition.</span></span><br><span class=\"line\"><span class=\"comment\">  */</span></span><br><span class=\"line\"> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span></span>(split: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>]</span><br></pre></td></tr></table></figure>\n<p>从注释可以看出，compute 方法应该由子类实现，用以计算给定分区并生成一个迭代器。如果不考虑缓存和 checkpoint 的情况下，简化一下代码:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">final</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">iterator</span></span>(split: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>] = &#123;</span><br><span class=\"line\">   compute(split,context)</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>那么实际上 iterator 的功能是: <strong>接受一个分区，对这个分区进行计算，并返回计算结果的迭代器</strong>。而之所以 compute 需要子类来实现，是因为只需要在子类中实现不同的 compute 方法，就能产生不同类型的 RDD。既然不同的RDD 有不同的功能，我们想知道之前的例子是如何进行迭代计算的，就需要知道这个例子中涉及到了哪些 RDD。</p>\n<p>首先查看<code>SparkContext</code>中与<code>parallelize</code>相关的部分代码:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">parallelize</span></span>[<span class=\"type\">T</span>: <span class=\"type\">ClassTag</span>](</span><br><span class=\"line\">    seq: <span class=\"type\">Seq</span>[<span class=\"type\">T</span>],</span><br><span class=\"line\">    numSlices: <span class=\"type\">Int</span> = defaultParallelism): <span class=\"type\">RDD</span>[<span class=\"type\">T</span>] = withScope &#123;</span><br><span class=\"line\">  assertNotStopped()</span><br><span class=\"line\">  <span class=\"keyword\">new</span> <span class=\"type\">ParallelCollectionRDD</span>[<span class=\"type\">T</span>](<span class=\"keyword\">this</span>, seq, numSlices, <span class=\"type\">Map</span>[<span class=\"type\">Int</span>, <span class=\"type\">Seq</span>[<span class=\"type\">String</span>]]())</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可以看到，<code>parallelize</code>实际返回了一个<code>ParallelCollectionRDD</code>。在<code>ParallelCollectionRDD</code>中并没有对<code>filter</code>方法进行重写，因此我们查看<code>RDD</code>中的<code>filter</code>方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">filter</span></span>(f: <span class=\"type\">T</span> =&gt; <span class=\"type\">Boolean</span>): <span class=\"type\">RDD</span>[<span class=\"type\">T</span>] = withScope &#123;</span><br><span class=\"line\">   <span class=\"keyword\">val</span> cleanF = sc.clean(f)</span><br><span class=\"line\">   <span class=\"keyword\">new</span> <span class=\"type\">MapPartitionsRDD</span>[<span class=\"type\">T</span>, <span class=\"type\">T</span>](</span><br><span class=\"line\">     <span class=\"keyword\">this</span>,</span><br><span class=\"line\">     (context, pid, iter) =&gt; iter.filter(cleanF),</span><br><span class=\"line\">     preservesPartitioning = <span class=\"literal\">true</span>)</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>filter 方法返回了<code>MapPartitionsRDD</code>。为了方便起见，我将 ParallelCollectionRDD 类型的 RDD 称为 a，MapPartitionsRDD 类型的 RDD 成为B。那么先看一下 b 的 compute 方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span>[spark] <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MapPartitionsRDD</span>[<span class=\"type\">U</span>: <span class=\"type\">ClassTag</span>, <span class=\"type\">T</span>: <span class=\"type\">ClassTag</span>](<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    var prev: <span class=\"type\">RDD</span>[<span class=\"type\">T</span>],</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    f: (<span class=\"type\">TaskContext</span>, <span class=\"type\">Int</span>, <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>]</span>) <span class=\"title\">=&gt;</span> <span class=\"title\">Iterator</span>[<span class=\"type\">U</span>],  <span class=\"title\">//</span> (<span class=\"params\"><span class=\"type\">TaskContext</span>, partition index, iterator</span>)</span></span><br><span class=\"line\"><span class=\"class\">    <span class=\"title\">preservesPartitioning</span></span>: <span class=\"type\">Boolean</span> = <span class=\"literal\">false</span>)</span><br><span class=\"line\">  <span class=\"keyword\">extends</span> <span class=\"type\">RDD</span>[<span class=\"type\">U</span>](prev) &#123;</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"keyword\">val</span> partitioner = <span class=\"keyword\">if</span> (preservesPartitioning) firstParent[<span class=\"type\">T</span>].partitioner <span class=\"keyword\">else</span> <span class=\"type\">None</span></span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPartitions</span></span>: <span class=\"type\">Array</span>[<span class=\"type\">Partition</span>] = firstParent[<span class=\"type\">T</span>].partitions</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span></span>(split: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">U</span>] =</span><br><span class=\"line\">    f(context, split.index, firstParent[<span class=\"type\">T</span>].iterator(split, context))</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">clearDependencies</span></span>() &#123;</span><br><span class=\"line\">    <span class=\"keyword\">super</span>.clearDependencies()</span><br><span class=\"line\">    prev = <span class=\"literal\">null</span></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>这里可以看到 compute 方法实际上调用 f 这个函数。而 f 这个函数是在 filter 方法中传入的<code>(context, pid, iter) =&gt; iter.filter(cleanF)</code> 。那么实际上 compute 进行的计算为:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(context, split.index, firstParent[<span class=\"type\">T</span>].iterator(split, context)) =&gt; firstParent[<span class=\"type\">T</span>].iterator(split, context).filter(cleanF)</span><br></pre></td></tr></table></figure>\n<p>前两个参数并没有用到，也就是最终的方法可以简化为:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">firstParent[<span class=\"type\">T</span>].iterator(split, context) =&gt; firstParent[<span class=\"type\">T</span>].iterator(split, context).filter(cleanF)</span><br></pre></td></tr></table></figure>\n<p>这里出现了一个<code>firstParent</code>，我们知道 RDD 之间存在依赖关系，如果rdd3 依赖 rdd2，rdd2 依赖 rdd1。rdd2 与 rdd3 都是rdd1 的一个父rdd, spark也将其称为血缘关系。而故名意思 rdd2 是 rdd3 的 firstParent。在这个例子中 a 是 b的 firstParent。那么在b 的 compute 中又调用了 a 的 iterator 方法。而 a 的 iterator 方法又会调用a 的 comput 方法。用箭头表示调用关系:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">b.iterotr -&gt; b.compute -&gt; a.iterotr -&gt; a.compute -&gt;| 调用</span><br><span class=\"line\">b.iterotr &lt;- b.compute &lt;- a.iterotr &lt;- a.compute &lt;-| 返回</span><br></pre></td></tr></table></figure>\n<p>而这里我们也可以看出来，b 调用 iterotr 返回的数据，是使用 b 中的方法对 a 返回数据进行处理。也就是b 的计算结果依赖于a的计算结果。如果将一个rdd比作一个函数，大概就是 <code>fb(fa())</code>，这样的。此时，我们已经得到了关于 spark 迭代的最简单的模型，假如我们有 n 个 rdd。那么之间的调用依赖关系便是:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">n.iterotr -&gt; n.compute -&gt; (n-1).iterotr -&gt; (n-1).compute -&gt;...-&gt; 1.iterotr -&gt; 1.compute-&gt;| </span><br><span class=\"line\">n.iterotr &lt;- n.compute &lt;- (n-1).iterotr &lt;- (n-1).compute &lt;-...&lt;- 1.iterotr &lt;- 1.compute&lt;-|</span><br></pre></td></tr></table></figure>\n<p>那么细心的同学应该注意到了，所有的数据都是源自于第一个 RDD。这里把它称为源 RDD。例如本例中产生的 RDD 以及 textFile 方法产生的HadoopRDD都属于源RDD。既然属于源 RDD ，那么这个 RDD 一定会与内存或磁盘中的源数据产生交互，否则就没有真实的数据来源。这里的源数据是指内存的数组、本地文件、或者是HDFS上的分布式文件等。</p>\n<p>那么我们再继续看 a 的 compute 方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span></span>(s: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>] = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">new</span> <span class=\"type\">InterruptibleIterator</span>(context,s.asInstanceOf[<span class=\"type\">ParallelCollectionPartition</span>[<span class=\"type\">T</span>]].iterator)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可以看到 a 的 compute 根据 ParallelCollectionPartition 类的 iterator直接生成了一个迭代器。再看看ParallelCollectionPartition 的定义:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rivate[spark] <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ParallelCollectionPartition</span>[<span class=\"type\">T</span>: <span class=\"type\">ClassTag</span>](<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    var rddId: <span class=\"type\">Long</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    var slice: <span class=\"type\">Int</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    var values: <span class=\"type\">Seq</span>[<span class=\"type\">T</span>]</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">  </span>) <span class=\"keyword\">extends</span> <span class=\"title\">Partition</span> <span class=\"keyword\">with</span> <span class=\"title\">Serializable</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">iterator</span></span>: <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>] = values.iterator</span><br><span class=\"line\">  ...</span><br></pre></td></tr></table></figure>\n<p>iterator 是来自于 values 的一个迭代器，显然，values是内存中的数据。这说明 a 的 compute 方法直接返回了一个与源数据相关的迭代器。而这里 ParallelCollectionPartition 的数据是如何传入的，又是在什么时候被传入的呢?在<strong>分区</strong>小节将会提到。</p>\n<p>到这里，我们已经基本了解了 spark 的迭代计算的原理以及数据来源。但是仍然还有许多不了解的地方，例如 iterotr 会接受一个  Partition 作为参数。那么这个Partition参数到底起到了什么作用。其次，我们知道 rdd 的 iterotr 方法 是从后开始往前调用，那么又是谁调用的最后一个 rdd 的 itertor 方法呢。接下来我们就探索一下与分区相关的内容.</p>\n<h3 id=\"分区\"><a href=\"#分区\" class=\"headerlink\" title=\"分区\"></a>分区</h3><p>提到分区，大部分都是类似于“RDD 中最小的数据单元”，“只是数据的抽象分区，而不是真正的持有数据”等等这样的描述。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * An identifier for a partition in an RDD.</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">trait</span> <span class=\"title\">Partition</span> <span class=\"keyword\">extends</span> <span class=\"title\">Serializable</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">index</span></span>: <span class=\"type\">Int</span></span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hashCode</span></span>(): <span class=\"type\">Int</span> = index</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">equals</span></span>(other: <span class=\"type\">Any</span>): <span class=\"type\">Boolean</span> = <span class=\"keyword\">super</span>.equals(other)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可以看到分区的类十分的简单，只有一个 index 属性。 当将分区这个参数传入 itertor 时表示我们需要 index 对应的分区的数据经过一系列计算的之后的迭代器。那么显然，在源 RDD 中一定会有更据分区的index获取对应数据的代码部分。而我们又知道，对于<code>ParallelCollectionPartition</code>这个分区类是直接持有数据的迭代器，因此我们只需要知道这个类如何被建立，便知道了是如何根据分区获取数据的。我们在整个工程中搜索 ParallelCollectionPartition，发现:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPartitions</span></span>: <span class=\"type\">Array</span>[<span class=\"type\">Partition</span>] = &#123;</span><br><span class=\"line\">   <span class=\"keyword\">val</span> slices = <span class=\"type\">ParallelCollectionRDD</span>.slice(data, numSlices).toArray</span><br><span class=\"line\">   slices.indices.map(i =&gt; <span class=\"keyword\">new</span> <span class=\"type\">ParallelCollectionPartition</span>(id, i, slices(i))).toArray</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>中生成了一系列的分区对象。其中 ParallelCollectionRDD.slice() 方法根据总共有多少个分区来将 data  进行切分。而 data 我们在调用 parallelize() 方法时传入的。看到这里就应该明白了，<strong>ParallelCollectionRDD 会将我们输入的数组进行切分，然后将利用分区对象持有数据的迭代器，当调用到 itertor 方法时就返回该分区的迭代器，供后续的RDD</strong>进行计算。虽然这里我们只看了 ParallelCollectionRDD 的原代码，但是其他例如 HadoopRDD 的远离基本相同。 只不过一个是从内存中获取数据进行切分，另一个是从磁盘上获取数据进行切分。</p>\n<p>但是直到这里，我们仍然不知道 a 中的 itertor 方法的分区对象是如何传入的，因为这个方法间接被 b 的 itertor 方法调用。 而 b 的  itertor 方法同样也需要在外部被调用 ，因此要解决这个问题只需要找到 b 的 itertor 被调用的地方。不过我们首先可以根据代码猜测一下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span></span>(s: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>] = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">new</span> <span class=\"type\">InterruptibleIterator</span>(context,s.asInstanceOf[<span class=\"type\">ParallelCollectionPartition</span>[<span class=\"type\">T</span>]].iterator)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>a 中的 compute 方法直接将传入的分区对象转为了 ParallelCollectionPartition 并获取了对应的迭代器。根据对象间强转的</p>\n<p>规则，传入的分区对象只能是 ParallelCollectionPartition 类型 ，因为其父类 RDD 并没有 iterator 方法。 并且传入的ParallelCollectionPartition 一定是持有源数据迭代器的对象，否则在 a 的 compute 中就无法向后返回迭代器。而且我们知道，spark 的计算是<strong>惰性计算</strong>，在调用 action 算子之后才会真正的开始计算。那么可以猜测，b 的 iterator 也是在调用了 count 方法之后才被调用的。为了证明这一点，我们继续查看代码:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">count</span></span>(): <span class=\"type\">Long</span> = sc.runJob(<span class=\"keyword\">this</span>, <span class=\"type\">Utils</span>.getIteratorSize _).sum</span><br></pre></td></tr></table></figure>\n<p>可以看到是调用了 SparkContext 中的runJob方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">runJob</span></span>[<span class=\"type\">T</span>, <span class=\"type\">U</span>: <span class=\"type\">ClassTag</span>](</span><br><span class=\"line\">    ...</span><br><span class=\"line\">   dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)</span><br><span class=\"line\">   ...</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>而 SparkContext 中的 runJob 又继续调用了 DAGScheduler 中的 runJob 方法， 继而调用了 submitJob 方法。之后由经历了 DAGSchedulerEventProcessLoop.post DAGSchedulerEventProcessLoop.doOnReceive 等方法，最后在 submitMissingTasks 看到建立了 ResultTask 对象。由于一个分区的数据会被一个task 处理，因此我们猜测在 ＲesultTask 中会有关于对应 rdd 调用 iterator 的信息，果然，在ResultTask中找到了 runTask 方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">runTask</span></span>(context: <span class=\"type\">TaskContext</span>): <span class=\"type\">U</span> = &#123;</span><br><span class=\"line\">    <span class=\"comment\">// Deserialize the RDD and the func using the broadcast variables.</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> threadMXBean = <span class=\"type\">ManagementFactory</span>.getThreadMXBean</span><br><span class=\"line\">    <span class=\"keyword\">val</span> deserializeStartTime = <span class=\"type\">System</span>.currentTimeMillis()</span><br><span class=\"line\">    <span class=\"keyword\">val</span> deserializeStartCpuTime = <span class=\"keyword\">if</span> (threadMXBean.isCurrentThreadCpuTimeSupported) &#123;</span><br><span class=\"line\">      threadMXBean.getCurrentThreadCpuTime</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> <span class=\"number\">0</span>L</span><br><span class=\"line\">    <span class=\"keyword\">val</span> ser = <span class=\"type\">SparkEnv</span>.get.closureSerializer.newInstance()</span><br><span class=\"line\">    <span class=\"keyword\">val</span> (rdd, func) = ser.deserialize[(<span class=\"type\">RDD</span>[<span class=\"type\">T</span>], (<span class=\"type\">TaskContext</span>, <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>]) =&gt; <span class=\"type\">U</span>)](</span><br><span class=\"line\">      <span class=\"type\">ByteBuffer</span>.wrap(taskBinary.value), <span class=\"type\">Thread</span>.currentThread.getContextClassLoader)</span><br><span class=\"line\">    _executorDeserializeTime = <span class=\"type\">System</span>.currentTimeMillis() - deserializeStartTime</span><br><span class=\"line\">    _executorDeserializeCpuTime = <span class=\"keyword\">if</span> (threadMXBean.isCurrentThreadCpuTimeSupported) &#123;</span><br><span class=\"line\">      threadMXBean.getCurrentThreadCpuTime - deserializeStartCpuTime</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> <span class=\"number\">0</span>L</span><br><span class=\"line\"></span><br><span class=\"line\">    func(context, rdd.iterator(partition, context))<span class=\"comment\">//这里就 b 的iterator被真正调用的地方</span></span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>那么我们可以看到最后一行有一个rdd的调用，而这个rdd 是反徐序列化得的，很明显，这个rdd就是 b。此时，b 调用 iterator 方法的地方找到了，但是分区对象partition依然没有找到从哪里传入。由于 partition 是 ResultTask 构造时传入的，我们回到 submitMissingTasks 中，创建ResultTask时分区参数对应的为变量 part 。 </p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//从需要计算的分区map中获取分区，并生成task</span></span><br><span class=\"line\"> partitionsToCompute.map &#123; id =&gt;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> p: <span class=\"type\">Int</span> = stage.partitions(id)</span><br><span class=\"line\">      <span class=\"keyword\">val</span> part = partitions(p)</span><br><span class=\"line\">      <span class=\"keyword\">val</span> locs = taskIdToLocations(id)</span><br><span class=\"line\">      <span class=\"keyword\">new</span> <span class=\"type\">ResultTask</span>(stage.id, stage.latestInfo.attemptNumber,</span><br><span class=\"line\">        taskBinary, part, locs, id, properties, serializedTaskMetrics,</span><br><span class=\"line\">        <span class=\"type\">Option</span>(jobId), <span class=\"type\">Option</span>(sc.applicationId), sc.applicationAttemptId)</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>继续往上看，可以看到 part 是 从 partitions 中获取的。这里我们也可以看出 <strong>一个任务对应一个分区数据</strong>。继续往上看，发现<code>partitions = stage.rdd.partitions</code>。实际上来自于 Stage 中的 rdd 中的 partitions。那么看Stage 对 rdd 这个属性的描述:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> *</span><br><span class=\"line\"> * <span class=\"meta\">@param</span> rdd <span class=\"type\">RDD</span> that <span class=\"keyword\">this</span> stage runs on: <span class=\"keyword\">for</span> a shuffle map stage, it<span class=\"symbol\">'s</span> the <span class=\"type\">RDD</span> we run map tasks</span><br><span class=\"line\"> *   on, <span class=\"keyword\">while</span> <span class=\"keyword\">for</span> a result stage, it<span class=\"symbol\">'s</span> the target <span class=\"type\">RDD</span> that we ran an action on</span><br><span class=\"line\"> *   这里的意思是 rdd 表示调用 action 算子的 rdd 也就是 b </span><br><span class=\"line\"> */</span><br><span class=\"line\"><span class=\"keyword\">private</span>[scheduler] <span class=\"keyword\">abstract</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Stage</span>(<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val id: <span class=\"type\">Int</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val rdd: <span class=\"type\">RDD</span>[_],</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val numTasks: <span class=\"type\">Int</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val parents: <span class=\"type\">List</span>[<span class=\"type\">Stage</span>],</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val firstJobId: <span class=\"type\">Int</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val callSite: <span class=\"type\">CallSite</span></span>)</span></span><br><span class=\"line\"><span class=\"class\">  <span class=\"keyword\">extends</span> <span class=\"title\">Logging</span> </span>&#123;</span><br></pre></td></tr></table></figure>\n<p>我们发现一个惊人的事实 ，这个 rdd 居然也是 b。也就是说，我们在 runTask函数中实际调用的方法是这样的:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func(context, b.iterator(b.partitions(index), context))<span class=\"comment\">//这里就 b 的iterator被真正调用的地方</span></span><br></pre></td></tr></table></figure>\n<p>index 表示该task对应需要执行的分区标号。随即我们继续看 b 中的 partitions 的定义:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPartitions</span></span>: <span class=\"type\">Array</span>[<span class=\"type\">Partition</span>] = firstParent[<span class=\"type\">T</span>].partitions</span><br></pre></td></tr></table></figure>\n<p>说明这个 partitions 来自于 a。 而 a 中的 partitions 实际上又来自于:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPartitions</span></span>: <span class=\"type\">Array</span>[<span class=\"type\">Partition</span>] = &#123;</span><br><span class=\"line\">   <span class=\"keyword\">val</span> slices = <span class=\"type\">ParallelCollectionRDD</span>.slice(data, numSlices).toArray</span><br><span class=\"line\">   slices.indices.map(i =&gt; <span class=\"keyword\">new</span> <span class=\"type\">ParallelCollectionPartition</span>(id, i, slices(i))).toArray</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>这样最终 runTask 函数实际调用的方法为:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func(context, b.iterator(a.partitions(index), context))<span class=\"comment\">//这里就 b 的iterator被真正调用的地方</span></span><br></pre></td></tr></table></figure>\n<p>这和我们猜测的相同，这个分区确实为 ParallelCollectionPartition 对象，并且也持有了源数据的迭代器。这里其实有一个很巧妙的地方，虽然 RDD  的迭代计算是从后往前调用，但是传入源 RDD 的分区对象依然来自于源 RDD 自身。</p>\n<p>到了这里，我们也就明白分区对象是如何和数据关联起来的。ParallelCollectionPartition 对象中存在分区编号 index 以及 源数据的迭代器，通过分区编号就能获取到源数据的迭代器。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>经过如此大篇幅的讲解，终于对 spark 的迭代计算以及分区做了一个简单的分析。虽然还有很多地方不完善，例如还有其它类型的分区对象没有讲解，但是我相信你能完全看懂这篇文章所讲的内容，其他类型的分区对象对于你来说也很简单了。作者水平有限，并且这也是作者的第一篇关于 spark 源码阅读的博客，难免有遗漏和错误。如果想如给作者提建议和意见的话，请联系作者的微信或直接在 github 上提 issue 。</p>\n"},{"title":"Spark源码阅读计划-第四部分-shuffle read","date":"2018-06-18T02:38:44.000Z","author":"lishion","toc":true,"_content":"\n拖了这么久终于把 shuffle read 部分的源码看了一遍了。虽然 shuffle read 再数据合并部分的逻辑要比 shuffle 简单，但是由于这个过程中 executor 要到 master 拉取 shuffle write 结果信息，就涉及到 spark 的 block manager 的一些东西，因此完整的 shuffle read 过程依然是很复杂的。但是由于我还是太菜了，对于 block manage 这一部分看的一知半解，因此关于 executor 与 master 进行元数据交互的部分也就不会写得很详细，当然还是会涉及到一些。有关 block manage 的这部分以后应该会写到，先立个 flag 在这里吧。\n\n## MapStatus\t\n\n在 shuffle write 完成之后会返回一个 MapStatus，MapStatus记录了 BlockManagerId 以及最终每个分区的大小。其中 BlockManagerId 包含了 BlockManager 所在的 host 以及 port 等信息。返回的 MapStatus 最终会被 Driver 端获并存储以便于 mapper 端获取。 这里要稍微提一下 BlockManager 的知识，Spark 利用 BlockManager 对数据进行读写，而 Block 就是其中的基本单位。每一个 Block 拥有一个 id。而通过BlockManager 则可以操作这些 Block 。因此 mapper 端只需要知道 BlockManager 的位置以及所需要的文件在哪些 Block 就能获取到对应的数据。这里通过 MapOutputTracker 通过 shuffleid 对一次 shuffle write 中所有的 mapper 产生的 MapStatus 进行了记录。\n\n在每一个 mapper 的 shuffle task 结束后，MapOutputTracker就会将其返回的 MapStatues 进行注册。在 DAGScheduler 中可以看到:\n\n```scala\n case smt: ShuffleMapTask =>\n              val shuffleStage = stage.asInstanceOf[ShuffleMapStage]\n              val status = event.result.asInstanceOf[MapStatus]\n              val execId = status.location.executorId\n\t\t\t  \n              mapOutputTracker.registerMapOutput(\n                shuffleStage.shuffleDep.shuffleId, smt.partitionId, status)\n```\n\n这里的 MapOutputTracker 实际的类型为 MapOutputTrackerMaster 。继续看关于注册的代码:\n\n```scala\n def registerMapOutput(shuffleId: Int, mapId: Int, status: MapStatus) {\n    shuffleStatuses(shuffleId).addMapOutput(mapId, status)\n  }\n```\n\nshuffleStatuses 实际上是一个 hashmap:\n\n```scala\nval shuffleStatuses = new ConcurrentHashMap[Int, ShuffleStatus]().asScala\n```\n\nShuffleStatus 是一个类，里面包含了一个 MapStatus 数组。也就是说，通过在 shuffle write 之前注册的 shuffle 所用的 shuffleid 作为索引存储了 shuffle write 过程产生的 MapStatus。而 MapOutputTrackerMaster 是用于 Driver 端的。用于 Executor 的则是 MapOutputTrackerWorker 。在 MapOutputTrackerWorker 中一开始是没有 shuffleStatuses 的。需要从 MapOutputTrackerMaster 中获取。\n\n## shuffle read\n\nshuffle read 代码开始于 Shuffled 中的 compute 方法:\n\n```scala\noverride def compute(split: Partition, context: TaskContext): Iterator[(K, C)] = {\n    val dep = dependencies.head.asInstanceOf[ShuffleDependency[K, V, C]]\n    SparkEnv.get.shuffleManager.getReader(dep.shuffleHandle, split.index, split.index + 1, context)\n      .read()\n      .asInstanceOf[Iterator[(K, C)]]\n  }\n```\n\n可以看到这里传入了需要计算的分区。代码中获取了一个 reader 并调用了其 read 方法。 getReader 代码如下:\n\n```scala\n  override def getReader[K, C](\n      handle: ShuffleHandle,\n      startPartition: Int,\n      endPartition: Int,\n      context: TaskContext): ShuffleReader[K, C] = {\n    new BlockStoreShuffleReader(\n      handle.asInstanceOf[BaseShuffleHandle[K, _, C]], startPartition, endPartition, context)\n  }\n```\n\n实际上是返回了一个 BlockStoreShuffleReader 。read 方法较长，准备一段一段的讲解:\n\n```scala\noverride def read(): Iterator[Product2[K, C]]\n```\n\n方法返回了一个 Iterator。方法的开始定义了一个:\n\n```scala\n val wrappedStreams = new ShuffleBlockFetcherIterator(\n      context,\n      blockManager.shuffleClient,\n      blockManager,\n      // 获取 blockManagerId 与　blockId\n      mapOutputTracker.getMapSizesByExecutorId(handle.shuffleId, startPartition, endPartition),\n      serializerManager.wrapStream,\n      // Note: we use getSizeAsMb when no suffix is provided for backwards compatibility\n      SparkEnv.get.conf.getSizeAsMb(\"spark.reducer.maxSizeInFlight\", \"48m\") * 1024 * 1024,\n      SparkEnv.get.conf.getInt(\"spark.reducer.maxReqsInFlight\", Int.MaxValue),\n      SparkEnv.get.conf.get(config.REDUCER_MAX_BLOCKS_IN_FLIGHT_PER_ADDRESS),\n      SparkEnv.get.conf.get(config.MAX_REMOTE_BLOCK_SIZE_FETCH_TO_MEM),\n      SparkEnv.get.conf.getBoolean(\"spark.shuffle.detectCorrupt\", true))\n```\n\n这个 wrappedStreams 实际上获取数据的输入流。看看:\n\n```scala\n override def getMapSizesByExecutorId(shuffleId: Int, startPartition: Int, endPartition: Int)\n      : Iterator[(BlockManagerId, Seq[(BlockId, Long)])] = {\n    logDebug(s\"Fetching outputs for shuffle $shuffleId, partitions $startPartition-$endPartition\")\n    // 通过本次的 shuffleId 先获取 mapper 端产生的 MapStatuses\n    val statuses = getStatuses(shuffleId)\n    try {\n       // \n      MapOutputTracker.convertMapStatuses(shuffleId, startPartition, endPartition, statuses)\n    } catch {\n      case e: MetadataFetchFailedException =>\n        // We experienced a fetch failure so our mapStatuses cache is outdated; clear it:\n        mapStatuses.clear()\n        throw e\n    }\n  }\n```\n\n随后调用了 convertMapStatuses:\n\n```scala\ndef convertMapStatuses(\n      shuffleId: Int,\n      startPartition: Int,\n      endPartition: Int,\n      statuses: Array[MapStatus]): Iterator[(BlockManagerId, Seq[(BlockId, Long)])] = {\n    assert (statuses != null)\n    val splitsByAddress = new HashMap[BlockManagerId, ListBuffer[(BlockId, Long)]]\n    for ((status, mapId) <- statuses.iterator.zipWithIndex) {\n      if (status == null) {\n        val errorMessage = s\"Missing an output location for shuffle $shuffleId\"\n        logError(errorMessage)\n        throw new MetadataFetchFailedException(shuffleId, startPartition, errorMessage)\n      } else {\n        for (part <- startPartition until endPartition) {\n          val size = status.getSizeForBlock(part) // 这里只获取了需要处理的分区对应的数据\n          if (size != 0) {\n            // 这里获取到需要计算的分区数据所在的 block。其实就是由 shuffleId, mapId, part\n            // 这三个组成的\n            splitsByAddress.getOrElseUpdate(status.location, ListBuffer()) +=\n                ((ShuffleBlockId(shuffleId, mapId, part), size))\n          }\n        }\n      }\n    }\n    splitsByAddress.iterator\n  }\n```\n\n这里涉及到 mapId 和 part 这两个变量。实际上，这两个都是分区的 Id 。只不过 mapId 是 mapper 端对应的分区Id，而 part 是经过 shuffle 之后 reducer 端对应的分区Id。通过`convertMapStatuses`就可以得到需要从哪些 Block 拉取数据。\n\n``` scala\n// 获取　block　对应的输入流\nval recordIter = wrappedStreams.flatMap { case (blockId, wrappedStream) =>\n      // Note: the asKeyValueIterator below wraps a key/value iterator inside of a\n      // NextIterator. The NextIterator makes sure that close() is called on the\n      // underlying InputStream when all records have been read.\n      serializerInstance.deserializeStream(wrappedStream).asKeyValueIterator\n    }\n\n    // Update the context task metrics for each record read.\n    val readMetrics = context.taskMetrics.createTempShuffleReadMetrics()\n    val metricIter = CompletionIterator[(Any, Any), Iterator[(Any, Any)]](\n      recordIter.map { record =>\n        readMetrics.incRecordsRead(1)\n        record\n      },\n      context.taskMetrics().mergeShuffleReadMetrics())\n\n    // An interruptible iterator must be used here in order to support task cancellation\n    //\n    val interruptibleIter = new InterruptibleIterator[(Any, Any)](context, metricIter)\n\t\n    val aggregatedIter: Iterator[Product2[K, C]] = if (dep.aggregator.isDefined) {\n      　// 如果需要聚合\n        if (dep.mapSideCombine) {\n        // We are reading values that are already combined\n        val combinedKeyValuesIterator = interruptibleIter.asInstanceOf[Iterator[(K, C)]]\n        dep.aggregator.get.combineCombinersByKey(combinedKeyValuesIterator, context)\n      } else {\n        // We don't know the value type, but also don't care -- the dependency *should*\n        // have made sure its compatible w/ this aggregator, which will convert the value\n        // type to the combined type C\n        val keyValuesIterator = interruptibleIter.asInstanceOf[Iterator[(K, Nothing)]]\n        dep.aggregator.get.combineValuesByKey(keyValuesIterator, context)\n      }\n    } else {\n      interruptibleIter.asInstanceOf[Iterator[Product2[K, C]]]\n    }\n```\n\n这里进行聚合的代码实际上在 combineCombinersByKey 中:\n\n```scala\n  def combineCombinersByKey(\n      iter: Iterator[_ <: Product2[K, C]],\n      context: TaskContext): Iterator[(K, C)] = {\n    val combiners = new ExternalAppendOnlyMap[K, C, C](identity, mergeCombiners, mergeCombiners)\n    // 这里的 insertAll 和　shuffle write 中的 insertAll 效果一样，会排序生成临时文件\n    combiners.insertAll(iter)\n    updateMetrics(context, combiners)\n    combiners.iterator\n  }\n```\n\n最终返回了 iterator:\n\n```scala\n override def iterator: Iterator[(K, C)] = {\n    if (currentMap == null) {\n      throw new IllegalStateException(\n        \"ExternalAppendOnlyMap.iterator is destructive and should only be called once.\")\n    }\n     //　如果没有生成临时文件\n    if (spilledMaps.isEmpty) {\n      CompletionIterator[(K, C), Iterator[(K, C)]](\n        destructiveIterator(currentMap.iterator), freeCurrentMap())\n    } else {\n      new ExternalIterator()\n    }\n```\n\n如果有零时文件生成会返回一个 ExternalIterator :\n\n```scala\nprivate val mergeHeap = new mutable.PriorityQueue[StreamBuffer]\n    private val sortedMap = CompletionIterator[(K, C), Iterator[(K, C)]](destructiveIterator(\n      currentMap.destructiveSortedIterator(keyComparator)), freeCurrentMap())\n    // 这里合并了临时文件与内存中的数据\n    private val inputStreams = (Seq(sortedMap) ++ spilledMaps).map(it => it.buffered)\n    inputStreams.foreach { it =>\n      val kcPairs = new ArrayBuffer[(K, C)]\n      readNextHashCode(it, kcPairs)\n      if (kcPairs.length > 0) {\n        mergeHeap.enqueue(new StreamBuffer(it, kcPairs))\n      }\n    }\n```\n\n这里涉及到了一个很重要的类 ArrayBuffer:\n\n```scala\n    private class StreamBuffer(\n        val iterator: BufferedIterator[(K, C)],\n        val pairs: ArrayBuffer[(K, C)])\n      extends Comparable[StreamBuffer] {\n      def isEmpty: Boolean = pairs.length == 0\n      def minKeyHash: Int = {\n        assert(pairs.length > 0)\n        hashKey(pairs.head)\n      }\n      override def compareTo(other: StreamBuffer): Int = {\n        if (other.minKeyHash < minKeyHash) -1 else if (other.minKeyHash == minKeyHash) 0 else 1\n      }\n```\n\nStreamBuffer 实际上维持了一个 iterator 与一个数组。并且重写了 compareTo 方法。是通过数据中第一个元素的 key 也就是minKeyHash来比较大小。而代码:\n\n```scala\n readNextHashCode(it, kcPairs)\n      if (kcPairs.length > 0) {\n        mergeHeap.enqueue(new StreamBuffer(it, kcPairs))\n      }\n```\n\n再看 next 方法:\n\n```scala\n    override def next(): (K, C) = {\n      if (mergeHeap.isEmpty) {\n        throw new NoSuchElementException\n      }\n      // Select a key from the StreamBuffer that holds the lowest key hash\n      //　这里需要注意的是每一个 StreamBuffer 中的数组的元素是按照 key 经过排序的，mergeHeap 中的 StreamBuffer 也是按照 minKeyHash 进行排序的。也就是从 mergeHeap 每取出一个StreamBuffer，其对应的数组中 key 的 hash 一定是目前 mergeHeap 中所有数组中 key 最小的，如果能理解这一点，那这里的 merge 就基本可以理解了。\n      val minBuffer = mergeHeap.dequeue()\n      val minPairs = minBuffer.pairs\n      val minHash = minBuffer.minKeyHash\n      val minPair = removeFromBuffer(minPairs, 0)\n      val minKey = minPair._1\n      var minCombiner = minPair._2\n      assert(hashKey(minPair) == minHash)\n\n      // For all other streams that may have this key (i.e. have the same minimum key hash),\n      // merge in the corresponding value (if any) from that stream\n      val mergedBuffers = ArrayBuffer[StreamBuffer](minBuffer)\n      // 如果下一个 StreamBuffer 中的 minKeyHash 相同，则可能会含有相同的 key，则需要合并。这里由于是经过排序的，所以不用遍历所有的 StreamBuffer。只要下一个不同，则后面一定不会有与当前 minKeyHash 相同的的 StreamBuffer。\n      while (mergeHeap.nonEmpty && mergeHeap.head.minKeyHash == minHash) {\n        val newBuffer = mergeHeap.dequeue()\n        // 这里如果有相同的 key 则进行合并\n        // 注意，hash 相同 key 不一定相同\n        minCombiner = mergeIfKeyExists(minKey, minCombiner, newBuffer)\n        mergedBuffers += newBuffer\n      }\n\n      // Repopulate each visited stream buffer and add it back to the queue if it is non-empty\n      \n      mergedBuffers.foreach { buffer =>\n        // 如果 key 被合并完了，就需要读取下一批hash相同的key的数据到ArrayBuffer的数组中\n        if (buffer.isEmpty) {\n          readNextHashCode(buffer.iterator, buffer.pairs)\n        }\n        if (!buffer.isEmpty) {\n          // 刚才dequeue的 ArrayBuffer的数组中可能有没合并完的数据 \n          // 或者有新读取的数据则需要继续放入 mergeHeap 中进行合并 \n          mergeHeap.enqueue(buffer)\n        }\n      }\n      // 返回合并完的数据\n      // 注意这里的 key 只是按照 hash 进行排序的，在 ExternalSorter 才是按照用户定义的排序方式进行排序\n      (minKey, minCombiner)\n    }\n```\n\n如果不需要排序，shuffle read 就算完成了。但是如果需要排序则:\n\n```scala\n val resultIter = dep.keyOrdering match {\n      case Some(keyOrd: Ordering[K]) =>\n        // Create an ExternalSorter to sort the data.\n        val sorter =\n          new ExternalSorter[K, C, C](context, ordering = Some(keyOrd), serializer = dep.serializer)\n        sorter.insertAll(aggregatedIter)\n        context.taskMetrics().incMemoryBytesSpilled(sorter.memoryBytesSpilled)\n        context.taskMetrics().incDiskBytesSpilled(sorter.diskBytesSpilled)\n        context.taskMetrics().incPeakExecutionMemory(sorter.peakMemoryUsedBytes)\n        // Use completion callback to stop sorter if task was finished/cancelled.\n        context.addTaskCompletionListener(_ => {\n          sorter.stop()\n        })\n        CompletionIterator[Product2[K, C], Iterator[Product2[K, C]]](sorter.iterator, sorter.stop())\n      case None =>\n        aggregatedIter\n    }\n\n    resultIter match {\n      case _: InterruptibleIterator[Product2[K, C]] => resultIter\n      case _ =>\n        // Use another interruptible iterator here to support task cancellation as aggregator\n        // or(and) sorter may have consumed previous interruptible iterator.\n        new InterruptibleIterator[Product2[K, C]](context, resultIter)\n    }\n```\n\n这里依然使用了 ExternalSorter 进行排序，而最终使用了 ExternalSorter 的 iterator 方法。\n\n```scala\n  def iterator: Iterator[Product2[K, C]] = {\n    isShuffleSort = false\n    partitionedIterator.flatMap(pair => pair._2)\n  }\n\n```\n\n其中 partitionedIterator 方法在 shuffle write 部分已经讲的很清楚了。有兴趣的可以去看看。这里 shuffle read 就基本结束了。其中从其他 Executor 拉取数据的部分由于涉及很多，这里就基本没有怎么讲解。以后可能会专门开一篇文章。","source":"_posts/Spark源码阅读计划-第四部分-shuffle-read.md","raw":"---\ntitle: Spark源码阅读计划-第四部分-shuffle read\ndate: 2018-06-18 10:38:44\ntags:\n  - Spark\n  - 编程\ncategories: Spark源码阅读计划\nauthor: lishion\ntoc: true\n---\n\n拖了这么久终于把 shuffle read 部分的源码看了一遍了。虽然 shuffle read 再数据合并部分的逻辑要比 shuffle 简单，但是由于这个过程中 executor 要到 master 拉取 shuffle write 结果信息，就涉及到 spark 的 block manager 的一些东西，因此完整的 shuffle read 过程依然是很复杂的。但是由于我还是太菜了，对于 block manage 这一部分看的一知半解，因此关于 executor 与 master 进行元数据交互的部分也就不会写得很详细，当然还是会涉及到一些。有关 block manage 的这部分以后应该会写到，先立个 flag 在这里吧。\n\n## MapStatus\t\n\n在 shuffle write 完成之后会返回一个 MapStatus，MapStatus记录了 BlockManagerId 以及最终每个分区的大小。其中 BlockManagerId 包含了 BlockManager 所在的 host 以及 port 等信息。返回的 MapStatus 最终会被 Driver 端获并存储以便于 mapper 端获取。 这里要稍微提一下 BlockManager 的知识，Spark 利用 BlockManager 对数据进行读写，而 Block 就是其中的基本单位。每一个 Block 拥有一个 id。而通过BlockManager 则可以操作这些 Block 。因此 mapper 端只需要知道 BlockManager 的位置以及所需要的文件在哪些 Block 就能获取到对应的数据。这里通过 MapOutputTracker 通过 shuffleid 对一次 shuffle write 中所有的 mapper 产生的 MapStatus 进行了记录。\n\n在每一个 mapper 的 shuffle task 结束后，MapOutputTracker就会将其返回的 MapStatues 进行注册。在 DAGScheduler 中可以看到:\n\n```scala\n case smt: ShuffleMapTask =>\n              val shuffleStage = stage.asInstanceOf[ShuffleMapStage]\n              val status = event.result.asInstanceOf[MapStatus]\n              val execId = status.location.executorId\n\t\t\t  \n              mapOutputTracker.registerMapOutput(\n                shuffleStage.shuffleDep.shuffleId, smt.partitionId, status)\n```\n\n这里的 MapOutputTracker 实际的类型为 MapOutputTrackerMaster 。继续看关于注册的代码:\n\n```scala\n def registerMapOutput(shuffleId: Int, mapId: Int, status: MapStatus) {\n    shuffleStatuses(shuffleId).addMapOutput(mapId, status)\n  }\n```\n\nshuffleStatuses 实际上是一个 hashmap:\n\n```scala\nval shuffleStatuses = new ConcurrentHashMap[Int, ShuffleStatus]().asScala\n```\n\nShuffleStatus 是一个类，里面包含了一个 MapStatus 数组。也就是说，通过在 shuffle write 之前注册的 shuffle 所用的 shuffleid 作为索引存储了 shuffle write 过程产生的 MapStatus。而 MapOutputTrackerMaster 是用于 Driver 端的。用于 Executor 的则是 MapOutputTrackerWorker 。在 MapOutputTrackerWorker 中一开始是没有 shuffleStatuses 的。需要从 MapOutputTrackerMaster 中获取。\n\n## shuffle read\n\nshuffle read 代码开始于 Shuffled 中的 compute 方法:\n\n```scala\noverride def compute(split: Partition, context: TaskContext): Iterator[(K, C)] = {\n    val dep = dependencies.head.asInstanceOf[ShuffleDependency[K, V, C]]\n    SparkEnv.get.shuffleManager.getReader(dep.shuffleHandle, split.index, split.index + 1, context)\n      .read()\n      .asInstanceOf[Iterator[(K, C)]]\n  }\n```\n\n可以看到这里传入了需要计算的分区。代码中获取了一个 reader 并调用了其 read 方法。 getReader 代码如下:\n\n```scala\n  override def getReader[K, C](\n      handle: ShuffleHandle,\n      startPartition: Int,\n      endPartition: Int,\n      context: TaskContext): ShuffleReader[K, C] = {\n    new BlockStoreShuffleReader(\n      handle.asInstanceOf[BaseShuffleHandle[K, _, C]], startPartition, endPartition, context)\n  }\n```\n\n实际上是返回了一个 BlockStoreShuffleReader 。read 方法较长，准备一段一段的讲解:\n\n```scala\noverride def read(): Iterator[Product2[K, C]]\n```\n\n方法返回了一个 Iterator。方法的开始定义了一个:\n\n```scala\n val wrappedStreams = new ShuffleBlockFetcherIterator(\n      context,\n      blockManager.shuffleClient,\n      blockManager,\n      // 获取 blockManagerId 与　blockId\n      mapOutputTracker.getMapSizesByExecutorId(handle.shuffleId, startPartition, endPartition),\n      serializerManager.wrapStream,\n      // Note: we use getSizeAsMb when no suffix is provided for backwards compatibility\n      SparkEnv.get.conf.getSizeAsMb(\"spark.reducer.maxSizeInFlight\", \"48m\") * 1024 * 1024,\n      SparkEnv.get.conf.getInt(\"spark.reducer.maxReqsInFlight\", Int.MaxValue),\n      SparkEnv.get.conf.get(config.REDUCER_MAX_BLOCKS_IN_FLIGHT_PER_ADDRESS),\n      SparkEnv.get.conf.get(config.MAX_REMOTE_BLOCK_SIZE_FETCH_TO_MEM),\n      SparkEnv.get.conf.getBoolean(\"spark.shuffle.detectCorrupt\", true))\n```\n\n这个 wrappedStreams 实际上获取数据的输入流。看看:\n\n```scala\n override def getMapSizesByExecutorId(shuffleId: Int, startPartition: Int, endPartition: Int)\n      : Iterator[(BlockManagerId, Seq[(BlockId, Long)])] = {\n    logDebug(s\"Fetching outputs for shuffle $shuffleId, partitions $startPartition-$endPartition\")\n    // 通过本次的 shuffleId 先获取 mapper 端产生的 MapStatuses\n    val statuses = getStatuses(shuffleId)\n    try {\n       // \n      MapOutputTracker.convertMapStatuses(shuffleId, startPartition, endPartition, statuses)\n    } catch {\n      case e: MetadataFetchFailedException =>\n        // We experienced a fetch failure so our mapStatuses cache is outdated; clear it:\n        mapStatuses.clear()\n        throw e\n    }\n  }\n```\n\n随后调用了 convertMapStatuses:\n\n```scala\ndef convertMapStatuses(\n      shuffleId: Int,\n      startPartition: Int,\n      endPartition: Int,\n      statuses: Array[MapStatus]): Iterator[(BlockManagerId, Seq[(BlockId, Long)])] = {\n    assert (statuses != null)\n    val splitsByAddress = new HashMap[BlockManagerId, ListBuffer[(BlockId, Long)]]\n    for ((status, mapId) <- statuses.iterator.zipWithIndex) {\n      if (status == null) {\n        val errorMessage = s\"Missing an output location for shuffle $shuffleId\"\n        logError(errorMessage)\n        throw new MetadataFetchFailedException(shuffleId, startPartition, errorMessage)\n      } else {\n        for (part <- startPartition until endPartition) {\n          val size = status.getSizeForBlock(part) // 这里只获取了需要处理的分区对应的数据\n          if (size != 0) {\n            // 这里获取到需要计算的分区数据所在的 block。其实就是由 shuffleId, mapId, part\n            // 这三个组成的\n            splitsByAddress.getOrElseUpdate(status.location, ListBuffer()) +=\n                ((ShuffleBlockId(shuffleId, mapId, part), size))\n          }\n        }\n      }\n    }\n    splitsByAddress.iterator\n  }\n```\n\n这里涉及到 mapId 和 part 这两个变量。实际上，这两个都是分区的 Id 。只不过 mapId 是 mapper 端对应的分区Id，而 part 是经过 shuffle 之后 reducer 端对应的分区Id。通过`convertMapStatuses`就可以得到需要从哪些 Block 拉取数据。\n\n``` scala\n// 获取　block　对应的输入流\nval recordIter = wrappedStreams.flatMap { case (blockId, wrappedStream) =>\n      // Note: the asKeyValueIterator below wraps a key/value iterator inside of a\n      // NextIterator. The NextIterator makes sure that close() is called on the\n      // underlying InputStream when all records have been read.\n      serializerInstance.deserializeStream(wrappedStream).asKeyValueIterator\n    }\n\n    // Update the context task metrics for each record read.\n    val readMetrics = context.taskMetrics.createTempShuffleReadMetrics()\n    val metricIter = CompletionIterator[(Any, Any), Iterator[(Any, Any)]](\n      recordIter.map { record =>\n        readMetrics.incRecordsRead(1)\n        record\n      },\n      context.taskMetrics().mergeShuffleReadMetrics())\n\n    // An interruptible iterator must be used here in order to support task cancellation\n    //\n    val interruptibleIter = new InterruptibleIterator[(Any, Any)](context, metricIter)\n\t\n    val aggregatedIter: Iterator[Product2[K, C]] = if (dep.aggregator.isDefined) {\n      　// 如果需要聚合\n        if (dep.mapSideCombine) {\n        // We are reading values that are already combined\n        val combinedKeyValuesIterator = interruptibleIter.asInstanceOf[Iterator[(K, C)]]\n        dep.aggregator.get.combineCombinersByKey(combinedKeyValuesIterator, context)\n      } else {\n        // We don't know the value type, but also don't care -- the dependency *should*\n        // have made sure its compatible w/ this aggregator, which will convert the value\n        // type to the combined type C\n        val keyValuesIterator = interruptibleIter.asInstanceOf[Iterator[(K, Nothing)]]\n        dep.aggregator.get.combineValuesByKey(keyValuesIterator, context)\n      }\n    } else {\n      interruptibleIter.asInstanceOf[Iterator[Product2[K, C]]]\n    }\n```\n\n这里进行聚合的代码实际上在 combineCombinersByKey 中:\n\n```scala\n  def combineCombinersByKey(\n      iter: Iterator[_ <: Product2[K, C]],\n      context: TaskContext): Iterator[(K, C)] = {\n    val combiners = new ExternalAppendOnlyMap[K, C, C](identity, mergeCombiners, mergeCombiners)\n    // 这里的 insertAll 和　shuffle write 中的 insertAll 效果一样，会排序生成临时文件\n    combiners.insertAll(iter)\n    updateMetrics(context, combiners)\n    combiners.iterator\n  }\n```\n\n最终返回了 iterator:\n\n```scala\n override def iterator: Iterator[(K, C)] = {\n    if (currentMap == null) {\n      throw new IllegalStateException(\n        \"ExternalAppendOnlyMap.iterator is destructive and should only be called once.\")\n    }\n     //　如果没有生成临时文件\n    if (spilledMaps.isEmpty) {\n      CompletionIterator[(K, C), Iterator[(K, C)]](\n        destructiveIterator(currentMap.iterator), freeCurrentMap())\n    } else {\n      new ExternalIterator()\n    }\n```\n\n如果有零时文件生成会返回一个 ExternalIterator :\n\n```scala\nprivate val mergeHeap = new mutable.PriorityQueue[StreamBuffer]\n    private val sortedMap = CompletionIterator[(K, C), Iterator[(K, C)]](destructiveIterator(\n      currentMap.destructiveSortedIterator(keyComparator)), freeCurrentMap())\n    // 这里合并了临时文件与内存中的数据\n    private val inputStreams = (Seq(sortedMap) ++ spilledMaps).map(it => it.buffered)\n    inputStreams.foreach { it =>\n      val kcPairs = new ArrayBuffer[(K, C)]\n      readNextHashCode(it, kcPairs)\n      if (kcPairs.length > 0) {\n        mergeHeap.enqueue(new StreamBuffer(it, kcPairs))\n      }\n    }\n```\n\n这里涉及到了一个很重要的类 ArrayBuffer:\n\n```scala\n    private class StreamBuffer(\n        val iterator: BufferedIterator[(K, C)],\n        val pairs: ArrayBuffer[(K, C)])\n      extends Comparable[StreamBuffer] {\n      def isEmpty: Boolean = pairs.length == 0\n      def minKeyHash: Int = {\n        assert(pairs.length > 0)\n        hashKey(pairs.head)\n      }\n      override def compareTo(other: StreamBuffer): Int = {\n        if (other.minKeyHash < minKeyHash) -1 else if (other.minKeyHash == minKeyHash) 0 else 1\n      }\n```\n\nStreamBuffer 实际上维持了一个 iterator 与一个数组。并且重写了 compareTo 方法。是通过数据中第一个元素的 key 也就是minKeyHash来比较大小。而代码:\n\n```scala\n readNextHashCode(it, kcPairs)\n      if (kcPairs.length > 0) {\n        mergeHeap.enqueue(new StreamBuffer(it, kcPairs))\n      }\n```\n\n再看 next 方法:\n\n```scala\n    override def next(): (K, C) = {\n      if (mergeHeap.isEmpty) {\n        throw new NoSuchElementException\n      }\n      // Select a key from the StreamBuffer that holds the lowest key hash\n      //　这里需要注意的是每一个 StreamBuffer 中的数组的元素是按照 key 经过排序的，mergeHeap 中的 StreamBuffer 也是按照 minKeyHash 进行排序的。也就是从 mergeHeap 每取出一个StreamBuffer，其对应的数组中 key 的 hash 一定是目前 mergeHeap 中所有数组中 key 最小的，如果能理解这一点，那这里的 merge 就基本可以理解了。\n      val minBuffer = mergeHeap.dequeue()\n      val minPairs = minBuffer.pairs\n      val minHash = minBuffer.minKeyHash\n      val minPair = removeFromBuffer(minPairs, 0)\n      val minKey = minPair._1\n      var minCombiner = minPair._2\n      assert(hashKey(minPair) == minHash)\n\n      // For all other streams that may have this key (i.e. have the same minimum key hash),\n      // merge in the corresponding value (if any) from that stream\n      val mergedBuffers = ArrayBuffer[StreamBuffer](minBuffer)\n      // 如果下一个 StreamBuffer 中的 minKeyHash 相同，则可能会含有相同的 key，则需要合并。这里由于是经过排序的，所以不用遍历所有的 StreamBuffer。只要下一个不同，则后面一定不会有与当前 minKeyHash 相同的的 StreamBuffer。\n      while (mergeHeap.nonEmpty && mergeHeap.head.minKeyHash == minHash) {\n        val newBuffer = mergeHeap.dequeue()\n        // 这里如果有相同的 key 则进行合并\n        // 注意，hash 相同 key 不一定相同\n        minCombiner = mergeIfKeyExists(minKey, minCombiner, newBuffer)\n        mergedBuffers += newBuffer\n      }\n\n      // Repopulate each visited stream buffer and add it back to the queue if it is non-empty\n      \n      mergedBuffers.foreach { buffer =>\n        // 如果 key 被合并完了，就需要读取下一批hash相同的key的数据到ArrayBuffer的数组中\n        if (buffer.isEmpty) {\n          readNextHashCode(buffer.iterator, buffer.pairs)\n        }\n        if (!buffer.isEmpty) {\n          // 刚才dequeue的 ArrayBuffer的数组中可能有没合并完的数据 \n          // 或者有新读取的数据则需要继续放入 mergeHeap 中进行合并 \n          mergeHeap.enqueue(buffer)\n        }\n      }\n      // 返回合并完的数据\n      // 注意这里的 key 只是按照 hash 进行排序的，在 ExternalSorter 才是按照用户定义的排序方式进行排序\n      (minKey, minCombiner)\n    }\n```\n\n如果不需要排序，shuffle read 就算完成了。但是如果需要排序则:\n\n```scala\n val resultIter = dep.keyOrdering match {\n      case Some(keyOrd: Ordering[K]) =>\n        // Create an ExternalSorter to sort the data.\n        val sorter =\n          new ExternalSorter[K, C, C](context, ordering = Some(keyOrd), serializer = dep.serializer)\n        sorter.insertAll(aggregatedIter)\n        context.taskMetrics().incMemoryBytesSpilled(sorter.memoryBytesSpilled)\n        context.taskMetrics().incDiskBytesSpilled(sorter.diskBytesSpilled)\n        context.taskMetrics().incPeakExecutionMemory(sorter.peakMemoryUsedBytes)\n        // Use completion callback to stop sorter if task was finished/cancelled.\n        context.addTaskCompletionListener(_ => {\n          sorter.stop()\n        })\n        CompletionIterator[Product2[K, C], Iterator[Product2[K, C]]](sorter.iterator, sorter.stop())\n      case None =>\n        aggregatedIter\n    }\n\n    resultIter match {\n      case _: InterruptibleIterator[Product2[K, C]] => resultIter\n      case _ =>\n        // Use another interruptible iterator here to support task cancellation as aggregator\n        // or(and) sorter may have consumed previous interruptible iterator.\n        new InterruptibleIterator[Product2[K, C]](context, resultIter)\n    }\n```\n\n这里依然使用了 ExternalSorter 进行排序，而最终使用了 ExternalSorter 的 iterator 方法。\n\n```scala\n  def iterator: Iterator[Product2[K, C]] = {\n    isShuffleSort = false\n    partitionedIterator.flatMap(pair => pair._2)\n  }\n\n```\n\n其中 partitionedIterator 方法在 shuffle write 部分已经讲的很清楚了。有兴趣的可以去看看。这里 shuffle read 就基本结束了。其中从其他 Executor 拉取数据的部分由于涉及很多，这里就基本没有怎么讲解。以后可能会专门开一篇文章。","slug":"Spark源码阅读计划-第四部分-shuffle-read","published":1,"updated":"2018-06-29T17:06:08.923Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjj08c9nj0008ksngz14yvt75","content":"<p>拖了这么久终于把 shuffle read 部分的源码看了一遍了。虽然 shuffle read 再数据合并部分的逻辑要比 shuffle 简单，但是由于这个过程中 executor 要到 master 拉取 shuffle write 结果信息，就涉及到 spark 的 block manager 的一些东西，因此完整的 shuffle read 过程依然是很复杂的。但是由于我还是太菜了，对于 block manage 这一部分看的一知半解，因此关于 executor 与 master 进行元数据交互的部分也就不会写得很详细，当然还是会涉及到一些。有关 block manage 的这部分以后应该会写到，先立个 flag 在这里吧。</p>\n<h2 id=\"MapStatus\"><a href=\"#MapStatus\" class=\"headerlink\" title=\"MapStatus\"></a>MapStatus</h2><p>在 shuffle write 完成之后会返回一个 MapStatus，MapStatus记录了 BlockManagerId 以及最终每个分区的大小。其中 BlockManagerId 包含了 BlockManager 所在的 host 以及 port 等信息。返回的 MapStatus 最终会被 Driver 端获并存储以便于 mapper 端获取。 这里要稍微提一下 BlockManager 的知识，Spark 利用 BlockManager 对数据进行读写，而 Block 就是其中的基本单位。每一个 Block 拥有一个 id。而通过BlockManager 则可以操作这些 Block 。因此 mapper 端只需要知道 BlockManager 的位置以及所需要的文件在哪些 Block 就能获取到对应的数据。这里通过 MapOutputTracker 通过 shuffleid 对一次 shuffle write 中所有的 mapper 产生的 MapStatus 进行了记录。</p>\n<p>在每一个 mapper 的 shuffle task 结束后，MapOutputTracker就会将其返回的 MapStatues 进行注册。在 DAGScheduler 中可以看到:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">case</span> smt: <span class=\"type\">ShuffleMapTask</span> =&gt;</span><br><span class=\"line\">             <span class=\"keyword\">val</span> shuffleStage = stage.asInstanceOf[<span class=\"type\">ShuffleMapStage</span>]</span><br><span class=\"line\">             <span class=\"keyword\">val</span> status = event.result.asInstanceOf[<span class=\"type\">MapStatus</span>]</span><br><span class=\"line\">             <span class=\"keyword\">val</span> execId = status.location.executorId</span><br><span class=\"line\">\t\t  </span><br><span class=\"line\">             mapOutputTracker.registerMapOutput(</span><br><span class=\"line\">               shuffleStage.shuffleDep.shuffleId, smt.partitionId, status)</span><br></pre></td></tr></table></figure>\n<p>这里的 MapOutputTracker 实际的类型为 MapOutputTrackerMaster 。继续看关于注册的代码:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">registerMapOutput</span></span>(shuffleId: <span class=\"type\">Int</span>, mapId: <span class=\"type\">Int</span>, status: <span class=\"type\">MapStatus</span>) &#123;</span><br><span class=\"line\">   shuffleStatuses(shuffleId).addMapOutput(mapId, status)</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>shuffleStatuses 实际上是一个 hashmap:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> shuffleStatuses = <span class=\"keyword\">new</span> <span class=\"type\">ConcurrentHashMap</span>[<span class=\"type\">Int</span>, <span class=\"type\">ShuffleStatus</span>]().asScala</span><br></pre></td></tr></table></figure>\n<p>ShuffleStatus 是一个类，里面包含了一个 MapStatus 数组。也就是说，通过在 shuffle write 之前注册的 shuffle 所用的 shuffleid 作为索引存储了 shuffle write 过程产生的 MapStatus。而 MapOutputTrackerMaster 是用于 Driver 端的。用于 Executor 的则是 MapOutputTrackerWorker 。在 MapOutputTrackerWorker 中一开始是没有 shuffleStatuses 的。需要从 MapOutputTrackerMaster 中获取。</p>\n<h2 id=\"shuffle-read\"><a href=\"#shuffle-read\" class=\"headerlink\" title=\"shuffle read\"></a>shuffle read</h2><p>shuffle read 代码开始于 Shuffled 中的 compute 方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span></span>(split: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>)] = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> dep = dependencies.head.asInstanceOf[<span class=\"type\">ShuffleDependency</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>, <span class=\"type\">C</span>]]</span><br><span class=\"line\">    <span class=\"type\">SparkEnv</span>.get.shuffleManager.getReader(dep.shuffleHandle, split.index, split.index + <span class=\"number\">1</span>, context)</span><br><span class=\"line\">      .read()</span><br><span class=\"line\">      .asInstanceOf[<span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>)]]</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>可以看到这里传入了需要计算的分区。代码中获取了一个 reader 并调用了其 read 方法。 getReader 代码如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getReader</span></span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>](</span><br><span class=\"line\">    handle: <span class=\"type\">ShuffleHandle</span>,</span><br><span class=\"line\">    startPartition: <span class=\"type\">Int</span>,</span><br><span class=\"line\">    endPartition: <span class=\"type\">Int</span>,</span><br><span class=\"line\">    context: <span class=\"type\">TaskContext</span>): <span class=\"type\">ShuffleReader</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>] = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">new</span> <span class=\"type\">BlockStoreShuffleReader</span>(</span><br><span class=\"line\">    handle.asInstanceOf[<span class=\"type\">BaseShuffleHandle</span>[<span class=\"type\">K</span>, _, <span class=\"type\">C</span>]], startPartition, endPartition, context)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>实际上是返回了一个 BlockStoreShuffleReader 。read 方法较长，准备一段一段的讲解:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">read</span></span>(): <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]]</span><br></pre></td></tr></table></figure>\n<p>方法返回了一个 Iterator。方法的开始定义了一个:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> wrappedStreams = <span class=\"keyword\">new</span> <span class=\"type\">ShuffleBlockFetcherIterator</span>(</span><br><span class=\"line\">     context,</span><br><span class=\"line\">     blockManager.shuffleClient,</span><br><span class=\"line\">     blockManager,</span><br><span class=\"line\">     <span class=\"comment\">// 获取 blockManagerId 与　blockId</span></span><br><span class=\"line\">     mapOutputTracker.getMapSizesByExecutorId(handle.shuffleId, startPartition, endPartition),</span><br><span class=\"line\">     serializerManager.wrapStream,</span><br><span class=\"line\">     <span class=\"comment\">// Note: we use getSizeAsMb when no suffix is provided for backwards compatibility</span></span><br><span class=\"line\">     <span class=\"type\">SparkEnv</span>.get.conf.getSizeAsMb(<span class=\"string\">\"spark.reducer.maxSizeInFlight\"</span>, <span class=\"string\">\"48m\"</span>) * <span class=\"number\">1024</span> * <span class=\"number\">1024</span>,</span><br><span class=\"line\">     <span class=\"type\">SparkEnv</span>.get.conf.getInt(<span class=\"string\">\"spark.reducer.maxReqsInFlight\"</span>, <span class=\"type\">Int</span>.<span class=\"type\">MaxValue</span>),</span><br><span class=\"line\">     <span class=\"type\">SparkEnv</span>.get.conf.get(config.<span class=\"type\">REDUCER_MAX_BLOCKS_IN_FLIGHT_PER_ADDRESS</span>),</span><br><span class=\"line\">     <span class=\"type\">SparkEnv</span>.get.conf.get(config.<span class=\"type\">MAX_REMOTE_BLOCK_SIZE_FETCH_TO_MEM</span>),</span><br><span class=\"line\">     <span class=\"type\">SparkEnv</span>.get.conf.getBoolean(<span class=\"string\">\"spark.shuffle.detectCorrupt\"</span>, <span class=\"literal\">true</span>))</span><br></pre></td></tr></table></figure>\n<p>这个 wrappedStreams 实际上获取数据的输入流。看看:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getMapSizesByExecutorId</span></span>(shuffleId: <span class=\"type\">Int</span>, startPartition: <span class=\"type\">Int</span>, endPartition: <span class=\"type\">Int</span>)</span><br><span class=\"line\">     : <span class=\"type\">Iterator</span>[(<span class=\"type\">BlockManagerId</span>, <span class=\"type\">Seq</span>[(<span class=\"type\">BlockId</span>, <span class=\"type\">Long</span>)])] = &#123;</span><br><span class=\"line\">   logDebug(<span class=\"string\">s\"Fetching outputs for shuffle <span class=\"subst\">$shuffleId</span>, partitions <span class=\"subst\">$startPartition</span>-<span class=\"subst\">$endPartition</span>\"</span>)</span><br><span class=\"line\">   <span class=\"comment\">// 通过本次的 shuffleId 先获取 mapper 端产生的 MapStatuses</span></span><br><span class=\"line\">   <span class=\"keyword\">val</span> statuses = getStatuses(shuffleId)</span><br><span class=\"line\">   <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">      <span class=\"comment\">// </span></span><br><span class=\"line\">     <span class=\"type\">MapOutputTracker</span>.convertMapStatuses(shuffleId, startPartition, endPartition, statuses)</span><br><span class=\"line\">   &#125; <span class=\"keyword\">catch</span> &#123;</span><br><span class=\"line\">     <span class=\"keyword\">case</span> e: <span class=\"type\">MetadataFetchFailedException</span> =&gt;</span><br><span class=\"line\">       <span class=\"comment\">// We experienced a fetch failure so our mapStatuses cache is outdated; clear it:</span></span><br><span class=\"line\">       mapStatuses.clear()</span><br><span class=\"line\">       <span class=\"keyword\">throw</span> e</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>随后调用了 convertMapStatuses:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">convertMapStatuses</span></span>(</span><br><span class=\"line\">      shuffleId: <span class=\"type\">Int</span>,</span><br><span class=\"line\">      startPartition: <span class=\"type\">Int</span>,</span><br><span class=\"line\">      endPartition: <span class=\"type\">Int</span>,</span><br><span class=\"line\">      statuses: <span class=\"type\">Array</span>[<span class=\"type\">MapStatus</span>]): <span class=\"type\">Iterator</span>[(<span class=\"type\">BlockManagerId</span>, <span class=\"type\">Seq</span>[(<span class=\"type\">BlockId</span>, <span class=\"type\">Long</span>)])] = &#123;</span><br><span class=\"line\">    assert (statuses != <span class=\"literal\">null</span>)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> splitsByAddress = <span class=\"keyword\">new</span> <span class=\"type\">HashMap</span>[<span class=\"type\">BlockManagerId</span>, <span class=\"type\">ListBuffer</span>[(<span class=\"type\">BlockId</span>, <span class=\"type\">Long</span>)]]</span><br><span class=\"line\">    <span class=\"keyword\">for</span> ((status, mapId) &lt;- statuses.iterator.zipWithIndex) &#123;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (status == <span class=\"literal\">null</span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">val</span> errorMessage = <span class=\"string\">s\"Missing an output location for shuffle <span class=\"subst\">$shuffleId</span>\"</span></span><br><span class=\"line\">        logError(errorMessage)</span><br><span class=\"line\">        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">MetadataFetchFailedException</span>(shuffleId, startPartition, errorMessage)</span><br><span class=\"line\">      &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (part &lt;- startPartition until endPartition) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">val</span> size = status.getSizeForBlock(part) <span class=\"comment\">// 这里只获取了需要处理的分区对应的数据</span></span><br><span class=\"line\">          <span class=\"keyword\">if</span> (size != <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">            <span class=\"comment\">// 这里获取到需要计算的分区数据所在的 block。其实就是由 shuffleId, mapId, part</span></span><br><span class=\"line\">            <span class=\"comment\">// 这三个组成的</span></span><br><span class=\"line\">            splitsByAddress.getOrElseUpdate(status.location, <span class=\"type\">ListBuffer</span>()) +=</span><br><span class=\"line\">                ((<span class=\"type\">ShuffleBlockId</span>(shuffleId, mapId, part), size))</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    splitsByAddress.iterator</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>这里涉及到 mapId 和 part 这两个变量。实际上，这两个都是分区的 Id 。只不过 mapId 是 mapper 端对应的分区Id，而 part 是经过 shuffle 之后 reducer 端对应的分区Id。通过<code>convertMapStatuses</code>就可以得到需要从哪些 Block 拉取数据。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 获取　block　对应的输入流</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> recordIter = wrappedStreams.flatMap &#123; <span class=\"keyword\">case</span> (blockId, wrappedStream) =&gt;</span><br><span class=\"line\">      <span class=\"comment\">// Note: the asKeyValueIterator below wraps a key/value iterator inside of a</span></span><br><span class=\"line\">      <span class=\"comment\">// NextIterator. The NextIterator makes sure that close() is called on the</span></span><br><span class=\"line\">      <span class=\"comment\">// underlying InputStream when all records have been read.</span></span><br><span class=\"line\">      serializerInstance.deserializeStream(wrappedStream).asKeyValueIterator</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// Update the context task metrics for each record read.</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> readMetrics = context.taskMetrics.createTempShuffleReadMetrics()</span><br><span class=\"line\">    <span class=\"keyword\">val</span> metricIter = <span class=\"type\">CompletionIterator</span>[(<span class=\"type\">Any</span>, <span class=\"type\">Any</span>), <span class=\"type\">Iterator</span>[(<span class=\"type\">Any</span>, <span class=\"type\">Any</span>)]](</span><br><span class=\"line\">      recordIter.map &#123; record =&gt;</span><br><span class=\"line\">        readMetrics.incRecordsRead(<span class=\"number\">1</span>)</span><br><span class=\"line\">        record</span><br><span class=\"line\">      &#125;,</span><br><span class=\"line\">      context.taskMetrics().mergeShuffleReadMetrics())</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// An interruptible iterator must be used here in order to support task cancellation</span></span><br><span class=\"line\">    <span class=\"comment\">//</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> interruptibleIter = <span class=\"keyword\">new</span> <span class=\"type\">InterruptibleIterator</span>[(<span class=\"type\">Any</span>, <span class=\"type\">Any</span>)](context, metricIter)</span><br><span class=\"line\">\t</span><br><span class=\"line\">    <span class=\"keyword\">val</span> aggregatedIter: <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] = <span class=\"keyword\">if</span> (dep.aggregator.isDefined) &#123;</span><br><span class=\"line\">      　<span class=\"comment\">// 如果需要聚合</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (dep.mapSideCombine) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// We are reading values that are already combined</span></span><br><span class=\"line\">        <span class=\"keyword\">val</span> combinedKeyValuesIterator = interruptibleIter.asInstanceOf[<span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>)]]</span><br><span class=\"line\">        dep.aggregator.get.combineCombinersByKey(combinedKeyValuesIterator, context)</span><br><span class=\"line\">      &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// We don't know the value type, but also don't care -- the dependency *should*</span></span><br><span class=\"line\">        <span class=\"comment\">// have made sure its compatible w/ this aggregator, which will convert the value</span></span><br><span class=\"line\">        <span class=\"comment\">// type to the combined type C</span></span><br><span class=\"line\">        <span class=\"keyword\">val</span> keyValuesIterator = interruptibleIter.asInstanceOf[<span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">Nothing</span>)]]</span><br><span class=\"line\">        dep.aggregator.get.combineValuesByKey(keyValuesIterator, context)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">      interruptibleIter.asInstanceOf[<span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]]]</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>这里进行聚合的代码实际上在 combineCombinersByKey 中:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">combineCombinersByKey</span></span>(</span><br><span class=\"line\">    iter: <span class=\"type\">Iterator</span>[_ &lt;: <span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]],</span><br><span class=\"line\">    context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>)] = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">val</span> combiners = <span class=\"keyword\">new</span> <span class=\"type\">ExternalAppendOnlyMap</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>, <span class=\"type\">C</span>](identity, mergeCombiners, mergeCombiners)</span><br><span class=\"line\">  <span class=\"comment\">// 这里的 insertAll 和　shuffle write 中的 insertAll 效果一样，会排序生成临时文件</span></span><br><span class=\"line\">  combiners.insertAll(iter)</span><br><span class=\"line\">  updateMetrics(context, combiners)</span><br><span class=\"line\">  combiners.iterator</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>最终返回了 iterator:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">iterator</span></span>: <span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>)] = &#123;</span><br><span class=\"line\">   <span class=\"keyword\">if</span> (currentMap == <span class=\"literal\">null</span>) &#123;</span><br><span class=\"line\">     <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">IllegalStateException</span>(</span><br><span class=\"line\">       <span class=\"string\">\"ExternalAppendOnlyMap.iterator is destructive and should only be called once.\"</span>)</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">    <span class=\"comment\">//　如果没有生成临时文件</span></span><br><span class=\"line\">   <span class=\"keyword\">if</span> (spilledMaps.isEmpty) &#123;</span><br><span class=\"line\">     <span class=\"type\">CompletionIterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>), <span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>)]](</span><br><span class=\"line\">       destructiveIterator(currentMap.iterator), freeCurrentMap())</span><br><span class=\"line\">   &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">     <span class=\"keyword\">new</span> <span class=\"type\">ExternalIterator</span>()</span><br><span class=\"line\">   &#125;</span><br></pre></td></tr></table></figure>\n<p>如果有零时文件生成会返回一个 ExternalIterator :</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"keyword\">val</span> mergeHeap = <span class=\"keyword\">new</span> mutable.<span class=\"type\">PriorityQueue</span>[<span class=\"type\">StreamBuffer</span>]</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> sortedMap = <span class=\"type\">CompletionIterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>), <span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>)]](destructiveIterator(</span><br><span class=\"line\">      currentMap.destructiveSortedIterator(keyComparator)), freeCurrentMap())</span><br><span class=\"line\">    <span class=\"comment\">// 这里合并了临时文件与内存中的数据</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> inputStreams = (<span class=\"type\">Seq</span>(sortedMap) ++ spilledMaps).map(it =&gt; it.buffered)</span><br><span class=\"line\">    inputStreams.foreach &#123; it =&gt;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> kcPairs = <span class=\"keyword\">new</span> <span class=\"type\">ArrayBuffer</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>)]</span><br><span class=\"line\">      readNextHashCode(it, kcPairs)</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (kcPairs.length &gt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">        mergeHeap.enqueue(<span class=\"keyword\">new</span> <span class=\"type\">StreamBuffer</span>(it, kcPairs))</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>这里涉及到了一个很重要的类 ArrayBuffer:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">StreamBuffer</span>(<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val iterator: <span class=\"type\">BufferedIterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span></span>)],</span></span><br><span class=\"line\"><span class=\"class\">    <span class=\"title\">val</span> <span class=\"title\">pairs</span></span>: <span class=\"type\">ArrayBuffer</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>)])</span><br><span class=\"line\">  <span class=\"keyword\">extends</span> <span class=\"type\">Comparable</span>[<span class=\"type\">StreamBuffer</span>] &#123;</span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">isEmpty</span></span>: <span class=\"type\">Boolean</span> = pairs.length == <span class=\"number\">0</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">minKeyHash</span></span>: <span class=\"type\">Int</span> = &#123;</span><br><span class=\"line\">    assert(pairs.length &gt; <span class=\"number\">0</span>)</span><br><span class=\"line\">    hashKey(pairs.head)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compareTo</span></span>(other: <span class=\"type\">StreamBuffer</span>): <span class=\"type\">Int</span> = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (other.minKeyHash &lt; minKeyHash) <span class=\"number\">-1</span> <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (other.minKeyHash == minKeyHash) <span class=\"number\">0</span> <span class=\"keyword\">else</span> <span class=\"number\">1</span></span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>StreamBuffer 实际上维持了一个 iterator 与一个数组。并且重写了 compareTo 方法。是通过数据中第一个元素的 key 也就是minKeyHash来比较大小。而代码:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">readNextHashCode(it, kcPairs)</span><br><span class=\"line\">     <span class=\"keyword\">if</span> (kcPairs.length &gt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">       mergeHeap.enqueue(<span class=\"keyword\">new</span> <span class=\"type\">StreamBuffer</span>(it, kcPairs))</span><br><span class=\"line\">     &#125;</span><br></pre></td></tr></table></figure>\n<p>再看 next 方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">next</span></span>(): (<span class=\"type\">K</span>, <span class=\"type\">C</span>) = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (mergeHeap.isEmpty) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">NoSuchElementException</span></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"comment\">// Select a key from the StreamBuffer that holds the lowest key hash</span></span><br><span class=\"line\">  <span class=\"comment\">//　这里需要注意的是每一个 StreamBuffer 中的数组的元素是按照 key 经过排序的，mergeHeap 中的 StreamBuffer 也是按照 minKeyHash 进行排序的。也就是从 mergeHeap 每取出一个StreamBuffer，其对应的数组中 key 的 hash 一定是目前 mergeHeap 中所有数组中 key 最小的，如果能理解这一点，那这里的 merge 就基本可以理解了。</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> minBuffer = mergeHeap.dequeue()</span><br><span class=\"line\">  <span class=\"keyword\">val</span> minPairs = minBuffer.pairs</span><br><span class=\"line\">  <span class=\"keyword\">val</span> minHash = minBuffer.minKeyHash</span><br><span class=\"line\">  <span class=\"keyword\">val</span> minPair = removeFromBuffer(minPairs, <span class=\"number\">0</span>)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> minKey = minPair._1</span><br><span class=\"line\">  <span class=\"keyword\">var</span> minCombiner = minPair._2</span><br><span class=\"line\">  assert(hashKey(minPair) == minHash)</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">// For all other streams that may have this key (i.e. have the same minimum key hash),</span></span><br><span class=\"line\">  <span class=\"comment\">// merge in the corresponding value (if any) from that stream</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> mergedBuffers = <span class=\"type\">ArrayBuffer</span>[<span class=\"type\">StreamBuffer</span>](minBuffer)</span><br><span class=\"line\">  <span class=\"comment\">// 如果下一个 StreamBuffer 中的 minKeyHash 相同，则可能会含有相同的 key，则需要合并。这里由于是经过排序的，所以不用遍历所有的 StreamBuffer。只要下一个不同，则后面一定不会有与当前 minKeyHash 相同的的 StreamBuffer。</span></span><br><span class=\"line\">  <span class=\"keyword\">while</span> (mergeHeap.nonEmpty &amp;&amp; mergeHeap.head.minKeyHash == minHash) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> newBuffer = mergeHeap.dequeue()</span><br><span class=\"line\">    <span class=\"comment\">// 这里如果有相同的 key 则进行合并</span></span><br><span class=\"line\">    <span class=\"comment\">// 注意，hash 相同 key 不一定相同</span></span><br><span class=\"line\">    minCombiner = mergeIfKeyExists(minKey, minCombiner, newBuffer)</span><br><span class=\"line\">    mergedBuffers += newBuffer</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">// Repopulate each visited stream buffer and add it back to the queue if it is non-empty</span></span><br><span class=\"line\">  </span><br><span class=\"line\">  mergedBuffers.foreach &#123; buffer =&gt;</span><br><span class=\"line\">    <span class=\"comment\">// 如果 key 被合并完了，就需要读取下一批hash相同的key的数据到ArrayBuffer的数组中</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (buffer.isEmpty) &#123;</span><br><span class=\"line\">      readNextHashCode(buffer.iterator, buffer.pairs)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (!buffer.isEmpty) &#123;</span><br><span class=\"line\">      <span class=\"comment\">// 刚才dequeue的 ArrayBuffer的数组中可能有没合并完的数据 </span></span><br><span class=\"line\">      <span class=\"comment\">// 或者有新读取的数据则需要继续放入 mergeHeap 中进行合并 </span></span><br><span class=\"line\">      mergeHeap.enqueue(buffer)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"comment\">// 返回合并完的数据</span></span><br><span class=\"line\">  <span class=\"comment\">// 注意这里的 key 只是按照 hash 进行排序的，在 ExternalSorter 才是按照用户定义的排序方式进行排序</span></span><br><span class=\"line\">  (minKey, minCombiner)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>如果不需要排序，shuffle read 就算完成了。但是如果需要排序则:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> resultIter = dep.keyOrdering <span class=\"keyword\">match</span> &#123;</span><br><span class=\"line\">     <span class=\"keyword\">case</span> <span class=\"type\">Some</span>(keyOrd: <span class=\"type\">Ordering</span>[<span class=\"type\">K</span>]) =&gt;</span><br><span class=\"line\">       <span class=\"comment\">// Create an ExternalSorter to sort the data.</span></span><br><span class=\"line\">       <span class=\"keyword\">val</span> sorter =</span><br><span class=\"line\">         <span class=\"keyword\">new</span> <span class=\"type\">ExternalSorter</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>, <span class=\"type\">C</span>](context, ordering = <span class=\"type\">Some</span>(keyOrd), serializer = dep.serializer)</span><br><span class=\"line\">       sorter.insertAll(aggregatedIter)</span><br><span class=\"line\">       context.taskMetrics().incMemoryBytesSpilled(sorter.memoryBytesSpilled)</span><br><span class=\"line\">       context.taskMetrics().incDiskBytesSpilled(sorter.diskBytesSpilled)</span><br><span class=\"line\">       context.taskMetrics().incPeakExecutionMemory(sorter.peakMemoryUsedBytes)</span><br><span class=\"line\">       <span class=\"comment\">// Use completion callback to stop sorter if task was finished/cancelled.</span></span><br><span class=\"line\">       context.addTaskCompletionListener(_ =&gt; &#123;</span><br><span class=\"line\">         sorter.stop()</span><br><span class=\"line\">       &#125;)</span><br><span class=\"line\">       <span class=\"type\">CompletionIterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>], <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]]](sorter.iterator, sorter.stop())</span><br><span class=\"line\">     <span class=\"keyword\">case</span> <span class=\"type\">None</span> =&gt;</span><br><span class=\"line\">       aggregatedIter</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">   resultIter <span class=\"keyword\">match</span> &#123;</span><br><span class=\"line\">     <span class=\"keyword\">case</span> _: <span class=\"type\">InterruptibleIterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] =&gt; resultIter</span><br><span class=\"line\">     <span class=\"keyword\">case</span> _ =&gt;</span><br><span class=\"line\">       <span class=\"comment\">// Use another interruptible iterator here to support task cancellation as aggregator</span></span><br><span class=\"line\">       <span class=\"comment\">// or(and) sorter may have consumed previous interruptible iterator.</span></span><br><span class=\"line\">       <span class=\"keyword\">new</span> <span class=\"type\">InterruptibleIterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]](context, resultIter)</span><br><span class=\"line\">   &#125;</span><br></pre></td></tr></table></figure>\n<p>这里依然使用了 ExternalSorter 进行排序，而最终使用了 ExternalSorter 的 iterator 方法。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">iterator</span></span>: <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] = &#123;</span><br><span class=\"line\">  isShuffleSort = <span class=\"literal\">false</span></span><br><span class=\"line\">  partitionedIterator.flatMap(pair =&gt; pair._2)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>其中 partitionedIterator 方法在 shuffle write 部分已经讲的很清楚了。有兴趣的可以去看看。这里 shuffle read 就基本结束了。其中从其他 Executor 拉取数据的部分由于涉及很多，这里就基本没有怎么讲解。以后可能会专门开一篇文章。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>拖了这么久终于把 shuffle read 部分的源码看了一遍了。虽然 shuffle read 再数据合并部分的逻辑要比 shuffle 简单，但是由于这个过程中 executor 要到 master 拉取 shuffle write 结果信息，就涉及到 spark 的 block manager 的一些东西，因此完整的 shuffle read 过程依然是很复杂的。但是由于我还是太菜了，对于 block manage 这一部分看的一知半解，因此关于 executor 与 master 进行元数据交互的部分也就不会写得很详细，当然还是会涉及到一些。有关 block manage 的这部分以后应该会写到，先立个 flag 在这里吧。</p>\n<h2 id=\"MapStatus\"><a href=\"#MapStatus\" class=\"headerlink\" title=\"MapStatus\"></a>MapStatus</h2><p>在 shuffle write 完成之后会返回一个 MapStatus，MapStatus记录了 BlockManagerId 以及最终每个分区的大小。其中 BlockManagerId 包含了 BlockManager 所在的 host 以及 port 等信息。返回的 MapStatus 最终会被 Driver 端获并存储以便于 mapper 端获取。 这里要稍微提一下 BlockManager 的知识，Spark 利用 BlockManager 对数据进行读写，而 Block 就是其中的基本单位。每一个 Block 拥有一个 id。而通过BlockManager 则可以操作这些 Block 。因此 mapper 端只需要知道 BlockManager 的位置以及所需要的文件在哪些 Block 就能获取到对应的数据。这里通过 MapOutputTracker 通过 shuffleid 对一次 shuffle write 中所有的 mapper 产生的 MapStatus 进行了记录。</p>\n<p>在每一个 mapper 的 shuffle task 结束后，MapOutputTracker就会将其返回的 MapStatues 进行注册。在 DAGScheduler 中可以看到:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">case</span> smt: <span class=\"type\">ShuffleMapTask</span> =&gt;</span><br><span class=\"line\">             <span class=\"keyword\">val</span> shuffleStage = stage.asInstanceOf[<span class=\"type\">ShuffleMapStage</span>]</span><br><span class=\"line\">             <span class=\"keyword\">val</span> status = event.result.asInstanceOf[<span class=\"type\">MapStatus</span>]</span><br><span class=\"line\">             <span class=\"keyword\">val</span> execId = status.location.executorId</span><br><span class=\"line\">\t\t  </span><br><span class=\"line\">             mapOutputTracker.registerMapOutput(</span><br><span class=\"line\">               shuffleStage.shuffleDep.shuffleId, smt.partitionId, status)</span><br></pre></td></tr></table></figure>\n<p>这里的 MapOutputTracker 实际的类型为 MapOutputTrackerMaster 。继续看关于注册的代码:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">registerMapOutput</span></span>(shuffleId: <span class=\"type\">Int</span>, mapId: <span class=\"type\">Int</span>, status: <span class=\"type\">MapStatus</span>) &#123;</span><br><span class=\"line\">   shuffleStatuses(shuffleId).addMapOutput(mapId, status)</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>shuffleStatuses 实际上是一个 hashmap:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> shuffleStatuses = <span class=\"keyword\">new</span> <span class=\"type\">ConcurrentHashMap</span>[<span class=\"type\">Int</span>, <span class=\"type\">ShuffleStatus</span>]().asScala</span><br></pre></td></tr></table></figure>\n<p>ShuffleStatus 是一个类，里面包含了一个 MapStatus 数组。也就是说，通过在 shuffle write 之前注册的 shuffle 所用的 shuffleid 作为索引存储了 shuffle write 过程产生的 MapStatus。而 MapOutputTrackerMaster 是用于 Driver 端的。用于 Executor 的则是 MapOutputTrackerWorker 。在 MapOutputTrackerWorker 中一开始是没有 shuffleStatuses 的。需要从 MapOutputTrackerMaster 中获取。</p>\n<h2 id=\"shuffle-read\"><a href=\"#shuffle-read\" class=\"headerlink\" title=\"shuffle read\"></a>shuffle read</h2><p>shuffle read 代码开始于 Shuffled 中的 compute 方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span></span>(split: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>)] = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> dep = dependencies.head.asInstanceOf[<span class=\"type\">ShuffleDependency</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>, <span class=\"type\">C</span>]]</span><br><span class=\"line\">    <span class=\"type\">SparkEnv</span>.get.shuffleManager.getReader(dep.shuffleHandle, split.index, split.index + <span class=\"number\">1</span>, context)</span><br><span class=\"line\">      .read()</span><br><span class=\"line\">      .asInstanceOf[<span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>)]]</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>可以看到这里传入了需要计算的分区。代码中获取了一个 reader 并调用了其 read 方法。 getReader 代码如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getReader</span></span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>](</span><br><span class=\"line\">    handle: <span class=\"type\">ShuffleHandle</span>,</span><br><span class=\"line\">    startPartition: <span class=\"type\">Int</span>,</span><br><span class=\"line\">    endPartition: <span class=\"type\">Int</span>,</span><br><span class=\"line\">    context: <span class=\"type\">TaskContext</span>): <span class=\"type\">ShuffleReader</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>] = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">new</span> <span class=\"type\">BlockStoreShuffleReader</span>(</span><br><span class=\"line\">    handle.asInstanceOf[<span class=\"type\">BaseShuffleHandle</span>[<span class=\"type\">K</span>, _, <span class=\"type\">C</span>]], startPartition, endPartition, context)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>实际上是返回了一个 BlockStoreShuffleReader 。read 方法较长，准备一段一段的讲解:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">read</span></span>(): <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]]</span><br></pre></td></tr></table></figure>\n<p>方法返回了一个 Iterator。方法的开始定义了一个:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> wrappedStreams = <span class=\"keyword\">new</span> <span class=\"type\">ShuffleBlockFetcherIterator</span>(</span><br><span class=\"line\">     context,</span><br><span class=\"line\">     blockManager.shuffleClient,</span><br><span class=\"line\">     blockManager,</span><br><span class=\"line\">     <span class=\"comment\">// 获取 blockManagerId 与　blockId</span></span><br><span class=\"line\">     mapOutputTracker.getMapSizesByExecutorId(handle.shuffleId, startPartition, endPartition),</span><br><span class=\"line\">     serializerManager.wrapStream,</span><br><span class=\"line\">     <span class=\"comment\">// Note: we use getSizeAsMb when no suffix is provided for backwards compatibility</span></span><br><span class=\"line\">     <span class=\"type\">SparkEnv</span>.get.conf.getSizeAsMb(<span class=\"string\">\"spark.reducer.maxSizeInFlight\"</span>, <span class=\"string\">\"48m\"</span>) * <span class=\"number\">1024</span> * <span class=\"number\">1024</span>,</span><br><span class=\"line\">     <span class=\"type\">SparkEnv</span>.get.conf.getInt(<span class=\"string\">\"spark.reducer.maxReqsInFlight\"</span>, <span class=\"type\">Int</span>.<span class=\"type\">MaxValue</span>),</span><br><span class=\"line\">     <span class=\"type\">SparkEnv</span>.get.conf.get(config.<span class=\"type\">REDUCER_MAX_BLOCKS_IN_FLIGHT_PER_ADDRESS</span>),</span><br><span class=\"line\">     <span class=\"type\">SparkEnv</span>.get.conf.get(config.<span class=\"type\">MAX_REMOTE_BLOCK_SIZE_FETCH_TO_MEM</span>),</span><br><span class=\"line\">     <span class=\"type\">SparkEnv</span>.get.conf.getBoolean(<span class=\"string\">\"spark.shuffle.detectCorrupt\"</span>, <span class=\"literal\">true</span>))</span><br></pre></td></tr></table></figure>\n<p>这个 wrappedStreams 实际上获取数据的输入流。看看:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getMapSizesByExecutorId</span></span>(shuffleId: <span class=\"type\">Int</span>, startPartition: <span class=\"type\">Int</span>, endPartition: <span class=\"type\">Int</span>)</span><br><span class=\"line\">     : <span class=\"type\">Iterator</span>[(<span class=\"type\">BlockManagerId</span>, <span class=\"type\">Seq</span>[(<span class=\"type\">BlockId</span>, <span class=\"type\">Long</span>)])] = &#123;</span><br><span class=\"line\">   logDebug(<span class=\"string\">s\"Fetching outputs for shuffle <span class=\"subst\">$shuffleId</span>, partitions <span class=\"subst\">$startPartition</span>-<span class=\"subst\">$endPartition</span>\"</span>)</span><br><span class=\"line\">   <span class=\"comment\">// 通过本次的 shuffleId 先获取 mapper 端产生的 MapStatuses</span></span><br><span class=\"line\">   <span class=\"keyword\">val</span> statuses = getStatuses(shuffleId)</span><br><span class=\"line\">   <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">      <span class=\"comment\">// </span></span><br><span class=\"line\">     <span class=\"type\">MapOutputTracker</span>.convertMapStatuses(shuffleId, startPartition, endPartition, statuses)</span><br><span class=\"line\">   &#125; <span class=\"keyword\">catch</span> &#123;</span><br><span class=\"line\">     <span class=\"keyword\">case</span> e: <span class=\"type\">MetadataFetchFailedException</span> =&gt;</span><br><span class=\"line\">       <span class=\"comment\">// We experienced a fetch failure so our mapStatuses cache is outdated; clear it:</span></span><br><span class=\"line\">       mapStatuses.clear()</span><br><span class=\"line\">       <span class=\"keyword\">throw</span> e</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>随后调用了 convertMapStatuses:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">convertMapStatuses</span></span>(</span><br><span class=\"line\">      shuffleId: <span class=\"type\">Int</span>,</span><br><span class=\"line\">      startPartition: <span class=\"type\">Int</span>,</span><br><span class=\"line\">      endPartition: <span class=\"type\">Int</span>,</span><br><span class=\"line\">      statuses: <span class=\"type\">Array</span>[<span class=\"type\">MapStatus</span>]): <span class=\"type\">Iterator</span>[(<span class=\"type\">BlockManagerId</span>, <span class=\"type\">Seq</span>[(<span class=\"type\">BlockId</span>, <span class=\"type\">Long</span>)])] = &#123;</span><br><span class=\"line\">    assert (statuses != <span class=\"literal\">null</span>)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> splitsByAddress = <span class=\"keyword\">new</span> <span class=\"type\">HashMap</span>[<span class=\"type\">BlockManagerId</span>, <span class=\"type\">ListBuffer</span>[(<span class=\"type\">BlockId</span>, <span class=\"type\">Long</span>)]]</span><br><span class=\"line\">    <span class=\"keyword\">for</span> ((status, mapId) &lt;- statuses.iterator.zipWithIndex) &#123;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (status == <span class=\"literal\">null</span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">val</span> errorMessage = <span class=\"string\">s\"Missing an output location for shuffle <span class=\"subst\">$shuffleId</span>\"</span></span><br><span class=\"line\">        logError(errorMessage)</span><br><span class=\"line\">        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">MetadataFetchFailedException</span>(shuffleId, startPartition, errorMessage)</span><br><span class=\"line\">      &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (part &lt;- startPartition until endPartition) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">val</span> size = status.getSizeForBlock(part) <span class=\"comment\">// 这里只获取了需要处理的分区对应的数据</span></span><br><span class=\"line\">          <span class=\"keyword\">if</span> (size != <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">            <span class=\"comment\">// 这里获取到需要计算的分区数据所在的 block。其实就是由 shuffleId, mapId, part</span></span><br><span class=\"line\">            <span class=\"comment\">// 这三个组成的</span></span><br><span class=\"line\">            splitsByAddress.getOrElseUpdate(status.location, <span class=\"type\">ListBuffer</span>()) +=</span><br><span class=\"line\">                ((<span class=\"type\">ShuffleBlockId</span>(shuffleId, mapId, part), size))</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    splitsByAddress.iterator</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>这里涉及到 mapId 和 part 这两个变量。实际上，这两个都是分区的 Id 。只不过 mapId 是 mapper 端对应的分区Id，而 part 是经过 shuffle 之后 reducer 端对应的分区Id。通过<code>convertMapStatuses</code>就可以得到需要从哪些 Block 拉取数据。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 获取　block　对应的输入流</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> recordIter = wrappedStreams.flatMap &#123; <span class=\"keyword\">case</span> (blockId, wrappedStream) =&gt;</span><br><span class=\"line\">      <span class=\"comment\">// Note: the asKeyValueIterator below wraps a key/value iterator inside of a</span></span><br><span class=\"line\">      <span class=\"comment\">// NextIterator. The NextIterator makes sure that close() is called on the</span></span><br><span class=\"line\">      <span class=\"comment\">// underlying InputStream when all records have been read.</span></span><br><span class=\"line\">      serializerInstance.deserializeStream(wrappedStream).asKeyValueIterator</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// Update the context task metrics for each record read.</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> readMetrics = context.taskMetrics.createTempShuffleReadMetrics()</span><br><span class=\"line\">    <span class=\"keyword\">val</span> metricIter = <span class=\"type\">CompletionIterator</span>[(<span class=\"type\">Any</span>, <span class=\"type\">Any</span>), <span class=\"type\">Iterator</span>[(<span class=\"type\">Any</span>, <span class=\"type\">Any</span>)]](</span><br><span class=\"line\">      recordIter.map &#123; record =&gt;</span><br><span class=\"line\">        readMetrics.incRecordsRead(<span class=\"number\">1</span>)</span><br><span class=\"line\">        record</span><br><span class=\"line\">      &#125;,</span><br><span class=\"line\">      context.taskMetrics().mergeShuffleReadMetrics())</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// An interruptible iterator must be used here in order to support task cancellation</span></span><br><span class=\"line\">    <span class=\"comment\">//</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> interruptibleIter = <span class=\"keyword\">new</span> <span class=\"type\">InterruptibleIterator</span>[(<span class=\"type\">Any</span>, <span class=\"type\">Any</span>)](context, metricIter)</span><br><span class=\"line\">\t</span><br><span class=\"line\">    <span class=\"keyword\">val</span> aggregatedIter: <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] = <span class=\"keyword\">if</span> (dep.aggregator.isDefined) &#123;</span><br><span class=\"line\">      　<span class=\"comment\">// 如果需要聚合</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (dep.mapSideCombine) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// We are reading values that are already combined</span></span><br><span class=\"line\">        <span class=\"keyword\">val</span> combinedKeyValuesIterator = interruptibleIter.asInstanceOf[<span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>)]]</span><br><span class=\"line\">        dep.aggregator.get.combineCombinersByKey(combinedKeyValuesIterator, context)</span><br><span class=\"line\">      &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// We don't know the value type, but also don't care -- the dependency *should*</span></span><br><span class=\"line\">        <span class=\"comment\">// have made sure its compatible w/ this aggregator, which will convert the value</span></span><br><span class=\"line\">        <span class=\"comment\">// type to the combined type C</span></span><br><span class=\"line\">        <span class=\"keyword\">val</span> keyValuesIterator = interruptibleIter.asInstanceOf[<span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">Nothing</span>)]]</span><br><span class=\"line\">        dep.aggregator.get.combineValuesByKey(keyValuesIterator, context)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">      interruptibleIter.asInstanceOf[<span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]]]</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>这里进行聚合的代码实际上在 combineCombinersByKey 中:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">combineCombinersByKey</span></span>(</span><br><span class=\"line\">    iter: <span class=\"type\">Iterator</span>[_ &lt;: <span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]],</span><br><span class=\"line\">    context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>)] = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">val</span> combiners = <span class=\"keyword\">new</span> <span class=\"type\">ExternalAppendOnlyMap</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>, <span class=\"type\">C</span>](identity, mergeCombiners, mergeCombiners)</span><br><span class=\"line\">  <span class=\"comment\">// 这里的 insertAll 和　shuffle write 中的 insertAll 效果一样，会排序生成临时文件</span></span><br><span class=\"line\">  combiners.insertAll(iter)</span><br><span class=\"line\">  updateMetrics(context, combiners)</span><br><span class=\"line\">  combiners.iterator</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>最终返回了 iterator:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">iterator</span></span>: <span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>)] = &#123;</span><br><span class=\"line\">   <span class=\"keyword\">if</span> (currentMap == <span class=\"literal\">null</span>) &#123;</span><br><span class=\"line\">     <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">IllegalStateException</span>(</span><br><span class=\"line\">       <span class=\"string\">\"ExternalAppendOnlyMap.iterator is destructive and should only be called once.\"</span>)</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">    <span class=\"comment\">//　如果没有生成临时文件</span></span><br><span class=\"line\">   <span class=\"keyword\">if</span> (spilledMaps.isEmpty) &#123;</span><br><span class=\"line\">     <span class=\"type\">CompletionIterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>), <span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>)]](</span><br><span class=\"line\">       destructiveIterator(currentMap.iterator), freeCurrentMap())</span><br><span class=\"line\">   &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">     <span class=\"keyword\">new</span> <span class=\"type\">ExternalIterator</span>()</span><br><span class=\"line\">   &#125;</span><br></pre></td></tr></table></figure>\n<p>如果有零时文件生成会返回一个 ExternalIterator :</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"keyword\">val</span> mergeHeap = <span class=\"keyword\">new</span> mutable.<span class=\"type\">PriorityQueue</span>[<span class=\"type\">StreamBuffer</span>]</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> sortedMap = <span class=\"type\">CompletionIterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>), <span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>)]](destructiveIterator(</span><br><span class=\"line\">      currentMap.destructiveSortedIterator(keyComparator)), freeCurrentMap())</span><br><span class=\"line\">    <span class=\"comment\">// 这里合并了临时文件与内存中的数据</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> inputStreams = (<span class=\"type\">Seq</span>(sortedMap) ++ spilledMaps).map(it =&gt; it.buffered)</span><br><span class=\"line\">    inputStreams.foreach &#123; it =&gt;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> kcPairs = <span class=\"keyword\">new</span> <span class=\"type\">ArrayBuffer</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>)]</span><br><span class=\"line\">      readNextHashCode(it, kcPairs)</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (kcPairs.length &gt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">        mergeHeap.enqueue(<span class=\"keyword\">new</span> <span class=\"type\">StreamBuffer</span>(it, kcPairs))</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>这里涉及到了一个很重要的类 ArrayBuffer:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">StreamBuffer</span>(<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val iterator: <span class=\"type\">BufferedIterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span></span>)],</span></span><br><span class=\"line\"><span class=\"class\">    <span class=\"title\">val</span> <span class=\"title\">pairs</span></span>: <span class=\"type\">ArrayBuffer</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>)])</span><br><span class=\"line\">  <span class=\"keyword\">extends</span> <span class=\"type\">Comparable</span>[<span class=\"type\">StreamBuffer</span>] &#123;</span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">isEmpty</span></span>: <span class=\"type\">Boolean</span> = pairs.length == <span class=\"number\">0</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">minKeyHash</span></span>: <span class=\"type\">Int</span> = &#123;</span><br><span class=\"line\">    assert(pairs.length &gt; <span class=\"number\">0</span>)</span><br><span class=\"line\">    hashKey(pairs.head)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compareTo</span></span>(other: <span class=\"type\">StreamBuffer</span>): <span class=\"type\">Int</span> = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (other.minKeyHash &lt; minKeyHash) <span class=\"number\">-1</span> <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (other.minKeyHash == minKeyHash) <span class=\"number\">0</span> <span class=\"keyword\">else</span> <span class=\"number\">1</span></span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>StreamBuffer 实际上维持了一个 iterator 与一个数组。并且重写了 compareTo 方法。是通过数据中第一个元素的 key 也就是minKeyHash来比较大小。而代码:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">readNextHashCode(it, kcPairs)</span><br><span class=\"line\">     <span class=\"keyword\">if</span> (kcPairs.length &gt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">       mergeHeap.enqueue(<span class=\"keyword\">new</span> <span class=\"type\">StreamBuffer</span>(it, kcPairs))</span><br><span class=\"line\">     &#125;</span><br></pre></td></tr></table></figure>\n<p>再看 next 方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">next</span></span>(): (<span class=\"type\">K</span>, <span class=\"type\">C</span>) = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (mergeHeap.isEmpty) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">NoSuchElementException</span></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"comment\">// Select a key from the StreamBuffer that holds the lowest key hash</span></span><br><span class=\"line\">  <span class=\"comment\">//　这里需要注意的是每一个 StreamBuffer 中的数组的元素是按照 key 经过排序的，mergeHeap 中的 StreamBuffer 也是按照 minKeyHash 进行排序的。也就是从 mergeHeap 每取出一个StreamBuffer，其对应的数组中 key 的 hash 一定是目前 mergeHeap 中所有数组中 key 最小的，如果能理解这一点，那这里的 merge 就基本可以理解了。</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> minBuffer = mergeHeap.dequeue()</span><br><span class=\"line\">  <span class=\"keyword\">val</span> minPairs = minBuffer.pairs</span><br><span class=\"line\">  <span class=\"keyword\">val</span> minHash = minBuffer.minKeyHash</span><br><span class=\"line\">  <span class=\"keyword\">val</span> minPair = removeFromBuffer(minPairs, <span class=\"number\">0</span>)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> minKey = minPair._1</span><br><span class=\"line\">  <span class=\"keyword\">var</span> minCombiner = minPair._2</span><br><span class=\"line\">  assert(hashKey(minPair) == minHash)</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">// For all other streams that may have this key (i.e. have the same minimum key hash),</span></span><br><span class=\"line\">  <span class=\"comment\">// merge in the corresponding value (if any) from that stream</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> mergedBuffers = <span class=\"type\">ArrayBuffer</span>[<span class=\"type\">StreamBuffer</span>](minBuffer)</span><br><span class=\"line\">  <span class=\"comment\">// 如果下一个 StreamBuffer 中的 minKeyHash 相同，则可能会含有相同的 key，则需要合并。这里由于是经过排序的，所以不用遍历所有的 StreamBuffer。只要下一个不同，则后面一定不会有与当前 minKeyHash 相同的的 StreamBuffer。</span></span><br><span class=\"line\">  <span class=\"keyword\">while</span> (mergeHeap.nonEmpty &amp;&amp; mergeHeap.head.minKeyHash == minHash) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> newBuffer = mergeHeap.dequeue()</span><br><span class=\"line\">    <span class=\"comment\">// 这里如果有相同的 key 则进行合并</span></span><br><span class=\"line\">    <span class=\"comment\">// 注意，hash 相同 key 不一定相同</span></span><br><span class=\"line\">    minCombiner = mergeIfKeyExists(minKey, minCombiner, newBuffer)</span><br><span class=\"line\">    mergedBuffers += newBuffer</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">// Repopulate each visited stream buffer and add it back to the queue if it is non-empty</span></span><br><span class=\"line\">  </span><br><span class=\"line\">  mergedBuffers.foreach &#123; buffer =&gt;</span><br><span class=\"line\">    <span class=\"comment\">// 如果 key 被合并完了，就需要读取下一批hash相同的key的数据到ArrayBuffer的数组中</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (buffer.isEmpty) &#123;</span><br><span class=\"line\">      readNextHashCode(buffer.iterator, buffer.pairs)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (!buffer.isEmpty) &#123;</span><br><span class=\"line\">      <span class=\"comment\">// 刚才dequeue的 ArrayBuffer的数组中可能有没合并完的数据 </span></span><br><span class=\"line\">      <span class=\"comment\">// 或者有新读取的数据则需要继续放入 mergeHeap 中进行合并 </span></span><br><span class=\"line\">      mergeHeap.enqueue(buffer)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"comment\">// 返回合并完的数据</span></span><br><span class=\"line\">  <span class=\"comment\">// 注意这里的 key 只是按照 hash 进行排序的，在 ExternalSorter 才是按照用户定义的排序方式进行排序</span></span><br><span class=\"line\">  (minKey, minCombiner)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>如果不需要排序，shuffle read 就算完成了。但是如果需要排序则:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> resultIter = dep.keyOrdering <span class=\"keyword\">match</span> &#123;</span><br><span class=\"line\">     <span class=\"keyword\">case</span> <span class=\"type\">Some</span>(keyOrd: <span class=\"type\">Ordering</span>[<span class=\"type\">K</span>]) =&gt;</span><br><span class=\"line\">       <span class=\"comment\">// Create an ExternalSorter to sort the data.</span></span><br><span class=\"line\">       <span class=\"keyword\">val</span> sorter =</span><br><span class=\"line\">         <span class=\"keyword\">new</span> <span class=\"type\">ExternalSorter</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>, <span class=\"type\">C</span>](context, ordering = <span class=\"type\">Some</span>(keyOrd), serializer = dep.serializer)</span><br><span class=\"line\">       sorter.insertAll(aggregatedIter)</span><br><span class=\"line\">       context.taskMetrics().incMemoryBytesSpilled(sorter.memoryBytesSpilled)</span><br><span class=\"line\">       context.taskMetrics().incDiskBytesSpilled(sorter.diskBytesSpilled)</span><br><span class=\"line\">       context.taskMetrics().incPeakExecutionMemory(sorter.peakMemoryUsedBytes)</span><br><span class=\"line\">       <span class=\"comment\">// Use completion callback to stop sorter if task was finished/cancelled.</span></span><br><span class=\"line\">       context.addTaskCompletionListener(_ =&gt; &#123;</span><br><span class=\"line\">         sorter.stop()</span><br><span class=\"line\">       &#125;)</span><br><span class=\"line\">       <span class=\"type\">CompletionIterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>], <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]]](sorter.iterator, sorter.stop())</span><br><span class=\"line\">     <span class=\"keyword\">case</span> <span class=\"type\">None</span> =&gt;</span><br><span class=\"line\">       aggregatedIter</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">   resultIter <span class=\"keyword\">match</span> &#123;</span><br><span class=\"line\">     <span class=\"keyword\">case</span> _: <span class=\"type\">InterruptibleIterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] =&gt; resultIter</span><br><span class=\"line\">     <span class=\"keyword\">case</span> _ =&gt;</span><br><span class=\"line\">       <span class=\"comment\">// Use another interruptible iterator here to support task cancellation as aggregator</span></span><br><span class=\"line\">       <span class=\"comment\">// or(and) sorter may have consumed previous interruptible iterator.</span></span><br><span class=\"line\">       <span class=\"keyword\">new</span> <span class=\"type\">InterruptibleIterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]](context, resultIter)</span><br><span class=\"line\">   &#125;</span><br></pre></td></tr></table></figure>\n<p>这里依然使用了 ExternalSorter 进行排序，而最终使用了 ExternalSorter 的 iterator 方法。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">iterator</span></span>: <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] = &#123;</span><br><span class=\"line\">  isShuffleSort = <span class=\"literal\">false</span></span><br><span class=\"line\">  partitionedIterator.flatMap(pair =&gt; pair._2)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>其中 partitionedIterator 方法在 shuffle write 部分已经讲的很清楚了。有兴趣的可以去看看。这里 shuffle read 就基本结束了。其中从其他 Executor 拉取数据的部分由于涉及很多，这里就基本没有怎么讲解。以后可能会专门开一篇文章。</p>\n"},{"title":"使用git + hexo 建立个人博客","date":"2018-06-07T02:42:45.000Z","_content":"\n## 安装 hexo\n\n1. 安装`node`\n2. 安装`npm`\n3. 安装`hexo: npm install -g hexo-cli`\n\n\n## 建立一个新博客\n\n1. `hexo init blog`:使用该命令会在blog目录下建立一个博客并且在`source/_posts/hello-world.md`生成一篇名为`hello world`的文章，随后你可以选择删除它并新建自己的文章。\n2. `cd blog`\n3. `hexo g`:使用该命令将 `source/_posts/hello-world.md` 渲染为`html、css、js`静态资源\n4. `hexo s`:开启服务器。然后http://localhost:4000/\n\n## 关联至github\n\n1. 新建仓库`xxx.github.io`，这里 xxx 可以是你想要取的名字，但是必须以`github.io`结尾\n\n2. 此时可以访问`https://xxx.github.io`，但是没有内容\n\n3. 修改`blog`目录下配置文件`_config.yml`，找到`deploy`选项，修改(新增)为:\n\n   ```yaml\n   deploy:\n     type: git\n     repository: git@github.com:xxx/xxx.github.io.git \n     branch: master\n   ```\n\n4. 安装插件`npm install hexo-deployer-git --save`\n\n5. `hexo d -g` 生成内容后部署\n\n6. 访问`https://xxx.github.io`，应该要延迟一段时间才能看到效果\n\n## 更换主题\n\n由于初始的主题不怎么好看，可以选择更换一下主题。官方主题地址为[hexo-themes](https://hexo.io/themes/)。本教程中采用[maupassant-hexo](https://www.haomwei.com/technology/maupassant-hexo.html#%E6%94%AF%E6%8C%81%E8%AF%AD%E8%A8%80)为主题\n\n1. 由于大部分的主题都托管在`github`上，在`blog`目录下运行:\n\n   `git clone https://github.com/tufu9441/maupassant-hexo.git themes/maupassant` \n\n   `themes/xxx`是`hexo`存放`xxx`主题的目录\n\n2. `npm install hexo-renderer-pug --save`\n\n3. `npm install hexo-renderer-sass --save --registry=https://registry.npm.taobao.org`\n\n4. 修改`_config.yml`中主题为`theme: maupassant`\n\n5. `hexo g`重新生成\n\n6. `hexo s`开启服务器\n\n主题还有许多可用的配置，请参照上面给出的链接进行设置\n\n## 目录、tag\n\n需要`归档`和`tag`只需要在`markdown`上加上一些`YAML`头部信息:\n\n```\n---\ntitle: hello\ncategories: 杂谈\ntag: 杂七杂八\n---\n```\n\n即可\n\n","source":"_posts/使用git+hexo建立个人博客.md","raw":"---\ntitle: 使用git + hexo 建立个人博客\ncategories: 工具\ndate: 2018-06-07 10:42:45\ntag: \n  - 工具\n  - 教程\n---\n\n## 安装 hexo\n\n1. 安装`node`\n2. 安装`npm`\n3. 安装`hexo: npm install -g hexo-cli`\n\n\n## 建立一个新博客\n\n1. `hexo init blog`:使用该命令会在blog目录下建立一个博客并且在`source/_posts/hello-world.md`生成一篇名为`hello world`的文章，随后你可以选择删除它并新建自己的文章。\n2. `cd blog`\n3. `hexo g`:使用该命令将 `source/_posts/hello-world.md` 渲染为`html、css、js`静态资源\n4. `hexo s`:开启服务器。然后http://localhost:4000/\n\n## 关联至github\n\n1. 新建仓库`xxx.github.io`，这里 xxx 可以是你想要取的名字，但是必须以`github.io`结尾\n\n2. 此时可以访问`https://xxx.github.io`，但是没有内容\n\n3. 修改`blog`目录下配置文件`_config.yml`，找到`deploy`选项，修改(新增)为:\n\n   ```yaml\n   deploy:\n     type: git\n     repository: git@github.com:xxx/xxx.github.io.git \n     branch: master\n   ```\n\n4. 安装插件`npm install hexo-deployer-git --save`\n\n5. `hexo d -g` 生成内容后部署\n\n6. 访问`https://xxx.github.io`，应该要延迟一段时间才能看到效果\n\n## 更换主题\n\n由于初始的主题不怎么好看，可以选择更换一下主题。官方主题地址为[hexo-themes](https://hexo.io/themes/)。本教程中采用[maupassant-hexo](https://www.haomwei.com/technology/maupassant-hexo.html#%E6%94%AF%E6%8C%81%E8%AF%AD%E8%A8%80)为主题\n\n1. 由于大部分的主题都托管在`github`上，在`blog`目录下运行:\n\n   `git clone https://github.com/tufu9441/maupassant-hexo.git themes/maupassant` \n\n   `themes/xxx`是`hexo`存放`xxx`主题的目录\n\n2. `npm install hexo-renderer-pug --save`\n\n3. `npm install hexo-renderer-sass --save --registry=https://registry.npm.taobao.org`\n\n4. 修改`_config.yml`中主题为`theme: maupassant`\n\n5. `hexo g`重新生成\n\n6. `hexo s`开启服务器\n\n主题还有许多可用的配置，请参照上面给出的链接进行设置\n\n## 目录、tag\n\n需要`归档`和`tag`只需要在`markdown`上加上一些`YAML`头部信息:\n\n```\n---\ntitle: hello\ncategories: 杂谈\ntag: 杂七杂八\n---\n```\n\n即可\n\n","slug":"使用git+hexo建立个人博客","published":1,"updated":"2018-06-29T17:06:08.923Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjj08c9nl0009ksng0qobgvrk","content":"<h2 id=\"安装-hexo\"><a href=\"#安装-hexo\" class=\"headerlink\" title=\"安装 hexo\"></a>安装 hexo</h2><ol>\n<li>安装<code>node</code></li>\n<li>安装<code>npm</code></li>\n<li>安装<code>hexo: npm install -g hexo-cli</code></li>\n</ol>\n<h2 id=\"建立一个新博客\"><a href=\"#建立一个新博客\" class=\"headerlink\" title=\"建立一个新博客\"></a>建立一个新博客</h2><ol>\n<li><code>hexo init blog</code>:使用该命令会在blog目录下建立一个博客并且在<code>source/_posts/hello-world.md</code>生成一篇名为<code>hello world</code>的文章，随后你可以选择删除它并新建自己的文章。</li>\n<li><code>cd blog</code></li>\n<li><code>hexo g</code>:使用该命令将 <code>source/_posts/hello-world.md</code> 渲染为<code>html、css、js</code>静态资源</li>\n<li><code>hexo s</code>:开启服务器。然后<a href=\"http://localhost:4000/\" target=\"_blank\" rel=\"noopener\">http://localhost:4000/</a></li>\n</ol>\n<h2 id=\"关联至github\"><a href=\"#关联至github\" class=\"headerlink\" title=\"关联至github\"></a>关联至github</h2><ol>\n<li><p>新建仓库<code>xxx.github.io</code>，这里 xxx 可以是你想要取的名字，但是必须以<code>github.io</code>结尾</p>\n</li>\n<li><p>此时可以访问<code>https://xxx.github.io</code>，但是没有内容</p>\n</li>\n<li><p>修改<code>blog</code>目录下配置文件<code>_config.yml</code>，找到<code>deploy</code>选项，修改(新增)为:</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">deploy:</span></span><br><span class=\"line\"><span class=\"attr\">  type:</span> <span class=\"string\">git</span></span><br><span class=\"line\"><span class=\"attr\">  repository:</span> <span class=\"string\">git@github.com:xxx/xxx.github.io.git</span> </span><br><span class=\"line\"><span class=\"attr\">  branch:</span> <span class=\"string\">master</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>安装插件<code>npm install hexo-deployer-git --save</code></p>\n</li>\n<li><p><code>hexo d -g</code> 生成内容后部署</p>\n</li>\n<li><p>访问<code>https://xxx.github.io</code>，应该要延迟一段时间才能看到效果</p>\n</li>\n</ol>\n<h2 id=\"更换主题\"><a href=\"#更换主题\" class=\"headerlink\" title=\"更换主题\"></a>更换主题</h2><p>由于初始的主题不怎么好看，可以选择更换一下主题。官方主题地址为<a href=\"https://hexo.io/themes/\" target=\"_blank\" rel=\"noopener\">hexo-themes</a>。本教程中采用<a href=\"https://www.haomwei.com/technology/maupassant-hexo.html#%E6%94%AF%E6%8C%81%E8%AF%AD%E8%A8%80\" target=\"_blank\" rel=\"noopener\">maupassant-hexo</a>为主题</p>\n<ol>\n<li><p>由于大部分的主题都托管在<code>github</code>上，在<code>blog</code>目录下运行:</p>\n<p><code>git clone https://github.com/tufu9441/maupassant-hexo.git themes/maupassant</code> </p>\n<p><code>themes/xxx</code>是<code>hexo</code>存放<code>xxx</code>主题的目录</p>\n</li>\n<li><p><code>npm install hexo-renderer-pug --save</code></p>\n</li>\n<li><p><code>npm install hexo-renderer-sass --save --registry=https://registry.npm.taobao.org</code></p>\n</li>\n<li><p>修改<code>_config.yml</code>中主题为<code>theme: maupassant</code></p>\n</li>\n<li><p><code>hexo g</code>重新生成</p>\n</li>\n<li><p><code>hexo s</code>开启服务器</p>\n</li>\n</ol>\n<p>主题还有许多可用的配置，请参照上面给出的链接进行设置</p>\n<h2 id=\"目录、tag\"><a href=\"#目录、tag\" class=\"headerlink\" title=\"目录、tag\"></a>目录、tag</h2><p>需要<code>归档</code>和<code>tag</code>只需要在<code>markdown</code>上加上一些<code>YAML</code>头部信息:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">---</span><br><span class=\"line\">title: hello</span><br><span class=\"line\">categories: 杂谈</span><br><span class=\"line\">tag: 杂七杂八</span><br><span class=\"line\">---</span><br></pre></td></tr></table></figure>\n<p>即可</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"安装-hexo\"><a href=\"#安装-hexo\" class=\"headerlink\" title=\"安装 hexo\"></a>安装 hexo</h2><ol>\n<li>安装<code>node</code></li>\n<li>安装<code>npm</code></li>\n<li>安装<code>hexo: npm install -g hexo-cli</code></li>\n</ol>\n<h2 id=\"建立一个新博客\"><a href=\"#建立一个新博客\" class=\"headerlink\" title=\"建立一个新博客\"></a>建立一个新博客</h2><ol>\n<li><code>hexo init blog</code>:使用该命令会在blog目录下建立一个博客并且在<code>source/_posts/hello-world.md</code>生成一篇名为<code>hello world</code>的文章，随后你可以选择删除它并新建自己的文章。</li>\n<li><code>cd blog</code></li>\n<li><code>hexo g</code>:使用该命令将 <code>source/_posts/hello-world.md</code> 渲染为<code>html、css、js</code>静态资源</li>\n<li><code>hexo s</code>:开启服务器。然后<a href=\"http://localhost:4000/\" target=\"_blank\" rel=\"noopener\">http://localhost:4000/</a></li>\n</ol>\n<h2 id=\"关联至github\"><a href=\"#关联至github\" class=\"headerlink\" title=\"关联至github\"></a>关联至github</h2><ol>\n<li><p>新建仓库<code>xxx.github.io</code>，这里 xxx 可以是你想要取的名字，但是必须以<code>github.io</code>结尾</p>\n</li>\n<li><p>此时可以访问<code>https://xxx.github.io</code>，但是没有内容</p>\n</li>\n<li><p>修改<code>blog</code>目录下配置文件<code>_config.yml</code>，找到<code>deploy</code>选项，修改(新增)为:</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">deploy:</span></span><br><span class=\"line\"><span class=\"attr\">  type:</span> <span class=\"string\">git</span></span><br><span class=\"line\"><span class=\"attr\">  repository:</span> <span class=\"string\">git@github.com:xxx/xxx.github.io.git</span> </span><br><span class=\"line\"><span class=\"attr\">  branch:</span> <span class=\"string\">master</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>安装插件<code>npm install hexo-deployer-git --save</code></p>\n</li>\n<li><p><code>hexo d -g</code> 生成内容后部署</p>\n</li>\n<li><p>访问<code>https://xxx.github.io</code>，应该要延迟一段时间才能看到效果</p>\n</li>\n</ol>\n<h2 id=\"更换主题\"><a href=\"#更换主题\" class=\"headerlink\" title=\"更换主题\"></a>更换主题</h2><p>由于初始的主题不怎么好看，可以选择更换一下主题。官方主题地址为<a href=\"https://hexo.io/themes/\" target=\"_blank\" rel=\"noopener\">hexo-themes</a>。本教程中采用<a href=\"https://www.haomwei.com/technology/maupassant-hexo.html#%E6%94%AF%E6%8C%81%E8%AF%AD%E8%A8%80\" target=\"_blank\" rel=\"noopener\">maupassant-hexo</a>为主题</p>\n<ol>\n<li><p>由于大部分的主题都托管在<code>github</code>上，在<code>blog</code>目录下运行:</p>\n<p><code>git clone https://github.com/tufu9441/maupassant-hexo.git themes/maupassant</code> </p>\n<p><code>themes/xxx</code>是<code>hexo</code>存放<code>xxx</code>主题的目录</p>\n</li>\n<li><p><code>npm install hexo-renderer-pug --save</code></p>\n</li>\n<li><p><code>npm install hexo-renderer-sass --save --registry=https://registry.npm.taobao.org</code></p>\n</li>\n<li><p>修改<code>_config.yml</code>中主题为<code>theme: maupassant</code></p>\n</li>\n<li><p><code>hexo g</code>重新生成</p>\n</li>\n<li><p><code>hexo s</code>开启服务器</p>\n</li>\n</ol>\n<p>主题还有许多可用的配置，请参照上面给出的链接进行设置</p>\n<h2 id=\"目录、tag\"><a href=\"#目录、tag\" class=\"headerlink\" title=\"目录、tag\"></a>目录、tag</h2><p>需要<code>归档</code>和<code>tag</code>只需要在<code>markdown</code>上加上一些<code>YAML</code>头部信息:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">---</span><br><span class=\"line\">title: hello</span><br><span class=\"line\">categories: 杂谈</span><br><span class=\"line\">tag: 杂七杂八</span><br><span class=\"line\">---</span><br></pre></td></tr></table></figure>\n<p>即可</p>\n"},{"title":"使用travis自动部署hexo到github","date":"2018-06-20T12:37:50.000Z","author":"lishion","toc":true,"_content":"\n在上一章[使用hexo+github搭建自己的博客](https://lishion.github.io/2018/06/07/%E4%BD%BF%E7%94%A8git+hexo%E5%BB%BA%E7%AB%8B%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/)介绍了如何使用 hexo 以及 github 搭建自己的博客。但是随后我们就遇到了一个问题: 如何我们想在多台电脑上编辑我们的博客应该怎么办。当然，我们可以选择新建一个仓库|分支利用 github 在多台电脑上同步。但是这种方式有一点繁琐，每次写作之前我们要保证源文件的同步 还要保证生成新的资源并部署到 github。例如一次典型的写作我们需要:\n\n```bash\ngit pull origin resource:resource # 假设我们的资源文件存储在 origin 分支\nhexo new xxx\ngit push origin resource\nhexo g\nhexo d\n```\n\n使用 travis 就能让我们省去渲染和部署的命令，变成:\n\n```shell~~\ngit pull origin resource:resource # 假设我们的资源文件存储在 origin 分支\nhexo new xxx\ngit push origin resource\n```\n\n~~难道就只能节省两个命令吗?~~\n\n~~哇，一共就五个命令。节省了两个等于节省了百分之四十啊!你说赛高不赛高。~~\n\n## 使用 travis 进行自动部署\n\n​\t看看廖雪峰大佬对 [travis](http://www.ruanyifeng.com/blog/2017/12/travis_ci_tutorial.html) 的介绍:\n\n> Travis CI 提供的是持续集成服务（Continuous Integration，简称 CI）。它绑定 Github 上面的项目，只要有新的代码，就会自动抓取。然后，提供一个运行环境，执行测试，完成构建，还能部署到服务器。\n\n简单的说 travis 提供了一个平台，这个平台可以监听你 Github 上仓库的变化，如果发生改变。则会将你的代码拉取到 travis 的服务器上，并执行你事先定义好的操作。\n\n### 初识 travis\n\n只要你拥有 github 的账户就可以直接登录 [travis官网](https://www.travis-ci.org/)使用。如果没有。。。。那就去注册一个。登录之后的页面大概是这样的:\n\n{% asset_img travis-mainpage.png travis主页 %}\n\n当然如果你没有使用过那左边的仓库栏和右边的信息栏都是空吧的，这个时候点击仓库栏上面的+号，添加一个仓库。\n\n{% asset_img travis-add.png 新增关联仓库 %}\n\n当然这里我们要选择的是 hexo 部署到的仓库。\n\n到这里，关联我们的仓库到 trvias 就完成了。但是我们还需要创建一个分支来保存我们的资源文件，因此，在我们的博客的根目录运行:\n\n```bash\ngit check out -b resource\ngit add *\ngit commit -m \"add resource\" \ngit add remote xxx # 这里的xxx就是博客对应的仓库\ngit push -u origin resource\n```\n\n这样把我们的资源文件同步到 github 上。\n\n### 配置文件\n\n当然，一个仓库有很多分支，我们还需要在根目录下配置一个 .travis.yml 文件来让 travis 知道我们需要构建哪一个分支。._config.yml 同时还告诉了 travis 在构建前需要做什么，构建后需要做什么。例如，我的 .travis.yml 如下:\n\n```yaml\nlanguage: node_js　# 使用什么语言\nnode_js:\n  - \"10\" # 语言版本\nbranches:\n  only:\n  - resource # 只构建 resource 分支\nscript:\n  - hexo g # 构建时执行的命令\ninstall:\n  - npm install  hexo # 由于 travis 的服务器默认是不带 hexo 所以需要安装\n  - npm install  hexo-cli\n  - npm install hexo-deployer-git\n  - sed -i'' \"s~git@github.com:~https://${TOKEN}@github.com/~\" _config.yml\nafter_success:\n  - hexo d # 构建完成后执行的命令 这里就是进行部署\nnotifications: # 设置通知项\n  email:\n    - 544670411@qq.com\n~                       \n```\n\n### 配置授权码\n\n现在还存在一个问题。travis 是需要从 github 拉取到他的服务器上，并且还需要通过 hexo 将构建后的文件 push 到 github，然而在 travis 的服务器上是不存在我们的 ssh 公钥的。也就会说目前 travis 还对无法操作我们的 github 仓库。所以我们还需要生成一个 github 授权码:\n\n{% asset_img author.png 新增授权码 %}\n\n然后在 travis 中配置这个授权码:\n\n{% asset_img token-add.png 配置授权码 %}\n\n其中名字可以自定义。但是配置了这个授权码如何使用呢?答案就是在 .travis.yml 中配置:\n\n```yaml\ninstall:\n  ...\n  - sed -i'' \"s~git@github.com:~https://${TOKEN}@github.com/~\" _config.yml\n```\n\n这里的 ${TOKEN} 需要将 TOKEN 替换为你刚才设置的名字。\n\n### 开始使用 \n\n到这里，整个设置部分就完成了。现在只需要 git pull origin resource 。然后登录 travis 就能看到项目正在构建了。从日志可以看到构建成功还是失败。以后每次写文章就只需要:\n\n```\ngit pull origin resource:resource\nhexo new xxx\ngit push orgin resource\n```\n\ntravis 就能帮你完成构建和部署的功能，是不是方便多了?\n\n\n\n\n\n\n","source":"_posts/使用travis自动部署hexo到github.md","raw":"---\ntitle: 使用travis自动部署hexo到github\ndate: 2018-06-20 20:37:50\ntags:\n  - 工具\n  - 教程\ncategories: 工具\nauthor: lishion\ntoc: true\n\n---\n\n在上一章[使用hexo+github搭建自己的博客](https://lishion.github.io/2018/06/07/%E4%BD%BF%E7%94%A8git+hexo%E5%BB%BA%E7%AB%8B%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/)介绍了如何使用 hexo 以及 github 搭建自己的博客。但是随后我们就遇到了一个问题: 如何我们想在多台电脑上编辑我们的博客应该怎么办。当然，我们可以选择新建一个仓库|分支利用 github 在多台电脑上同步。但是这种方式有一点繁琐，每次写作之前我们要保证源文件的同步 还要保证生成新的资源并部署到 github。例如一次典型的写作我们需要:\n\n```bash\ngit pull origin resource:resource # 假设我们的资源文件存储在 origin 分支\nhexo new xxx\ngit push origin resource\nhexo g\nhexo d\n```\n\n使用 travis 就能让我们省去渲染和部署的命令，变成:\n\n```shell~~\ngit pull origin resource:resource # 假设我们的资源文件存储在 origin 分支\nhexo new xxx\ngit push origin resource\n```\n\n~~难道就只能节省两个命令吗?~~\n\n~~哇，一共就五个命令。节省了两个等于节省了百分之四十啊!你说赛高不赛高。~~\n\n## 使用 travis 进行自动部署\n\n​\t看看廖雪峰大佬对 [travis](http://www.ruanyifeng.com/blog/2017/12/travis_ci_tutorial.html) 的介绍:\n\n> Travis CI 提供的是持续集成服务（Continuous Integration，简称 CI）。它绑定 Github 上面的项目，只要有新的代码，就会自动抓取。然后，提供一个运行环境，执行测试，完成构建，还能部署到服务器。\n\n简单的说 travis 提供了一个平台，这个平台可以监听你 Github 上仓库的变化，如果发生改变。则会将你的代码拉取到 travis 的服务器上，并执行你事先定义好的操作。\n\n### 初识 travis\n\n只要你拥有 github 的账户就可以直接登录 [travis官网](https://www.travis-ci.org/)使用。如果没有。。。。那就去注册一个。登录之后的页面大概是这样的:\n\n{% asset_img travis-mainpage.png travis主页 %}\n\n当然如果你没有使用过那左边的仓库栏和右边的信息栏都是空吧的，这个时候点击仓库栏上面的+号，添加一个仓库。\n\n{% asset_img travis-add.png 新增关联仓库 %}\n\n当然这里我们要选择的是 hexo 部署到的仓库。\n\n到这里，关联我们的仓库到 trvias 就完成了。但是我们还需要创建一个分支来保存我们的资源文件，因此，在我们的博客的根目录运行:\n\n```bash\ngit check out -b resource\ngit add *\ngit commit -m \"add resource\" \ngit add remote xxx # 这里的xxx就是博客对应的仓库\ngit push -u origin resource\n```\n\n这样把我们的资源文件同步到 github 上。\n\n### 配置文件\n\n当然，一个仓库有很多分支，我们还需要在根目录下配置一个 .travis.yml 文件来让 travis 知道我们需要构建哪一个分支。._config.yml 同时还告诉了 travis 在构建前需要做什么，构建后需要做什么。例如，我的 .travis.yml 如下:\n\n```yaml\nlanguage: node_js　# 使用什么语言\nnode_js:\n  - \"10\" # 语言版本\nbranches:\n  only:\n  - resource # 只构建 resource 分支\nscript:\n  - hexo g # 构建时执行的命令\ninstall:\n  - npm install  hexo # 由于 travis 的服务器默认是不带 hexo 所以需要安装\n  - npm install  hexo-cli\n  - npm install hexo-deployer-git\n  - sed -i'' \"s~git@github.com:~https://${TOKEN}@github.com/~\" _config.yml\nafter_success:\n  - hexo d # 构建完成后执行的命令 这里就是进行部署\nnotifications: # 设置通知项\n  email:\n    - 544670411@qq.com\n~                       \n```\n\n### 配置授权码\n\n现在还存在一个问题。travis 是需要从 github 拉取到他的服务器上，并且还需要通过 hexo 将构建后的文件 push 到 github，然而在 travis 的服务器上是不存在我们的 ssh 公钥的。也就会说目前 travis 还对无法操作我们的 github 仓库。所以我们还需要生成一个 github 授权码:\n\n{% asset_img author.png 新增授权码 %}\n\n然后在 travis 中配置这个授权码:\n\n{% asset_img token-add.png 配置授权码 %}\n\n其中名字可以自定义。但是配置了这个授权码如何使用呢?答案就是在 .travis.yml 中配置:\n\n```yaml\ninstall:\n  ...\n  - sed -i'' \"s~git@github.com:~https://${TOKEN}@github.com/~\" _config.yml\n```\n\n这里的 ${TOKEN} 需要将 TOKEN 替换为你刚才设置的名字。\n\n### 开始使用 \n\n到这里，整个设置部分就完成了。现在只需要 git pull origin resource 。然后登录 travis 就能看到项目正在构建了。从日志可以看到构建成功还是失败。以后每次写文章就只需要:\n\n```\ngit pull origin resource:resource\nhexo new xxx\ngit push orgin resource\n```\n\ntravis 就能帮你完成构建和部署的功能，是不是方便多了?\n\n\n\n\n\n\n","slug":"使用travis自动部署hexo到github","published":1,"updated":"2018-06-29T17:06:08.923Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjj08c9nm000cksngsxsi1nf3","content":"<p>在上一章<a href=\"https://lishion.github.io/2018/06/07/%E4%BD%BF%E7%94%A8git+hexo%E5%BB%BA%E7%AB%8B%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/\" target=\"_blank\" rel=\"noopener\">使用hexo+github搭建自己的博客</a>介绍了如何使用 hexo 以及 github 搭建自己的博客。但是随后我们就遇到了一个问题: 如何我们想在多台电脑上编辑我们的博客应该怎么办。当然，我们可以选择新建一个仓库|分支利用 github 在多台电脑上同步。但是这种方式有一点繁琐，每次写作之前我们要保证源文件的同步 还要保证生成新的资源并部署到 github。例如一次典型的写作我们需要:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git pull origin resource:resource <span class=\"comment\"># 假设我们的资源文件存储在 origin 分支</span></span><br><span class=\"line\">hexo new xxx</span><br><span class=\"line\">git push origin resource</span><br><span class=\"line\">hexo g</span><br><span class=\"line\">hexo d</span><br></pre></td></tr></table></figure>\n<p>使用 travis 就能让我们省去渲染和部署的命令，变成:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git pull origin resource:resource # 假设我们的资源文件存储在 origin 分支</span><br><span class=\"line\">hexo new xxx</span><br><span class=\"line\">git push origin resource</span><br></pre></td></tr></table></figure>\n<p><del>难道就只能节省两个命令吗?</del></p>\n<p><del>哇，一共就五个命令。节省了两个等于节省了百分之四十啊!你说赛高不赛高。</del></p>\n<h2 id=\"使用-travis-进行自动部署\"><a href=\"#使用-travis-进行自动部署\" class=\"headerlink\" title=\"使用 travis 进行自动部署\"></a>使用 travis 进行自动部署</h2><p>​    看看廖雪峰大佬对 <a href=\"http://www.ruanyifeng.com/blog/2017/12/travis_ci_tutorial.html\" target=\"_blank\" rel=\"noopener\">travis</a> 的介绍:</p>\n<blockquote>\n<p>Travis CI 提供的是持续集成服务（Continuous Integration，简称 CI）。它绑定 Github 上面的项目，只要有新的代码，就会自动抓取。然后，提供一个运行环境，执行测试，完成构建，还能部署到服务器。</p>\n</blockquote>\n<p>简单的说 travis 提供了一个平台，这个平台可以监听你 Github 上仓库的变化，如果发生改变。则会将你的代码拉取到 travis 的服务器上，并执行你事先定义好的操作。</p>\n<h3 id=\"初识-travis\"><a href=\"#初识-travis\" class=\"headerlink\" title=\"初识 travis\"></a>初识 travis</h3><p>只要你拥有 github 的账户就可以直接登录 <a href=\"https://www.travis-ci.org/\" target=\"_blank\" rel=\"noopener\">travis官网</a>使用。如果没有。。。。那就去注册一个。登录之后的页面大概是这样的:</p>\n<img src=\"/2018/06/20/使用travis自动部署hexo到github/travis-mainpage.png\" title=\"travis主页\">\n<p>当然如果你没有使用过那左边的仓库栏和右边的信息栏都是空吧的，这个时候点击仓库栏上面的+号，添加一个仓库。</p>\n<img src=\"/2018/06/20/使用travis自动部署hexo到github/travis-add.png\" title=\"新增关联仓库\">\n<p>当然这里我们要选择的是 hexo 部署到的仓库。</p>\n<p>到这里，关联我们的仓库到 trvias 就完成了。但是我们还需要创建一个分支来保存我们的资源文件，因此，在我们的博客的根目录运行:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git check out -b resource</span><br><span class=\"line\">git add *</span><br><span class=\"line\">git commit -m <span class=\"string\">\"add resource\"</span> </span><br><span class=\"line\">git add remote xxx <span class=\"comment\"># 这里的xxx就是博客对应的仓库</span></span><br><span class=\"line\">git push -u origin resource</span><br></pre></td></tr></table></figure>\n<p>这样把我们的资源文件同步到 github 上。</p>\n<h3 id=\"配置文件\"><a href=\"#配置文件\" class=\"headerlink\" title=\"配置文件\"></a>配置文件</h3><p>当然，一个仓库有很多分支，我们还需要在根目录下配置一个 .travis.yml 文件来让 travis 知道我们需要构建哪一个分支。._config.yml 同时还告诉了 travis 在构建前需要做什么，构建后需要做什么。例如，我的 .travis.yml 如下:</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">language:</span> <span class=\"string\">node_js</span>　<span class=\"comment\"># 使用什么语言</span></span><br><span class=\"line\"><span class=\"attr\">node_js:</span></span><br><span class=\"line\"><span class=\"bullet\">  -</span> <span class=\"string\">\"10\"</span> <span class=\"comment\"># 语言版本</span></span><br><span class=\"line\"><span class=\"attr\">branches:</span></span><br><span class=\"line\"><span class=\"attr\">  only:</span></span><br><span class=\"line\"><span class=\"bullet\">  -</span> <span class=\"string\">resource</span> <span class=\"comment\"># 只构建 resource 分支</span></span><br><span class=\"line\"><span class=\"attr\">script:</span></span><br><span class=\"line\"><span class=\"bullet\">  -</span> <span class=\"string\">hexo</span> <span class=\"string\">g</span> <span class=\"comment\"># 构建时执行的命令</span></span><br><span class=\"line\"><span class=\"attr\">install:</span></span><br><span class=\"line\"><span class=\"bullet\">  -</span> <span class=\"string\">npm</span> <span class=\"string\">install</span>  <span class=\"string\">hexo</span> <span class=\"comment\"># 由于 travis 的服务器默认是不带 hexo 所以需要安装</span></span><br><span class=\"line\"><span class=\"bullet\">  -</span> <span class=\"string\">npm</span> <span class=\"string\">install</span>  <span class=\"string\">hexo-cli</span></span><br><span class=\"line\"><span class=\"bullet\">  -</span> <span class=\"string\">npm</span> <span class=\"string\">install</span> <span class=\"string\">hexo-deployer-git</span></span><br><span class=\"line\"><span class=\"bullet\">  -</span> <span class=\"string\">sed</span> <span class=\"bullet\">-i''</span> <span class=\"string\">\"s~git@github.com:~https://$&#123;TOKEN&#125;@github.com/~\"</span> <span class=\"string\">_config.yml</span></span><br><span class=\"line\"><span class=\"attr\">after_success:</span></span><br><span class=\"line\"><span class=\"bullet\">  -</span> <span class=\"string\">hexo</span> <span class=\"string\">d</span> <span class=\"comment\"># 构建完成后执行的命令 这里就是进行部署</span></span><br><span class=\"line\"><span class=\"attr\">notifications:</span> <span class=\"comment\"># 设置通知项</span></span><br><span class=\"line\"><span class=\"attr\">  email:</span></span><br><span class=\"line\"><span class=\"bullet\">    -</span> <span class=\"number\">544670411</span><span class=\"string\">@qq.com</span></span><br><span class=\"line\"><span class=\"string\">~</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"配置授权码\"><a href=\"#配置授权码\" class=\"headerlink\" title=\"配置授权码\"></a>配置授权码</h3><p>现在还存在一个问题。travis 是需要从 github 拉取到他的服务器上，并且还需要通过 hexo 将构建后的文件 push 到 github，然而在 travis 的服务器上是不存在我们的 ssh 公钥的。也就会说目前 travis 还对无法操作我们的 github 仓库。所以我们还需要生成一个 github 授权码:</p>\n<img src=\"/2018/06/20/使用travis自动部署hexo到github/author.png\" title=\"新增授权码\">\n<p>然后在 travis 中配置这个授权码:</p>\n<img src=\"/2018/06/20/使用travis自动部署hexo到github/token-add.png\" title=\"配置授权码\">\n<p>其中名字可以自定义。但是配置了这个授权码如何使用呢?答案就是在 .travis.yml 中配置:</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">install:</span></span><br><span class=\"line\">  <span class=\"string\">...</span></span><br><span class=\"line\"><span class=\"bullet\">  -</span> <span class=\"string\">sed</span> <span class=\"bullet\">-i''</span> <span class=\"string\">\"s~git@github.com:~https://$&#123;TOKEN&#125;@github.com/~\"</span> <span class=\"string\">_config.yml</span></span><br></pre></td></tr></table></figure>\n<p>这里的 ${TOKEN} 需要将 TOKEN 替换为你刚才设置的名字。</p>\n<h3 id=\"开始使用\"><a href=\"#开始使用\" class=\"headerlink\" title=\"开始使用\"></a>开始使用</h3><p>到这里，整个设置部分就完成了。现在只需要 git pull origin resource 。然后登录 travis 就能看到项目正在构建了。从日志可以看到构建成功还是失败。以后每次写文章就只需要:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git pull origin resource:resource</span><br><span class=\"line\">hexo new xxx</span><br><span class=\"line\">git push orgin resource</span><br></pre></td></tr></table></figure>\n<p>travis 就能帮你完成构建和部署的功能，是不是方便多了?</p>\n","site":{"data":{}},"excerpt":"","more":"<p>在上一章<a href=\"https://lishion.github.io/2018/06/07/%E4%BD%BF%E7%94%A8git+hexo%E5%BB%BA%E7%AB%8B%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/\" target=\"_blank\" rel=\"noopener\">使用hexo+github搭建自己的博客</a>介绍了如何使用 hexo 以及 github 搭建自己的博客。但是随后我们就遇到了一个问题: 如何我们想在多台电脑上编辑我们的博客应该怎么办。当然，我们可以选择新建一个仓库|分支利用 github 在多台电脑上同步。但是这种方式有一点繁琐，每次写作之前我们要保证源文件的同步 还要保证生成新的资源并部署到 github。例如一次典型的写作我们需要:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git pull origin resource:resource <span class=\"comment\"># 假设我们的资源文件存储在 origin 分支</span></span><br><span class=\"line\">hexo new xxx</span><br><span class=\"line\">git push origin resource</span><br><span class=\"line\">hexo g</span><br><span class=\"line\">hexo d</span><br></pre></td></tr></table></figure>\n<p>使用 travis 就能让我们省去渲染和部署的命令，变成:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git pull origin resource:resource # 假设我们的资源文件存储在 origin 分支</span><br><span class=\"line\">hexo new xxx</span><br><span class=\"line\">git push origin resource</span><br></pre></td></tr></table></figure>\n<p><del>难道就只能节省两个命令吗?</del></p>\n<p><del>哇，一共就五个命令。节省了两个等于节省了百分之四十啊!你说赛高不赛高。</del></p>\n<h2 id=\"使用-travis-进行自动部署\"><a href=\"#使用-travis-进行自动部署\" class=\"headerlink\" title=\"使用 travis 进行自动部署\"></a>使用 travis 进行自动部署</h2><p>​    看看廖雪峰大佬对 <a href=\"http://www.ruanyifeng.com/blog/2017/12/travis_ci_tutorial.html\" target=\"_blank\" rel=\"noopener\">travis</a> 的介绍:</p>\n<blockquote>\n<p>Travis CI 提供的是持续集成服务（Continuous Integration，简称 CI）。它绑定 Github 上面的项目，只要有新的代码，就会自动抓取。然后，提供一个运行环境，执行测试，完成构建，还能部署到服务器。</p>\n</blockquote>\n<p>简单的说 travis 提供了一个平台，这个平台可以监听你 Github 上仓库的变化，如果发生改变。则会将你的代码拉取到 travis 的服务器上，并执行你事先定义好的操作。</p>\n<h3 id=\"初识-travis\"><a href=\"#初识-travis\" class=\"headerlink\" title=\"初识 travis\"></a>初识 travis</h3><p>只要你拥有 github 的账户就可以直接登录 <a href=\"https://www.travis-ci.org/\" target=\"_blank\" rel=\"noopener\">travis官网</a>使用。如果没有。。。。那就去注册一个。登录之后的页面大概是这样的:</p>\n<img src=\"/2018/06/20/使用travis自动部署hexo到github/travis-mainpage.png\" title=\"travis主页\">\n<p>当然如果你没有使用过那左边的仓库栏和右边的信息栏都是空吧的，这个时候点击仓库栏上面的+号，添加一个仓库。</p>\n<img src=\"/2018/06/20/使用travis自动部署hexo到github/travis-add.png\" title=\"新增关联仓库\">\n<p>当然这里我们要选择的是 hexo 部署到的仓库。</p>\n<p>到这里，关联我们的仓库到 trvias 就完成了。但是我们还需要创建一个分支来保存我们的资源文件，因此，在我们的博客的根目录运行:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git check out -b resource</span><br><span class=\"line\">git add *</span><br><span class=\"line\">git commit -m <span class=\"string\">\"add resource\"</span> </span><br><span class=\"line\">git add remote xxx <span class=\"comment\"># 这里的xxx就是博客对应的仓库</span></span><br><span class=\"line\">git push -u origin resource</span><br></pre></td></tr></table></figure>\n<p>这样把我们的资源文件同步到 github 上。</p>\n<h3 id=\"配置文件\"><a href=\"#配置文件\" class=\"headerlink\" title=\"配置文件\"></a>配置文件</h3><p>当然，一个仓库有很多分支，我们还需要在根目录下配置一个 .travis.yml 文件来让 travis 知道我们需要构建哪一个分支。._config.yml 同时还告诉了 travis 在构建前需要做什么，构建后需要做什么。例如，我的 .travis.yml 如下:</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">language:</span> <span class=\"string\">node_js</span>　<span class=\"comment\"># 使用什么语言</span></span><br><span class=\"line\"><span class=\"attr\">node_js:</span></span><br><span class=\"line\"><span class=\"bullet\">  -</span> <span class=\"string\">\"10\"</span> <span class=\"comment\"># 语言版本</span></span><br><span class=\"line\"><span class=\"attr\">branches:</span></span><br><span class=\"line\"><span class=\"attr\">  only:</span></span><br><span class=\"line\"><span class=\"bullet\">  -</span> <span class=\"string\">resource</span> <span class=\"comment\"># 只构建 resource 分支</span></span><br><span class=\"line\"><span class=\"attr\">script:</span></span><br><span class=\"line\"><span class=\"bullet\">  -</span> <span class=\"string\">hexo</span> <span class=\"string\">g</span> <span class=\"comment\"># 构建时执行的命令</span></span><br><span class=\"line\"><span class=\"attr\">install:</span></span><br><span class=\"line\"><span class=\"bullet\">  -</span> <span class=\"string\">npm</span> <span class=\"string\">install</span>  <span class=\"string\">hexo</span> <span class=\"comment\"># 由于 travis 的服务器默认是不带 hexo 所以需要安装</span></span><br><span class=\"line\"><span class=\"bullet\">  -</span> <span class=\"string\">npm</span> <span class=\"string\">install</span>  <span class=\"string\">hexo-cli</span></span><br><span class=\"line\"><span class=\"bullet\">  -</span> <span class=\"string\">npm</span> <span class=\"string\">install</span> <span class=\"string\">hexo-deployer-git</span></span><br><span class=\"line\"><span class=\"bullet\">  -</span> <span class=\"string\">sed</span> <span class=\"bullet\">-i''</span> <span class=\"string\">\"s~git@github.com:~https://$&#123;TOKEN&#125;@github.com/~\"</span> <span class=\"string\">_config.yml</span></span><br><span class=\"line\"><span class=\"attr\">after_success:</span></span><br><span class=\"line\"><span class=\"bullet\">  -</span> <span class=\"string\">hexo</span> <span class=\"string\">d</span> <span class=\"comment\"># 构建完成后执行的命令 这里就是进行部署</span></span><br><span class=\"line\"><span class=\"attr\">notifications:</span> <span class=\"comment\"># 设置通知项</span></span><br><span class=\"line\"><span class=\"attr\">  email:</span></span><br><span class=\"line\"><span class=\"bullet\">    -</span> <span class=\"number\">544670411</span><span class=\"string\">@qq.com</span></span><br><span class=\"line\"><span class=\"string\">~</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"配置授权码\"><a href=\"#配置授权码\" class=\"headerlink\" title=\"配置授权码\"></a>配置授权码</h3><p>现在还存在一个问题。travis 是需要从 github 拉取到他的服务器上，并且还需要通过 hexo 将构建后的文件 push 到 github，然而在 travis 的服务器上是不存在我们的 ssh 公钥的。也就会说目前 travis 还对无法操作我们的 github 仓库。所以我们还需要生成一个 github 授权码:</p>\n<img src=\"/2018/06/20/使用travis自动部署hexo到github/author.png\" title=\"新增授权码\">\n<p>然后在 travis 中配置这个授权码:</p>\n<img src=\"/2018/06/20/使用travis自动部署hexo到github/token-add.png\" title=\"配置授权码\">\n<p>其中名字可以自定义。但是配置了这个授权码如何使用呢?答案就是在 .travis.yml 中配置:</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">install:</span></span><br><span class=\"line\">  <span class=\"string\">...</span></span><br><span class=\"line\"><span class=\"bullet\">  -</span> <span class=\"string\">sed</span> <span class=\"bullet\">-i''</span> <span class=\"string\">\"s~git@github.com:~https://$&#123;TOKEN&#125;@github.com/~\"</span> <span class=\"string\">_config.yml</span></span><br></pre></td></tr></table></figure>\n<p>这里的 ${TOKEN} 需要将 TOKEN 替换为你刚才设置的名字。</p>\n<h3 id=\"开始使用\"><a href=\"#开始使用\" class=\"headerlink\" title=\"开始使用\"></a>开始使用</h3><p>到这里，整个设置部分就完成了。现在只需要 git pull origin resource 。然后登录 travis 就能看到项目正在构建了。从日志可以看到构建成功还是失败。以后每次写文章就只需要:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git pull origin resource:resource</span><br><span class=\"line\">hexo new xxx</span><br><span class=\"line\">git push orgin resource</span><br></pre></td></tr></table></figure>\n<p>travis 就能帮你完成构建和部署的功能，是不是方便多了?</p>\n"},{"title":"利用idea搭建spark-shell开发环境","date":"2018-06-22T06:54:33.000Z","author":"lishion","toc":true,"_content":"\n之前一直使用 vscode 写的 spark-shell 脚本，然后上传到服务器跑。但是 vscode 对 scala 支持实在是太差了，连基本的自动补全和语法检查都不支持。后来实在是忍受不了了，决定使用 idea 搭建一个编写 spark-shell 脚本的环境。**注意搭建这个环境主要是为了自动补全和语法检测，并不是为了编写可以运行的代码**。当然你按照这个方法搭建只需要配置一下调试器应该就可以运行了。\n\n## 用 maven 搭建 scala 运行环境\n\n既然要写 spark-shell 脚本。事先肯定得搭建好 scala 的运行环境。至于像 JDK 这一类的依赖可以装也可以不装，可以使用 idea 自带的。scala项目可以使用 sbt 搭建也可以使用 maven 搭建。由于对 sbt 不是很熟悉，再加上 sbt 在国内慢如蜗牛，多疑还是选择 maven。\n\n步骤如下:\n\n1. 安装 scala 插件，建议直接下载离线版本，直接安装下载好的 zip 文件\n\n2. 新建一个 maven 项目，稍后添加支持 scala\n\n3. 新增 scala sdk：\n\n   选择 `FIle -> setting -> Libraries -> 点击+ ->Scala SDK `\n\n   应该会有多个选项，选择一个合适的版本就可以\n\n到此如果可以新建 scala class 就说明安装成功了\n\n##引入 spark 依赖\n\n引入 spark 依赖其实很简单，只需在 pom.xml 中添加就可以 。一般来说只需要引入 spark-core 就行。如果还需要使用 spark sql 或则其他的依赖可以直接在[spark-maven](http://mvnrepository.com/artifact/org.apache.spark)中查找。\n\n但是，例如像 sc spark 这种在 spark-shell 准备好的对象在我们搭建的环境中是不存在的。为了避免每次都手动生成这些对象，我们可以在根目录新建一个 Env 类。然后在类中准备好 sc spark 对象。然后写脚本的时候只需要引入这个类就可以。例如:\n\n```scala\n// Env.scala\nimport org.apache.spark.{SparkConf,SparkContext}\nimport org.apache.spark.sql.SparkSession\n// 注意一定是 object\nobject Env {\n    val sc = new SparkContext(new SparkConf().setMaster(\"123\").setAppName(\"123\"))\n    val spark = new SparkSession()\n}\n```\n\n然后需要写不同功能的脚本只需要新建一个包，然后引入:\n\n```scala\nimport Env._\nsc.xxx\nspark.sql.xx\n```\n\n就可以获取到 sc 以及 spark 对象。","source":"_posts/利用idea搭建spark-shell开发环境.md","raw":"---\ntitle: 利用idea搭建spark-shell开发环境\ndate: 2018-06-22 14:54:33\ntags:\n  - 工具\n  - 教程\n  - 编程\n  - Spark\ncategories: 工具\nauthor: lishion\ntoc: true\n---\n\n之前一直使用 vscode 写的 spark-shell 脚本，然后上传到服务器跑。但是 vscode 对 scala 支持实在是太差了，连基本的自动补全和语法检查都不支持。后来实在是忍受不了了，决定使用 idea 搭建一个编写 spark-shell 脚本的环境。**注意搭建这个环境主要是为了自动补全和语法检测，并不是为了编写可以运行的代码**。当然你按照这个方法搭建只需要配置一下调试器应该就可以运行了。\n\n## 用 maven 搭建 scala 运行环境\n\n既然要写 spark-shell 脚本。事先肯定得搭建好 scala 的运行环境。至于像 JDK 这一类的依赖可以装也可以不装，可以使用 idea 自带的。scala项目可以使用 sbt 搭建也可以使用 maven 搭建。由于对 sbt 不是很熟悉，再加上 sbt 在国内慢如蜗牛，多疑还是选择 maven。\n\n步骤如下:\n\n1. 安装 scala 插件，建议直接下载离线版本，直接安装下载好的 zip 文件\n\n2. 新建一个 maven 项目，稍后添加支持 scala\n\n3. 新增 scala sdk：\n\n   选择 `FIle -> setting -> Libraries -> 点击+ ->Scala SDK `\n\n   应该会有多个选项，选择一个合适的版本就可以\n\n到此如果可以新建 scala class 就说明安装成功了\n\n##引入 spark 依赖\n\n引入 spark 依赖其实很简单，只需在 pom.xml 中添加就可以 。一般来说只需要引入 spark-core 就行。如果还需要使用 spark sql 或则其他的依赖可以直接在[spark-maven](http://mvnrepository.com/artifact/org.apache.spark)中查找。\n\n但是，例如像 sc spark 这种在 spark-shell 准备好的对象在我们搭建的环境中是不存在的。为了避免每次都手动生成这些对象，我们可以在根目录新建一个 Env 类。然后在类中准备好 sc spark 对象。然后写脚本的时候只需要引入这个类就可以。例如:\n\n```scala\n// Env.scala\nimport org.apache.spark.{SparkConf,SparkContext}\nimport org.apache.spark.sql.SparkSession\n// 注意一定是 object\nobject Env {\n    val sc = new SparkContext(new SparkConf().setMaster(\"123\").setAppName(\"123\"))\n    val spark = new SparkSession()\n}\n```\n\n然后需要写不同功能的脚本只需要新建一个包，然后引入:\n\n```scala\nimport Env._\nsc.xxx\nspark.sql.xx\n```\n\n就可以获取到 sc 以及 spark 对象。","slug":"利用idea搭建spark-shell开发环境","published":1,"updated":"2018-06-29T17:06:08.923Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjj08c9np000dksngathterys","content":"<p>之前一直使用 vscode 写的 spark-shell 脚本，然后上传到服务器跑。但是 vscode 对 scala 支持实在是太差了，连基本的自动补全和语法检查都不支持。后来实在是忍受不了了，决定使用 idea 搭建一个编写 spark-shell 脚本的环境。<strong>注意搭建这个环境主要是为了自动补全和语法检测，并不是为了编写可以运行的代码</strong>。当然你按照这个方法搭建只需要配置一下调试器应该就可以运行了。</p>\n<h2 id=\"用-maven-搭建-scala-运行环境\"><a href=\"#用-maven-搭建-scala-运行环境\" class=\"headerlink\" title=\"用 maven 搭建 scala 运行环境\"></a>用 maven 搭建 scala 运行环境</h2><p>既然要写 spark-shell 脚本。事先肯定得搭建好 scala 的运行环境。至于像 JDK 这一类的依赖可以装也可以不装，可以使用 idea 自带的。scala项目可以使用 sbt 搭建也可以使用 maven 搭建。由于对 sbt 不是很熟悉，再加上 sbt 在国内慢如蜗牛，多疑还是选择 maven。</p>\n<p>步骤如下:</p>\n<ol>\n<li><p>安装 scala 插件，建议直接下载离线版本，直接安装下载好的 zip 文件</p>\n</li>\n<li><p>新建一个 maven 项目，稍后添加支持 scala</p>\n</li>\n<li><p>新增 scala sdk：</p>\n<p>选择 <code>FIle -&gt; setting -&gt; Libraries -&gt; 点击+ -&gt;Scala SDK</code></p>\n<p>应该会有多个选项，选择一个合适的版本就可以</p>\n</li>\n</ol>\n<p>到此如果可以新建 scala class 就说明安装成功了</p>\n<p>##引入 spark 依赖</p>\n<p>引入 spark 依赖其实很简单，只需在 pom.xml 中添加就可以 。一般来说只需要引入 spark-core 就行。如果还需要使用 spark sql 或则其他的依赖可以直接在<a href=\"http://mvnrepository.com/artifact/org.apache.spark\" target=\"_blank\" rel=\"noopener\">spark-maven</a>中查找。</p>\n<p>但是，例如像 sc spark 这种在 spark-shell 准备好的对象在我们搭建的环境中是不存在的。为了避免每次都手动生成这些对象，我们可以在根目录新建一个 Env 类。然后在类中准备好 sc spark 对象。然后写脚本的时候只需要引入这个类就可以。例如:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Env.scala</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.&#123;<span class=\"type\">SparkConf</span>,<span class=\"type\">SparkContext</span>&#125;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.sql.<span class=\"type\">SparkSession</span></span><br><span class=\"line\"><span class=\"comment\">// 注意一定是 object</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Env</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> sc = <span class=\"keyword\">new</span> <span class=\"type\">SparkContext</span>(<span class=\"keyword\">new</span> <span class=\"type\">SparkConf</span>().setMaster(<span class=\"string\">\"123\"</span>).setAppName(<span class=\"string\">\"123\"</span>))</span><br><span class=\"line\">    <span class=\"keyword\">val</span> spark = <span class=\"keyword\">new</span> <span class=\"type\">SparkSession</span>()</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>然后需要写不同功能的脚本只需要新建一个包，然后引入:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> <span class=\"type\">Env</span>._</span><br><span class=\"line\">sc.xxx</span><br><span class=\"line\">spark.sql.xx</span><br></pre></td></tr></table></figure>\n<p>就可以获取到 sc 以及 spark 对象。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>之前一直使用 vscode 写的 spark-shell 脚本，然后上传到服务器跑。但是 vscode 对 scala 支持实在是太差了，连基本的自动补全和语法检查都不支持。后来实在是忍受不了了，决定使用 idea 搭建一个编写 spark-shell 脚本的环境。<strong>注意搭建这个环境主要是为了自动补全和语法检测，并不是为了编写可以运行的代码</strong>。当然你按照这个方法搭建只需要配置一下调试器应该就可以运行了。</p>\n<h2 id=\"用-maven-搭建-scala-运行环境\"><a href=\"#用-maven-搭建-scala-运行环境\" class=\"headerlink\" title=\"用 maven 搭建 scala 运行环境\"></a>用 maven 搭建 scala 运行环境</h2><p>既然要写 spark-shell 脚本。事先肯定得搭建好 scala 的运行环境。至于像 JDK 这一类的依赖可以装也可以不装，可以使用 idea 自带的。scala项目可以使用 sbt 搭建也可以使用 maven 搭建。由于对 sbt 不是很熟悉，再加上 sbt 在国内慢如蜗牛，多疑还是选择 maven。</p>\n<p>步骤如下:</p>\n<ol>\n<li><p>安装 scala 插件，建议直接下载离线版本，直接安装下载好的 zip 文件</p>\n</li>\n<li><p>新建一个 maven 项目，稍后添加支持 scala</p>\n</li>\n<li><p>新增 scala sdk：</p>\n<p>选择 <code>FIle -&gt; setting -&gt; Libraries -&gt; 点击+ -&gt;Scala SDK</code></p>\n<p>应该会有多个选项，选择一个合适的版本就可以</p>\n</li>\n</ol>\n<p>到此如果可以新建 scala class 就说明安装成功了</p>\n<p>##引入 spark 依赖</p>\n<p>引入 spark 依赖其实很简单，只需在 pom.xml 中添加就可以 。一般来说只需要引入 spark-core 就行。如果还需要使用 spark sql 或则其他的依赖可以直接在<a href=\"http://mvnrepository.com/artifact/org.apache.spark\" target=\"_blank\" rel=\"noopener\">spark-maven</a>中查找。</p>\n<p>但是，例如像 sc spark 这种在 spark-shell 准备好的对象在我们搭建的环境中是不存在的。为了避免每次都手动生成这些对象，我们可以在根目录新建一个 Env 类。然后在类中准备好 sc spark 对象。然后写脚本的时候只需要引入这个类就可以。例如:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Env.scala</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.&#123;<span class=\"type\">SparkConf</span>,<span class=\"type\">SparkContext</span>&#125;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.sql.<span class=\"type\">SparkSession</span></span><br><span class=\"line\"><span class=\"comment\">// 注意一定是 object</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Env</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> sc = <span class=\"keyword\">new</span> <span class=\"type\">SparkContext</span>(<span class=\"keyword\">new</span> <span class=\"type\">SparkConf</span>().setMaster(<span class=\"string\">\"123\"</span>).setAppName(<span class=\"string\">\"123\"</span>))</span><br><span class=\"line\">    <span class=\"keyword\">val</span> spark = <span class=\"keyword\">new</span> <span class=\"type\">SparkSession</span>()</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>然后需要写不同功能的脚本只需要新建一个包，然后引入:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> <span class=\"type\">Env</span>._</span><br><span class=\"line\">sc.xxx</span><br><span class=\"line\">spark.sql.xx</span><br></pre></td></tr></table></figure>\n<p>就可以获取到 sc 以及 spark 对象。</p>\n"},{"title":"在hexo中书写latex公式","date":"2018-06-28T05:27:26.000Z","author":"lishion","toc":true,"_content":"\n昨天在写笔记的时候发现在 typora 中可以正确显示的 latex 公式 hexo 无法正确渲染。hexo 默认渲染器不支持 latex公式。需要安装插件才能使用。这里我采用了[hexo-renderer-markdown-it-plus](https://github.com/CHENXCHEN/hexo-renderer-markdown-it-plus) 替换默认的渲染器。其中公式渲染采用 [katex](https://khan.github.io/KaTeX/) 使用步骤如下:\n\n1. 安装依赖:\n\n   `npm uninstall hexo-renderer-marked`\n\n   `npm install hexo-renderer-markdown-it-plus`\n\n   `npm install markdown-it-katex`\n\n2. 修改配置文件\n\n   在`_config.yml`中添加:\n\n   ```yaml\n   markdown_it_plus:\n       highlight: true\n       html: true\n       xhtmlOut: true\n       breaks: true\n       langPrefix:\n       linkify: true\n       typographer:\n       quotes: “”‘’\n       pre_class: highlight\n   ```\n\n3. 修改主题模板:\n\n      主题的 head 模板位于: `themes/next-gux/layout/_partials/head.swig` 中。将\n\n      ```\n      <link href=\"https://cdn.bootcss.com/KaTeX/0.7.1/katex.min.css\" rel=\"stylesheet\">\n      ```\n\n      添加到`head.swig`文件末尾。\n\n   完成以上步骤即可 hexo 即可正确的渲染 latex 公式。\n\n","source":"_posts/在hexo中书写latex公式.md","raw":"---\ntitle: 在hexo中书写latex公式\ndate: 2018-06-28 13:27:26\ntags:\n  - 教程\n  - 工具\ncategories: Spark源码阅读计划\nauthor: lishion\ntoc: true\n---\n\n昨天在写笔记的时候发现在 typora 中可以正确显示的 latex 公式 hexo 无法正确渲染。hexo 默认渲染器不支持 latex公式。需要安装插件才能使用。这里我采用了[hexo-renderer-markdown-it-plus](https://github.com/CHENXCHEN/hexo-renderer-markdown-it-plus) 替换默认的渲染器。其中公式渲染采用 [katex](https://khan.github.io/KaTeX/) 使用步骤如下:\n\n1. 安装依赖:\n\n   `npm uninstall hexo-renderer-marked`\n\n   `npm install hexo-renderer-markdown-it-plus`\n\n   `npm install markdown-it-katex`\n\n2. 修改配置文件\n\n   在`_config.yml`中添加:\n\n   ```yaml\n   markdown_it_plus:\n       highlight: true\n       html: true\n       xhtmlOut: true\n       breaks: true\n       langPrefix:\n       linkify: true\n       typographer:\n       quotes: “”‘’\n       pre_class: highlight\n   ```\n\n3. 修改主题模板:\n\n      主题的 head 模板位于: `themes/next-gux/layout/_partials/head.swig` 中。将\n\n      ```\n      <link href=\"https://cdn.bootcss.com/KaTeX/0.7.1/katex.min.css\" rel=\"stylesheet\">\n      ```\n\n      添加到`head.swig`文件末尾。\n\n   完成以上步骤即可 hexo 即可正确的渲染 latex 公式。\n\n","slug":"在hexo中书写latex公式","published":1,"updated":"2018-06-29T17:06:08.923Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjj08c9nv000hksngvq8tcheu","content":"<p>昨天在写笔记的时候发现在 typora 中可以正确显示的 latex 公式 hexo 无法正确渲染。hexo 默认渲染器不支持 latex公式。需要安装插件才能使用。这里我采用了<a href=\"https://github.com/CHENXCHEN/hexo-renderer-markdown-it-plus\" target=\"_blank\" rel=\"noopener\">hexo-renderer-markdown-it-plus</a> 替换默认的渲染器。其中公式渲染采用 <a href=\"https://khan.github.io/KaTeX/\" target=\"_blank\" rel=\"noopener\">katex</a> 使用步骤如下:</p>\n<ol>\n<li><p>安装依赖:</p>\n<p><code>npm uninstall hexo-renderer-marked</code></p>\n<p><code>npm install hexo-renderer-markdown-it-plus</code></p>\n<p><code>npm install markdown-it-katex</code></p>\n</li>\n<li><p>修改配置文件</p>\n<p>在<code>_config.yml</code>中添加:</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">markdown_it_plus:</span></span><br><span class=\"line\"><span class=\"attr\">    highlight:</span> <span class=\"literal\">true</span></span><br><span class=\"line\"><span class=\"attr\">    html:</span> <span class=\"literal\">true</span></span><br><span class=\"line\"><span class=\"attr\">    xhtmlOut:</span> <span class=\"literal\">true</span></span><br><span class=\"line\"><span class=\"attr\">    breaks:</span> <span class=\"literal\">true</span></span><br><span class=\"line\"><span class=\"attr\">    langPrefix:</span></span><br><span class=\"line\"><span class=\"attr\">    linkify:</span> <span class=\"literal\">true</span></span><br><span class=\"line\"><span class=\"attr\">    typographer:</span></span><br><span class=\"line\"><span class=\"attr\">    quotes:</span> <span class=\"string\">“”‘’</span></span><br><span class=\"line\"><span class=\"attr\">    pre_class:</span> <span class=\"string\">highlight</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>修改主题模板:</p>\n<p>   主题的 head 模板位于: <code>themes/next-gux/layout/_partials/head.swig</code> 中。将</p>\n   <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;link href=&quot;https://cdn.bootcss.com/KaTeX/0.7.1/katex.min.css&quot; rel=&quot;stylesheet&quot;&gt;</span><br></pre></td></tr></table></figure>\n<p>   添加到<code>head.swig</code>文件末尾。</p>\n<p>完成以上步骤即可 hexo 即可正确的渲染 latex 公式。</p>\n</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<p>昨天在写笔记的时候发现在 typora 中可以正确显示的 latex 公式 hexo 无法正确渲染。hexo 默认渲染器不支持 latex公式。需要安装插件才能使用。这里我采用了<a href=\"https://github.com/CHENXCHEN/hexo-renderer-markdown-it-plus\" target=\"_blank\" rel=\"noopener\">hexo-renderer-markdown-it-plus</a> 替换默认的渲染器。其中公式渲染采用 <a href=\"https://khan.github.io/KaTeX/\" target=\"_blank\" rel=\"noopener\">katex</a> 使用步骤如下:</p>\n<ol>\n<li><p>安装依赖:</p>\n<p><code>npm uninstall hexo-renderer-marked</code></p>\n<p><code>npm install hexo-renderer-markdown-it-plus</code></p>\n<p><code>npm install markdown-it-katex</code></p>\n</li>\n<li><p>修改配置文件</p>\n<p>在<code>_config.yml</code>中添加:</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">markdown_it_plus:</span></span><br><span class=\"line\"><span class=\"attr\">    highlight:</span> <span class=\"literal\">true</span></span><br><span class=\"line\"><span class=\"attr\">    html:</span> <span class=\"literal\">true</span></span><br><span class=\"line\"><span class=\"attr\">    xhtmlOut:</span> <span class=\"literal\">true</span></span><br><span class=\"line\"><span class=\"attr\">    breaks:</span> <span class=\"literal\">true</span></span><br><span class=\"line\"><span class=\"attr\">    langPrefix:</span></span><br><span class=\"line\"><span class=\"attr\">    linkify:</span> <span class=\"literal\">true</span></span><br><span class=\"line\"><span class=\"attr\">    typographer:</span></span><br><span class=\"line\"><span class=\"attr\">    quotes:</span> <span class=\"string\">“”‘’</span></span><br><span class=\"line\"><span class=\"attr\">    pre_class:</span> <span class=\"string\">highlight</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>修改主题模板:</p>\n<p>   主题的 head 模板位于: <code>themes/next-gux/layout/_partials/head.swig</code> 中。将</p>\n   <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;link href=&quot;https://cdn.bootcss.com/KaTeX/0.7.1/katex.min.css&quot; rel=&quot;stylesheet&quot;&gt;</span><br></pre></td></tr></table></figure>\n<p>   添加到<code>head.swig</code>文件末尾。</p>\n<p>完成以上步骤即可 hexo 即可正确的渲染 latex 公式。</p>\n</li>\n</ol>\n"},{"title":"常用 Linux 命令系列","author":"lishion","toc":true,"date":"2018-06-09T02:42:45.000Z","_content":"\n记录一下用到的 Linux 命令\n\n## sort\n\nsort 可以用于对文本进行排序，常用的参数有:\n\n> * -t : 指定分隔符，默认为空格\n> * -n : 按照数字规则排序\n> * -k : 按空格分隔后的排序列数\n\n例如:\n\n```bash\n# sort_test\n1:2:3\n3:2:1\n2:3:4\n5:6:11\nsort -nk -t : sort_test # 按数字排序\n3:2:1\n1:2:3\n2:3:4\n5:6:11\nsort -k -t : sort_test # 按首字母进行排序\n3:2:1\n5:6:11\n1:2:3\n2:3:4\n\n```\n\n","source":"_posts/常用-linux-命令系列.md","raw":"---\ntitle: 常用 Linux 命令系列\ncategories: 笔记\nauthor: lishion\ntoc: true\ndate: 2018-06-09 10:42:45\ntags: \n  - Linux\n  - 编程\n---\n\n记录一下用到的 Linux 命令\n\n## sort\n\nsort 可以用于对文本进行排序，常用的参数有:\n\n> * -t : 指定分隔符，默认为空格\n> * -n : 按照数字规则排序\n> * -k : 按空格分隔后的排序列数\n\n例如:\n\n```bash\n# sort_test\n1:2:3\n3:2:1\n2:3:4\n5:6:11\nsort -nk -t : sort_test # 按数字排序\n3:2:1\n1:2:3\n2:3:4\n5:6:11\nsort -k -t : sort_test # 按首字母进行排序\n3:2:1\n5:6:11\n1:2:3\n2:3:4\n\n```\n\n","slug":"常用-linux-命令系列","published":1,"updated":"2018-06-29T17:06:08.923Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjj08c9nx000kksngoer0230f","content":"<p>记录一下用到的 Linux 命令</p>\n<h2 id=\"sort\"><a href=\"#sort\" class=\"headerlink\" title=\"sort\"></a>sort</h2><p>sort 可以用于对文本进行排序，常用的参数有:</p>\n<blockquote>\n<ul>\n<li>-t : 指定分隔符，默认为空格</li>\n<li>-n : 按照数字规则排序</li>\n<li>-k : 按空格分隔后的排序列数</li>\n</ul>\n</blockquote>\n<p>例如:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># sort_test</span></span><br><span class=\"line\">1:2:3</span><br><span class=\"line\">3:2:1</span><br><span class=\"line\">2:3:4</span><br><span class=\"line\">5:6:11</span><br><span class=\"line\">sort -nk -t : sort_test <span class=\"comment\"># 按数字排序</span></span><br><span class=\"line\">3:2:1</span><br><span class=\"line\">1:2:3</span><br><span class=\"line\">2:3:4</span><br><span class=\"line\">5:6:11</span><br><span class=\"line\">sort -k -t : sort_test <span class=\"comment\"># 按首字母进行排序</span></span><br><span class=\"line\">3:2:1</span><br><span class=\"line\">5:6:11</span><br><span class=\"line\">1:2:3</span><br><span class=\"line\">2:3:4</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<p>记录一下用到的 Linux 命令</p>\n<h2 id=\"sort\"><a href=\"#sort\" class=\"headerlink\" title=\"sort\"></a>sort</h2><p>sort 可以用于对文本进行排序，常用的参数有:</p>\n<blockquote>\n<ul>\n<li>-t : 指定分隔符，默认为空格</li>\n<li>-n : 按照数字规则排序</li>\n<li>-k : 按空格分隔后的排序列数</li>\n</ul>\n</blockquote>\n<p>例如:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># sort_test</span></span><br><span class=\"line\">1:2:3</span><br><span class=\"line\">3:2:1</span><br><span class=\"line\">2:3:4</span><br><span class=\"line\">5:6:11</span><br><span class=\"line\">sort -nk -t : sort_test <span class=\"comment\"># 按数字排序</span></span><br><span class=\"line\">3:2:1</span><br><span class=\"line\">1:2:3</span><br><span class=\"line\">2:3:4</span><br><span class=\"line\">5:6:11</span><br><span class=\"line\">sort -k -t : sort_test <span class=\"comment\"># 按首字母进行排序</span></span><br><span class=\"line\">3:2:1</span><br><span class=\"line\">5:6:11</span><br><span class=\"line\">1:2:3</span><br><span class=\"line\">2:3:4</span><br></pre></td></tr></table></figure>\n"},{"title":"数据挖掘基础-最大似然估计与最大后验估计","date":"2018-06-28T06:47:50.000Z","author":"lishion","toc":true,"_content":"\n在某些时候，我们知道观测数据是由某个概率模型产生的，但是我们不知道模型的参数。而通过观测数据估计模型参数就称为参数估计。参数估计有很多种方法：最大释然估计、最大后验估计、采样等。这篇文章主要讨论最大释然估计与最大后验证估计。\n\n## 最大似然估计\n\n在给定模型参数$ \\theta $，观测到观测值 $X1$ 的概率为 $p(\\mathbf{X}|\\theta)$ ，这称之为似然函数。顾名思义，最大释然估计就是在已知观测值 $\\mathbf{X}$ 的情况下估计$\\theta$，使得 $p(\\mathbf{X}|\\theta)$ 最大。\n\n举一个例子，抛一枚硬币 6 次，假设这枚硬币正面向上的概率为$\\theta$，观测到的结果如下：\n\n```正 反 正 正 正 反```\n利用这个结果对 $\\theta$ 进行一个估计。首先写出似然函数:\n$$\np(\\mathbf{X}|\\theta)=(1-\\theta)\\theta^3(1-\\theta)\n$$\n令导数为 0 ，求得 $\\theta = \\frac 2 3$。\n\n有的同学看到这里嘴巴张成了 o 型。抛 6 次 4 次正面向上$\\theta=\\frac{2}{3}$，那抛 1 次正好有 1 次正面向上岂不是$\\theta=1$?其实这里你已经开始使用 $\\theta$ 的先验概率了。也就是说，在你以前的观察中，$\\theta=0.5$的概率是最高的。而$\\theta=1$是几乎不可能的。这也就是我们接下来要讲的**最大后验估计**。\n\n## 最大后验估计\n\n在上面提到，一般来说都会认为 $\\theta=0.5$ 概率较大，而 $\\theta=1$ 概率较小。这其实相当于我们认为 $\\theta$ 也具有一个分布，例如正太分布。也就是将$\\theta$看作一个随机变量。这里改变一下符号，令 $\\Theta$ 表示需要估计的参数，而 $\\theta$ 表示它一个具体的取值 。而最大后验估计的思想就是:\n$$\n\\mathop{max}_\\theta \\ p(\\Theta=\\theta|\\mathbf{X})\n$$\n\n也就是在观测到$\\mathbf{X}$之后，估计$\\Theta$并使得 $p(\\Theta|\\mathbf{X})$ 最大。利用贝叶斯公式：\n$$\np(\\Theta=\\theta|\\mathbf{X}) = \\frac{p(\\mathbf{X}|\\Theta=\\theta)P(\\Theta=\\theta)}{p(\\mathbf{X})}\n$$\n\n这里的分母是一个常数，与 $\\Theta$ 无关。于是我们只需要最大化分子就可以，于是最后为:\n$$\n\\mathop{max}_\\theta \\ p(\\mathbf{X}|\\Theta=\\theta)P(\\Theta=\\theta)\n$$\n可以看出，这里比最大释然估计多出了一个先验概率 $P(\\Theta=\\theta)$。\n$$\np(\\mathbf{X}|\\Theta=\\theta)\n$$\n例如这里我们假设$\\Theta \\backsim \\it{N(0.5,0.1)}$。由于计算比较复杂，这里直接画出函数图像为:\n\n{% asset_img func_img.png 后验函数图像 %}\n\n从图像可以估计出$\\theta=0.55$，这个结果就比最大似然估计更加贴近实际。\n\n## 总结\n\n一般来说，在知道参数正确分布的时候，最大后验估计要优于最大释然估计。而最大似然估计与最大后验估计的差别用一句话概括为:\n\n> 最大似然估计的估计量是一个**未知常量**，而最大后验估计的估计量是一个**未知随机变量**\n\n\n\n","source":"_posts/数据挖掘基础-最大似然估计与最大后验估计.md","raw":"---\ntitle: 数据挖掘基础-最大似然估计与最大后验估计\ndate: 2018-06-28 14:47:50\ntags:\n  - 数据挖掘\n  - 机器学习\ncategories: 数据挖掘基础\nauthor: lishion\ntoc: true\n---\n\n在某些时候，我们知道观测数据是由某个概率模型产生的，但是我们不知道模型的参数。而通过观测数据估计模型参数就称为参数估计。参数估计有很多种方法：最大释然估计、最大后验估计、采样等。这篇文章主要讨论最大释然估计与最大后验证估计。\n\n## 最大似然估计\n\n在给定模型参数$ \\theta $，观测到观测值 $X1$ 的概率为 $p(\\mathbf{X}|\\theta)$ ，这称之为似然函数。顾名思义，最大释然估计就是在已知观测值 $\\mathbf{X}$ 的情况下估计$\\theta$，使得 $p(\\mathbf{X}|\\theta)$ 最大。\n\n举一个例子，抛一枚硬币 6 次，假设这枚硬币正面向上的概率为$\\theta$，观测到的结果如下：\n\n```正 反 正 正 正 反```\n利用这个结果对 $\\theta$ 进行一个估计。首先写出似然函数:\n$$\np(\\mathbf{X}|\\theta)=(1-\\theta)\\theta^3(1-\\theta)\n$$\n令导数为 0 ，求得 $\\theta = \\frac 2 3$。\n\n有的同学看到这里嘴巴张成了 o 型。抛 6 次 4 次正面向上$\\theta=\\frac{2}{3}$，那抛 1 次正好有 1 次正面向上岂不是$\\theta=1$?其实这里你已经开始使用 $\\theta$ 的先验概率了。也就是说，在你以前的观察中，$\\theta=0.5$的概率是最高的。而$\\theta=1$是几乎不可能的。这也就是我们接下来要讲的**最大后验估计**。\n\n## 最大后验估计\n\n在上面提到，一般来说都会认为 $\\theta=0.5$ 概率较大，而 $\\theta=1$ 概率较小。这其实相当于我们认为 $\\theta$ 也具有一个分布，例如正太分布。也就是将$\\theta$看作一个随机变量。这里改变一下符号，令 $\\Theta$ 表示需要估计的参数，而 $\\theta$ 表示它一个具体的取值 。而最大后验估计的思想就是:\n$$\n\\mathop{max}_\\theta \\ p(\\Theta=\\theta|\\mathbf{X})\n$$\n\n也就是在观测到$\\mathbf{X}$之后，估计$\\Theta$并使得 $p(\\Theta|\\mathbf{X})$ 最大。利用贝叶斯公式：\n$$\np(\\Theta=\\theta|\\mathbf{X}) = \\frac{p(\\mathbf{X}|\\Theta=\\theta)P(\\Theta=\\theta)}{p(\\mathbf{X})}\n$$\n\n这里的分母是一个常数，与 $\\Theta$ 无关。于是我们只需要最大化分子就可以，于是最后为:\n$$\n\\mathop{max}_\\theta \\ p(\\mathbf{X}|\\Theta=\\theta)P(\\Theta=\\theta)\n$$\n可以看出，这里比最大释然估计多出了一个先验概率 $P(\\Theta=\\theta)$。\n$$\np(\\mathbf{X}|\\Theta=\\theta)\n$$\n例如这里我们假设$\\Theta \\backsim \\it{N(0.5,0.1)}$。由于计算比较复杂，这里直接画出函数图像为:\n\n{% asset_img func_img.png 后验函数图像 %}\n\n从图像可以估计出$\\theta=0.55$，这个结果就比最大似然估计更加贴近实际。\n\n## 总结\n\n一般来说，在知道参数正确分布的时候，最大后验估计要优于最大释然估计。而最大似然估计与最大后验估计的差别用一句话概括为:\n\n> 最大似然估计的估计量是一个**未知常量**，而最大后验估计的估计量是一个**未知随机变量**\n\n\n\n","slug":"数据挖掘基础-最大似然估计与最大后验估计","published":1,"updated":"2018-06-29T17:06:08.923Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjj08c9nz000pksng0m8atk4m","content":"<p>在某些时候，我们知道观测数据是由某个概率模型产生的，但是我们不知道模型的参数。而通过观测数据估计模型参数就称为参数估计。参数估计有很多种方法：最大释然估计、最大后验估计、采样等。这篇文章主要讨论最大释然估计与最大后验证估计。</p>\n<h2 id=\"最大似然估计\"><a href=\"#最大似然估计\" class=\"headerlink\" title=\"最大似然估计\"></a>最大似然估计</h2><p>在给定模型参数$ \\theta $，观测到观测值 $X1$ 的概率为 $p(\\mathbf{X}|\\theta)$ ，这称之为似然函数。顾名思义，最大释然估计就是在已知观测值 $\\mathbf{X}$ 的情况下估计$\\theta$，使得 $p(\\mathbf{X}|\\theta)$ 最大。</p>\n<p>举一个例子，抛一枚硬币 6 次，假设这枚硬币正面向上的概率为$\\theta$，观测到的结果如下：</p>\n<p><code>正 反 正 正 正 反</code><br>利用这个结果对 $\\theta$ 进行一个估计。首先写出似然函数:<br>$$<br>p(\\mathbf{X}|\\theta)=(1-\\theta)\\theta^3(1-\\theta)<br>$$<br>令导数为 0 ，求得 $\\theta = \\frac 2 3$。</p>\n<p>有的同学看到这里嘴巴张成了 o 型。抛 6 次 4 次正面向上$\\theta=\\frac{2}{3}$，那抛 1 次正好有 1 次正面向上岂不是$\\theta=1$?其实这里你已经开始使用 $\\theta$ 的先验概率了。也就是说，在你以前的观察中，$\\theta=0.5$的概率是最高的。而$\\theta=1$是几乎不可能的。这也就是我们接下来要讲的<strong>最大后验估计</strong>。</p>\n<h2 id=\"最大后验估计\"><a href=\"#最大后验估计\" class=\"headerlink\" title=\"最大后验估计\"></a>最大后验估计</h2><p>在上面提到，一般来说都会认为 $\\theta=0.5$ 概率较大，而 $\\theta=1$ 概率较小。这其实相当于我们认为 $\\theta$ 也具有一个分布，例如正太分布。也就是将$\\theta$看作一个随机变量。这里改变一下符号，令 $\\Theta$ 表示需要估计的参数，而 $\\theta$ 表示它一个具体的取值 。而最大后验估计的思想就是:<br>$$<br>\\mathop{max}_\\theta \\ p(\\Theta=\\theta|\\mathbf{X})<br>$$</p>\n<p>也就是在观测到$\\mathbf{X}$之后，估计$\\Theta$并使得 $p(\\Theta|\\mathbf{X})$ 最大。利用贝叶斯公式：<br>$$<br>p(\\Theta=\\theta|\\mathbf{X}) = \\frac{p(\\mathbf{X}|\\Theta=\\theta)P(\\Theta=\\theta)}{p(\\mathbf{X})}<br>$$</p>\n<p>这里的分母是一个常数，与 $\\Theta$ 无关。于是我们只需要最大化分子就可以，于是最后为:<br>$$<br>\\mathop{max}_\\theta \\ p(\\mathbf{X}|\\Theta=\\theta)P(\\Theta=\\theta)<br>$$<br>可以看出，这里比最大释然估计多出了一个先验概率 $P(\\Theta=\\theta)$。<br>$$<br>p(\\mathbf{X}|\\Theta=\\theta)<br>$$<br>例如这里我们假设$\\Theta \\backsim \\it{N(0.5,0.1)}$。由于计算比较复杂，这里直接画出函数图像为:</p>\n<img src=\"/2018/06/28/数据挖掘基础-最大似然估计与最大后验估计/func_img.png\" title=\"后验函数图像\">\n<p>从图像可以估计出$\\theta=0.55$，这个结果就比最大似然估计更加贴近实际。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>一般来说，在知道参数正确分布的时候，最大后验估计要优于最大释然估计。而最大似然估计与最大后验估计的差别用一句话概括为:</p>\n<blockquote>\n<p>最大似然估计的估计量是一个<strong>未知常量</strong>，而最大后验估计的估计量是一个<strong>未知随机变量</strong></p>\n</blockquote>\n","site":{"data":{}},"excerpt":"","more":"<p>在某些时候，我们知道观测数据是由某个概率模型产生的，但是我们不知道模型的参数。而通过观测数据估计模型参数就称为参数估计。参数估计有很多种方法：最大释然估计、最大后验估计、采样等。这篇文章主要讨论最大释然估计与最大后验证估计。</p>\n<h2 id=\"最大似然估计\"><a href=\"#最大似然估计\" class=\"headerlink\" title=\"最大似然估计\"></a>最大似然估计</h2><p>在给定模型参数$ \\theta $，观测到观测值 $X1$ 的概率为 $p(\\mathbf{X}|\\theta)$ ，这称之为似然函数。顾名思义，最大释然估计就是在已知观测值 $\\mathbf{X}$ 的情况下估计$\\theta$，使得 $p(\\mathbf{X}|\\theta)$ 最大。</p>\n<p>举一个例子，抛一枚硬币 6 次，假设这枚硬币正面向上的概率为$\\theta$，观测到的结果如下：</p>\n<p><code>正 反 正 正 正 反</code><br>利用这个结果对 $\\theta$ 进行一个估计。首先写出似然函数:<br>$$<br>p(\\mathbf{X}|\\theta)=(1-\\theta)\\theta^3(1-\\theta)<br>$$<br>令导数为 0 ，求得 $\\theta = \\frac 2 3$。</p>\n<p>有的同学看到这里嘴巴张成了 o 型。抛 6 次 4 次正面向上$\\theta=\\frac{2}{3}$，那抛 1 次正好有 1 次正面向上岂不是$\\theta=1$?其实这里你已经开始使用 $\\theta$ 的先验概率了。也就是说，在你以前的观察中，$\\theta=0.5$的概率是最高的。而$\\theta=1$是几乎不可能的。这也就是我们接下来要讲的<strong>最大后验估计</strong>。</p>\n<h2 id=\"最大后验估计\"><a href=\"#最大后验估计\" class=\"headerlink\" title=\"最大后验估计\"></a>最大后验估计</h2><p>在上面提到，一般来说都会认为 $\\theta=0.5$ 概率较大，而 $\\theta=1$ 概率较小。这其实相当于我们认为 $\\theta$ 也具有一个分布，例如正太分布。也就是将$\\theta$看作一个随机变量。这里改变一下符号，令 $\\Theta$ 表示需要估计的参数，而 $\\theta$ 表示它一个具体的取值 。而最大后验估计的思想就是:<br>$$<br>\\mathop{max}_\\theta \\ p(\\Theta=\\theta|\\mathbf{X})<br>$$</p>\n<p>也就是在观测到$\\mathbf{X}$之后，估计$\\Theta$并使得 $p(\\Theta|\\mathbf{X})$ 最大。利用贝叶斯公式：<br>$$<br>p(\\Theta=\\theta|\\mathbf{X}) = \\frac{p(\\mathbf{X}|\\Theta=\\theta)P(\\Theta=\\theta)}{p(\\mathbf{X})}<br>$$</p>\n<p>这里的分母是一个常数，与 $\\Theta$ 无关。于是我们只需要最大化分子就可以，于是最后为:<br>$$<br>\\mathop{max}_\\theta \\ p(\\mathbf{X}|\\Theta=\\theta)P(\\Theta=\\theta)<br>$$<br>可以看出，这里比最大释然估计多出了一个先验概率 $P(\\Theta=\\theta)$。<br>$$<br>p(\\mathbf{X}|\\Theta=\\theta)<br>$$<br>例如这里我们假设$\\Theta \\backsim \\it{N(0.5,0.1)}$。由于计算比较复杂，这里直接画出函数图像为:</p>\n<img src=\"/2018/06/28/数据挖掘基础-最大似然估计与最大后验估计/func_img.png\" title=\"后验函数图像\">\n<p>从图像可以估计出$\\theta=0.55$，这个结果就比最大似然估计更加贴近实际。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>一般来说，在知道参数正确分布的时候，最大后验估计要优于最大释然估计。而最大似然估计与最大后验估计的差别用一句话概括为:</p>\n<blockquote>\n<p>最大似然估计的估计量是一个<strong>未知常量</strong>，而最大后验估计的估计量是一个<strong>未知随机变量</strong></p>\n</blockquote>\n"},{"title":"数据挖掘基础-熵","date":"2018-06-27T06:29:49.000Z","author":"lishion","toc":true,"_content":"\n熵最初是一个热力学中表征物质状态的参量，是体系混乱程度的度量。香农大佬在*通讯的数学原理*这篇论文中用来表示一个信源所发出的信息具有的平均信息量。可以说如果没有香农大佬这篇论文，就没有现在的现代通信体系。虽然信源熵这个概念出自通信，但是在其他的领域也有很广泛的应用。在数据挖掘和机器学习中也有所体现。由于作者专业本身就是通信，写今天这篇文章主要是想通过一些简单易懂的例子来描述一下熵的实际意义，帮助一些非通信的同学了解这个概念。\n\n## 信息有大小吗？如何定量描述?\n\n在香农大佬发表论文之前，人们没有办法对信息进行量化。例如**这个苹果是红的**这条消息，它到底包含了多少的信息呢，能够用数字来衡量吗？为了解决这个问题，香农大佬在他论文里描述到：**一条消息所包含的信息量，应该与这个消息所包含的事件发生的概率成反比**。这句话的意思是：如果一个事件发生的概率越大，那么这个事件发生时所包含的信息量就越少；如果一个事件发生的概率越小，那么这个事件包含的信息量就越大。\n\n就拿刚才那个红苹果的例子来举例：例如你昨天从超市买了一个红苹果放在桌子上，今天有一条消息告诉你**桌子上的苹果是红色的**。由于昨天购买的红苹果今天任然是红苹果的概率几乎是百分之百，因此这条信息的所包含的信息量几乎为0。如果有一条消息告诉你:**桌子上的苹果是金黄色的**，由于你购买一个红苹果放在桌上而变成金黄色的概率几乎为0，因此这条消息所包含的信息量就非常大。它可能意味着你购买了一个具有特异功能的苹果或者昨晚有小偷进入到你的家里把你桌子上苹果偷偷的涂成金黄色再悄悄离开。\n\n所以，香农大佬提出了自信息量的的概念:对于消息 x ，它发生的概率为 p。那么 x 的自信息量定为:\n$$\nI(x) = -log(p)\n$$\n这里 log 的底数可以是 2 或是 e。如果是 2 那么对应的单位为 bit 否则为 nat。   \n\n在很多时候，消息源都不止发出一条消息。那么如何衡量一个消息源的所包含的信息量呢?这里可以采用我们在统计学中**期望值**的思想：对于一个可以发出 n 条消息的消息源 X ，其中消息 $m_i$ 所发生的概率为 $p_i$ 。那么这个消息所包含的平均自信息量为:\n$$\nE(X)=-\\sum_{i=1}^n\\frac{1}{p_i}log(p)\n$$\n这也就是熵的定义。\n\n## 信息增益\n\n信息增益在数据挖掘中也有许多的应用，例如在决策树选择属性进行划分的时候要优先选择信息增益较大的属性。简答来说，信息增益就是在收到了一条消息后，信息源的不确定性的减少程度。再举一个例子。 \n\n有一天你的朋友问了你这样一个问题: 我在超市买了一个苹果，并把它藏在那大海的深处，请问你知道它是生的还是熟的吗？此时，你不知道关于这个苹果的任何信息，猜中这个苹果是生的还是熟的概率为50%。也就是说，苹果这个信息源有50%的可能会发出：我是生的这条消息。或者以50%的消息发出：我是熟的这条消息。那么我们计算一个这个苹果此时的熵为:\n$$\n-\\frac{1}{2}log\\frac{1}{2}-\\frac{1}{2}log\\frac{1}{2}=1(bit)\n$$\n此时你的朋友着你迷茫的双眼，他准备给你点提示: 我藏在大海深处的苹果是红色的。于是你到楼下超市看了一圈，并做出如下统计:\n\n|  颜色  | 成熟 | 非成熟 |\n| :----: | :--: | :----: |\n|  红色  |  90  |   10   |\n| 非红色 |  10  |   90   |\n\n\n\n这时你使用你在大学学到有关条件概率计算出。也就是说，当我们知道一个苹果是红色的，并且为熟的概率为 $\\frac{9}{10}$ 。那么此时苹果的熵为:\n$$\n-\\frac{9}{10}log\\frac{9}{10}-\\frac{1}{10}log\\frac{1}{10}=0.469(bit)\n$$\n\n\n可以看出，这个**苹果是红色的**这条消息使得信源的消息减少了 $1 - 0.469 = 0.531(bit)$\n\n\n\n","source":"_posts/数据挖掘基础-熵.md","raw":"---\ntitle: 数据挖掘基础-熵\ndate: 2018-06-27 14:29:49\ntags:\n  - 数据挖掘\n  - 机器学习\ncategories: 数据挖掘基础\nauthor: lishion\ntoc: true\n---\n\n熵最初是一个热力学中表征物质状态的参量，是体系混乱程度的度量。香农大佬在*通讯的数学原理*这篇论文中用来表示一个信源所发出的信息具有的平均信息量。可以说如果没有香农大佬这篇论文，就没有现在的现代通信体系。虽然信源熵这个概念出自通信，但是在其他的领域也有很广泛的应用。在数据挖掘和机器学习中也有所体现。由于作者专业本身就是通信，写今天这篇文章主要是想通过一些简单易懂的例子来描述一下熵的实际意义，帮助一些非通信的同学了解这个概念。\n\n## 信息有大小吗？如何定量描述?\n\n在香农大佬发表论文之前，人们没有办法对信息进行量化。例如**这个苹果是红的**这条消息，它到底包含了多少的信息呢，能够用数字来衡量吗？为了解决这个问题，香农大佬在他论文里描述到：**一条消息所包含的信息量，应该与这个消息所包含的事件发生的概率成反比**。这句话的意思是：如果一个事件发生的概率越大，那么这个事件发生时所包含的信息量就越少；如果一个事件发生的概率越小，那么这个事件包含的信息量就越大。\n\n就拿刚才那个红苹果的例子来举例：例如你昨天从超市买了一个红苹果放在桌子上，今天有一条消息告诉你**桌子上的苹果是红色的**。由于昨天购买的红苹果今天任然是红苹果的概率几乎是百分之百，因此这条信息的所包含的信息量几乎为0。如果有一条消息告诉你:**桌子上的苹果是金黄色的**，由于你购买一个红苹果放在桌上而变成金黄色的概率几乎为0，因此这条消息所包含的信息量就非常大。它可能意味着你购买了一个具有特异功能的苹果或者昨晚有小偷进入到你的家里把你桌子上苹果偷偷的涂成金黄色再悄悄离开。\n\n所以，香农大佬提出了自信息量的的概念:对于消息 x ，它发生的概率为 p。那么 x 的自信息量定为:\n$$\nI(x) = -log(p)\n$$\n这里 log 的底数可以是 2 或是 e。如果是 2 那么对应的单位为 bit 否则为 nat。   \n\n在很多时候，消息源都不止发出一条消息。那么如何衡量一个消息源的所包含的信息量呢?这里可以采用我们在统计学中**期望值**的思想：对于一个可以发出 n 条消息的消息源 X ，其中消息 $m_i$ 所发生的概率为 $p_i$ 。那么这个消息所包含的平均自信息量为:\n$$\nE(X)=-\\sum_{i=1}^n\\frac{1}{p_i}log(p)\n$$\n这也就是熵的定义。\n\n## 信息增益\n\n信息增益在数据挖掘中也有许多的应用，例如在决策树选择属性进行划分的时候要优先选择信息增益较大的属性。简答来说，信息增益就是在收到了一条消息后，信息源的不确定性的减少程度。再举一个例子。 \n\n有一天你的朋友问了你这样一个问题: 我在超市买了一个苹果，并把它藏在那大海的深处，请问你知道它是生的还是熟的吗？此时，你不知道关于这个苹果的任何信息，猜中这个苹果是生的还是熟的概率为50%。也就是说，苹果这个信息源有50%的可能会发出：我是生的这条消息。或者以50%的消息发出：我是熟的这条消息。那么我们计算一个这个苹果此时的熵为:\n$$\n-\\frac{1}{2}log\\frac{1}{2}-\\frac{1}{2}log\\frac{1}{2}=1(bit)\n$$\n此时你的朋友着你迷茫的双眼，他准备给你点提示: 我藏在大海深处的苹果是红色的。于是你到楼下超市看了一圈，并做出如下统计:\n\n|  颜色  | 成熟 | 非成熟 |\n| :----: | :--: | :----: |\n|  红色  |  90  |   10   |\n| 非红色 |  10  |   90   |\n\n\n\n这时你使用你在大学学到有关条件概率计算出。也就是说，当我们知道一个苹果是红色的，并且为熟的概率为 $\\frac{9}{10}$ 。那么此时苹果的熵为:\n$$\n-\\frac{9}{10}log\\frac{9}{10}-\\frac{1}{10}log\\frac{1}{10}=0.469(bit)\n$$\n\n\n可以看出，这个**苹果是红色的**这条消息使得信源的消息减少了 $1 - 0.469 = 0.531(bit)$\n\n\n\n","slug":"数据挖掘基础-熵","published":1,"updated":"2018-06-29T17:06:08.923Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjj08c9o0000rksngaojsz4g6","content":"<p>熵最初是一个热力学中表征物质状态的参量，是体系混乱程度的度量。香农大佬在<em>通讯的数学原理</em>这篇论文中用来表示一个信源所发出的信息具有的平均信息量。可以说如果没有香农大佬这篇论文，就没有现在的现代通信体系。虽然信源熵这个概念出自通信，但是在其他的领域也有很广泛的应用。在数据挖掘和机器学习中也有所体现。由于作者专业本身就是通信，写今天这篇文章主要是想通过一些简单易懂的例子来描述一下熵的实际意义，帮助一些非通信的同学了解这个概念。</p>\n<h2 id=\"信息有大小吗？如何定量描述\"><a href=\"#信息有大小吗？如何定量描述\" class=\"headerlink\" title=\"信息有大小吗？如何定量描述?\"></a>信息有大小吗？如何定量描述?</h2><p>在香农大佬发表论文之前，人们没有办法对信息进行量化。例如<strong>这个苹果是红的</strong>这条消息，它到底包含了多少的信息呢，能够用数字来衡量吗？为了解决这个问题，香农大佬在他论文里描述到：<strong>一条消息所包含的信息量，应该与这个消息所包含的事件发生的概率成反比</strong>。这句话的意思是：如果一个事件发生的概率越大，那么这个事件发生时所包含的信息量就越少；如果一个事件发生的概率越小，那么这个事件包含的信息量就越大。</p>\n<p>就拿刚才那个红苹果的例子来举例：例如你昨天从超市买了一个红苹果放在桌子上，今天有一条消息告诉你<strong>桌子上的苹果是红色的</strong>。由于昨天购买的红苹果今天任然是红苹果的概率几乎是百分之百，因此这条信息的所包含的信息量几乎为0。如果有一条消息告诉你:<strong>桌子上的苹果是金黄色的</strong>，由于你购买一个红苹果放在桌上而变成金黄色的概率几乎为0，因此这条消息所包含的信息量就非常大。它可能意味着你购买了一个具有特异功能的苹果或者昨晚有小偷进入到你的家里把你桌子上苹果偷偷的涂成金黄色再悄悄离开。</p>\n<p>所以，香农大佬提出了自信息量的的概念:对于消息 x ，它发生的概率为 p。那么 x 的自信息量定为:<br>$$<br>I(x) = -log(p)<br>$$<br>这里 log 的底数可以是 2 或是 e。如果是 2 那么对应的单位为 bit 否则为 nat。   </p>\n<p>在很多时候，消息源都不止发出一条消息。那么如何衡量一个消息源的所包含的信息量呢?这里可以采用我们在统计学中<strong>期望值</strong>的思想：对于一个可以发出 n 条消息的消息源 X ，其中消息 $m_i$ 所发生的概率为 $p_i$ 。那么这个消息所包含的平均自信息量为:<br>$$<br>E(X)=-\\sum_{i=1}^n\\frac{1}{p_i}log(p)<br>$$<br>这也就是熵的定义。</p>\n<h2 id=\"信息增益\"><a href=\"#信息增益\" class=\"headerlink\" title=\"信息增益\"></a>信息增益</h2><p>信息增益在数据挖掘中也有许多的应用，例如在决策树选择属性进行划分的时候要优先选择信息增益较大的属性。简答来说，信息增益就是在收到了一条消息后，信息源的不确定性的减少程度。再举一个例子。 </p>\n<p>有一天你的朋友问了你这样一个问题: 我在超市买了一个苹果，并把它藏在那大海的深处，请问你知道它是生的还是熟的吗？此时，你不知道关于这个苹果的任何信息，猜中这个苹果是生的还是熟的概率为50%。也就是说，苹果这个信息源有50%的可能会发出：我是生的这条消息。或者以50%的消息发出：我是熟的这条消息。那么我们计算一个这个苹果此时的熵为:<br>$$<br>-\\frac{1}{2}log\\frac{1}{2}-\\frac{1}{2}log\\frac{1}{2}=1(bit)<br>$$<br>此时你的朋友着你迷茫的双眼，他准备给你点提示: 我藏在大海深处的苹果是红色的。于是你到楼下超市看了一圈，并做出如下统计:</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">颜色</th>\n<th style=\"text-align:center\">成熟</th>\n<th style=\"text-align:center\">非成熟</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">红色</td>\n<td style=\"text-align:center\">90</td>\n<td style=\"text-align:center\">10</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">非红色</td>\n<td style=\"text-align:center\">10</td>\n<td style=\"text-align:center\">90</td>\n</tr>\n</tbody>\n</table>\n<p>这时你使用你在大学学到有关条件概率计算出。也就是说，当我们知道一个苹果是红色的，并且为熟的概率为 $\\frac{9}{10}$ 。那么此时苹果的熵为:<br>$$<br>-\\frac{9}{10}log\\frac{9}{10}-\\frac{1}{10}log\\frac{1}{10}=0.469(bit)<br>$$</p>\n<p>可以看出，这个<strong>苹果是红色的</strong>这条消息使得信源的消息减少了 $1 - 0.469 = 0.531(bit)$</p>\n","site":{"data":{}},"excerpt":"","more":"<p>熵最初是一个热力学中表征物质状态的参量，是体系混乱程度的度量。香农大佬在<em>通讯的数学原理</em>这篇论文中用来表示一个信源所发出的信息具有的平均信息量。可以说如果没有香农大佬这篇论文，就没有现在的现代通信体系。虽然信源熵这个概念出自通信，但是在其他的领域也有很广泛的应用。在数据挖掘和机器学习中也有所体现。由于作者专业本身就是通信，写今天这篇文章主要是想通过一些简单易懂的例子来描述一下熵的实际意义，帮助一些非通信的同学了解这个概念。</p>\n<h2 id=\"信息有大小吗？如何定量描述\"><a href=\"#信息有大小吗？如何定量描述\" class=\"headerlink\" title=\"信息有大小吗？如何定量描述?\"></a>信息有大小吗？如何定量描述?</h2><p>在香农大佬发表论文之前，人们没有办法对信息进行量化。例如<strong>这个苹果是红的</strong>这条消息，它到底包含了多少的信息呢，能够用数字来衡量吗？为了解决这个问题，香农大佬在他论文里描述到：<strong>一条消息所包含的信息量，应该与这个消息所包含的事件发生的概率成反比</strong>。这句话的意思是：如果一个事件发生的概率越大，那么这个事件发生时所包含的信息量就越少；如果一个事件发生的概率越小，那么这个事件包含的信息量就越大。</p>\n<p>就拿刚才那个红苹果的例子来举例：例如你昨天从超市买了一个红苹果放在桌子上，今天有一条消息告诉你<strong>桌子上的苹果是红色的</strong>。由于昨天购买的红苹果今天任然是红苹果的概率几乎是百分之百，因此这条信息的所包含的信息量几乎为0。如果有一条消息告诉你:<strong>桌子上的苹果是金黄色的</strong>，由于你购买一个红苹果放在桌上而变成金黄色的概率几乎为0，因此这条消息所包含的信息量就非常大。它可能意味着你购买了一个具有特异功能的苹果或者昨晚有小偷进入到你的家里把你桌子上苹果偷偷的涂成金黄色再悄悄离开。</p>\n<p>所以，香农大佬提出了自信息量的的概念:对于消息 x ，它发生的概率为 p。那么 x 的自信息量定为:<br>$$<br>I(x) = -log(p)<br>$$<br>这里 log 的底数可以是 2 或是 e。如果是 2 那么对应的单位为 bit 否则为 nat。   </p>\n<p>在很多时候，消息源都不止发出一条消息。那么如何衡量一个消息源的所包含的信息量呢?这里可以采用我们在统计学中<strong>期望值</strong>的思想：对于一个可以发出 n 条消息的消息源 X ，其中消息 $m_i$ 所发生的概率为 $p_i$ 。那么这个消息所包含的平均自信息量为:<br>$$<br>E(X)=-\\sum_{i=1}^n\\frac{1}{p_i}log(p)<br>$$<br>这也就是熵的定义。</p>\n<h2 id=\"信息增益\"><a href=\"#信息增益\" class=\"headerlink\" title=\"信息增益\"></a>信息增益</h2><p>信息增益在数据挖掘中也有许多的应用，例如在决策树选择属性进行划分的时候要优先选择信息增益较大的属性。简答来说，信息增益就是在收到了一条消息后，信息源的不确定性的减少程度。再举一个例子。 </p>\n<p>有一天你的朋友问了你这样一个问题: 我在超市买了一个苹果，并把它藏在那大海的深处，请问你知道它是生的还是熟的吗？此时，你不知道关于这个苹果的任何信息，猜中这个苹果是生的还是熟的概率为50%。也就是说，苹果这个信息源有50%的可能会发出：我是生的这条消息。或者以50%的消息发出：我是熟的这条消息。那么我们计算一个这个苹果此时的熵为:<br>$$<br>-\\frac{1}{2}log\\frac{1}{2}-\\frac{1}{2}log\\frac{1}{2}=1(bit)<br>$$<br>此时你的朋友着你迷茫的双眼，他准备给你点提示: 我藏在大海深处的苹果是红色的。于是你到楼下超市看了一圈，并做出如下统计:</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">颜色</th>\n<th style=\"text-align:center\">成熟</th>\n<th style=\"text-align:center\">非成熟</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">红色</td>\n<td style=\"text-align:center\">90</td>\n<td style=\"text-align:center\">10</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">非红色</td>\n<td style=\"text-align:center\">10</td>\n<td style=\"text-align:center\">90</td>\n</tr>\n</tbody>\n</table>\n<p>这时你使用你在大学学到有关条件概率计算出。也就是说，当我们知道一个苹果是红色的，并且为熟的概率为 $\\frac{9}{10}$ 。那么此时苹果的熵为:<br>$$<br>-\\frac{9}{10}log\\frac{9}{10}-\\frac{1}{10}log\\frac{1}{10}=0.469(bit)<br>$$</p>\n<p>可以看出，这个<strong>苹果是红色的</strong>这条消息使得信源的消息减少了 $1 - 0.469 = 0.531(bit)$</p>\n"},{"title":"spark源码阅读计划 - 第二部分 - shuffle write","date":"2018-06-08T05:46:45.000Z","author":"lishion","toc":true,"_content":"## 写在开始的话\n\nshuffle 作为 spark 最重要的一部分，也是很多初学者感到头痛的一部分。简单来说，shuffle 分为两个部分 : shuffle write 和 shuffle read。shuffle write 在 map 端进行，而shuffle read 在 reduce 端进行。本章就准备就 shuffle write 做一个详细的分析。\n\n## 为什么需要shuffle\n\n在上一章[Spark-源码阅读计划-第一部分-迭代计算](https://lishion.github.io/2018/06/06/Spark-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E8%AE%A1%E5%88%92-%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86-%E8%BF%AD%E4%BB%A3%E8%AE%A1%E7%AE%97/)提到过每一个分区的数据最后都会由一个task来处理，那么当task 执行的类似于 reduceByKey 这种算子的时候一定需要满足相同的 key 在同一个分区这个条件，否则就不能按照 key 进行reduce了。但是当我们从数据源获取数据并进行处理后，相同的 key 不一定在同一个分区。那么在一个 map 端将相同的key 使用某种方式聚集在同一个分区并写入临时文件的过程就称为 shuffle write；而在 reduce 从 map 拉去执行 task 所需要的数据就是 shuffle read；这两个步骤组成了shuffle。\n\n## 一个小例子\n\n本章同样以一个小例子作为本章讲解的中心，例如一个统计词频的代码:\n\n```scala\nsc.parallelize(List(\"hello\",\"world\",\"hello\",\"spark\")).map(x=>(x,1)).reduceByKey(_+_)\n```\n\n相信有一点基础的同学都知道 reduceByKey 这算子会产生 shuffle。但是大部分的同学都不知 shuffle 是什么时候产生的，shuffle 到底做了些什么，这也是这一章的重点。\n\n## shuffle task\n\n在上一章提到过当 job 提交后会生成 result task。而在需要 shuffle 则会生成 ShuffleMapTask。这里涉及到 job 提交以及 stage 生成的的知识，将会在下一章提到。这里我们直接开始看 ShuffleMapTask 中的 runtask :\n\n```scala\n override def runTask(context: TaskContext): MapStatus = {\n    // Deserialize the RDD using the broadcast variable.\n ...\n    var writer: ShuffleWriter[Any, Any] = null\n    try {\n      val manager = SparkEnv.get.shuffleManager\n      writer = manager.getWriter[Any, Any](dep.shuffleHandle, partitionId, context)\n       // 这里依然是rdd调用iterator方法的地方\n      writer.write(rdd.iterator(partition, context).asInstanceOf[Iterator[_ <: Product2[Any, Any]]])\n      writer.stop(success = true).get\n    } catch {\n      case e: Exception =>\n        try {\n          if (writer != null) {\n            writer.stop(success = false)\n          }\n        } catch {\n          case e: Exception =>\n            log.debug(\"Could not stop writer\", e)\n        }\n        throw e\n    }\n  }\n```\n\n可以看到首先获得一个 shuffleWriter。spark 中有许多不同类型的 writer ，默认使用的是 SortShuffleWriter。SortShuffleWriter 中的 write 方法实现了 shuffle write。源代码如下:\n\n```scala\n  override def write(records: Iterator[Product2[K, V]]): Unit = {\n      //如果需要 map 端聚\n    sorter = if (dep.mapSideCombine) {\n      new ExternalSorter[K, V, C](\n        context, dep.aggregator, Some(dep.partitioner), dep.keyOrdering, dep.serializer)\n    } else {\n      new ExternalSorter[K, V, V](\n        context, aggregator = None, Some(dep.partitioner), ordering = None, dep.serializer)\n    }\n    sorter.insertAll(records)\n    val output = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)\n    val tmp = Utils.tempFileWith(output)\n    try { \n      val blockId = ShuffleBlockId(dep.shuffleId, mapId, IndexShuffleBlockResolver.NOOP_REDUCE_ID)\n      val partitionLengths = sorter.writePartitionedFile(blockId, tmp)\n      shuffleBlockResolver.writeIndexFileAndCommit(dep.shuffleId, mapId, partitionLengths, tmp)\n      mapStatus = MapStatus(blockManager.shuffleServerId, partitionLengths)\n    } finally {\n      if (tmp.exists() && !tmp.delete()) {\n        logError(s\"Error while deleting temp file ${tmp.getAbsolutePath}\")\n      }\n    }\n  }\n```\n\n使用了 ExternalSorter 的 insertAll 方法插入了上一个 RDD 生成的数据的迭代器。\n\n#### insertAll\n\n```scala\ndef insertAll(records: Iterator[Product2[K, V]]): Unit = {\n    // TODO: stop combining if we find that the reduction factor isn't high\n    val shouldCombine = aggregator.isDefined\n    // 直接看需要 map 端聚合的情况\n    // 不需要聚合的情况类似\n    if (shouldCombine) {\n      // Combine values in-memory first using our AppendOnlyMap\n       \n      val mergeValue = aggregator.get.mergeValue \n      val createCombiner = aggregator.get.createCombiner\n      var kv: Product2[K, V] = null\n      // 如果 key 不存在则直接创建 Combiner ，否则使用 mergeValue 将 value 添加到创建的 Combiner中\n      val update = (hadValue: Boolean, oldValue: C) => {\n        if (hadValue) mergeValue(oldValue, kv._2) else createCombiner(kv._2)\n      }\n      while (records.hasNext) {\n        addElementsRead()\n        kv = records.next()\n        // 更新值\n        // 这里需要注意的是之前的数据是(k,v)类型，这里转换成了((part,k),v)\n        // 其中 part 是 key 对应的分区\n        map.changeValue((getPartition(kv._1), kv._1), update)\n        maybeSpillCollection(usingMap = true)\n      }\n    } else {\n      // Stick values into our buffer\n      while (records.hasNext) {\n        addElementsRead()\n        val kv = records.next()\n        buffer.insert(getPartition(kv._1), kv._1, kv._2.asInstanceOf[C])\n        maybeSpillCollection(usingMap = false)\n      }\n    }\n  }\n```\n\n在这里使用 getPartition() 进行了数据的**分区**，也就是将不同的 key 与其分区对应起来。默认使用的是 HashPartitioner ，有兴趣的同学可以去阅读源码。再继续看 changeValue 方法。在这里使用非常重要的数据结构来存放分区后的数据，也就是代码中的 map 类型为 PartitionedAppendOnlyMap。其中是使用hask表存放数据，并且存放的方式为表中的偶数索index引存放key, index + 1 存放对应的value。例如:\n\n```\nk1|v1|null|k2|v2|k3|v3|null|....\n```\n\n之所以里面由空值，是因为在插入的时候使用了二次探测法。来看一看 changeValue 方法:\n\nPartitionedAppendOnlyMap 中的  changeValue 方法继承自 \n\n```scala\n  def changeValue(key: K, updateFunc: (Boolean, V) => V): V = {\n    assert(!destroyed, destructionMessage)\n    val k = key.asInstanceOf[AnyRef]\n    if (k.eq(null)) {\n      if (!haveNullValue) {\n        incrementSize()\n      }\n      nullValue = updateFunc(haveNullValue, nullValue)\n      haveNullValue = true\n      return nullValue\n    }\n    var pos = rehash(k.hashCode) & mask\n    var i = 1\n    while (true) {\n      // 偶数位置为 key\n      val curKey = data(2 * pos)\n       // 如果该key不存在，就直接插入\n      if (curKey.eq(null)) {\n        val newValue = updateFunc(false, null.asInstanceOf[V])\n        data(2 * pos) = k\n        data(2 * pos + 1) = newValue.asInstanceOf[AnyRef]\n        incrementSize()\n        return newValue\n      } else if (k.eq(curKey) || k.equals(curKey)) {//如果 key 存在，则进行聚合\n        \n        val newValue = updateFunc(true, data(2 * pos + 1).asInstanceOf[V])\n        data(2 * pos + 1) = newValue.asInstanceOf[AnyRef]\n        return newValue\n      } else { // 否则进行下一次探测\n        val delta = i\n        pos = (pos + delta) & mask\n        i += 1\n      }\n    }\n    null.asInstanceOf[V] // Never reached but needed to keep compiler happy\n  }\n```\n\n再调用 insertAll 进行了数据的聚合|插入之后，调用了 maybeSpillCollection 方法。这个方法的作用是判断 map 占用了内存，如果内存达到一定的值，则将内存中的数据 spill 到临时文件中。\n\n```scala\nprivate def maybeSpillCollection(usingMap: Boolean): Unit = {\n    var estimatedSize = 0L\n    if (usingMap) {\n      estimatedSize = map.estimateSize()\n      if (maybeSpill(map, estimatedSize)) {\n        map = new PartitionedAppendOnlyMap[K, C]\n      }\n    } else {\n      estimatedSize = buffer.estimateSize()\n      if (maybeSpill(buffer, estimatedSize)) {\n        buffer = new PartitionedPairBuffer[K, C]\n      }\n    }\n\n    if (estimatedSize > _peakMemoryUsedBytes) {\n      _peakMemoryUsedBytes = estimatedSize\n    }\n  }\n```\n\nmaybeSpillCollection 继续调用了 maybeSpill 方法，这个方法继承自 Spillable 中\n\n```scala\nprotected def maybeSpill(collection: C, currentMemory: Long): Boolean = {\n    var shouldSpill = false\n    if (elementsRead % 32 == 0 && currentMemory >= myMemoryThreshold) {\n      // Claim up to double our current memory from the shuffle memory pool\n      val amountToRequest = 2 * currentMemory - myMemoryThreshold\n      val granted = acquireMemory(amountToRequest)\n      myMemoryThreshold += granted\n      // If we were granted too little memory to grow further (either tryToAcquire returned 0,\n      // or we already had more memory than myMemoryThreshold), spill the current collection\n      shouldSpill = currentMemory >= myMemoryThreshold\n    }\n    shouldSpill = shouldSpill || _elementsRead > numElementsForceSpillThreshold\n    // Actually spill\n    if (shouldSpill) {\n      _spillCount += 1\n      logSpillage(currentMemory)\n      spill(collection)\n      _elementsRead = 0\n      _memoryBytesSpilled += currentMemory\n      releaseMemory()\n    }\n    shouldSpill\n  }\n```\n\n该方法首先判断了是否需要 spill，如果需要则调用 spill() 方法将内存中的数据写入临时文件中。spill 方法如下:\n\n```scala\n override protected[this] def spill(collection: WritablePartitionedPairCollection[K, C]): Unit = {\n    val inMemoryIterator = collection.destructiveSortedWritablePartitionedIterator(comparator)\n    val spillFile = spillMemoryIteratorToDisk(inMemoryIterator)\n    spills += spillFile\n  }\n```\n\n其中的  comparator 如下:\n\n```scala\n private def comparator: Option[Comparator[K]] = {\n    if (ordering.isDefined || aggregator.isDefined) {\n      Some(keyComparator)\n    } else {\n      None\n    }\n  }\n```\n\n也就是:\n\n```scala\n  private val keyComparator: Comparator[K] = ordering.getOrElse(new Comparator[K] {\n    override def compare(a: K, b: K): Int = {\n      val h1 = if (a == null) 0 else a.hashCode()\n      val h2 = if (b == null) 0 else b.hashCode()\n      if (h1 < h2) -1 else if (h1 == h2) 0 else 1\n    }\n  })\n```\n\ndestructiveSortedWritablePartitionedIterator 位于 WritablePartitionedPairCollection 类中，实现如下:\n\n```scala\ndef destructiveSortedWritablePartitionedIterator(keyComparator: Option[Comparator[K]])\n    : WritablePartitionedIterator = {\n    //这里调用的其实是　PartitionedAppendOnlyMap　中重写的　partitionedDestructiveSortedIterator    \n    val it = partitionedDestructiveSortedIterator(keyComparator)\n    // 这里还实现了一个　writeNext　的方法，后面会用到\n    new WritablePartitionedIterator {\n      private[this] var cur = if (it.hasNext) it.next() else null\n       // 在map 中的数据其实是((partition,k),v)\n       // 这里只写入了(k,v)\n      def writeNext(writer: DiskBlockObjectWriter): Unit = {\n        writer.write(cur._1._2, cur._2)\n        cur = if (it.hasNext) it.next() else null\n      }\n      def hasNext(): Boolean = cur != null\n      def nextPartition(): Int = cur._1._1\n    }\n  }\n```\n\n接下来看看partitionedDestructiveSortedIterator:\n\n```scala\ndef partitionedDestructiveSortedIterator(keyComparator: Option[Comparator[K]])\n    : Iterator[((Int, K), V)] = {\n    val comparator = keyComparator.map(partitionKeyComparator).getOrElse(partitionComparator)\n    destructiveSortedIterator(comparator)\n }\n```\n\nkeyComparator.map(partitionKeyComparator)　等于 partitionKeyComparator(keyComparator)，再看下面的partitionKeyComparator，说明比较器是先对分区进行比较，如果分区相同，再对key的hash值进行比较:\n\n```scala\n def partitionKeyComparator[K](keyComparator: Comparator[K]): Comparator[(Int, K)] = {\n    new Comparator[(Int, K)] {\n      override def compare(a: (Int, K), b: (Int, K)): Int = {\n        val partitionDiff = a._1 - b._1\n        if (partitionDiff != 0) {\n          partitionDiff\n        } else {\n          keyComparator.compare(a._2, b._2)\n        }\n      }\n    }\n  }\n```\n\n然后看destructiveSortedIterator(),这是在 PartitionedAppendOnlyMap 的父类 destructiveSortedIterator 中实现的:\n\n```scala\n def destructiveSortedIterator(keyComparator: Comparator[K]): Iterator[(K, V)] = {\n    destroyed = true\n    // Pack KV pairs into the front of the underlying array\n    // 这里是因为　PartitionedAppendOnlyMap　采用的二次探测法　,kv对之间存在　null值，所以先把非空的kv对全部移动到hash表的前端\n    var keyIndex, newIndex = 0\n    while (keyIndex < capacity) {\n      if (data(2 * keyIndex) != null) {\n        data(2 * newIndex) = data(2 * keyIndex)\n        data(2 * newIndex + 1) = data(2 * keyIndex + 1)\n        newIndex += 1\n      }\n      keyIndex += 1\n    }\n    assert(curSize == newIndex + (if (haveNullValue) 1 else 0))\n    // 这里对给定数据范围为 0 to newIndex，利用　keyComparator　比较器进行排序\n    // 也就是对 map 中的数据根据　(partition,key) 进行排序\n    new Sorter(new KVArraySortDataFormat[K, AnyRef]).sort(data, 0, newIndex, keyComparator)\n\n    // 定义迭代器\n    new Iterator[(K, V)] {\n      var i = 0\n      var nullValueReady = haveNullValue\n      def hasNext: Boolean = (i < newIndex || nullValueReady)\n      def next(): (K, V) = {\n        if (nullValueReady) {\n          nullValueReady = false\n          (null.asInstanceOf[K], nullValue)\n        } else {\n          val item = (data(2 * i).asInstanceOf[K], data(2 * i + 1).asInstanceOf[V])\n          i += 1\n          item\n        }\n      }\n    }\n  }\n```\n\n可以看出整个　`val inMemoryIterator = collection.destructiveSortedWritablePartitionedIterator(comparator)`　这一步其实就是在对　map　或 buffer 进行排序，并且实现了一个 writeNext() 方法供后续调调用。随后调用`val spillFile = spillMemoryIteratorToDisk(inMemoryIterator)`将排序后的缓存写入文件中:\n\n```scala\nprivate[this] def spillMemoryIteratorToDisk(inMemoryIterator: WritablePartitionedIterator)\n      : SpilledFile = {\n   \n    val (blockId, file) = diskBlockManager.createTempShuffleBlock()\n    // These variables are reset after each flush\n    var objectsWritten: Long = 0\n    val spillMetrics: ShuffleWriteMetrics = new ShuffleWriteMetrics\n    val writer: DiskBlockObjectWriter =\n      blockManager.getDiskWriter(blockId, file, serInstance, fileBufferSize, spillMetrics)\n    // List of batch sizes (bytes) in the order they are written to disk\n    val batchSizes = new ArrayBuffer[Long]\n    // How many elements we have in each partition\n    // 用于记录每一个分区有多少条数据\n    // 由于刚才数据写入文件没有写入key对应的分区，因此需要记录每一个分区有多少条数据，然后根据数据的偏移量就可以判断该数据属于哪个分区　\n    val elementsPerPartition = new Array[Long](numPartitions)\n    // Flush the disk writer's contents to disk, and update relevant variables.\n    // The writer is committed at the end of this process.\n    def flush(): Unit = {\n      val segment = writer.commitAndGet()\n      batchSizes += segment.length\n      _diskBytesSpilled += segment.length\n      objectsWritten = 0\n    }\n    var success = false\n    try {\n      while (inMemoryIterator.hasNext) {\n        val partitionId = inMemoryIterator.nextPartition()\n        require(partitionId >= 0 && partitionId < numPartitions,\n          s\"partition Id: ${partitionId} should be in the range [0, ${numPartitions})\")\n        inMemoryIterator.writeNext(writer)\n        elementsPerPartition(partitionId) += 1\n        objectsWritten += 1\n        if (objectsWritten == serializerBatchSize) {　//写入　serializerBatchSize　条数据便刷新一次缓存\n          // batchSize 在类中定义的如下:\n          // 可以看出如果不存在配置默认为　10000　条\n          // private val serializerBatchSize = conf.getLong(\"spark.shuffle.spill.batchSize\", 10000)\n          flush()\n        }\n      }\n      if (objectsWritten > 0) {\n        flush()\n      } else {\n        writer.revertPartialWritesAndClose()\n      }\n      success = true\n    } finally {\n      if (success) {\n        writer.close()\n      } else {\n        // This code path only happens if an exception was thrown above before we set success;\n        // close our stuff and let the exception be thrown further\n        writer.revertPartialWritesAndClose()\n        if (file.exists()) {\n          if (!file.delete()) {\n            logWarning(s\"Error deleting ${file}\")\n          }\n        }\n      }\n    }\n    //最后记录临时文件的信息\n    SpilledFile(file, blockId, batchSizes.toArray, elementsPerPartition)\n\n  }\n```\n\n这就是整个　insertAll()　方法:\n\n merge 达到溢出值后进行排序并写入临时文件，这里要注意的是并非所有的缓存文件都被写入到临时文件中，所以接下来需要将临时文件中的数据加上内存中的数据进行一次排序写入到一个大文件中。\n\n```scala\n//首先获取需要写入的文件:　\n   val output = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)\n　　val tmp = Utils.tempFileWith(output)\n    try { \n      val blockId = ShuffleBlockId(dep.shuffleId, mapId, IndexShuffleBlockResolver.NOOP_REDUCE_ID)\n      val partitionLengths = sorter.writePartitionedFile(blockId, tmp)//这一句是重点 下面会讲解\n      shuffleBlockResolver.writeIndexFileAndCommit(dep.shuffleId, mapId, partitionLengths, tmp)\n      mapStatus = MapStatus(blockManager.shuffleServerId, partitionLengths)\n    } finally {\n      if (tmp.exists() && !tmp.delete()) {\n        logError(s\"Error while deleting temp file ${tmp.getAbsolutePath}\")\n    }\n```\n#### writePartitionedFile\n\n继续看 writePartitionedFile :\n\n```scala\n//  ExternalSorter 中的　writePartitionedFile　方法\n   def writePartitionedFile(\n      blockId: BlockId,\n      outputFile: File): Array[Long] = {\n\n    // Track location of each range in the output file\n    val lengths = new Array[Long](numPartitions)\n    val writer = blockManager.getDiskWriter(blockId, outputFile, serInstance, fileBufferSize,\n      context.taskMetrics().shuffleWriteMetrics)\n\n    // 这里的spills 就是　刚才产生的临时文件的数据　如果为空逻辑就比较简单，直接进行排序并写入文件\n    if (spills.isEmpty) {\n      // Case where we only have in-memory data\n      val collection = if (aggregator.isDefined) map else buffer\n      val it = collection.destructiveSortedWritablePartitionedIterator(comparator)　//这里同样使用　destructiveSortedWritablePartitionedIterator　进行排序\n      while (it.hasNext) {\n        val partitionId = it.nextPartition()\n        while (it.hasNext && it.nextPartition() == partitionId) {\n          it.writeNext(writer)\n        }\n        val segment = writer.commitAndGet()\n        lengths(partitionId) = segment.length\n      }\n    } else {//重点看不为空的时候，这里调用了　partitionedIterator　方法\n      // We must perform merge-sort; get an iterator by partition and write everything directly.\n      for ((id, elements) <- this.partitionedIterator) {\n        if (elements.hasNext) {\n          for (elem <- elements) {\n            writer.write(elem._1, elem._2)\n          }\n          val segment = writer.commitAndGet()\n          lengths(id) = segment.length\n        }\n      }\n    }\n\n    writer.close()\n    context.taskMetrics().incMemoryBytesSpilled(memoryBytesSpilled)\n    context.taskMetrics().incDiskBytesSpilled(diskBytesSpilled)\n    context.taskMetrics().incPeakExecutionMemory(peakMemoryUsedBytes)\n\n    lengths\n  }\n```\n\n然后是 partitionedIterator 方法:\n\n```scala\ndef partitionedIterator: Iterator[(Int, Iterator[Product2[K, C]])] = {\n    val usingMap = aggregator.isDefined\n    val collection: WritablePartitionedPairCollection[K, C] = if (usingMap) map else buffer\n    if (spills.isEmpty) {// 这里又一次判断了是否为空，直接看有临时文件的部分\n      // Special case: if we have only in-memory data, we don't need to merge streams, and perhaps\n      // we don't even need to sort by anything other than partition ID\n      if (!ordering.isDefined) {\n        // The user hasn't requested sorted keys, so only sort by partition ID, not key\n   groupByPartition(destructiveIterator(collection.partitionedDestructiveSortedIterator(None)))\n      } else {\n        // We do need to sort by both partition ID and key\n        groupByPartition(destructiveIterator(\n          collection.partitionedDestructiveSortedIterator(Some(keyComparator))))\n      }\n    } else {\n      // Merge spilled and in-memory data\n      // 这里传入了临时文件　spills　和　排序后的缓存文件\n      merge(spills, destructiveIterator(\n        collection.partitionedDestructiveSortedIterator(comparator)))\n    }\n  }\n```\n\nmerge 方法:\n\n```scala\n // merge方法\n   // inMemory　是根据(partion,hash(k)) 排序后的内存数据\n   private def merge(spills: Seq[SpilledFile], inMemory: Iterator[((Int, K), C)])\n      : Iterator[(Int, Iterator[Product2[K, C]])] = {\n    //　将所有缓存文件转化为 SpillReader \n    val readers = spills.map(new SpillReader(_))\n    // buffered方法只是比普通的　itertor 方法多提供一个　head　方法，例如:\n    // val a = List(1,2,3,4,5)\n    // val b = a.iterator.buffered\n    // b.head : 1\n    // b.next : 1\n    // b.head : 2\n    // b.next : 2\n    val inMemBuffered = inMemory.buffered\n    (0 until numPartitions).iterator.map { p =>\n      // 获得分区对应数据的迭代器 \n      // 这里的　IteratorForPartition　就是得到分区 p 对应的数据的迭代器，逻辑比较简单，就不单独拿出来分析\n      val inMemIterator = new IteratorForPartition(p, inMemBuffered)\n      \n      //这里获取所有文件的第　p　个分区　并与内存中的第　p 个分区进行合并\n      val iterators = readers.map(_.readNextPartition()) ++ Seq(inMemIterator)\n　　　// 这里的　iterators　是由　多个 iterator 组成的，其中每一个都是关于 p 分区数据的 iterator\n      if (aggregator.isDefined) {\n        // Perform partial aggregation across partitions\n        (p, mergeWithAggregation(\n          iterators, aggregator.get.mergeCombiners, keyComparator, ordering.isDefined))\n      } else if (ordering.isDefined) {\n        // No aggregator given, but we have an ordering (e.g. used by reduce tasks in sortByKey);\n        // sort the elements without trying to merge them\n        (p, mergeSort(iterators, ordering.get))\n      } else {\n        (p, iterators.iterator.flatten)\n      }\n    }\n```\nmergeWithAggregation\n```scala\n\n      private def mergeWithAggregation(\n      iterators: Seq[Iterator[Product2[K, C]]],\n      mergeCombiners: (C, C) => C,\n      comparator: Comparator[K],\n      totalOrder: Boolean)\n      : Iterator[Product2[K, C]] =\n  {\n    if (!totalOrder) {\n      new Iterator[Iterator[Product2[K, C]]] {\n        val sorted = mergeSort(iterators, comparator).buffered\n        val keys = new ArrayBuffer[K]\n        val combiners = new ArrayBuffer[C]\n        override def hasNext: Boolean = sorted.hasNext\n        override def next(): Iterator[Product2[K, C]] = {\n          if (!hasNext) {\n            throw new NoSuchElementException\n          }\n          keys.clear()\n          combiners.clear()\n          val firstPair = sorted.next()\n          keys += firstPair._1\n          combiners += firstPair._2\n          val key = firstPair._1\n          while (sorted.hasNext && comparator.compare(sorted.head._1, key) == 0) {\n            val pair = sorted.next()\n            var i = 0\n            var foundKey = false\n            while (i < keys.size && !foundKey) {\n              if (keys(i) == pair._1) {\n                combiners(i) = mergeCombiners(combiners(i), pair._2)\n                foundKey = true\n              }\n              i += 1\n            }\n            if (!foundKey) {\n              keys += pair._1\n              combiners += pair._2\n            }\n          }\n          keys.iterator.zip(combiners.iterator)\n        }\n      }.flatMap(i => i)\n    } else {\n      // We have a total ordering, so the objects with the same key are sequential.\n      new Iterator[Product2[K, C]] {\n        val sorted = mergeSort(iterators, comparator).buffered\n        override def hasNext: Boolean = sorted.hasNext\n        override def next(): Product2[K, C] = {\n          if (!hasNext) {\n            throw new NoSuchElementException\n          }\n          val elem = sorted.next()\n          val k = elem._1\n          var c = elem._2\n          // 这里的意思是可能存在多个相同的key 分布在不同临时文件或内存中\n          // 所以还需要将不同的 key 对应的值进行合并 \n          while (sorted.hasNext && sorted.head._1 == k) {\n            val pair = sorted.next()\n            c = mergeCombiners(c, pair._2)\n          }\n          (k, c)//返回\n        }\n      }\n    }\n  }\n```\n\n继续看 mergeSort 方法，首先选除头元素最小对应那个分区，然后选出这个分区的第一个元素，那么这个元素一定是最小的，然后将剩余的元素放回去:\n\n```scala\n\n  private def mergeSort(iterators: Seq[Iterator[Product2[K, C]]], comparator: Comparator[K])\n      : Iterator[Product2[K, C]] =\n  {\n    val bufferedIters = iterators.filter(_.hasNext).map(_.buffered)\n    type Iter = BufferedIterator[Product2[K, C]]\n    //选取头元素最小的分区\n    val heap = new mutable.PriorityQueue[Iter]()(new Ordering[Iter] {\n      // Use the reverse of comparator.compare because PriorityQueue dequeues the max\n      override def compare(x: Iter, y: Iter): Int = -comparator.compare(x.head._1, y.head._1)\n    })\n    heap.enqueue(bufferedIters: _*) \n    new Iterator[Product2[K, C]] {\n      override def hasNext: Boolean = !heap.isEmpty\n      override def next(): Product2[K, C] = {\n        if (!hasNext) {\n          throw new NoSuchElementException\n        }\n        val firstBuf = heap.dequeue()\n        val firstPair = firstBuf.next()\n        if (firstBuf.hasNext) {\n          heap.enqueue(firstBuf)\n        }\n        firstPair\n      }\n    }\n  }\n```\n\n接下来就使用 writeIndexFileAndCommit 将每一个分区的数据条数写入索引文件，便于 reduce 端获取。\n\n## 总结\n\nshuflle write 相比起 shuffe read 要复杂很多，因此本文的篇幅也较长。其实在 shuffle write 有两个较为重要的方法，其中一个是 insertAll。这个方法的作用就是将数据缓存到一个可以添加的 hash 表中。如果表占用的内存达到的一定值，就对其进行排序。排序方式先按照分区进行排序，如果分区相同则按key的hash进行排序，随后将排序后的数据写入临时文件中，这个过程可能会生成多个临时文件。\n\n然后 writePartitionedFile 的作用是最后将内存中的数据和临时文件中的数据进行全部排序，写入到一个大文件中。\n\n最后将每一个分区对应的索引数据写入文件。整个shuffle write 阶段就完成了\n","source":"_posts/源码阅读计划-第二部分-shuffle-write.md","raw":"---\ntitle: spark源码阅读计划 - 第二部分 - shuffle write\ndate: 2018-06-08 13:46:45\ntags:\n  - Spark\n  - 编程\ncategories: Spark源码阅读计划\nauthor: lishion\ntoc: true\n---\n## 写在开始的话\n\nshuffle 作为 spark 最重要的一部分，也是很多初学者感到头痛的一部分。简单来说，shuffle 分为两个部分 : shuffle write 和 shuffle read。shuffle write 在 map 端进行，而shuffle read 在 reduce 端进行。本章就准备就 shuffle write 做一个详细的分析。\n\n## 为什么需要shuffle\n\n在上一章[Spark-源码阅读计划-第一部分-迭代计算](https://lishion.github.io/2018/06/06/Spark-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E8%AE%A1%E5%88%92-%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86-%E8%BF%AD%E4%BB%A3%E8%AE%A1%E7%AE%97/)提到过每一个分区的数据最后都会由一个task来处理，那么当task 执行的类似于 reduceByKey 这种算子的时候一定需要满足相同的 key 在同一个分区这个条件，否则就不能按照 key 进行reduce了。但是当我们从数据源获取数据并进行处理后，相同的 key 不一定在同一个分区。那么在一个 map 端将相同的key 使用某种方式聚集在同一个分区并写入临时文件的过程就称为 shuffle write；而在 reduce 从 map 拉去执行 task 所需要的数据就是 shuffle read；这两个步骤组成了shuffle。\n\n## 一个小例子\n\n本章同样以一个小例子作为本章讲解的中心，例如一个统计词频的代码:\n\n```scala\nsc.parallelize(List(\"hello\",\"world\",\"hello\",\"spark\")).map(x=>(x,1)).reduceByKey(_+_)\n```\n\n相信有一点基础的同学都知道 reduceByKey 这算子会产生 shuffle。但是大部分的同学都不知 shuffle 是什么时候产生的，shuffle 到底做了些什么，这也是这一章的重点。\n\n## shuffle task\n\n在上一章提到过当 job 提交后会生成 result task。而在需要 shuffle 则会生成 ShuffleMapTask。这里涉及到 job 提交以及 stage 生成的的知识，将会在下一章提到。这里我们直接开始看 ShuffleMapTask 中的 runtask :\n\n```scala\n override def runTask(context: TaskContext): MapStatus = {\n    // Deserialize the RDD using the broadcast variable.\n ...\n    var writer: ShuffleWriter[Any, Any] = null\n    try {\n      val manager = SparkEnv.get.shuffleManager\n      writer = manager.getWriter[Any, Any](dep.shuffleHandle, partitionId, context)\n       // 这里依然是rdd调用iterator方法的地方\n      writer.write(rdd.iterator(partition, context).asInstanceOf[Iterator[_ <: Product2[Any, Any]]])\n      writer.stop(success = true).get\n    } catch {\n      case e: Exception =>\n        try {\n          if (writer != null) {\n            writer.stop(success = false)\n          }\n        } catch {\n          case e: Exception =>\n            log.debug(\"Could not stop writer\", e)\n        }\n        throw e\n    }\n  }\n```\n\n可以看到首先获得一个 shuffleWriter。spark 中有许多不同类型的 writer ，默认使用的是 SortShuffleWriter。SortShuffleWriter 中的 write 方法实现了 shuffle write。源代码如下:\n\n```scala\n  override def write(records: Iterator[Product2[K, V]]): Unit = {\n      //如果需要 map 端聚\n    sorter = if (dep.mapSideCombine) {\n      new ExternalSorter[K, V, C](\n        context, dep.aggregator, Some(dep.partitioner), dep.keyOrdering, dep.serializer)\n    } else {\n      new ExternalSorter[K, V, V](\n        context, aggregator = None, Some(dep.partitioner), ordering = None, dep.serializer)\n    }\n    sorter.insertAll(records)\n    val output = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)\n    val tmp = Utils.tempFileWith(output)\n    try { \n      val blockId = ShuffleBlockId(dep.shuffleId, mapId, IndexShuffleBlockResolver.NOOP_REDUCE_ID)\n      val partitionLengths = sorter.writePartitionedFile(blockId, tmp)\n      shuffleBlockResolver.writeIndexFileAndCommit(dep.shuffleId, mapId, partitionLengths, tmp)\n      mapStatus = MapStatus(blockManager.shuffleServerId, partitionLengths)\n    } finally {\n      if (tmp.exists() && !tmp.delete()) {\n        logError(s\"Error while deleting temp file ${tmp.getAbsolutePath}\")\n      }\n    }\n  }\n```\n\n使用了 ExternalSorter 的 insertAll 方法插入了上一个 RDD 生成的数据的迭代器。\n\n#### insertAll\n\n```scala\ndef insertAll(records: Iterator[Product2[K, V]]): Unit = {\n    // TODO: stop combining if we find that the reduction factor isn't high\n    val shouldCombine = aggregator.isDefined\n    // 直接看需要 map 端聚合的情况\n    // 不需要聚合的情况类似\n    if (shouldCombine) {\n      // Combine values in-memory first using our AppendOnlyMap\n       \n      val mergeValue = aggregator.get.mergeValue \n      val createCombiner = aggregator.get.createCombiner\n      var kv: Product2[K, V] = null\n      // 如果 key 不存在则直接创建 Combiner ，否则使用 mergeValue 将 value 添加到创建的 Combiner中\n      val update = (hadValue: Boolean, oldValue: C) => {\n        if (hadValue) mergeValue(oldValue, kv._2) else createCombiner(kv._2)\n      }\n      while (records.hasNext) {\n        addElementsRead()\n        kv = records.next()\n        // 更新值\n        // 这里需要注意的是之前的数据是(k,v)类型，这里转换成了((part,k),v)\n        // 其中 part 是 key 对应的分区\n        map.changeValue((getPartition(kv._1), kv._1), update)\n        maybeSpillCollection(usingMap = true)\n      }\n    } else {\n      // Stick values into our buffer\n      while (records.hasNext) {\n        addElementsRead()\n        val kv = records.next()\n        buffer.insert(getPartition(kv._1), kv._1, kv._2.asInstanceOf[C])\n        maybeSpillCollection(usingMap = false)\n      }\n    }\n  }\n```\n\n在这里使用 getPartition() 进行了数据的**分区**，也就是将不同的 key 与其分区对应起来。默认使用的是 HashPartitioner ，有兴趣的同学可以去阅读源码。再继续看 changeValue 方法。在这里使用非常重要的数据结构来存放分区后的数据，也就是代码中的 map 类型为 PartitionedAppendOnlyMap。其中是使用hask表存放数据，并且存放的方式为表中的偶数索index引存放key, index + 1 存放对应的value。例如:\n\n```\nk1|v1|null|k2|v2|k3|v3|null|....\n```\n\n之所以里面由空值，是因为在插入的时候使用了二次探测法。来看一看 changeValue 方法:\n\nPartitionedAppendOnlyMap 中的  changeValue 方法继承自 \n\n```scala\n  def changeValue(key: K, updateFunc: (Boolean, V) => V): V = {\n    assert(!destroyed, destructionMessage)\n    val k = key.asInstanceOf[AnyRef]\n    if (k.eq(null)) {\n      if (!haveNullValue) {\n        incrementSize()\n      }\n      nullValue = updateFunc(haveNullValue, nullValue)\n      haveNullValue = true\n      return nullValue\n    }\n    var pos = rehash(k.hashCode) & mask\n    var i = 1\n    while (true) {\n      // 偶数位置为 key\n      val curKey = data(2 * pos)\n       // 如果该key不存在，就直接插入\n      if (curKey.eq(null)) {\n        val newValue = updateFunc(false, null.asInstanceOf[V])\n        data(2 * pos) = k\n        data(2 * pos + 1) = newValue.asInstanceOf[AnyRef]\n        incrementSize()\n        return newValue\n      } else if (k.eq(curKey) || k.equals(curKey)) {//如果 key 存在，则进行聚合\n        \n        val newValue = updateFunc(true, data(2 * pos + 1).asInstanceOf[V])\n        data(2 * pos + 1) = newValue.asInstanceOf[AnyRef]\n        return newValue\n      } else { // 否则进行下一次探测\n        val delta = i\n        pos = (pos + delta) & mask\n        i += 1\n      }\n    }\n    null.asInstanceOf[V] // Never reached but needed to keep compiler happy\n  }\n```\n\n再调用 insertAll 进行了数据的聚合|插入之后，调用了 maybeSpillCollection 方法。这个方法的作用是判断 map 占用了内存，如果内存达到一定的值，则将内存中的数据 spill 到临时文件中。\n\n```scala\nprivate def maybeSpillCollection(usingMap: Boolean): Unit = {\n    var estimatedSize = 0L\n    if (usingMap) {\n      estimatedSize = map.estimateSize()\n      if (maybeSpill(map, estimatedSize)) {\n        map = new PartitionedAppendOnlyMap[K, C]\n      }\n    } else {\n      estimatedSize = buffer.estimateSize()\n      if (maybeSpill(buffer, estimatedSize)) {\n        buffer = new PartitionedPairBuffer[K, C]\n      }\n    }\n\n    if (estimatedSize > _peakMemoryUsedBytes) {\n      _peakMemoryUsedBytes = estimatedSize\n    }\n  }\n```\n\nmaybeSpillCollection 继续调用了 maybeSpill 方法，这个方法继承自 Spillable 中\n\n```scala\nprotected def maybeSpill(collection: C, currentMemory: Long): Boolean = {\n    var shouldSpill = false\n    if (elementsRead % 32 == 0 && currentMemory >= myMemoryThreshold) {\n      // Claim up to double our current memory from the shuffle memory pool\n      val amountToRequest = 2 * currentMemory - myMemoryThreshold\n      val granted = acquireMemory(amountToRequest)\n      myMemoryThreshold += granted\n      // If we were granted too little memory to grow further (either tryToAcquire returned 0,\n      // or we already had more memory than myMemoryThreshold), spill the current collection\n      shouldSpill = currentMemory >= myMemoryThreshold\n    }\n    shouldSpill = shouldSpill || _elementsRead > numElementsForceSpillThreshold\n    // Actually spill\n    if (shouldSpill) {\n      _spillCount += 1\n      logSpillage(currentMemory)\n      spill(collection)\n      _elementsRead = 0\n      _memoryBytesSpilled += currentMemory\n      releaseMemory()\n    }\n    shouldSpill\n  }\n```\n\n该方法首先判断了是否需要 spill，如果需要则调用 spill() 方法将内存中的数据写入临时文件中。spill 方法如下:\n\n```scala\n override protected[this] def spill(collection: WritablePartitionedPairCollection[K, C]): Unit = {\n    val inMemoryIterator = collection.destructiveSortedWritablePartitionedIterator(comparator)\n    val spillFile = spillMemoryIteratorToDisk(inMemoryIterator)\n    spills += spillFile\n  }\n```\n\n其中的  comparator 如下:\n\n```scala\n private def comparator: Option[Comparator[K]] = {\n    if (ordering.isDefined || aggregator.isDefined) {\n      Some(keyComparator)\n    } else {\n      None\n    }\n  }\n```\n\n也就是:\n\n```scala\n  private val keyComparator: Comparator[K] = ordering.getOrElse(new Comparator[K] {\n    override def compare(a: K, b: K): Int = {\n      val h1 = if (a == null) 0 else a.hashCode()\n      val h2 = if (b == null) 0 else b.hashCode()\n      if (h1 < h2) -1 else if (h1 == h2) 0 else 1\n    }\n  })\n```\n\ndestructiveSortedWritablePartitionedIterator 位于 WritablePartitionedPairCollection 类中，实现如下:\n\n```scala\ndef destructiveSortedWritablePartitionedIterator(keyComparator: Option[Comparator[K]])\n    : WritablePartitionedIterator = {\n    //这里调用的其实是　PartitionedAppendOnlyMap　中重写的　partitionedDestructiveSortedIterator    \n    val it = partitionedDestructiveSortedIterator(keyComparator)\n    // 这里还实现了一个　writeNext　的方法，后面会用到\n    new WritablePartitionedIterator {\n      private[this] var cur = if (it.hasNext) it.next() else null\n       // 在map 中的数据其实是((partition,k),v)\n       // 这里只写入了(k,v)\n      def writeNext(writer: DiskBlockObjectWriter): Unit = {\n        writer.write(cur._1._2, cur._2)\n        cur = if (it.hasNext) it.next() else null\n      }\n      def hasNext(): Boolean = cur != null\n      def nextPartition(): Int = cur._1._1\n    }\n  }\n```\n\n接下来看看partitionedDestructiveSortedIterator:\n\n```scala\ndef partitionedDestructiveSortedIterator(keyComparator: Option[Comparator[K]])\n    : Iterator[((Int, K), V)] = {\n    val comparator = keyComparator.map(partitionKeyComparator).getOrElse(partitionComparator)\n    destructiveSortedIterator(comparator)\n }\n```\n\nkeyComparator.map(partitionKeyComparator)　等于 partitionKeyComparator(keyComparator)，再看下面的partitionKeyComparator，说明比较器是先对分区进行比较，如果分区相同，再对key的hash值进行比较:\n\n```scala\n def partitionKeyComparator[K](keyComparator: Comparator[K]): Comparator[(Int, K)] = {\n    new Comparator[(Int, K)] {\n      override def compare(a: (Int, K), b: (Int, K)): Int = {\n        val partitionDiff = a._1 - b._1\n        if (partitionDiff != 0) {\n          partitionDiff\n        } else {\n          keyComparator.compare(a._2, b._2)\n        }\n      }\n    }\n  }\n```\n\n然后看destructiveSortedIterator(),这是在 PartitionedAppendOnlyMap 的父类 destructiveSortedIterator 中实现的:\n\n```scala\n def destructiveSortedIterator(keyComparator: Comparator[K]): Iterator[(K, V)] = {\n    destroyed = true\n    // Pack KV pairs into the front of the underlying array\n    // 这里是因为　PartitionedAppendOnlyMap　采用的二次探测法　,kv对之间存在　null值，所以先把非空的kv对全部移动到hash表的前端\n    var keyIndex, newIndex = 0\n    while (keyIndex < capacity) {\n      if (data(2 * keyIndex) != null) {\n        data(2 * newIndex) = data(2 * keyIndex)\n        data(2 * newIndex + 1) = data(2 * keyIndex + 1)\n        newIndex += 1\n      }\n      keyIndex += 1\n    }\n    assert(curSize == newIndex + (if (haveNullValue) 1 else 0))\n    // 这里对给定数据范围为 0 to newIndex，利用　keyComparator　比较器进行排序\n    // 也就是对 map 中的数据根据　(partition,key) 进行排序\n    new Sorter(new KVArraySortDataFormat[K, AnyRef]).sort(data, 0, newIndex, keyComparator)\n\n    // 定义迭代器\n    new Iterator[(K, V)] {\n      var i = 0\n      var nullValueReady = haveNullValue\n      def hasNext: Boolean = (i < newIndex || nullValueReady)\n      def next(): (K, V) = {\n        if (nullValueReady) {\n          nullValueReady = false\n          (null.asInstanceOf[K], nullValue)\n        } else {\n          val item = (data(2 * i).asInstanceOf[K], data(2 * i + 1).asInstanceOf[V])\n          i += 1\n          item\n        }\n      }\n    }\n  }\n```\n\n可以看出整个　`val inMemoryIterator = collection.destructiveSortedWritablePartitionedIterator(comparator)`　这一步其实就是在对　map　或 buffer 进行排序，并且实现了一个 writeNext() 方法供后续调调用。随后调用`val spillFile = spillMemoryIteratorToDisk(inMemoryIterator)`将排序后的缓存写入文件中:\n\n```scala\nprivate[this] def spillMemoryIteratorToDisk(inMemoryIterator: WritablePartitionedIterator)\n      : SpilledFile = {\n   \n    val (blockId, file) = diskBlockManager.createTempShuffleBlock()\n    // These variables are reset after each flush\n    var objectsWritten: Long = 0\n    val spillMetrics: ShuffleWriteMetrics = new ShuffleWriteMetrics\n    val writer: DiskBlockObjectWriter =\n      blockManager.getDiskWriter(blockId, file, serInstance, fileBufferSize, spillMetrics)\n    // List of batch sizes (bytes) in the order they are written to disk\n    val batchSizes = new ArrayBuffer[Long]\n    // How many elements we have in each partition\n    // 用于记录每一个分区有多少条数据\n    // 由于刚才数据写入文件没有写入key对应的分区，因此需要记录每一个分区有多少条数据，然后根据数据的偏移量就可以判断该数据属于哪个分区　\n    val elementsPerPartition = new Array[Long](numPartitions)\n    // Flush the disk writer's contents to disk, and update relevant variables.\n    // The writer is committed at the end of this process.\n    def flush(): Unit = {\n      val segment = writer.commitAndGet()\n      batchSizes += segment.length\n      _diskBytesSpilled += segment.length\n      objectsWritten = 0\n    }\n    var success = false\n    try {\n      while (inMemoryIterator.hasNext) {\n        val partitionId = inMemoryIterator.nextPartition()\n        require(partitionId >= 0 && partitionId < numPartitions,\n          s\"partition Id: ${partitionId} should be in the range [0, ${numPartitions})\")\n        inMemoryIterator.writeNext(writer)\n        elementsPerPartition(partitionId) += 1\n        objectsWritten += 1\n        if (objectsWritten == serializerBatchSize) {　//写入　serializerBatchSize　条数据便刷新一次缓存\n          // batchSize 在类中定义的如下:\n          // 可以看出如果不存在配置默认为　10000　条\n          // private val serializerBatchSize = conf.getLong(\"spark.shuffle.spill.batchSize\", 10000)\n          flush()\n        }\n      }\n      if (objectsWritten > 0) {\n        flush()\n      } else {\n        writer.revertPartialWritesAndClose()\n      }\n      success = true\n    } finally {\n      if (success) {\n        writer.close()\n      } else {\n        // This code path only happens if an exception was thrown above before we set success;\n        // close our stuff and let the exception be thrown further\n        writer.revertPartialWritesAndClose()\n        if (file.exists()) {\n          if (!file.delete()) {\n            logWarning(s\"Error deleting ${file}\")\n          }\n        }\n      }\n    }\n    //最后记录临时文件的信息\n    SpilledFile(file, blockId, batchSizes.toArray, elementsPerPartition)\n\n  }\n```\n\n这就是整个　insertAll()　方法:\n\n merge 达到溢出值后进行排序并写入临时文件，这里要注意的是并非所有的缓存文件都被写入到临时文件中，所以接下来需要将临时文件中的数据加上内存中的数据进行一次排序写入到一个大文件中。\n\n```scala\n//首先获取需要写入的文件:　\n   val output = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)\n　　val tmp = Utils.tempFileWith(output)\n    try { \n      val blockId = ShuffleBlockId(dep.shuffleId, mapId, IndexShuffleBlockResolver.NOOP_REDUCE_ID)\n      val partitionLengths = sorter.writePartitionedFile(blockId, tmp)//这一句是重点 下面会讲解\n      shuffleBlockResolver.writeIndexFileAndCommit(dep.shuffleId, mapId, partitionLengths, tmp)\n      mapStatus = MapStatus(blockManager.shuffleServerId, partitionLengths)\n    } finally {\n      if (tmp.exists() && !tmp.delete()) {\n        logError(s\"Error while deleting temp file ${tmp.getAbsolutePath}\")\n    }\n```\n#### writePartitionedFile\n\n继续看 writePartitionedFile :\n\n```scala\n//  ExternalSorter 中的　writePartitionedFile　方法\n   def writePartitionedFile(\n      blockId: BlockId,\n      outputFile: File): Array[Long] = {\n\n    // Track location of each range in the output file\n    val lengths = new Array[Long](numPartitions)\n    val writer = blockManager.getDiskWriter(blockId, outputFile, serInstance, fileBufferSize,\n      context.taskMetrics().shuffleWriteMetrics)\n\n    // 这里的spills 就是　刚才产生的临时文件的数据　如果为空逻辑就比较简单，直接进行排序并写入文件\n    if (spills.isEmpty) {\n      // Case where we only have in-memory data\n      val collection = if (aggregator.isDefined) map else buffer\n      val it = collection.destructiveSortedWritablePartitionedIterator(comparator)　//这里同样使用　destructiveSortedWritablePartitionedIterator　进行排序\n      while (it.hasNext) {\n        val partitionId = it.nextPartition()\n        while (it.hasNext && it.nextPartition() == partitionId) {\n          it.writeNext(writer)\n        }\n        val segment = writer.commitAndGet()\n        lengths(partitionId) = segment.length\n      }\n    } else {//重点看不为空的时候，这里调用了　partitionedIterator　方法\n      // We must perform merge-sort; get an iterator by partition and write everything directly.\n      for ((id, elements) <- this.partitionedIterator) {\n        if (elements.hasNext) {\n          for (elem <- elements) {\n            writer.write(elem._1, elem._2)\n          }\n          val segment = writer.commitAndGet()\n          lengths(id) = segment.length\n        }\n      }\n    }\n\n    writer.close()\n    context.taskMetrics().incMemoryBytesSpilled(memoryBytesSpilled)\n    context.taskMetrics().incDiskBytesSpilled(diskBytesSpilled)\n    context.taskMetrics().incPeakExecutionMemory(peakMemoryUsedBytes)\n\n    lengths\n  }\n```\n\n然后是 partitionedIterator 方法:\n\n```scala\ndef partitionedIterator: Iterator[(Int, Iterator[Product2[K, C]])] = {\n    val usingMap = aggregator.isDefined\n    val collection: WritablePartitionedPairCollection[K, C] = if (usingMap) map else buffer\n    if (spills.isEmpty) {// 这里又一次判断了是否为空，直接看有临时文件的部分\n      // Special case: if we have only in-memory data, we don't need to merge streams, and perhaps\n      // we don't even need to sort by anything other than partition ID\n      if (!ordering.isDefined) {\n        // The user hasn't requested sorted keys, so only sort by partition ID, not key\n   groupByPartition(destructiveIterator(collection.partitionedDestructiveSortedIterator(None)))\n      } else {\n        // We do need to sort by both partition ID and key\n        groupByPartition(destructiveIterator(\n          collection.partitionedDestructiveSortedIterator(Some(keyComparator))))\n      }\n    } else {\n      // Merge spilled and in-memory data\n      // 这里传入了临时文件　spills　和　排序后的缓存文件\n      merge(spills, destructiveIterator(\n        collection.partitionedDestructiveSortedIterator(comparator)))\n    }\n  }\n```\n\nmerge 方法:\n\n```scala\n // merge方法\n   // inMemory　是根据(partion,hash(k)) 排序后的内存数据\n   private def merge(spills: Seq[SpilledFile], inMemory: Iterator[((Int, K), C)])\n      : Iterator[(Int, Iterator[Product2[K, C]])] = {\n    //　将所有缓存文件转化为 SpillReader \n    val readers = spills.map(new SpillReader(_))\n    // buffered方法只是比普通的　itertor 方法多提供一个　head　方法，例如:\n    // val a = List(1,2,3,4,5)\n    // val b = a.iterator.buffered\n    // b.head : 1\n    // b.next : 1\n    // b.head : 2\n    // b.next : 2\n    val inMemBuffered = inMemory.buffered\n    (0 until numPartitions).iterator.map { p =>\n      // 获得分区对应数据的迭代器 \n      // 这里的　IteratorForPartition　就是得到分区 p 对应的数据的迭代器，逻辑比较简单，就不单独拿出来分析\n      val inMemIterator = new IteratorForPartition(p, inMemBuffered)\n      \n      //这里获取所有文件的第　p　个分区　并与内存中的第　p 个分区进行合并\n      val iterators = readers.map(_.readNextPartition()) ++ Seq(inMemIterator)\n　　　// 这里的　iterators　是由　多个 iterator 组成的，其中每一个都是关于 p 分区数据的 iterator\n      if (aggregator.isDefined) {\n        // Perform partial aggregation across partitions\n        (p, mergeWithAggregation(\n          iterators, aggregator.get.mergeCombiners, keyComparator, ordering.isDefined))\n      } else if (ordering.isDefined) {\n        // No aggregator given, but we have an ordering (e.g. used by reduce tasks in sortByKey);\n        // sort the elements without trying to merge them\n        (p, mergeSort(iterators, ordering.get))\n      } else {\n        (p, iterators.iterator.flatten)\n      }\n    }\n```\nmergeWithAggregation\n```scala\n\n      private def mergeWithAggregation(\n      iterators: Seq[Iterator[Product2[K, C]]],\n      mergeCombiners: (C, C) => C,\n      comparator: Comparator[K],\n      totalOrder: Boolean)\n      : Iterator[Product2[K, C]] =\n  {\n    if (!totalOrder) {\n      new Iterator[Iterator[Product2[K, C]]] {\n        val sorted = mergeSort(iterators, comparator).buffered\n        val keys = new ArrayBuffer[K]\n        val combiners = new ArrayBuffer[C]\n        override def hasNext: Boolean = sorted.hasNext\n        override def next(): Iterator[Product2[K, C]] = {\n          if (!hasNext) {\n            throw new NoSuchElementException\n          }\n          keys.clear()\n          combiners.clear()\n          val firstPair = sorted.next()\n          keys += firstPair._1\n          combiners += firstPair._2\n          val key = firstPair._1\n          while (sorted.hasNext && comparator.compare(sorted.head._1, key) == 0) {\n            val pair = sorted.next()\n            var i = 0\n            var foundKey = false\n            while (i < keys.size && !foundKey) {\n              if (keys(i) == pair._1) {\n                combiners(i) = mergeCombiners(combiners(i), pair._2)\n                foundKey = true\n              }\n              i += 1\n            }\n            if (!foundKey) {\n              keys += pair._1\n              combiners += pair._2\n            }\n          }\n          keys.iterator.zip(combiners.iterator)\n        }\n      }.flatMap(i => i)\n    } else {\n      // We have a total ordering, so the objects with the same key are sequential.\n      new Iterator[Product2[K, C]] {\n        val sorted = mergeSort(iterators, comparator).buffered\n        override def hasNext: Boolean = sorted.hasNext\n        override def next(): Product2[K, C] = {\n          if (!hasNext) {\n            throw new NoSuchElementException\n          }\n          val elem = sorted.next()\n          val k = elem._1\n          var c = elem._2\n          // 这里的意思是可能存在多个相同的key 分布在不同临时文件或内存中\n          // 所以还需要将不同的 key 对应的值进行合并 \n          while (sorted.hasNext && sorted.head._1 == k) {\n            val pair = sorted.next()\n            c = mergeCombiners(c, pair._2)\n          }\n          (k, c)//返回\n        }\n      }\n    }\n  }\n```\n\n继续看 mergeSort 方法，首先选除头元素最小对应那个分区，然后选出这个分区的第一个元素，那么这个元素一定是最小的，然后将剩余的元素放回去:\n\n```scala\n\n  private def mergeSort(iterators: Seq[Iterator[Product2[K, C]]], comparator: Comparator[K])\n      : Iterator[Product2[K, C]] =\n  {\n    val bufferedIters = iterators.filter(_.hasNext).map(_.buffered)\n    type Iter = BufferedIterator[Product2[K, C]]\n    //选取头元素最小的分区\n    val heap = new mutable.PriorityQueue[Iter]()(new Ordering[Iter] {\n      // Use the reverse of comparator.compare because PriorityQueue dequeues the max\n      override def compare(x: Iter, y: Iter): Int = -comparator.compare(x.head._1, y.head._1)\n    })\n    heap.enqueue(bufferedIters: _*) \n    new Iterator[Product2[K, C]] {\n      override def hasNext: Boolean = !heap.isEmpty\n      override def next(): Product2[K, C] = {\n        if (!hasNext) {\n          throw new NoSuchElementException\n        }\n        val firstBuf = heap.dequeue()\n        val firstPair = firstBuf.next()\n        if (firstBuf.hasNext) {\n          heap.enqueue(firstBuf)\n        }\n        firstPair\n      }\n    }\n  }\n```\n\n接下来就使用 writeIndexFileAndCommit 将每一个分区的数据条数写入索引文件，便于 reduce 端获取。\n\n## 总结\n\nshuflle write 相比起 shuffe read 要复杂很多，因此本文的篇幅也较长。其实在 shuffle write 有两个较为重要的方法，其中一个是 insertAll。这个方法的作用就是将数据缓存到一个可以添加的 hash 表中。如果表占用的内存达到的一定值，就对其进行排序。排序方式先按照分区进行排序，如果分区相同则按key的hash进行排序，随后将排序后的数据写入临时文件中，这个过程可能会生成多个临时文件。\n\n然后 writePartitionedFile 的作用是最后将内存中的数据和临时文件中的数据进行全部排序，写入到一个大文件中。\n\n最后将每一个分区对应的索引数据写入文件。整个shuffle write 阶段就完成了\n","slug":"源码阅读计划-第二部分-shuffle-write","published":1,"updated":"2018-06-29T17:06:08.923Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjj08c9o2000vksng5uu7z7jp","content":"<h2 id=\"写在开始的话\"><a href=\"#写在开始的话\" class=\"headerlink\" title=\"写在开始的话\"></a>写在开始的话</h2><p>shuffle 作为 spark 最重要的一部分，也是很多初学者感到头痛的一部分。简单来说，shuffle 分为两个部分 : shuffle write 和 shuffle read。shuffle write 在 map 端进行，而shuffle read 在 reduce 端进行。本章就准备就 shuffle write 做一个详细的分析。</p>\n<h2 id=\"为什么需要shuffle\"><a href=\"#为什么需要shuffle\" class=\"headerlink\" title=\"为什么需要shuffle\"></a>为什么需要shuffle</h2><p>在上一章<a href=\"https://lishion.github.io/2018/06/06/Spark-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E8%AE%A1%E5%88%92-%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86-%E8%BF%AD%E4%BB%A3%E8%AE%A1%E7%AE%97/\" target=\"_blank\" rel=\"noopener\">Spark-源码阅读计划-第一部分-迭代计算</a>提到过每一个分区的数据最后都会由一个task来处理，那么当task 执行的类似于 reduceByKey 这种算子的时候一定需要满足相同的 key 在同一个分区这个条件，否则就不能按照 key 进行reduce了。但是当我们从数据源获取数据并进行处理后，相同的 key 不一定在同一个分区。那么在一个 map 端将相同的key 使用某种方式聚集在同一个分区并写入临时文件的过程就称为 shuffle write；而在 reduce 从 map 拉去执行 task 所需要的数据就是 shuffle read；这两个步骤组成了shuffle。</p>\n<h2 id=\"一个小例子\"><a href=\"#一个小例子\" class=\"headerlink\" title=\"一个小例子\"></a>一个小例子</h2><p>本章同样以一个小例子作为本章讲解的中心，例如一个统计词频的代码:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sc.parallelize(<span class=\"type\">List</span>(<span class=\"string\">\"hello\"</span>,<span class=\"string\">\"world\"</span>,<span class=\"string\">\"hello\"</span>,<span class=\"string\">\"spark\"</span>)).map(x=&gt;(x,<span class=\"number\">1</span>)).reduceByKey(_+_)</span><br></pre></td></tr></table></figure>\n<p>相信有一点基础的同学都知道 reduceByKey 这算子会产生 shuffle。但是大部分的同学都不知 shuffle 是什么时候产生的，shuffle 到底做了些什么，这也是这一章的重点。</p>\n<h2 id=\"shuffle-task\"><a href=\"#shuffle-task\" class=\"headerlink\" title=\"shuffle task\"></a>shuffle task</h2><p>在上一章提到过当 job 提交后会生成 result task。而在需要 shuffle 则会生成 ShuffleMapTask。这里涉及到 job 提交以及 stage 生成的的知识，将会在下一章提到。这里我们直接开始看 ShuffleMapTask 中的 runtask :</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">runTask</span></span>(context: <span class=\"type\">TaskContext</span>): <span class=\"type\">MapStatus</span> = &#123;</span><br><span class=\"line\">   <span class=\"comment\">// Deserialize the RDD using the broadcast variable.</span></span><br><span class=\"line\">...</span><br><span class=\"line\">   <span class=\"keyword\">var</span> writer: <span class=\"type\">ShuffleWriter</span>[<span class=\"type\">Any</span>, <span class=\"type\">Any</span>] = <span class=\"literal\">null</span></span><br><span class=\"line\">   <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">     <span class=\"keyword\">val</span> manager = <span class=\"type\">SparkEnv</span>.get.shuffleManager</span><br><span class=\"line\">     writer = manager.getWriter[<span class=\"type\">Any</span>, <span class=\"type\">Any</span>](dep.shuffleHandle, partitionId, context)</span><br><span class=\"line\">      <span class=\"comment\">// 这里依然是rdd调用iterator方法的地方</span></span><br><span class=\"line\">     writer.write(rdd.iterator(partition, context).asInstanceOf[<span class=\"type\">Iterator</span>[_ &lt;: <span class=\"type\">Product2</span>[<span class=\"type\">Any</span>, <span class=\"type\">Any</span>]]])</span><br><span class=\"line\">     writer.stop(success = <span class=\"literal\">true</span>).get</span><br><span class=\"line\">   &#125; <span class=\"keyword\">catch</span> &#123;</span><br><span class=\"line\">     <span class=\"keyword\">case</span> e: <span class=\"type\">Exception</span> =&gt;</span><br><span class=\"line\">       <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">         <span class=\"keyword\">if</span> (writer != <span class=\"literal\">null</span>) &#123;</span><br><span class=\"line\">           writer.stop(success = <span class=\"literal\">false</span>)</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">       &#125; <span class=\"keyword\">catch</span> &#123;</span><br><span class=\"line\">         <span class=\"keyword\">case</span> e: <span class=\"type\">Exception</span> =&gt;</span><br><span class=\"line\">           log.debug(<span class=\"string\">\"Could not stop writer\"</span>, e)</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">       <span class=\"keyword\">throw</span> e</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>可以看到首先获得一个 shuffleWriter。spark 中有许多不同类型的 writer ，默认使用的是 SortShuffleWriter。SortShuffleWriter 中的 write 方法实现了 shuffle write。源代码如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">write</span></span>(records: <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>]]): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">    <span class=\"comment\">//如果需要 map 端聚</span></span><br><span class=\"line\">  sorter = <span class=\"keyword\">if</span> (dep.mapSideCombine) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">ExternalSorter</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>, <span class=\"type\">C</span>](</span><br><span class=\"line\">      context, dep.aggregator, <span class=\"type\">Some</span>(dep.partitioner), dep.keyOrdering, dep.serializer)</span><br><span class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">ExternalSorter</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>, <span class=\"type\">V</span>](</span><br><span class=\"line\">      context, aggregator = <span class=\"type\">None</span>, <span class=\"type\">Some</span>(dep.partitioner), ordering = <span class=\"type\">None</span>, dep.serializer)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  sorter.insertAll(records)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> output = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> tmp = <span class=\"type\">Utils</span>.tempFileWith(output)</span><br><span class=\"line\">  <span class=\"keyword\">try</span> &#123; </span><br><span class=\"line\">    <span class=\"keyword\">val</span> blockId = <span class=\"type\">ShuffleBlockId</span>(dep.shuffleId, mapId, <span class=\"type\">IndexShuffleBlockResolver</span>.<span class=\"type\">NOOP_REDUCE_ID</span>)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> partitionLengths = sorter.writePartitionedFile(blockId, tmp)</span><br><span class=\"line\">    shuffleBlockResolver.writeIndexFileAndCommit(dep.shuffleId, mapId, partitionLengths, tmp)</span><br><span class=\"line\">    mapStatus = <span class=\"type\">MapStatus</span>(blockManager.shuffleServerId, partitionLengths)</span><br><span class=\"line\">  &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (tmp.exists() &amp;&amp; !tmp.delete()) &#123;</span><br><span class=\"line\">      logError(<span class=\"string\">s\"Error while deleting temp file <span class=\"subst\">$&#123;tmp.getAbsolutePath&#125;</span>\"</span>)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>使用了 ExternalSorter 的 insertAll 方法插入了上一个 RDD 生成的数据的迭代器。</p>\n<h4 id=\"insertAll\"><a href=\"#insertAll\" class=\"headerlink\" title=\"insertAll\"></a>insertAll</h4><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">insertAll</span></span>(records: <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>]]): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">    <span class=\"comment\">// <span class=\"doctag\">TODO:</span> stop combining if we find that the reduction factor isn't high</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> shouldCombine = aggregator.isDefined</span><br><span class=\"line\">    <span class=\"comment\">// 直接看需要 map 端聚合的情况</span></span><br><span class=\"line\">    <span class=\"comment\">// 不需要聚合的情况类似</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (shouldCombine) &#123;</span><br><span class=\"line\">      <span class=\"comment\">// Combine values in-memory first using our AppendOnlyMap</span></span><br><span class=\"line\">       </span><br><span class=\"line\">      <span class=\"keyword\">val</span> mergeValue = aggregator.get.mergeValue </span><br><span class=\"line\">      <span class=\"keyword\">val</span> createCombiner = aggregator.get.createCombiner</span><br><span class=\"line\">      <span class=\"keyword\">var</span> kv: <span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>] = <span class=\"literal\">null</span></span><br><span class=\"line\">      <span class=\"comment\">// 如果 key 不存在则直接创建 Combiner ，否则使用 mergeValue 将 value 添加到创建的 Combiner中</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> update = (hadValue: <span class=\"type\">Boolean</span>, oldValue: <span class=\"type\">C</span>) =&gt; &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (hadValue) mergeValue(oldValue, kv._2) <span class=\"keyword\">else</span> createCombiner(kv._2)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      <span class=\"keyword\">while</span> (records.hasNext) &#123;</span><br><span class=\"line\">        addElementsRead()</span><br><span class=\"line\">        kv = records.next()</span><br><span class=\"line\">        <span class=\"comment\">// 更新值</span></span><br><span class=\"line\">        <span class=\"comment\">// 这里需要注意的是之前的数据是(k,v)类型，这里转换成了((part,k),v)</span></span><br><span class=\"line\">        <span class=\"comment\">// 其中 part 是 key 对应的分区</span></span><br><span class=\"line\">        map.changeValue((getPartition(kv._1), kv._1), update)</span><br><span class=\"line\">        maybeSpillCollection(usingMap = <span class=\"literal\">true</span>)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">      <span class=\"comment\">// Stick values into our buffer</span></span><br><span class=\"line\">      <span class=\"keyword\">while</span> (records.hasNext) &#123;</span><br><span class=\"line\">        addElementsRead()</span><br><span class=\"line\">        <span class=\"keyword\">val</span> kv = records.next()</span><br><span class=\"line\">        buffer.insert(getPartition(kv._1), kv._1, kv._2.asInstanceOf[<span class=\"type\">C</span>])</span><br><span class=\"line\">        maybeSpillCollection(usingMap = <span class=\"literal\">false</span>)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>在这里使用 getPartition() 进行了数据的<strong>分区</strong>，也就是将不同的 key 与其分区对应起来。默认使用的是 HashPartitioner ，有兴趣的同学可以去阅读源码。再继续看 changeValue 方法。在这里使用非常重要的数据结构来存放分区后的数据，也就是代码中的 map 类型为 PartitionedAppendOnlyMap。其中是使用hask表存放数据，并且存放的方式为表中的偶数索index引存放key, index + 1 存放对应的value。例如:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">k1|v1|null|k2|v2|k3|v3|null|....</span><br></pre></td></tr></table></figure>\n<p>之所以里面由空值，是因为在插入的时候使用了二次探测法。来看一看 changeValue 方法:</p>\n<p>PartitionedAppendOnlyMap 中的  changeValue 方法继承自 </p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">changeValue</span></span>(key: <span class=\"type\">K</span>, updateFunc: (<span class=\"type\">Boolean</span>, <span class=\"type\">V</span>) =&gt; <span class=\"type\">V</span>): <span class=\"type\">V</span> = &#123;</span><br><span class=\"line\">  assert(!destroyed, destructionMessage)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> k = key.asInstanceOf[<span class=\"type\">AnyRef</span>]</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (k.eq(<span class=\"literal\">null</span>)) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (!haveNullValue) &#123;</span><br><span class=\"line\">      incrementSize()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    nullValue = updateFunc(haveNullValue, nullValue)</span><br><span class=\"line\">    haveNullValue = <span class=\"literal\">true</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> nullValue</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"keyword\">var</span> pos = rehash(k.hashCode) &amp; mask</span><br><span class=\"line\">  <span class=\"keyword\">var</span> i = <span class=\"number\">1</span></span><br><span class=\"line\">  <span class=\"keyword\">while</span> (<span class=\"literal\">true</span>) &#123;</span><br><span class=\"line\">    <span class=\"comment\">// 偶数位置为 key</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> curKey = data(<span class=\"number\">2</span> * pos)</span><br><span class=\"line\">     <span class=\"comment\">// 如果该key不存在，就直接插入</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (curKey.eq(<span class=\"literal\">null</span>)) &#123;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> newValue = updateFunc(<span class=\"literal\">false</span>, <span class=\"literal\">null</span>.asInstanceOf[<span class=\"type\">V</span>])</span><br><span class=\"line\">      data(<span class=\"number\">2</span> * pos) = k</span><br><span class=\"line\">      data(<span class=\"number\">2</span> * pos + <span class=\"number\">1</span>) = newValue.asInstanceOf[<span class=\"type\">AnyRef</span>]</span><br><span class=\"line\">      incrementSize()</span><br><span class=\"line\">      <span class=\"keyword\">return</span> newValue</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (k.eq(curKey) || k.equals(curKey)) &#123;<span class=\"comment\">//如果 key 存在，则进行聚合</span></span><br><span class=\"line\">      </span><br><span class=\"line\">      <span class=\"keyword\">val</span> newValue = updateFunc(<span class=\"literal\">true</span>, data(<span class=\"number\">2</span> * pos + <span class=\"number\">1</span>).asInstanceOf[<span class=\"type\">V</span>])</span><br><span class=\"line\">      data(<span class=\"number\">2</span> * pos + <span class=\"number\">1</span>) = newValue.asInstanceOf[<span class=\"type\">AnyRef</span>]</span><br><span class=\"line\">      <span class=\"keyword\">return</span> newValue</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123; <span class=\"comment\">// 否则进行下一次探测</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> delta = i</span><br><span class=\"line\">      pos = (pos + delta) &amp; mask</span><br><span class=\"line\">      i += <span class=\"number\">1</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"literal\">null</span>.asInstanceOf[<span class=\"type\">V</span>] <span class=\"comment\">// Never reached but needed to keep compiler happy</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>再调用 insertAll 进行了数据的聚合|插入之后，调用了 maybeSpillCollection 方法。这个方法的作用是判断 map 占用了内存，如果内存达到一定的值，则将内存中的数据 spill 到临时文件中。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">maybeSpillCollection</span></span>(usingMap: <span class=\"type\">Boolean</span>): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">var</span> estimatedSize = <span class=\"number\">0</span>L</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (usingMap) &#123;</span><br><span class=\"line\">      estimatedSize = map.estimateSize()</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (maybeSpill(map, estimatedSize)) &#123;</span><br><span class=\"line\">        map = <span class=\"keyword\">new</span> <span class=\"type\">PartitionedAppendOnlyMap</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">      estimatedSize = buffer.estimateSize()</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (maybeSpill(buffer, estimatedSize)) &#123;</span><br><span class=\"line\">        buffer = <span class=\"keyword\">new</span> <span class=\"type\">PartitionedPairBuffer</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (estimatedSize &gt; _peakMemoryUsedBytes) &#123;</span><br><span class=\"line\">      _peakMemoryUsedBytes = estimatedSize</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>maybeSpillCollection 继续调用了 maybeSpill 方法，这个方法继承自 Spillable 中</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">protected</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">maybeSpill</span></span>(collection: <span class=\"type\">C</span>, currentMemory: <span class=\"type\">Long</span>): <span class=\"type\">Boolean</span> = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">var</span> shouldSpill = <span class=\"literal\">false</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (elementsRead % <span class=\"number\">32</span> == <span class=\"number\">0</span> &amp;&amp; currentMemory &gt;= myMemoryThreshold) &#123;</span><br><span class=\"line\">      <span class=\"comment\">// Claim up to double our current memory from the shuffle memory pool</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> amountToRequest = <span class=\"number\">2</span> * currentMemory - myMemoryThreshold</span><br><span class=\"line\">      <span class=\"keyword\">val</span> granted = acquireMemory(amountToRequest)</span><br><span class=\"line\">      myMemoryThreshold += granted</span><br><span class=\"line\">      <span class=\"comment\">// If we were granted too little memory to grow further (either tryToAcquire returned 0,</span></span><br><span class=\"line\">      <span class=\"comment\">// or we already had more memory than myMemoryThreshold), spill the current collection</span></span><br><span class=\"line\">      shouldSpill = currentMemory &gt;= myMemoryThreshold</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    shouldSpill = shouldSpill || _elementsRead &gt; numElementsForceSpillThreshold</span><br><span class=\"line\">    <span class=\"comment\">// Actually spill</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (shouldSpill) &#123;</span><br><span class=\"line\">      _spillCount += <span class=\"number\">1</span></span><br><span class=\"line\">      logSpillage(currentMemory)</span><br><span class=\"line\">      spill(collection)</span><br><span class=\"line\">      _elementsRead = <span class=\"number\">0</span></span><br><span class=\"line\">      _memoryBytesSpilled += currentMemory</span><br><span class=\"line\">      releaseMemory()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    shouldSpill</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>该方法首先判断了是否需要 spill，如果需要则调用 spill() 方法将内存中的数据写入临时文件中。spill 方法如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"keyword\">protected</span>[<span class=\"keyword\">this</span>] <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">spill</span></span>(collection: <span class=\"type\">WritablePartitionedPairCollection</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">   <span class=\"keyword\">val</span> inMemoryIterator = collection.destructiveSortedWritablePartitionedIterator(comparator)</span><br><span class=\"line\">   <span class=\"keyword\">val</span> spillFile = spillMemoryIteratorToDisk(inMemoryIterator)</span><br><span class=\"line\">   spills += spillFile</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>其中的  comparator 如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">comparator</span></span>: <span class=\"type\">Option</span>[<span class=\"type\">Comparator</span>[<span class=\"type\">K</span>]] = &#123;</span><br><span class=\"line\">   <span class=\"keyword\">if</span> (ordering.isDefined || aggregator.isDefined) &#123;</span><br><span class=\"line\">     <span class=\"type\">Some</span>(keyComparator)</span><br><span class=\"line\">   &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">     <span class=\"type\">None</span></span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>也就是:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"keyword\">val</span> keyComparator: <span class=\"type\">Comparator</span>[<span class=\"type\">K</span>] = ordering.getOrElse(<span class=\"keyword\">new</span> <span class=\"type\">Comparator</span>[<span class=\"type\">K</span>] &#123;</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compare</span></span>(a: <span class=\"type\">K</span>, b: <span class=\"type\">K</span>): <span class=\"type\">Int</span> = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> h1 = <span class=\"keyword\">if</span> (a == <span class=\"literal\">null</span>) <span class=\"number\">0</span> <span class=\"keyword\">else</span> a.hashCode()</span><br><span class=\"line\">    <span class=\"keyword\">val</span> h2 = <span class=\"keyword\">if</span> (b == <span class=\"literal\">null</span>) <span class=\"number\">0</span> <span class=\"keyword\">else</span> b.hashCode()</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (h1 &lt; h2) <span class=\"number\">-1</span> <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (h1 == h2) <span class=\"number\">0</span> <span class=\"keyword\">else</span> <span class=\"number\">1</span></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;)</span><br></pre></td></tr></table></figure>\n<p>destructiveSortedWritablePartitionedIterator 位于 WritablePartitionedPairCollection 类中，实现如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">destructiveSortedWritablePartitionedIterator</span></span>(keyComparator: <span class=\"type\">Option</span>[<span class=\"type\">Comparator</span>[<span class=\"type\">K</span>]])</span><br><span class=\"line\">    : <span class=\"type\">WritablePartitionedIterator</span> = &#123;</span><br><span class=\"line\">    <span class=\"comment\">//这里调用的其实是　PartitionedAppendOnlyMap　中重写的　partitionedDestructiveSortedIterator    </span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> it = partitionedDestructiveSortedIterator(keyComparator)</span><br><span class=\"line\">    <span class=\"comment\">// 这里还实现了一个　writeNext　的方法，后面会用到</span></span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">WritablePartitionedIterator</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">private</span>[<span class=\"keyword\">this</span>] <span class=\"keyword\">var</span> cur = <span class=\"keyword\">if</span> (it.hasNext) it.next() <span class=\"keyword\">else</span> <span class=\"literal\">null</span></span><br><span class=\"line\">       <span class=\"comment\">// 在map 中的数据其实是((partition,k),v)</span></span><br><span class=\"line\">       <span class=\"comment\">// 这里只写入了(k,v)</span></span><br><span class=\"line\">      <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">writeNext</span></span>(writer: <span class=\"type\">DiskBlockObjectWriter</span>): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">        writer.write(cur._1._2, cur._2)</span><br><span class=\"line\">        cur = <span class=\"keyword\">if</span> (it.hasNext) it.next() <span class=\"keyword\">else</span> <span class=\"literal\">null</span></span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hasNext</span></span>(): <span class=\"type\">Boolean</span> = cur != <span class=\"literal\">null</span></span><br><span class=\"line\">      <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">nextPartition</span></span>(): <span class=\"type\">Int</span> = cur._1._1</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>接下来看看partitionedDestructiveSortedIterator:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">partitionedDestructiveSortedIterator</span></span>(keyComparator: <span class=\"type\">Option</span>[<span class=\"type\">Comparator</span>[<span class=\"type\">K</span>]])</span><br><span class=\"line\">    : <span class=\"type\">Iterator</span>[((<span class=\"type\">Int</span>, <span class=\"type\">K</span>), <span class=\"type\">V</span>)] = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> comparator = keyComparator.map(partitionKeyComparator).getOrElse(partitionComparator)</span><br><span class=\"line\">    destructiveSortedIterator(comparator)</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>keyComparator.map(partitionKeyComparator)　等于 partitionKeyComparator(keyComparator)，再看下面的partitionKeyComparator，说明比较器是先对分区进行比较，如果分区相同，再对key的hash值进行比较:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">partitionKeyComparator</span></span>[<span class=\"type\">K</span>](keyComparator: <span class=\"type\">Comparator</span>[<span class=\"type\">K</span>]): <span class=\"type\">Comparator</span>[(<span class=\"type\">Int</span>, <span class=\"type\">K</span>)] = &#123;</span><br><span class=\"line\">   <span class=\"keyword\">new</span> <span class=\"type\">Comparator</span>[(<span class=\"type\">Int</span>, <span class=\"type\">K</span>)] &#123;</span><br><span class=\"line\">     <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compare</span></span>(a: (<span class=\"type\">Int</span>, <span class=\"type\">K</span>), b: (<span class=\"type\">Int</span>, <span class=\"type\">K</span>)): <span class=\"type\">Int</span> = &#123;</span><br><span class=\"line\">       <span class=\"keyword\">val</span> partitionDiff = a._1 - b._1</span><br><span class=\"line\">       <span class=\"keyword\">if</span> (partitionDiff != <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">         partitionDiff</span><br><span class=\"line\">       &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">         keyComparator.compare(a._2, b._2)</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>然后看destructiveSortedIterator(),这是在 PartitionedAppendOnlyMap 的父类 destructiveSortedIterator 中实现的:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">destructiveSortedIterator</span></span>(keyComparator: <span class=\"type\">Comparator</span>[<span class=\"type\">K</span>]): <span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">V</span>)] = &#123;</span><br><span class=\"line\">   destroyed = <span class=\"literal\">true</span></span><br><span class=\"line\">   <span class=\"comment\">// Pack KV pairs into the front of the underlying array</span></span><br><span class=\"line\">   <span class=\"comment\">// 这里是因为　PartitionedAppendOnlyMap　采用的二次探测法　,kv对之间存在　null值，所以先把非空的kv对全部移动到hash表的前端</span></span><br><span class=\"line\">   <span class=\"keyword\">var</span> keyIndex, newIndex = <span class=\"number\">0</span></span><br><span class=\"line\">   <span class=\"keyword\">while</span> (keyIndex &lt; capacity) &#123;</span><br><span class=\"line\">     <span class=\"keyword\">if</span> (data(<span class=\"number\">2</span> * keyIndex) != <span class=\"literal\">null</span>) &#123;</span><br><span class=\"line\">       data(<span class=\"number\">2</span> * newIndex) = data(<span class=\"number\">2</span> * keyIndex)</span><br><span class=\"line\">       data(<span class=\"number\">2</span> * newIndex + <span class=\"number\">1</span>) = data(<span class=\"number\">2</span> * keyIndex + <span class=\"number\">1</span>)</span><br><span class=\"line\">       newIndex += <span class=\"number\">1</span></span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">     keyIndex += <span class=\"number\">1</span></span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   assert(curSize == newIndex + (<span class=\"keyword\">if</span> (haveNullValue) <span class=\"number\">1</span> <span class=\"keyword\">else</span> <span class=\"number\">0</span>))</span><br><span class=\"line\">   <span class=\"comment\">// 这里对给定数据范围为 0 to newIndex，利用　keyComparator　比较器进行排序</span></span><br><span class=\"line\">   <span class=\"comment\">// 也就是对 map 中的数据根据　(partition,key) 进行排序</span></span><br><span class=\"line\">   <span class=\"keyword\">new</span> <span class=\"type\">Sorter</span>(<span class=\"keyword\">new</span> <span class=\"type\">KVArraySortDataFormat</span>[<span class=\"type\">K</span>, <span class=\"type\">AnyRef</span>]).sort(data, <span class=\"number\">0</span>, newIndex, keyComparator)</span><br><span class=\"line\"></span><br><span class=\"line\">   <span class=\"comment\">// 定义迭代器</span></span><br><span class=\"line\">   <span class=\"keyword\">new</span> <span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">V</span>)] &#123;</span><br><span class=\"line\">     <span class=\"keyword\">var</span> i = <span class=\"number\">0</span></span><br><span class=\"line\">     <span class=\"keyword\">var</span> nullValueReady = haveNullValue</span><br><span class=\"line\">     <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hasNext</span></span>: <span class=\"type\">Boolean</span> = (i &lt; newIndex || nullValueReady)</span><br><span class=\"line\">     <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">next</span></span>(): (<span class=\"type\">K</span>, <span class=\"type\">V</span>) = &#123;</span><br><span class=\"line\">       <span class=\"keyword\">if</span> (nullValueReady) &#123;</span><br><span class=\"line\">         nullValueReady = <span class=\"literal\">false</span></span><br><span class=\"line\">         (<span class=\"literal\">null</span>.asInstanceOf[<span class=\"type\">K</span>], nullValue)</span><br><span class=\"line\">       &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">         <span class=\"keyword\">val</span> item = (data(<span class=\"number\">2</span> * i).asInstanceOf[<span class=\"type\">K</span>], data(<span class=\"number\">2</span> * i + <span class=\"number\">1</span>).asInstanceOf[<span class=\"type\">V</span>])</span><br><span class=\"line\">         i += <span class=\"number\">1</span></span><br><span class=\"line\">         item</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>可以看出整个　<code>val inMemoryIterator = collection.destructiveSortedWritablePartitionedIterator(comparator)</code>　这一步其实就是在对　map　或 buffer 进行排序，并且实现了一个 writeNext() 方法供后续调调用。随后调用<code>val spillFile = spillMemoryIteratorToDisk(inMemoryIterator)</code>将排序后的缓存写入文件中:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span>[<span class=\"keyword\">this</span>] <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">spillMemoryIteratorToDisk</span></span>(inMemoryIterator: <span class=\"type\">WritablePartitionedIterator</span>)</span><br><span class=\"line\">      : <span class=\"type\">SpilledFile</span> = &#123;</span><br><span class=\"line\">   </span><br><span class=\"line\">    <span class=\"keyword\">val</span> (blockId, file) = diskBlockManager.createTempShuffleBlock()</span><br><span class=\"line\">    <span class=\"comment\">// These variables are reset after each flush</span></span><br><span class=\"line\">    <span class=\"keyword\">var</span> objectsWritten: <span class=\"type\">Long</span> = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> spillMetrics: <span class=\"type\">ShuffleWriteMetrics</span> = <span class=\"keyword\">new</span> <span class=\"type\">ShuffleWriteMetrics</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> writer: <span class=\"type\">DiskBlockObjectWriter</span> =</span><br><span class=\"line\">      blockManager.getDiskWriter(blockId, file, serInstance, fileBufferSize, spillMetrics)</span><br><span class=\"line\">    <span class=\"comment\">// List of batch sizes (bytes) in the order they are written to disk</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> batchSizes = <span class=\"keyword\">new</span> <span class=\"type\">ArrayBuffer</span>[<span class=\"type\">Long</span>]</span><br><span class=\"line\">    <span class=\"comment\">// How many elements we have in each partition</span></span><br><span class=\"line\">    <span class=\"comment\">// 用于记录每一个分区有多少条数据</span></span><br><span class=\"line\">    <span class=\"comment\">// 由于刚才数据写入文件没有写入key对应的分区，因此需要记录每一个分区有多少条数据，然后根据数据的偏移量就可以判断该数据属于哪个分区　</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> elementsPerPartition = <span class=\"keyword\">new</span> <span class=\"type\">Array</span>[<span class=\"type\">Long</span>](numPartitions)</span><br><span class=\"line\">    <span class=\"comment\">// Flush the disk writer's contents to disk, and update relevant variables.</span></span><br><span class=\"line\">    <span class=\"comment\">// The writer is committed at the end of this process.</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">flush</span></span>(): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> segment = writer.commitAndGet()</span><br><span class=\"line\">      batchSizes += segment.length</span><br><span class=\"line\">      _diskBytesSpilled += segment.length</span><br><span class=\"line\">      objectsWritten = <span class=\"number\">0</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">var</span> success = <span class=\"literal\">false</span></span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">while</span> (inMemoryIterator.hasNext) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">val</span> partitionId = inMemoryIterator.nextPartition()</span><br><span class=\"line\">        require(partitionId &gt;= <span class=\"number\">0</span> &amp;&amp; partitionId &lt; numPartitions,</span><br><span class=\"line\">          <span class=\"string\">s\"partition Id: <span class=\"subst\">$&#123;partitionId&#125;</span> should be in the range [0, <span class=\"subst\">$&#123;numPartitions&#125;</span>)\"</span>)</span><br><span class=\"line\">        inMemoryIterator.writeNext(writer)</span><br><span class=\"line\">        elementsPerPartition(partitionId) += <span class=\"number\">1</span></span><br><span class=\"line\">        objectsWritten += <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (objectsWritten == serializerBatchSize) &#123;　<span class=\"comment\">//写入　serializerBatchSize　条数据便刷新一次缓存</span></span><br><span class=\"line\">          <span class=\"comment\">// batchSize 在类中定义的如下:</span></span><br><span class=\"line\">          <span class=\"comment\">// 可以看出如果不存在配置默认为　10000　条</span></span><br><span class=\"line\">          <span class=\"comment\">// private val serializerBatchSize = conf.getLong(\"spark.shuffle.spill.batchSize\", 10000)</span></span><br><span class=\"line\">          flush()</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (objectsWritten &gt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">        flush()</span><br><span class=\"line\">      &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        writer.revertPartialWritesAndClose()</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      success = <span class=\"literal\">true</span></span><br><span class=\"line\">    &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (success) &#123;</span><br><span class=\"line\">        writer.close()</span><br><span class=\"line\">      &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// This code path only happens if an exception was thrown above before we set success;</span></span><br><span class=\"line\">        <span class=\"comment\">// close our stuff and let the exception be thrown further</span></span><br><span class=\"line\">        writer.revertPartialWritesAndClose()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (file.exists()) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">if</span> (!file.delete()) &#123;</span><br><span class=\"line\">            logWarning(<span class=\"string\">s\"Error deleting <span class=\"subst\">$&#123;file&#125;</span>\"</span>)</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">//最后记录临时文件的信息</span></span><br><span class=\"line\">    <span class=\"type\">SpilledFile</span>(file, blockId, batchSizes.toArray, elementsPerPartition)</span><br><span class=\"line\"></span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>这就是整个　insertAll()　方法:</p>\n<p> merge 达到溢出值后进行排序并写入临时文件，这里要注意的是并非所有的缓存文件都被写入到临时文件中，所以接下来需要将临时文件中的数据加上内存中的数据进行一次排序写入到一个大文件中。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//首先获取需要写入的文件:　</span></span><br><span class=\"line\">   <span class=\"keyword\">val</span> output = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)</span><br><span class=\"line\">　　<span class=\"keyword\">val</span> tmp = <span class=\"type\">Utils</span>.tempFileWith(output)</span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123; </span><br><span class=\"line\">      <span class=\"keyword\">val</span> blockId = <span class=\"type\">ShuffleBlockId</span>(dep.shuffleId, mapId, <span class=\"type\">IndexShuffleBlockResolver</span>.<span class=\"type\">NOOP_REDUCE_ID</span>)</span><br><span class=\"line\">      <span class=\"keyword\">val</span> partitionLengths = sorter.writePartitionedFile(blockId, tmp)<span class=\"comment\">//这一句是重点 下面会讲解</span></span><br><span class=\"line\">      shuffleBlockResolver.writeIndexFileAndCommit(dep.shuffleId, mapId, partitionLengths, tmp)</span><br><span class=\"line\">      mapStatus = <span class=\"type\">MapStatus</span>(blockManager.shuffleServerId, partitionLengths)</span><br><span class=\"line\">    &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (tmp.exists() &amp;&amp; !tmp.delete()) &#123;</span><br><span class=\"line\">        logError(<span class=\"string\">s\"Error while deleting temp file <span class=\"subst\">$&#123;tmp.getAbsolutePath&#125;</span>\"</span>)</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"writePartitionedFile\"><a href=\"#writePartitionedFile\" class=\"headerlink\" title=\"writePartitionedFile\"></a>writePartitionedFile</h4><p>继续看 writePartitionedFile :</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//  ExternalSorter 中的　writePartitionedFile　方法</span></span><br><span class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">writePartitionedFile</span></span>(</span><br><span class=\"line\">      blockId: <span class=\"type\">BlockId</span>,</span><br><span class=\"line\">      outputFile: <span class=\"type\">File</span>): <span class=\"type\">Array</span>[<span class=\"type\">Long</span>] = &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// Track location of each range in the output file</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> lengths = <span class=\"keyword\">new</span> <span class=\"type\">Array</span>[<span class=\"type\">Long</span>](numPartitions)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> writer = blockManager.getDiskWriter(blockId, outputFile, serInstance, fileBufferSize,</span><br><span class=\"line\">      context.taskMetrics().shuffleWriteMetrics)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 这里的spills 就是　刚才产生的临时文件的数据　如果为空逻辑就比较简单，直接进行排序并写入文件</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (spills.isEmpty) &#123;</span><br><span class=\"line\">      <span class=\"comment\">// Case where we only have in-memory data</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> collection = <span class=\"keyword\">if</span> (aggregator.isDefined) map <span class=\"keyword\">else</span> buffer</span><br><span class=\"line\">      <span class=\"keyword\">val</span> it = collection.destructiveSortedWritablePartitionedIterator(comparator)　<span class=\"comment\">//这里同样使用　destructiveSortedWritablePartitionedIterator　进行排序</span></span><br><span class=\"line\">      <span class=\"keyword\">while</span> (it.hasNext) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">val</span> partitionId = it.nextPartition()</span><br><span class=\"line\">        <span class=\"keyword\">while</span> (it.hasNext &amp;&amp; it.nextPartition() == partitionId) &#123;</span><br><span class=\"line\">          it.writeNext(writer)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">val</span> segment = writer.commitAndGet()</span><br><span class=\"line\">        lengths(partitionId) = segment.length</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;<span class=\"comment\">//重点看不为空的时候，这里调用了　partitionedIterator　方法</span></span><br><span class=\"line\">      <span class=\"comment\">// We must perform merge-sort; get an iterator by partition and write everything directly.</span></span><br><span class=\"line\">      <span class=\"keyword\">for</span> ((id, elements) &lt;- <span class=\"keyword\">this</span>.partitionedIterator) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (elements.hasNext) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">for</span> (elem &lt;- elements) &#123;</span><br><span class=\"line\">            writer.write(elem._1, elem._2)</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">          <span class=\"keyword\">val</span> segment = writer.commitAndGet()</span><br><span class=\"line\">          lengths(id) = segment.length</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    writer.close()</span><br><span class=\"line\">    context.taskMetrics().incMemoryBytesSpilled(memoryBytesSpilled)</span><br><span class=\"line\">    context.taskMetrics().incDiskBytesSpilled(diskBytesSpilled)</span><br><span class=\"line\">    context.taskMetrics().incPeakExecutionMemory(peakMemoryUsedBytes)</span><br><span class=\"line\"></span><br><span class=\"line\">    lengths</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>然后是 partitionedIterator 方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">partitionedIterator</span></span>: <span class=\"type\">Iterator</span>[(<span class=\"type\">Int</span>, <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]])] = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> usingMap = aggregator.isDefined</span><br><span class=\"line\">    <span class=\"keyword\">val</span> collection: <span class=\"type\">WritablePartitionedPairCollection</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>] = <span class=\"keyword\">if</span> (usingMap) map <span class=\"keyword\">else</span> buffer</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (spills.isEmpty) &#123;<span class=\"comment\">// 这里又一次判断了是否为空，直接看有临时文件的部分</span></span><br><span class=\"line\">      <span class=\"comment\">// Special case: if we have only in-memory data, we don't need to merge streams, and perhaps</span></span><br><span class=\"line\">      <span class=\"comment\">// we don't even need to sort by anything other than partition ID</span></span><br><span class=\"line\">      <span class=\"keyword\">if</span> (!ordering.isDefined) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// The user hasn't requested sorted keys, so only sort by partition ID, not key</span></span><br><span class=\"line\">   groupByPartition(destructiveIterator(collection.partitionedDestructiveSortedIterator(<span class=\"type\">None</span>)))</span><br><span class=\"line\">      &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// We do need to sort by both partition ID and key</span></span><br><span class=\"line\">        groupByPartition(destructiveIterator(</span><br><span class=\"line\">          collection.partitionedDestructiveSortedIterator(<span class=\"type\">Some</span>(keyComparator))))</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">      <span class=\"comment\">// Merge spilled and in-memory data</span></span><br><span class=\"line\">      <span class=\"comment\">// 这里传入了临时文件　spills　和　排序后的缓存文件</span></span><br><span class=\"line\">      merge(spills, destructiveIterator(</span><br><span class=\"line\">        collection.partitionedDestructiveSortedIterator(comparator)))</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>merge 方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// merge方法</span></span><br><span class=\"line\">  <span class=\"comment\">// inMemory　是根据(partion,hash(k)) 排序后的内存数据</span></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">merge</span></span>(spills: <span class=\"type\">Seq</span>[<span class=\"type\">SpilledFile</span>], inMemory: <span class=\"type\">Iterator</span>[((<span class=\"type\">Int</span>, <span class=\"type\">K</span>), <span class=\"type\">C</span>)])</span><br><span class=\"line\">     : <span class=\"type\">Iterator</span>[(<span class=\"type\">Int</span>, <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]])] = &#123;</span><br><span class=\"line\">   <span class=\"comment\">//　将所有缓存文件转化为 SpillReader </span></span><br><span class=\"line\">   <span class=\"keyword\">val</span> readers = spills.map(<span class=\"keyword\">new</span> <span class=\"type\">SpillReader</span>(_))</span><br><span class=\"line\">   <span class=\"comment\">// buffered方法只是比普通的　itertor 方法多提供一个　head　方法，例如:</span></span><br><span class=\"line\">   <span class=\"comment\">// val a = List(1,2,3,4,5)</span></span><br><span class=\"line\">   <span class=\"comment\">// val b = a.iterator.buffered</span></span><br><span class=\"line\">   <span class=\"comment\">// b.head : 1</span></span><br><span class=\"line\">   <span class=\"comment\">// b.next : 1</span></span><br><span class=\"line\">   <span class=\"comment\">// b.head : 2</span></span><br><span class=\"line\">   <span class=\"comment\">// b.next : 2</span></span><br><span class=\"line\">   <span class=\"keyword\">val</span> inMemBuffered = inMemory.buffered</span><br><span class=\"line\">   (<span class=\"number\">0</span> until numPartitions).iterator.map &#123; p =&gt;</span><br><span class=\"line\">     <span class=\"comment\">// 获得分区对应数据的迭代器 </span></span><br><span class=\"line\">     <span class=\"comment\">// 这里的　IteratorForPartition　就是得到分区 p 对应的数据的迭代器，逻辑比较简单，就不单独拿出来分析</span></span><br><span class=\"line\">     <span class=\"keyword\">val</span> inMemIterator = <span class=\"keyword\">new</span> <span class=\"type\">IteratorForPartition</span>(p, inMemBuffered)</span><br><span class=\"line\">     </span><br><span class=\"line\">     <span class=\"comment\">//这里获取所有文件的第　p　个分区　并与内存中的第　p 个分区进行合并</span></span><br><span class=\"line\">     <span class=\"keyword\">val</span> iterators = readers.map(_.readNextPartition()) ++ <span class=\"type\">Seq</span>(inMemIterator)</span><br><span class=\"line\">　　　<span class=\"comment\">// 这里的　iterators　是由　多个 iterator 组成的，其中每一个都是关于 p 分区数据的 iterator</span></span><br><span class=\"line\">     <span class=\"keyword\">if</span> (aggregator.isDefined) &#123;</span><br><span class=\"line\">       <span class=\"comment\">// Perform partial aggregation across partitions</span></span><br><span class=\"line\">       (p, mergeWithAggregation(</span><br><span class=\"line\">         iterators, aggregator.get.mergeCombiners, keyComparator, ordering.isDefined))</span><br><span class=\"line\">     &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (ordering.isDefined) &#123;</span><br><span class=\"line\">       <span class=\"comment\">// No aggregator given, but we have an ordering (e.g. used by reduce tasks in sortByKey);</span></span><br><span class=\"line\">       <span class=\"comment\">// sort the elements without trying to merge them</span></span><br><span class=\"line\">       (p, mergeSort(iterators, ordering.get))</span><br><span class=\"line\">     &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">       (p, iterators.iterator.flatten)</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">   &#125;</span><br></pre></td></tr></table></figure>\n<p>mergeWithAggregation<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">mergeWithAggregation</span></span>(</span><br><span class=\"line\">    iterators: <span class=\"type\">Seq</span>[<span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]]],</span><br><span class=\"line\">    mergeCombiners: (<span class=\"type\">C</span>, <span class=\"type\">C</span>) =&gt; <span class=\"type\">C</span>,</span><br><span class=\"line\">    comparator: <span class=\"type\">Comparator</span>[<span class=\"type\">K</span>],</span><br><span class=\"line\">    totalOrder: <span class=\"type\">Boolean</span>)</span><br><span class=\"line\">    : <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] =</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (!totalOrder) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">Iterator</span>[<span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]]] &#123;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> sorted = mergeSort(iterators, comparator).buffered</span><br><span class=\"line\">      <span class=\"keyword\">val</span> keys = <span class=\"keyword\">new</span> <span class=\"type\">ArrayBuffer</span>[<span class=\"type\">K</span>]</span><br><span class=\"line\">      <span class=\"keyword\">val</span> combiners = <span class=\"keyword\">new</span> <span class=\"type\">ArrayBuffer</span>[<span class=\"type\">C</span>]</span><br><span class=\"line\">      <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hasNext</span></span>: <span class=\"type\">Boolean</span> = sorted.hasNext</span><br><span class=\"line\">      <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">next</span></span>(): <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] = &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!hasNext) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">NoSuchElementException</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        keys.clear()</span><br><span class=\"line\">        combiners.clear()</span><br><span class=\"line\">        <span class=\"keyword\">val</span> firstPair = sorted.next()</span><br><span class=\"line\">        keys += firstPair._1</span><br><span class=\"line\">        combiners += firstPair._2</span><br><span class=\"line\">        <span class=\"keyword\">val</span> key = firstPair._1</span><br><span class=\"line\">        <span class=\"keyword\">while</span> (sorted.hasNext &amp;&amp; comparator.compare(sorted.head._1, key) == <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">val</span> pair = sorted.next()</span><br><span class=\"line\">          <span class=\"keyword\">var</span> i = <span class=\"number\">0</span></span><br><span class=\"line\">          <span class=\"keyword\">var</span> foundKey = <span class=\"literal\">false</span></span><br><span class=\"line\">          <span class=\"keyword\">while</span> (i &lt; keys.size &amp;&amp; !foundKey) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (keys(i) == pair._1) &#123;</span><br><span class=\"line\">              combiners(i) = mergeCombiners(combiners(i), pair._2)</span><br><span class=\"line\">              foundKey = <span class=\"literal\">true</span></span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            i += <span class=\"number\">1</span></span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">          <span class=\"keyword\">if</span> (!foundKey) &#123;</span><br><span class=\"line\">            keys += pair._1</span><br><span class=\"line\">            combiners += pair._2</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        keys.iterator.zip(combiners.iterator)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;.flatMap(i =&gt; i)</span><br><span class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">// We have a total ordering, so the objects with the same key are sequential.</span></span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] &#123;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> sorted = mergeSort(iterators, comparator).buffered</span><br><span class=\"line\">      <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hasNext</span></span>: <span class=\"type\">Boolean</span> = sorted.hasNext</span><br><span class=\"line\">      <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">next</span></span>(): <span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>] = &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!hasNext) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">NoSuchElementException</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">val</span> elem = sorted.next()</span><br><span class=\"line\">        <span class=\"keyword\">val</span> k = elem._1</span><br><span class=\"line\">        <span class=\"keyword\">var</span> c = elem._2</span><br><span class=\"line\">        <span class=\"comment\">// 这里的意思是可能存在多个相同的key 分布在不同临时文件或内存中</span></span><br><span class=\"line\">        <span class=\"comment\">// 所以还需要将不同的 key 对应的值进行合并 </span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> (sorted.hasNext &amp;&amp; sorted.head._1 == k) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">val</span> pair = sorted.next()</span><br><span class=\"line\">          c = mergeCombiners(c, pair._2)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        (k, c)<span class=\"comment\">//返回</span></span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>继续看 mergeSort 方法，首先选除头元素最小对应那个分区，然后选出这个分区的第一个元素，那么这个元素一定是最小的，然后将剩余的元素放回去:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">mergeSort</span></span>(iterators: <span class=\"type\">Seq</span>[<span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]]], comparator: <span class=\"type\">Comparator</span>[<span class=\"type\">K</span>])</span><br><span class=\"line\">    : <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] =</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  <span class=\"keyword\">val</span> bufferedIters = iterators.filter(_.hasNext).map(_.buffered)</span><br><span class=\"line\">  <span class=\"class\"><span class=\"keyword\">type</span> <span class=\"title\">Iter</span> </span>= <span class=\"type\">BufferedIterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]]</span><br><span class=\"line\">  <span class=\"comment\">//选取头元素最小的分区</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> heap = <span class=\"keyword\">new</span> mutable.<span class=\"type\">PriorityQueue</span>[<span class=\"type\">Iter</span>]()(<span class=\"keyword\">new</span> <span class=\"type\">Ordering</span>[<span class=\"type\">Iter</span>] &#123;</span><br><span class=\"line\">    <span class=\"comment\">// Use the reverse of comparator.compare because PriorityQueue dequeues the max</span></span><br><span class=\"line\">    <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compare</span></span>(x: <span class=\"type\">Iter</span>, y: <span class=\"type\">Iter</span>): <span class=\"type\">Int</span> = -comparator.compare(x.head._1, y.head._1)</span><br><span class=\"line\">  &#125;)</span><br><span class=\"line\">  heap.enqueue(bufferedIters: _*) </span><br><span class=\"line\">  <span class=\"keyword\">new</span> <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] &#123;</span><br><span class=\"line\">    <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hasNext</span></span>: <span class=\"type\">Boolean</span> = !heap.isEmpty</span><br><span class=\"line\">    <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">next</span></span>(): <span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>] = &#123;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (!hasNext) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">NoSuchElementException</span></span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> firstBuf = heap.dequeue()</span><br><span class=\"line\">      <span class=\"keyword\">val</span> firstPair = firstBuf.next()</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (firstBuf.hasNext) &#123;</span><br><span class=\"line\">        heap.enqueue(firstBuf)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      firstPair</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>接下来就使用 writeIndexFileAndCommit 将每一个分区的数据条数写入索引文件，便于 reduce 端获取。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>shuflle write 相比起 shuffe read 要复杂很多，因此本文的篇幅也较长。其实在 shuffle write 有两个较为重要的方法，其中一个是 insertAll。这个方法的作用就是将数据缓存到一个可以添加的 hash 表中。如果表占用的内存达到的一定值，就对其进行排序。排序方式先按照分区进行排序，如果分区相同则按key的hash进行排序，随后将排序后的数据写入临时文件中，这个过程可能会生成多个临时文件。</p>\n<p>然后 writePartitionedFile 的作用是最后将内存中的数据和临时文件中的数据进行全部排序，写入到一个大文件中。</p>\n<p>最后将每一个分区对应的索引数据写入文件。整个shuffle write 阶段就完成了</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"写在开始的话\"><a href=\"#写在开始的话\" class=\"headerlink\" title=\"写在开始的话\"></a>写在开始的话</h2><p>shuffle 作为 spark 最重要的一部分，也是很多初学者感到头痛的一部分。简单来说，shuffle 分为两个部分 : shuffle write 和 shuffle read。shuffle write 在 map 端进行，而shuffle read 在 reduce 端进行。本章就准备就 shuffle write 做一个详细的分析。</p>\n<h2 id=\"为什么需要shuffle\"><a href=\"#为什么需要shuffle\" class=\"headerlink\" title=\"为什么需要shuffle\"></a>为什么需要shuffle</h2><p>在上一章<a href=\"https://lishion.github.io/2018/06/06/Spark-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E8%AE%A1%E5%88%92-%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86-%E8%BF%AD%E4%BB%A3%E8%AE%A1%E7%AE%97/\" target=\"_blank\" rel=\"noopener\">Spark-源码阅读计划-第一部分-迭代计算</a>提到过每一个分区的数据最后都会由一个task来处理，那么当task 执行的类似于 reduceByKey 这种算子的时候一定需要满足相同的 key 在同一个分区这个条件，否则就不能按照 key 进行reduce了。但是当我们从数据源获取数据并进行处理后，相同的 key 不一定在同一个分区。那么在一个 map 端将相同的key 使用某种方式聚集在同一个分区并写入临时文件的过程就称为 shuffle write；而在 reduce 从 map 拉去执行 task 所需要的数据就是 shuffle read；这两个步骤组成了shuffle。</p>\n<h2 id=\"一个小例子\"><a href=\"#一个小例子\" class=\"headerlink\" title=\"一个小例子\"></a>一个小例子</h2><p>本章同样以一个小例子作为本章讲解的中心，例如一个统计词频的代码:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sc.parallelize(<span class=\"type\">List</span>(<span class=\"string\">\"hello\"</span>,<span class=\"string\">\"world\"</span>,<span class=\"string\">\"hello\"</span>,<span class=\"string\">\"spark\"</span>)).map(x=&gt;(x,<span class=\"number\">1</span>)).reduceByKey(_+_)</span><br></pre></td></tr></table></figure>\n<p>相信有一点基础的同学都知道 reduceByKey 这算子会产生 shuffle。但是大部分的同学都不知 shuffle 是什么时候产生的，shuffle 到底做了些什么，这也是这一章的重点。</p>\n<h2 id=\"shuffle-task\"><a href=\"#shuffle-task\" class=\"headerlink\" title=\"shuffle task\"></a>shuffle task</h2><p>在上一章提到过当 job 提交后会生成 result task。而在需要 shuffle 则会生成 ShuffleMapTask。这里涉及到 job 提交以及 stage 生成的的知识，将会在下一章提到。这里我们直接开始看 ShuffleMapTask 中的 runtask :</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">runTask</span></span>(context: <span class=\"type\">TaskContext</span>): <span class=\"type\">MapStatus</span> = &#123;</span><br><span class=\"line\">   <span class=\"comment\">// Deserialize the RDD using the broadcast variable.</span></span><br><span class=\"line\">...</span><br><span class=\"line\">   <span class=\"keyword\">var</span> writer: <span class=\"type\">ShuffleWriter</span>[<span class=\"type\">Any</span>, <span class=\"type\">Any</span>] = <span class=\"literal\">null</span></span><br><span class=\"line\">   <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">     <span class=\"keyword\">val</span> manager = <span class=\"type\">SparkEnv</span>.get.shuffleManager</span><br><span class=\"line\">     writer = manager.getWriter[<span class=\"type\">Any</span>, <span class=\"type\">Any</span>](dep.shuffleHandle, partitionId, context)</span><br><span class=\"line\">      <span class=\"comment\">// 这里依然是rdd调用iterator方法的地方</span></span><br><span class=\"line\">     writer.write(rdd.iterator(partition, context).asInstanceOf[<span class=\"type\">Iterator</span>[_ &lt;: <span class=\"type\">Product2</span>[<span class=\"type\">Any</span>, <span class=\"type\">Any</span>]]])</span><br><span class=\"line\">     writer.stop(success = <span class=\"literal\">true</span>).get</span><br><span class=\"line\">   &#125; <span class=\"keyword\">catch</span> &#123;</span><br><span class=\"line\">     <span class=\"keyword\">case</span> e: <span class=\"type\">Exception</span> =&gt;</span><br><span class=\"line\">       <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">         <span class=\"keyword\">if</span> (writer != <span class=\"literal\">null</span>) &#123;</span><br><span class=\"line\">           writer.stop(success = <span class=\"literal\">false</span>)</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">       &#125; <span class=\"keyword\">catch</span> &#123;</span><br><span class=\"line\">         <span class=\"keyword\">case</span> e: <span class=\"type\">Exception</span> =&gt;</span><br><span class=\"line\">           log.debug(<span class=\"string\">\"Could not stop writer\"</span>, e)</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">       <span class=\"keyword\">throw</span> e</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>可以看到首先获得一个 shuffleWriter。spark 中有许多不同类型的 writer ，默认使用的是 SortShuffleWriter。SortShuffleWriter 中的 write 方法实现了 shuffle write。源代码如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">write</span></span>(records: <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>]]): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">    <span class=\"comment\">//如果需要 map 端聚</span></span><br><span class=\"line\">  sorter = <span class=\"keyword\">if</span> (dep.mapSideCombine) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">ExternalSorter</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>, <span class=\"type\">C</span>](</span><br><span class=\"line\">      context, dep.aggregator, <span class=\"type\">Some</span>(dep.partitioner), dep.keyOrdering, dep.serializer)</span><br><span class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">ExternalSorter</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>, <span class=\"type\">V</span>](</span><br><span class=\"line\">      context, aggregator = <span class=\"type\">None</span>, <span class=\"type\">Some</span>(dep.partitioner), ordering = <span class=\"type\">None</span>, dep.serializer)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  sorter.insertAll(records)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> output = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> tmp = <span class=\"type\">Utils</span>.tempFileWith(output)</span><br><span class=\"line\">  <span class=\"keyword\">try</span> &#123; </span><br><span class=\"line\">    <span class=\"keyword\">val</span> blockId = <span class=\"type\">ShuffleBlockId</span>(dep.shuffleId, mapId, <span class=\"type\">IndexShuffleBlockResolver</span>.<span class=\"type\">NOOP_REDUCE_ID</span>)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> partitionLengths = sorter.writePartitionedFile(blockId, tmp)</span><br><span class=\"line\">    shuffleBlockResolver.writeIndexFileAndCommit(dep.shuffleId, mapId, partitionLengths, tmp)</span><br><span class=\"line\">    mapStatus = <span class=\"type\">MapStatus</span>(blockManager.shuffleServerId, partitionLengths)</span><br><span class=\"line\">  &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (tmp.exists() &amp;&amp; !tmp.delete()) &#123;</span><br><span class=\"line\">      logError(<span class=\"string\">s\"Error while deleting temp file <span class=\"subst\">$&#123;tmp.getAbsolutePath&#125;</span>\"</span>)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>使用了 ExternalSorter 的 insertAll 方法插入了上一个 RDD 生成的数据的迭代器。</p>\n<h4 id=\"insertAll\"><a href=\"#insertAll\" class=\"headerlink\" title=\"insertAll\"></a>insertAll</h4><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">insertAll</span></span>(records: <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>]]): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">    <span class=\"comment\">// <span class=\"doctag\">TODO:</span> stop combining if we find that the reduction factor isn't high</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> shouldCombine = aggregator.isDefined</span><br><span class=\"line\">    <span class=\"comment\">// 直接看需要 map 端聚合的情况</span></span><br><span class=\"line\">    <span class=\"comment\">// 不需要聚合的情况类似</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (shouldCombine) &#123;</span><br><span class=\"line\">      <span class=\"comment\">// Combine values in-memory first using our AppendOnlyMap</span></span><br><span class=\"line\">       </span><br><span class=\"line\">      <span class=\"keyword\">val</span> mergeValue = aggregator.get.mergeValue </span><br><span class=\"line\">      <span class=\"keyword\">val</span> createCombiner = aggregator.get.createCombiner</span><br><span class=\"line\">      <span class=\"keyword\">var</span> kv: <span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>] = <span class=\"literal\">null</span></span><br><span class=\"line\">      <span class=\"comment\">// 如果 key 不存在则直接创建 Combiner ，否则使用 mergeValue 将 value 添加到创建的 Combiner中</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> update = (hadValue: <span class=\"type\">Boolean</span>, oldValue: <span class=\"type\">C</span>) =&gt; &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (hadValue) mergeValue(oldValue, kv._2) <span class=\"keyword\">else</span> createCombiner(kv._2)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      <span class=\"keyword\">while</span> (records.hasNext) &#123;</span><br><span class=\"line\">        addElementsRead()</span><br><span class=\"line\">        kv = records.next()</span><br><span class=\"line\">        <span class=\"comment\">// 更新值</span></span><br><span class=\"line\">        <span class=\"comment\">// 这里需要注意的是之前的数据是(k,v)类型，这里转换成了((part,k),v)</span></span><br><span class=\"line\">        <span class=\"comment\">// 其中 part 是 key 对应的分区</span></span><br><span class=\"line\">        map.changeValue((getPartition(kv._1), kv._1), update)</span><br><span class=\"line\">        maybeSpillCollection(usingMap = <span class=\"literal\">true</span>)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">      <span class=\"comment\">// Stick values into our buffer</span></span><br><span class=\"line\">      <span class=\"keyword\">while</span> (records.hasNext) &#123;</span><br><span class=\"line\">        addElementsRead()</span><br><span class=\"line\">        <span class=\"keyword\">val</span> kv = records.next()</span><br><span class=\"line\">        buffer.insert(getPartition(kv._1), kv._1, kv._2.asInstanceOf[<span class=\"type\">C</span>])</span><br><span class=\"line\">        maybeSpillCollection(usingMap = <span class=\"literal\">false</span>)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>在这里使用 getPartition() 进行了数据的<strong>分区</strong>，也就是将不同的 key 与其分区对应起来。默认使用的是 HashPartitioner ，有兴趣的同学可以去阅读源码。再继续看 changeValue 方法。在这里使用非常重要的数据结构来存放分区后的数据，也就是代码中的 map 类型为 PartitionedAppendOnlyMap。其中是使用hask表存放数据，并且存放的方式为表中的偶数索index引存放key, index + 1 存放对应的value。例如:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">k1|v1|null|k2|v2|k3|v3|null|....</span><br></pre></td></tr></table></figure>\n<p>之所以里面由空值，是因为在插入的时候使用了二次探测法。来看一看 changeValue 方法:</p>\n<p>PartitionedAppendOnlyMap 中的  changeValue 方法继承自 </p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">changeValue</span></span>(key: <span class=\"type\">K</span>, updateFunc: (<span class=\"type\">Boolean</span>, <span class=\"type\">V</span>) =&gt; <span class=\"type\">V</span>): <span class=\"type\">V</span> = &#123;</span><br><span class=\"line\">  assert(!destroyed, destructionMessage)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> k = key.asInstanceOf[<span class=\"type\">AnyRef</span>]</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (k.eq(<span class=\"literal\">null</span>)) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (!haveNullValue) &#123;</span><br><span class=\"line\">      incrementSize()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    nullValue = updateFunc(haveNullValue, nullValue)</span><br><span class=\"line\">    haveNullValue = <span class=\"literal\">true</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> nullValue</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"keyword\">var</span> pos = rehash(k.hashCode) &amp; mask</span><br><span class=\"line\">  <span class=\"keyword\">var</span> i = <span class=\"number\">1</span></span><br><span class=\"line\">  <span class=\"keyword\">while</span> (<span class=\"literal\">true</span>) &#123;</span><br><span class=\"line\">    <span class=\"comment\">// 偶数位置为 key</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> curKey = data(<span class=\"number\">2</span> * pos)</span><br><span class=\"line\">     <span class=\"comment\">// 如果该key不存在，就直接插入</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (curKey.eq(<span class=\"literal\">null</span>)) &#123;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> newValue = updateFunc(<span class=\"literal\">false</span>, <span class=\"literal\">null</span>.asInstanceOf[<span class=\"type\">V</span>])</span><br><span class=\"line\">      data(<span class=\"number\">2</span> * pos) = k</span><br><span class=\"line\">      data(<span class=\"number\">2</span> * pos + <span class=\"number\">1</span>) = newValue.asInstanceOf[<span class=\"type\">AnyRef</span>]</span><br><span class=\"line\">      incrementSize()</span><br><span class=\"line\">      <span class=\"keyword\">return</span> newValue</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (k.eq(curKey) || k.equals(curKey)) &#123;<span class=\"comment\">//如果 key 存在，则进行聚合</span></span><br><span class=\"line\">      </span><br><span class=\"line\">      <span class=\"keyword\">val</span> newValue = updateFunc(<span class=\"literal\">true</span>, data(<span class=\"number\">2</span> * pos + <span class=\"number\">1</span>).asInstanceOf[<span class=\"type\">V</span>])</span><br><span class=\"line\">      data(<span class=\"number\">2</span> * pos + <span class=\"number\">1</span>) = newValue.asInstanceOf[<span class=\"type\">AnyRef</span>]</span><br><span class=\"line\">      <span class=\"keyword\">return</span> newValue</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123; <span class=\"comment\">// 否则进行下一次探测</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> delta = i</span><br><span class=\"line\">      pos = (pos + delta) &amp; mask</span><br><span class=\"line\">      i += <span class=\"number\">1</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"literal\">null</span>.asInstanceOf[<span class=\"type\">V</span>] <span class=\"comment\">// Never reached but needed to keep compiler happy</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>再调用 insertAll 进行了数据的聚合|插入之后，调用了 maybeSpillCollection 方法。这个方法的作用是判断 map 占用了内存，如果内存达到一定的值，则将内存中的数据 spill 到临时文件中。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">maybeSpillCollection</span></span>(usingMap: <span class=\"type\">Boolean</span>): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">var</span> estimatedSize = <span class=\"number\">0</span>L</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (usingMap) &#123;</span><br><span class=\"line\">      estimatedSize = map.estimateSize()</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (maybeSpill(map, estimatedSize)) &#123;</span><br><span class=\"line\">        map = <span class=\"keyword\">new</span> <span class=\"type\">PartitionedAppendOnlyMap</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">      estimatedSize = buffer.estimateSize()</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (maybeSpill(buffer, estimatedSize)) &#123;</span><br><span class=\"line\">        buffer = <span class=\"keyword\">new</span> <span class=\"type\">PartitionedPairBuffer</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (estimatedSize &gt; _peakMemoryUsedBytes) &#123;</span><br><span class=\"line\">      _peakMemoryUsedBytes = estimatedSize</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>maybeSpillCollection 继续调用了 maybeSpill 方法，这个方法继承自 Spillable 中</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">protected</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">maybeSpill</span></span>(collection: <span class=\"type\">C</span>, currentMemory: <span class=\"type\">Long</span>): <span class=\"type\">Boolean</span> = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">var</span> shouldSpill = <span class=\"literal\">false</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (elementsRead % <span class=\"number\">32</span> == <span class=\"number\">0</span> &amp;&amp; currentMemory &gt;= myMemoryThreshold) &#123;</span><br><span class=\"line\">      <span class=\"comment\">// Claim up to double our current memory from the shuffle memory pool</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> amountToRequest = <span class=\"number\">2</span> * currentMemory - myMemoryThreshold</span><br><span class=\"line\">      <span class=\"keyword\">val</span> granted = acquireMemory(amountToRequest)</span><br><span class=\"line\">      myMemoryThreshold += granted</span><br><span class=\"line\">      <span class=\"comment\">// If we were granted too little memory to grow further (either tryToAcquire returned 0,</span></span><br><span class=\"line\">      <span class=\"comment\">// or we already had more memory than myMemoryThreshold), spill the current collection</span></span><br><span class=\"line\">      shouldSpill = currentMemory &gt;= myMemoryThreshold</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    shouldSpill = shouldSpill || _elementsRead &gt; numElementsForceSpillThreshold</span><br><span class=\"line\">    <span class=\"comment\">// Actually spill</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (shouldSpill) &#123;</span><br><span class=\"line\">      _spillCount += <span class=\"number\">1</span></span><br><span class=\"line\">      logSpillage(currentMemory)</span><br><span class=\"line\">      spill(collection)</span><br><span class=\"line\">      _elementsRead = <span class=\"number\">0</span></span><br><span class=\"line\">      _memoryBytesSpilled += currentMemory</span><br><span class=\"line\">      releaseMemory()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    shouldSpill</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>该方法首先判断了是否需要 spill，如果需要则调用 spill() 方法将内存中的数据写入临时文件中。spill 方法如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"keyword\">protected</span>[<span class=\"keyword\">this</span>] <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">spill</span></span>(collection: <span class=\"type\">WritablePartitionedPairCollection</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">   <span class=\"keyword\">val</span> inMemoryIterator = collection.destructiveSortedWritablePartitionedIterator(comparator)</span><br><span class=\"line\">   <span class=\"keyword\">val</span> spillFile = spillMemoryIteratorToDisk(inMemoryIterator)</span><br><span class=\"line\">   spills += spillFile</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>其中的  comparator 如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">comparator</span></span>: <span class=\"type\">Option</span>[<span class=\"type\">Comparator</span>[<span class=\"type\">K</span>]] = &#123;</span><br><span class=\"line\">   <span class=\"keyword\">if</span> (ordering.isDefined || aggregator.isDefined) &#123;</span><br><span class=\"line\">     <span class=\"type\">Some</span>(keyComparator)</span><br><span class=\"line\">   &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">     <span class=\"type\">None</span></span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>也就是:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"keyword\">val</span> keyComparator: <span class=\"type\">Comparator</span>[<span class=\"type\">K</span>] = ordering.getOrElse(<span class=\"keyword\">new</span> <span class=\"type\">Comparator</span>[<span class=\"type\">K</span>] &#123;</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compare</span></span>(a: <span class=\"type\">K</span>, b: <span class=\"type\">K</span>): <span class=\"type\">Int</span> = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> h1 = <span class=\"keyword\">if</span> (a == <span class=\"literal\">null</span>) <span class=\"number\">0</span> <span class=\"keyword\">else</span> a.hashCode()</span><br><span class=\"line\">    <span class=\"keyword\">val</span> h2 = <span class=\"keyword\">if</span> (b == <span class=\"literal\">null</span>) <span class=\"number\">0</span> <span class=\"keyword\">else</span> b.hashCode()</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (h1 &lt; h2) <span class=\"number\">-1</span> <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (h1 == h2) <span class=\"number\">0</span> <span class=\"keyword\">else</span> <span class=\"number\">1</span></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;)</span><br></pre></td></tr></table></figure>\n<p>destructiveSortedWritablePartitionedIterator 位于 WritablePartitionedPairCollection 类中，实现如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">destructiveSortedWritablePartitionedIterator</span></span>(keyComparator: <span class=\"type\">Option</span>[<span class=\"type\">Comparator</span>[<span class=\"type\">K</span>]])</span><br><span class=\"line\">    : <span class=\"type\">WritablePartitionedIterator</span> = &#123;</span><br><span class=\"line\">    <span class=\"comment\">//这里调用的其实是　PartitionedAppendOnlyMap　中重写的　partitionedDestructiveSortedIterator    </span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> it = partitionedDestructiveSortedIterator(keyComparator)</span><br><span class=\"line\">    <span class=\"comment\">// 这里还实现了一个　writeNext　的方法，后面会用到</span></span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">WritablePartitionedIterator</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">private</span>[<span class=\"keyword\">this</span>] <span class=\"keyword\">var</span> cur = <span class=\"keyword\">if</span> (it.hasNext) it.next() <span class=\"keyword\">else</span> <span class=\"literal\">null</span></span><br><span class=\"line\">       <span class=\"comment\">// 在map 中的数据其实是((partition,k),v)</span></span><br><span class=\"line\">       <span class=\"comment\">// 这里只写入了(k,v)</span></span><br><span class=\"line\">      <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">writeNext</span></span>(writer: <span class=\"type\">DiskBlockObjectWriter</span>): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">        writer.write(cur._1._2, cur._2)</span><br><span class=\"line\">        cur = <span class=\"keyword\">if</span> (it.hasNext) it.next() <span class=\"keyword\">else</span> <span class=\"literal\">null</span></span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hasNext</span></span>(): <span class=\"type\">Boolean</span> = cur != <span class=\"literal\">null</span></span><br><span class=\"line\">      <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">nextPartition</span></span>(): <span class=\"type\">Int</span> = cur._1._1</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>接下来看看partitionedDestructiveSortedIterator:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">partitionedDestructiveSortedIterator</span></span>(keyComparator: <span class=\"type\">Option</span>[<span class=\"type\">Comparator</span>[<span class=\"type\">K</span>]])</span><br><span class=\"line\">    : <span class=\"type\">Iterator</span>[((<span class=\"type\">Int</span>, <span class=\"type\">K</span>), <span class=\"type\">V</span>)] = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> comparator = keyComparator.map(partitionKeyComparator).getOrElse(partitionComparator)</span><br><span class=\"line\">    destructiveSortedIterator(comparator)</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>keyComparator.map(partitionKeyComparator)　等于 partitionKeyComparator(keyComparator)，再看下面的partitionKeyComparator，说明比较器是先对分区进行比较，如果分区相同，再对key的hash值进行比较:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">partitionKeyComparator</span></span>[<span class=\"type\">K</span>](keyComparator: <span class=\"type\">Comparator</span>[<span class=\"type\">K</span>]): <span class=\"type\">Comparator</span>[(<span class=\"type\">Int</span>, <span class=\"type\">K</span>)] = &#123;</span><br><span class=\"line\">   <span class=\"keyword\">new</span> <span class=\"type\">Comparator</span>[(<span class=\"type\">Int</span>, <span class=\"type\">K</span>)] &#123;</span><br><span class=\"line\">     <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compare</span></span>(a: (<span class=\"type\">Int</span>, <span class=\"type\">K</span>), b: (<span class=\"type\">Int</span>, <span class=\"type\">K</span>)): <span class=\"type\">Int</span> = &#123;</span><br><span class=\"line\">       <span class=\"keyword\">val</span> partitionDiff = a._1 - b._1</span><br><span class=\"line\">       <span class=\"keyword\">if</span> (partitionDiff != <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">         partitionDiff</span><br><span class=\"line\">       &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">         keyComparator.compare(a._2, b._2)</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>然后看destructiveSortedIterator(),这是在 PartitionedAppendOnlyMap 的父类 destructiveSortedIterator 中实现的:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">destructiveSortedIterator</span></span>(keyComparator: <span class=\"type\">Comparator</span>[<span class=\"type\">K</span>]): <span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">V</span>)] = &#123;</span><br><span class=\"line\">   destroyed = <span class=\"literal\">true</span></span><br><span class=\"line\">   <span class=\"comment\">// Pack KV pairs into the front of the underlying array</span></span><br><span class=\"line\">   <span class=\"comment\">// 这里是因为　PartitionedAppendOnlyMap　采用的二次探测法　,kv对之间存在　null值，所以先把非空的kv对全部移动到hash表的前端</span></span><br><span class=\"line\">   <span class=\"keyword\">var</span> keyIndex, newIndex = <span class=\"number\">0</span></span><br><span class=\"line\">   <span class=\"keyword\">while</span> (keyIndex &lt; capacity) &#123;</span><br><span class=\"line\">     <span class=\"keyword\">if</span> (data(<span class=\"number\">2</span> * keyIndex) != <span class=\"literal\">null</span>) &#123;</span><br><span class=\"line\">       data(<span class=\"number\">2</span> * newIndex) = data(<span class=\"number\">2</span> * keyIndex)</span><br><span class=\"line\">       data(<span class=\"number\">2</span> * newIndex + <span class=\"number\">1</span>) = data(<span class=\"number\">2</span> * keyIndex + <span class=\"number\">1</span>)</span><br><span class=\"line\">       newIndex += <span class=\"number\">1</span></span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">     keyIndex += <span class=\"number\">1</span></span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   assert(curSize == newIndex + (<span class=\"keyword\">if</span> (haveNullValue) <span class=\"number\">1</span> <span class=\"keyword\">else</span> <span class=\"number\">0</span>))</span><br><span class=\"line\">   <span class=\"comment\">// 这里对给定数据范围为 0 to newIndex，利用　keyComparator　比较器进行排序</span></span><br><span class=\"line\">   <span class=\"comment\">// 也就是对 map 中的数据根据　(partition,key) 进行排序</span></span><br><span class=\"line\">   <span class=\"keyword\">new</span> <span class=\"type\">Sorter</span>(<span class=\"keyword\">new</span> <span class=\"type\">KVArraySortDataFormat</span>[<span class=\"type\">K</span>, <span class=\"type\">AnyRef</span>]).sort(data, <span class=\"number\">0</span>, newIndex, keyComparator)</span><br><span class=\"line\"></span><br><span class=\"line\">   <span class=\"comment\">// 定义迭代器</span></span><br><span class=\"line\">   <span class=\"keyword\">new</span> <span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">V</span>)] &#123;</span><br><span class=\"line\">     <span class=\"keyword\">var</span> i = <span class=\"number\">0</span></span><br><span class=\"line\">     <span class=\"keyword\">var</span> nullValueReady = haveNullValue</span><br><span class=\"line\">     <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hasNext</span></span>: <span class=\"type\">Boolean</span> = (i &lt; newIndex || nullValueReady)</span><br><span class=\"line\">     <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">next</span></span>(): (<span class=\"type\">K</span>, <span class=\"type\">V</span>) = &#123;</span><br><span class=\"line\">       <span class=\"keyword\">if</span> (nullValueReady) &#123;</span><br><span class=\"line\">         nullValueReady = <span class=\"literal\">false</span></span><br><span class=\"line\">         (<span class=\"literal\">null</span>.asInstanceOf[<span class=\"type\">K</span>], nullValue)</span><br><span class=\"line\">       &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">         <span class=\"keyword\">val</span> item = (data(<span class=\"number\">2</span> * i).asInstanceOf[<span class=\"type\">K</span>], data(<span class=\"number\">2</span> * i + <span class=\"number\">1</span>).asInstanceOf[<span class=\"type\">V</span>])</span><br><span class=\"line\">         i += <span class=\"number\">1</span></span><br><span class=\"line\">         item</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>可以看出整个　<code>val inMemoryIterator = collection.destructiveSortedWritablePartitionedIterator(comparator)</code>　这一步其实就是在对　map　或 buffer 进行排序，并且实现了一个 writeNext() 方法供后续调调用。随后调用<code>val spillFile = spillMemoryIteratorToDisk(inMemoryIterator)</code>将排序后的缓存写入文件中:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span>[<span class=\"keyword\">this</span>] <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">spillMemoryIteratorToDisk</span></span>(inMemoryIterator: <span class=\"type\">WritablePartitionedIterator</span>)</span><br><span class=\"line\">      : <span class=\"type\">SpilledFile</span> = &#123;</span><br><span class=\"line\">   </span><br><span class=\"line\">    <span class=\"keyword\">val</span> (blockId, file) = diskBlockManager.createTempShuffleBlock()</span><br><span class=\"line\">    <span class=\"comment\">// These variables are reset after each flush</span></span><br><span class=\"line\">    <span class=\"keyword\">var</span> objectsWritten: <span class=\"type\">Long</span> = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> spillMetrics: <span class=\"type\">ShuffleWriteMetrics</span> = <span class=\"keyword\">new</span> <span class=\"type\">ShuffleWriteMetrics</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> writer: <span class=\"type\">DiskBlockObjectWriter</span> =</span><br><span class=\"line\">      blockManager.getDiskWriter(blockId, file, serInstance, fileBufferSize, spillMetrics)</span><br><span class=\"line\">    <span class=\"comment\">// List of batch sizes (bytes) in the order they are written to disk</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> batchSizes = <span class=\"keyword\">new</span> <span class=\"type\">ArrayBuffer</span>[<span class=\"type\">Long</span>]</span><br><span class=\"line\">    <span class=\"comment\">// How many elements we have in each partition</span></span><br><span class=\"line\">    <span class=\"comment\">// 用于记录每一个分区有多少条数据</span></span><br><span class=\"line\">    <span class=\"comment\">// 由于刚才数据写入文件没有写入key对应的分区，因此需要记录每一个分区有多少条数据，然后根据数据的偏移量就可以判断该数据属于哪个分区　</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> elementsPerPartition = <span class=\"keyword\">new</span> <span class=\"type\">Array</span>[<span class=\"type\">Long</span>](numPartitions)</span><br><span class=\"line\">    <span class=\"comment\">// Flush the disk writer's contents to disk, and update relevant variables.</span></span><br><span class=\"line\">    <span class=\"comment\">// The writer is committed at the end of this process.</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">flush</span></span>(): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> segment = writer.commitAndGet()</span><br><span class=\"line\">      batchSizes += segment.length</span><br><span class=\"line\">      _diskBytesSpilled += segment.length</span><br><span class=\"line\">      objectsWritten = <span class=\"number\">0</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">var</span> success = <span class=\"literal\">false</span></span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">while</span> (inMemoryIterator.hasNext) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">val</span> partitionId = inMemoryIterator.nextPartition()</span><br><span class=\"line\">        require(partitionId &gt;= <span class=\"number\">0</span> &amp;&amp; partitionId &lt; numPartitions,</span><br><span class=\"line\">          <span class=\"string\">s\"partition Id: <span class=\"subst\">$&#123;partitionId&#125;</span> should be in the range [0, <span class=\"subst\">$&#123;numPartitions&#125;</span>)\"</span>)</span><br><span class=\"line\">        inMemoryIterator.writeNext(writer)</span><br><span class=\"line\">        elementsPerPartition(partitionId) += <span class=\"number\">1</span></span><br><span class=\"line\">        objectsWritten += <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (objectsWritten == serializerBatchSize) &#123;　<span class=\"comment\">//写入　serializerBatchSize　条数据便刷新一次缓存</span></span><br><span class=\"line\">          <span class=\"comment\">// batchSize 在类中定义的如下:</span></span><br><span class=\"line\">          <span class=\"comment\">// 可以看出如果不存在配置默认为　10000　条</span></span><br><span class=\"line\">          <span class=\"comment\">// private val serializerBatchSize = conf.getLong(\"spark.shuffle.spill.batchSize\", 10000)</span></span><br><span class=\"line\">          flush()</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (objectsWritten &gt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">        flush()</span><br><span class=\"line\">      &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        writer.revertPartialWritesAndClose()</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      success = <span class=\"literal\">true</span></span><br><span class=\"line\">    &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (success) &#123;</span><br><span class=\"line\">        writer.close()</span><br><span class=\"line\">      &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// This code path only happens if an exception was thrown above before we set success;</span></span><br><span class=\"line\">        <span class=\"comment\">// close our stuff and let the exception be thrown further</span></span><br><span class=\"line\">        writer.revertPartialWritesAndClose()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (file.exists()) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">if</span> (!file.delete()) &#123;</span><br><span class=\"line\">            logWarning(<span class=\"string\">s\"Error deleting <span class=\"subst\">$&#123;file&#125;</span>\"</span>)</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">//最后记录临时文件的信息</span></span><br><span class=\"line\">    <span class=\"type\">SpilledFile</span>(file, blockId, batchSizes.toArray, elementsPerPartition)</span><br><span class=\"line\"></span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>这就是整个　insertAll()　方法:</p>\n<p> merge 达到溢出值后进行排序并写入临时文件，这里要注意的是并非所有的缓存文件都被写入到临时文件中，所以接下来需要将临时文件中的数据加上内存中的数据进行一次排序写入到一个大文件中。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//首先获取需要写入的文件:　</span></span><br><span class=\"line\">   <span class=\"keyword\">val</span> output = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)</span><br><span class=\"line\">　　<span class=\"keyword\">val</span> tmp = <span class=\"type\">Utils</span>.tempFileWith(output)</span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123; </span><br><span class=\"line\">      <span class=\"keyword\">val</span> blockId = <span class=\"type\">ShuffleBlockId</span>(dep.shuffleId, mapId, <span class=\"type\">IndexShuffleBlockResolver</span>.<span class=\"type\">NOOP_REDUCE_ID</span>)</span><br><span class=\"line\">      <span class=\"keyword\">val</span> partitionLengths = sorter.writePartitionedFile(blockId, tmp)<span class=\"comment\">//这一句是重点 下面会讲解</span></span><br><span class=\"line\">      shuffleBlockResolver.writeIndexFileAndCommit(dep.shuffleId, mapId, partitionLengths, tmp)</span><br><span class=\"line\">      mapStatus = <span class=\"type\">MapStatus</span>(blockManager.shuffleServerId, partitionLengths)</span><br><span class=\"line\">    &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (tmp.exists() &amp;&amp; !tmp.delete()) &#123;</span><br><span class=\"line\">        logError(<span class=\"string\">s\"Error while deleting temp file <span class=\"subst\">$&#123;tmp.getAbsolutePath&#125;</span>\"</span>)</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"writePartitionedFile\"><a href=\"#writePartitionedFile\" class=\"headerlink\" title=\"writePartitionedFile\"></a>writePartitionedFile</h4><p>继续看 writePartitionedFile :</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//  ExternalSorter 中的　writePartitionedFile　方法</span></span><br><span class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">writePartitionedFile</span></span>(</span><br><span class=\"line\">      blockId: <span class=\"type\">BlockId</span>,</span><br><span class=\"line\">      outputFile: <span class=\"type\">File</span>): <span class=\"type\">Array</span>[<span class=\"type\">Long</span>] = &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// Track location of each range in the output file</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> lengths = <span class=\"keyword\">new</span> <span class=\"type\">Array</span>[<span class=\"type\">Long</span>](numPartitions)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> writer = blockManager.getDiskWriter(blockId, outputFile, serInstance, fileBufferSize,</span><br><span class=\"line\">      context.taskMetrics().shuffleWriteMetrics)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 这里的spills 就是　刚才产生的临时文件的数据　如果为空逻辑就比较简单，直接进行排序并写入文件</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (spills.isEmpty) &#123;</span><br><span class=\"line\">      <span class=\"comment\">// Case where we only have in-memory data</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> collection = <span class=\"keyword\">if</span> (aggregator.isDefined) map <span class=\"keyword\">else</span> buffer</span><br><span class=\"line\">      <span class=\"keyword\">val</span> it = collection.destructiveSortedWritablePartitionedIterator(comparator)　<span class=\"comment\">//这里同样使用　destructiveSortedWritablePartitionedIterator　进行排序</span></span><br><span class=\"line\">      <span class=\"keyword\">while</span> (it.hasNext) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">val</span> partitionId = it.nextPartition()</span><br><span class=\"line\">        <span class=\"keyword\">while</span> (it.hasNext &amp;&amp; it.nextPartition() == partitionId) &#123;</span><br><span class=\"line\">          it.writeNext(writer)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">val</span> segment = writer.commitAndGet()</span><br><span class=\"line\">        lengths(partitionId) = segment.length</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;<span class=\"comment\">//重点看不为空的时候，这里调用了　partitionedIterator　方法</span></span><br><span class=\"line\">      <span class=\"comment\">// We must perform merge-sort; get an iterator by partition and write everything directly.</span></span><br><span class=\"line\">      <span class=\"keyword\">for</span> ((id, elements) &lt;- <span class=\"keyword\">this</span>.partitionedIterator) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (elements.hasNext) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">for</span> (elem &lt;- elements) &#123;</span><br><span class=\"line\">            writer.write(elem._1, elem._2)</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">          <span class=\"keyword\">val</span> segment = writer.commitAndGet()</span><br><span class=\"line\">          lengths(id) = segment.length</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    writer.close()</span><br><span class=\"line\">    context.taskMetrics().incMemoryBytesSpilled(memoryBytesSpilled)</span><br><span class=\"line\">    context.taskMetrics().incDiskBytesSpilled(diskBytesSpilled)</span><br><span class=\"line\">    context.taskMetrics().incPeakExecutionMemory(peakMemoryUsedBytes)</span><br><span class=\"line\"></span><br><span class=\"line\">    lengths</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>然后是 partitionedIterator 方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">partitionedIterator</span></span>: <span class=\"type\">Iterator</span>[(<span class=\"type\">Int</span>, <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]])] = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> usingMap = aggregator.isDefined</span><br><span class=\"line\">    <span class=\"keyword\">val</span> collection: <span class=\"type\">WritablePartitionedPairCollection</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>] = <span class=\"keyword\">if</span> (usingMap) map <span class=\"keyword\">else</span> buffer</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (spills.isEmpty) &#123;<span class=\"comment\">// 这里又一次判断了是否为空，直接看有临时文件的部分</span></span><br><span class=\"line\">      <span class=\"comment\">// Special case: if we have only in-memory data, we don't need to merge streams, and perhaps</span></span><br><span class=\"line\">      <span class=\"comment\">// we don't even need to sort by anything other than partition ID</span></span><br><span class=\"line\">      <span class=\"keyword\">if</span> (!ordering.isDefined) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// The user hasn't requested sorted keys, so only sort by partition ID, not key</span></span><br><span class=\"line\">   groupByPartition(destructiveIterator(collection.partitionedDestructiveSortedIterator(<span class=\"type\">None</span>)))</span><br><span class=\"line\">      &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// We do need to sort by both partition ID and key</span></span><br><span class=\"line\">        groupByPartition(destructiveIterator(</span><br><span class=\"line\">          collection.partitionedDestructiveSortedIterator(<span class=\"type\">Some</span>(keyComparator))))</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">      <span class=\"comment\">// Merge spilled and in-memory data</span></span><br><span class=\"line\">      <span class=\"comment\">// 这里传入了临时文件　spills　和　排序后的缓存文件</span></span><br><span class=\"line\">      merge(spills, destructiveIterator(</span><br><span class=\"line\">        collection.partitionedDestructiveSortedIterator(comparator)))</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>merge 方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// merge方法</span></span><br><span class=\"line\">  <span class=\"comment\">// inMemory　是根据(partion,hash(k)) 排序后的内存数据</span></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">merge</span></span>(spills: <span class=\"type\">Seq</span>[<span class=\"type\">SpilledFile</span>], inMemory: <span class=\"type\">Iterator</span>[((<span class=\"type\">Int</span>, <span class=\"type\">K</span>), <span class=\"type\">C</span>)])</span><br><span class=\"line\">     : <span class=\"type\">Iterator</span>[(<span class=\"type\">Int</span>, <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]])] = &#123;</span><br><span class=\"line\">   <span class=\"comment\">//　将所有缓存文件转化为 SpillReader </span></span><br><span class=\"line\">   <span class=\"keyword\">val</span> readers = spills.map(<span class=\"keyword\">new</span> <span class=\"type\">SpillReader</span>(_))</span><br><span class=\"line\">   <span class=\"comment\">// buffered方法只是比普通的　itertor 方法多提供一个　head　方法，例如:</span></span><br><span class=\"line\">   <span class=\"comment\">// val a = List(1,2,3,4,5)</span></span><br><span class=\"line\">   <span class=\"comment\">// val b = a.iterator.buffered</span></span><br><span class=\"line\">   <span class=\"comment\">// b.head : 1</span></span><br><span class=\"line\">   <span class=\"comment\">// b.next : 1</span></span><br><span class=\"line\">   <span class=\"comment\">// b.head : 2</span></span><br><span class=\"line\">   <span class=\"comment\">// b.next : 2</span></span><br><span class=\"line\">   <span class=\"keyword\">val</span> inMemBuffered = inMemory.buffered</span><br><span class=\"line\">   (<span class=\"number\">0</span> until numPartitions).iterator.map &#123; p =&gt;</span><br><span class=\"line\">     <span class=\"comment\">// 获得分区对应数据的迭代器 </span></span><br><span class=\"line\">     <span class=\"comment\">// 这里的　IteratorForPartition　就是得到分区 p 对应的数据的迭代器，逻辑比较简单，就不单独拿出来分析</span></span><br><span class=\"line\">     <span class=\"keyword\">val</span> inMemIterator = <span class=\"keyword\">new</span> <span class=\"type\">IteratorForPartition</span>(p, inMemBuffered)</span><br><span class=\"line\">     </span><br><span class=\"line\">     <span class=\"comment\">//这里获取所有文件的第　p　个分区　并与内存中的第　p 个分区进行合并</span></span><br><span class=\"line\">     <span class=\"keyword\">val</span> iterators = readers.map(_.readNextPartition()) ++ <span class=\"type\">Seq</span>(inMemIterator)</span><br><span class=\"line\">　　　<span class=\"comment\">// 这里的　iterators　是由　多个 iterator 组成的，其中每一个都是关于 p 分区数据的 iterator</span></span><br><span class=\"line\">     <span class=\"keyword\">if</span> (aggregator.isDefined) &#123;</span><br><span class=\"line\">       <span class=\"comment\">// Perform partial aggregation across partitions</span></span><br><span class=\"line\">       (p, mergeWithAggregation(</span><br><span class=\"line\">         iterators, aggregator.get.mergeCombiners, keyComparator, ordering.isDefined))</span><br><span class=\"line\">     &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (ordering.isDefined) &#123;</span><br><span class=\"line\">       <span class=\"comment\">// No aggregator given, but we have an ordering (e.g. used by reduce tasks in sortByKey);</span></span><br><span class=\"line\">       <span class=\"comment\">// sort the elements without trying to merge them</span></span><br><span class=\"line\">       (p, mergeSort(iterators, ordering.get))</span><br><span class=\"line\">     &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">       (p, iterators.iterator.flatten)</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">   &#125;</span><br></pre></td></tr></table></figure>\n<p>mergeWithAggregation<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">mergeWithAggregation</span></span>(</span><br><span class=\"line\">    iterators: <span class=\"type\">Seq</span>[<span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]]],</span><br><span class=\"line\">    mergeCombiners: (<span class=\"type\">C</span>, <span class=\"type\">C</span>) =&gt; <span class=\"type\">C</span>,</span><br><span class=\"line\">    comparator: <span class=\"type\">Comparator</span>[<span class=\"type\">K</span>],</span><br><span class=\"line\">    totalOrder: <span class=\"type\">Boolean</span>)</span><br><span class=\"line\">    : <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] =</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (!totalOrder) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">Iterator</span>[<span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]]] &#123;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> sorted = mergeSort(iterators, comparator).buffered</span><br><span class=\"line\">      <span class=\"keyword\">val</span> keys = <span class=\"keyword\">new</span> <span class=\"type\">ArrayBuffer</span>[<span class=\"type\">K</span>]</span><br><span class=\"line\">      <span class=\"keyword\">val</span> combiners = <span class=\"keyword\">new</span> <span class=\"type\">ArrayBuffer</span>[<span class=\"type\">C</span>]</span><br><span class=\"line\">      <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hasNext</span></span>: <span class=\"type\">Boolean</span> = sorted.hasNext</span><br><span class=\"line\">      <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">next</span></span>(): <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] = &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!hasNext) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">NoSuchElementException</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        keys.clear()</span><br><span class=\"line\">        combiners.clear()</span><br><span class=\"line\">        <span class=\"keyword\">val</span> firstPair = sorted.next()</span><br><span class=\"line\">        keys += firstPair._1</span><br><span class=\"line\">        combiners += firstPair._2</span><br><span class=\"line\">        <span class=\"keyword\">val</span> key = firstPair._1</span><br><span class=\"line\">        <span class=\"keyword\">while</span> (sorted.hasNext &amp;&amp; comparator.compare(sorted.head._1, key) == <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">val</span> pair = sorted.next()</span><br><span class=\"line\">          <span class=\"keyword\">var</span> i = <span class=\"number\">0</span></span><br><span class=\"line\">          <span class=\"keyword\">var</span> foundKey = <span class=\"literal\">false</span></span><br><span class=\"line\">          <span class=\"keyword\">while</span> (i &lt; keys.size &amp;&amp; !foundKey) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (keys(i) == pair._1) &#123;</span><br><span class=\"line\">              combiners(i) = mergeCombiners(combiners(i), pair._2)</span><br><span class=\"line\">              foundKey = <span class=\"literal\">true</span></span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            i += <span class=\"number\">1</span></span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">          <span class=\"keyword\">if</span> (!foundKey) &#123;</span><br><span class=\"line\">            keys += pair._1</span><br><span class=\"line\">            combiners += pair._2</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        keys.iterator.zip(combiners.iterator)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;.flatMap(i =&gt; i)</span><br><span class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">// We have a total ordering, so the objects with the same key are sequential.</span></span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] &#123;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> sorted = mergeSort(iterators, comparator).buffered</span><br><span class=\"line\">      <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hasNext</span></span>: <span class=\"type\">Boolean</span> = sorted.hasNext</span><br><span class=\"line\">      <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">next</span></span>(): <span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>] = &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!hasNext) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">NoSuchElementException</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">val</span> elem = sorted.next()</span><br><span class=\"line\">        <span class=\"keyword\">val</span> k = elem._1</span><br><span class=\"line\">        <span class=\"keyword\">var</span> c = elem._2</span><br><span class=\"line\">        <span class=\"comment\">// 这里的意思是可能存在多个相同的key 分布在不同临时文件或内存中</span></span><br><span class=\"line\">        <span class=\"comment\">// 所以还需要将不同的 key 对应的值进行合并 </span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> (sorted.hasNext &amp;&amp; sorted.head._1 == k) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">val</span> pair = sorted.next()</span><br><span class=\"line\">          c = mergeCombiners(c, pair._2)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        (k, c)<span class=\"comment\">//返回</span></span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>继续看 mergeSort 方法，首先选除头元素最小对应那个分区，然后选出这个分区的第一个元素，那么这个元素一定是最小的，然后将剩余的元素放回去:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">mergeSort</span></span>(iterators: <span class=\"type\">Seq</span>[<span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]]], comparator: <span class=\"type\">Comparator</span>[<span class=\"type\">K</span>])</span><br><span class=\"line\">    : <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] =</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  <span class=\"keyword\">val</span> bufferedIters = iterators.filter(_.hasNext).map(_.buffered)</span><br><span class=\"line\">  <span class=\"class\"><span class=\"keyword\">type</span> <span class=\"title\">Iter</span> </span>= <span class=\"type\">BufferedIterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]]</span><br><span class=\"line\">  <span class=\"comment\">//选取头元素最小的分区</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> heap = <span class=\"keyword\">new</span> mutable.<span class=\"type\">PriorityQueue</span>[<span class=\"type\">Iter</span>]()(<span class=\"keyword\">new</span> <span class=\"type\">Ordering</span>[<span class=\"type\">Iter</span>] &#123;</span><br><span class=\"line\">    <span class=\"comment\">// Use the reverse of comparator.compare because PriorityQueue dequeues the max</span></span><br><span class=\"line\">    <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compare</span></span>(x: <span class=\"type\">Iter</span>, y: <span class=\"type\">Iter</span>): <span class=\"type\">Int</span> = -comparator.compare(x.head._1, y.head._1)</span><br><span class=\"line\">  &#125;)</span><br><span class=\"line\">  heap.enqueue(bufferedIters: _*) </span><br><span class=\"line\">  <span class=\"keyword\">new</span> <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] &#123;</span><br><span class=\"line\">    <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hasNext</span></span>: <span class=\"type\">Boolean</span> = !heap.isEmpty</span><br><span class=\"line\">    <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">next</span></span>(): <span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>] = &#123;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (!hasNext) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">NoSuchElementException</span></span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> firstBuf = heap.dequeue()</span><br><span class=\"line\">      <span class=\"keyword\">val</span> firstPair = firstBuf.next()</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (firstBuf.hasNext) &#123;</span><br><span class=\"line\">        heap.enqueue(firstBuf)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      firstPair</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>接下来就使用 writeIndexFileAndCommit 将每一个分区的数据条数写入索引文件，便于 reduce 端获取。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>shuflle write 相比起 shuffe read 要复杂很多，因此本文的篇幅也较长。其实在 shuffle write 有两个较为重要的方法，其中一个是 insertAll。这个方法的作用就是将数据缓存到一个可以添加的 hash 表中。如果表占用的内存达到的一定值，就对其进行排序。排序方式先按照分区进行排序，如果分区相同则按key的hash进行排序，随后将排序后的数据写入临时文件中，这个过程可能会生成多个临时文件。</p>\n<p>然后 writePartitionedFile 的作用是最后将内存中的数据和临时文件中的数据进行全部排序，写入到一个大文件中。</p>\n<p>最后将每一个分区对应的索引数据写入文件。整个shuffle write 阶段就完成了</p>\n"}],"PostAsset":[{"_id":"source/_posts/数据挖掘基础-最大似然估计与最大后验估计/func_img.png","post":"cjj08c9nz000pksng0m8atk4m","slug":"func_img.png","modified":1,"renderable":1},{"_id":"source/_posts/使用travis自动部署hexo到github/author-code.png","post":"cjj08c9nm000cksngsxsi1nf3","slug":"author-code.png","modified":1,"renderable":1},{"_id":"source/_posts/使用travis自动部署hexo到github/author.png","post":"cjj08c9nm000cksngsxsi1nf3","slug":"author.png","modified":1,"renderable":1},{"_id":"source/_posts/使用travis自动部署hexo到github/token-add.png","post":"cjj08c9nm000cksngsxsi1nf3","slug":"token-add.png","modified":1,"renderable":1},{"_id":"source/_posts/使用travis自动部署hexo到github/travis-add.png","post":"cjj08c9nm000cksngsxsi1nf3","slug":"travis-add.png","modified":1,"renderable":1},{"_id":"source/_posts/使用travis自动部署hexo到github/travis-mainpage.png","post":"cjj08c9nm000cksngsxsi1nf3","slug":"travis-mainpage.png","modified":1,"renderable":1}],"PostCategory":[{"post_id":"cjj08c9nj0008ksngz14yvt75","category_id":"cjj08c9nf0004ksng27uy7ekv","_id":"cjj08c9nr000eksngk544ayve"},{"post_id":"cjj08c9n70000ksngd8b0qh3f","category_id":"cjj08c9nf0004ksng27uy7ekv","_id":"cjj08c9nv000iksngcsnb1k81"},{"post_id":"cjj08c9nc0002ksng6ebukqyc","category_id":"cjj08c9nl000aksngz0eu27px","_id":"cjj08c9ny000nksnga77huiv5"},{"post_id":"cjj08c9nv000hksngvq8tcheu","category_id":"cjj08c9nf0004ksng27uy7ekv","_id":"cjj08c9o1000sksngrzxqjf4v"},{"post_id":"cjj08c9nh0006ksngu4ndif7o","category_id":"cjj08c9nf0004ksng27uy7ekv","_id":"cjj08c9o3000wksngkqnj3hly"},{"post_id":"cjj08c9nl0009ksng0qobgvrk","category_id":"cjj08c9ny000mksngqb5zttc3","_id":"cjj08c9o4000zksngohu9gw44"},{"post_id":"cjj08c9o2000vksng5uu7z7jp","category_id":"cjj08c9nf0004ksng27uy7ekv","_id":"cjj08c9o50012ksngzt510gn8"},{"post_id":"cjj08c9nm000cksngsxsi1nf3","category_id":"cjj08c9ny000mksngqb5zttc3","_id":"cjj08c9o60016ksngsjsfdkb3"},{"post_id":"cjj08c9np000dksngathterys","category_id":"cjj08c9ny000mksngqb5zttc3","_id":"cjj08c9o70018ksngaj37bxff"},{"post_id":"cjj08c9nx000kksngoer0230f","category_id":"cjj08c9o60014ksng20vxucxz","_id":"cjj08c9o8001dksngub7zg1zw"},{"post_id":"cjj08c9nz000pksng0m8atk4m","category_id":"cjj08c9o7001bksngpzzdqv93","_id":"cjj08c9o9001jksngwh36yqsv"},{"post_id":"cjj08c9o0000rksngaojsz4g6","category_id":"cjj08c9o7001bksngpzzdqv93","_id":"cjj08c9oa001lksngox3b2t10"}],"PostTag":[{"post_id":"cjj08c9n70000ksngd8b0qh3f","tag_id":"cjj08c9nh0005ksngcq4gvanr","_id":"cjj08c9nw000jksngftcu079e"},{"post_id":"cjj08c9n70000ksngd8b0qh3f","tag_id":"cjj08c9nm000bksng3ytymuai","_id":"cjj08c9nx000lksngb3sl4rnr"},{"post_id":"cjj08c9nc0002ksng6ebukqyc","tag_id":"cjj08c9nh0005ksngcq4gvanr","_id":"cjj08c9nz000qksng7n3gks38"},{"post_id":"cjj08c9o2000vksng5uu7z7jp","tag_id":"cjj08c9nh0005ksngcq4gvanr","_id":"cjj08c9o50010ksngvsowz53w"},{"post_id":"cjj08c9o2000vksng5uu7z7jp","tag_id":"cjj08c9nm000bksng3ytymuai","_id":"cjj08c9o50011ksnggi15hn7f"},{"post_id":"cjj08c9nh0006ksngu4ndif7o","tag_id":"cjj08c9nh0005ksngcq4gvanr","_id":"cjj08c9o60015ksngisnsyrn6"},{"post_id":"cjj08c9nh0006ksngu4ndif7o","tag_id":"cjj08c9nm000bksng3ytymuai","_id":"cjj08c9o60017ksnggy1lnea8"},{"post_id":"cjj08c9nj0008ksngz14yvt75","tag_id":"cjj08c9nh0005ksngcq4gvanr","_id":"cjj08c9o7001aksngn8h65p3i"},{"post_id":"cjj08c9nj0008ksngz14yvt75","tag_id":"cjj08c9nm000bksng3ytymuai","_id":"cjj08c9o7001cksng6qksgq9n"},{"post_id":"cjj08c9nl0009ksng0qobgvrk","tag_id":"cjj08c9o60013ksngla391558","_id":"cjj08c9o8001gksngqq0mxiwo"},{"post_id":"cjj08c9nl0009ksng0qobgvrk","tag_id":"cjj08c9o70019ksng1vq0p8i8","_id":"cjj08c9o9001hksng1k1krsel"},{"post_id":"cjj08c9nm000cksngsxsi1nf3","tag_id":"cjj08c9o60013ksngla391558","_id":"cjj08c9ob001mksngpbd91p62"},{"post_id":"cjj08c9nm000cksngsxsi1nf3","tag_id":"cjj08c9o70019ksng1vq0p8i8","_id":"cjj08c9ob001nksngaojwg7oi"},{"post_id":"cjj08c9np000dksngathterys","tag_id":"cjj08c9o60013ksngla391558","_id":"cjj08c9oc001qksngc5a9z12s"},{"post_id":"cjj08c9np000dksngathterys","tag_id":"cjj08c9o70019ksng1vq0p8i8","_id":"cjj08c9od001rksngu99e85p7"},{"post_id":"cjj08c9np000dksngathterys","tag_id":"cjj08c9nm000bksng3ytymuai","_id":"cjj08c9od001tksngryfptjzg"},{"post_id":"cjj08c9np000dksngathterys","tag_id":"cjj08c9nh0005ksngcq4gvanr","_id":"cjj08c9oe001uksngyne2jk21"},{"post_id":"cjj08c9nv000hksngvq8tcheu","tag_id":"cjj08c9o70019ksng1vq0p8i8","_id":"cjj08c9oe001wksng9ufydgos"},{"post_id":"cjj08c9nv000hksngvq8tcheu","tag_id":"cjj08c9o60013ksngla391558","_id":"cjj08c9of001xksngqm4iszho"},{"post_id":"cjj08c9nx000kksngoer0230f","tag_id":"cjj08c9oe001vksng0irysuq2","_id":"cjj08c9of001zksnge1e10let"},{"post_id":"cjj08c9nx000kksngoer0230f","tag_id":"cjj08c9nm000bksng3ytymuai","_id":"cjj08c9of0020ksngeo4kuhjh"},{"post_id":"cjj08c9nz000pksng0m8atk4m","tag_id":"cjj08c9of001yksngsyn6ssh3","_id":"cjj08c9oj0023ksnggu0x76ev"},{"post_id":"cjj08c9nz000pksng0m8atk4m","tag_id":"cjj08c9og0021ksngxce36c2p","_id":"cjj08c9oj0024ksnglt91nhry"},{"post_id":"cjj08c9o0000rksngaojsz4g6","tag_id":"cjj08c9of001yksngsyn6ssh3","_id":"cjj08c9ol0026ksngr1d9irp9"},{"post_id":"cjj08c9o0000rksngaojsz4g6","tag_id":"cjj08c9og0021ksngxce36c2p","_id":"cjj08c9ol0027ksngkoyuc900"}],"Tag":[{"name":"Spark","_id":"cjj08c9nh0005ksngcq4gvanr"},{"name":"编程","_id":"cjj08c9nm000bksng3ytymuai"},{"name":"工具","_id":"cjj08c9o60013ksngla391558"},{"name":"教程","_id":"cjj08c9o70019ksng1vq0p8i8"},{"name":"Linux","_id":"cjj08c9oe001vksng0irysuq2"},{"name":"数据挖掘","_id":"cjj08c9of001yksngsyn6ssh3"},{"name":"机器学习","_id":"cjj08c9og0021ksngxce36c2p"}]}}