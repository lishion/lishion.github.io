{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/next-gux/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next-gux/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next-gux/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next-gux/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next-gux/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next-gux/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next-gux/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next-gux/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next-gux/source/images/loading.gif","path":"images/loading.gif","modified":0,"renderable":1},{"_id":"themes/next-gux/source/images/default_avatar.jpg","path":"images/default_avatar.jpg","modified":0,"renderable":1},{"_id":"themes/next-gux/source/images/placeholder.gif","path":"images/placeholder.gif","modified":0,"renderable":1},{"_id":"themes/next-gux/source/images/searchicon.png","path":"images/searchicon.png","modified":0,"renderable":1},{"_id":"themes/next-gux/source/js/bootstrap.scrollspy.js","path":"js/bootstrap.scrollspy.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/js/fancy-box.js","path":"js/fancy-box.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/js/helpers.js","path":"js/helpers.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/js/hook-duoshuo.js","path":"js/hook-duoshuo.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/js/lazyload.js","path":"js/lazyload.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/js/motion_fallback.js","path":"js/motion_fallback.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/js/nav-toggle.js","path":"js/nav-toggle.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/js/motion_global.js","path":"js/motion_global.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/js/ua-parser.min.js","path":"js/ua-parser.min.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/images/background.jpg","path":"images/background.jpg","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-default/icomoon.eot","path":"fonts/icon-default/icomoon.eot","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-default/icomoon.ttf","path":"fonts/icon-default/icomoon.ttf","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-default/icomoon.svg","path":"fonts/icon-default/icomoon.svg","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-default/icomoon.woff","path":"fonts/icon-default/icomoon.woff","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-default/selection.json","path":"fonts/icon-default/selection.json","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-feather/icomoon.eot","path":"fonts/icon-feather/icomoon.eot","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-feather/icomoon.svg","path":"fonts/icon-feather/icomoon.svg","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-feather/icomoon.ttf","path":"fonts/icon-feather/icomoon.ttf","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-feather/icomoon.woff","path":"fonts/icon-feather/icomoon.woff","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-feather/selection.json","path":"fonts/icon-feather/selection.json","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-fifty-shades/icomoon.eot","path":"fonts/icon-fifty-shades/icomoon.eot","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-fifty-shades/icomoon.svg","path":"fonts/icon-fifty-shades/icomoon.svg","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-fifty-shades/icomoon.ttf","path":"fonts/icon-fifty-shades/icomoon.ttf","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-fifty-shades/icomoon.woff","path":"fonts/icon-fifty-shades/icomoon.woff","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-fifty-shades/selection.json","path":"fonts/icon-fifty-shades/selection.json","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-icomoon/icomoon.eot","path":"fonts/icon-icomoon/icomoon.eot","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-icomoon/icomoon.svg","path":"fonts/icon-icomoon/icomoon.svg","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-icomoon/icomoon.woff","path":"fonts/icon-icomoon/icomoon.woff","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-icomoon/icomoon.ttf","path":"fonts/icon-icomoon/icomoon.ttf","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/fastclick/LICENSE","path":"vendors/fastclick/LICENSE","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/fastclick/README.md","path":"vendors/fastclick/README.md","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/fastclick/bower.json","path":"vendors/fastclick/bower.json","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-linecons/icomoon.eot","path":"fonts/icon-linecons/icomoon.eot","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-linecons/icomoon.svg","path":"fonts/icon-linecons/icomoon.svg","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-linecons/icomoon.ttf","path":"fonts/icon-linecons/icomoon.ttf","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-linecons/selection.json","path":"fonts/icon-linecons/selection.json","modified":0,"renderable":1},{"_id":"themes/next-gux/source/fonts/icon-linecons/icomoon.woff","path":"fonts/icon-linecons/icomoon.woff","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/velocity/bower.json","path":"vendors/velocity/bower.json","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/velocity/velocity.min.js","path":"vendors/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/velocity/velocity.ui.min.js","path":"vendors/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/velocity/velocity.ui.js","path":"vendors/velocity/velocity.ui.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/jquery/index.js","path":"vendors/jquery/index.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/blank.gif","path":"vendors/fancybox/source/blank.gif","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/fancybox_overlay.png","path":"vendors/fancybox/source/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/fancybox_loading.gif","path":"vendors/fancybox/source/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/fancybox_loading@2x.gif","path":"vendors/fancybox/source/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/fancybox_sprite.png","path":"vendors/fancybox/source/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/fancybox_sprite@2x.png","path":"vendors/fancybox/source/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/jquery.fancybox.css","path":"vendors/fancybox/source/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/jquery.fancybox.pack.js","path":"vendors/fancybox/source/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/jquery.fancybox.js","path":"vendors/fancybox/source/jquery.fancybox.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/fastclick/lib/fastclick.min.js","path":"vendors/fastclick/lib/fastclick.min.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/fastclick/lib/fastclick.js","path":"vendors/fastclick/lib/fastclick.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/velocity/velocity.js","path":"vendors/velocity/velocity.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/helpers/fancybox_buttons.png","path":"vendors/fancybox/source/helpers/fancybox_buttons.png","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"vendors/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/helpers/jquery.fancybox-media.js","path":"vendors/fancybox/source/helpers/jquery.fancybox-media.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"vendors/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"vendors/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":0,"renderable":1},{"_id":"themes/next-gux/source/vendors/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"vendors/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":0,"renderable":1}],"Cache":[{"_id":"themes/next-gux/.bowerrc","hash":"80e096fdc1cf912ee85dd9f7e6e77fd40cf60f10","modified":1529475725783},{"_id":"themes/next-gux/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1529475725783},{"_id":"themes/next-gux/.gitignore","hash":"689a812947ccb5de9ed3ecdc5f927030adfd1d7a","modified":1529475725783},{"_id":"themes/next-gux/.jshintrc","hash":"12c5e37da3432bee2219ed1c667076d54f1639c0","modified":1529475725783},{"_id":"themes/next-gux/README.md","hash":"57c54b159185e346012669a5595b72a5b31cc91f","modified":1529475725783},{"_id":"themes/next-gux/_config.yml","hash":"eebb0388f030d33d8e25e7e3426cb5342d7a0fa0","modified":1530582823739},{"_id":"themes/next-gux/_config.yml.bak","hash":"f18ce03aaee919abef64fd6df1772935b577c3e0","modified":1529475725783},{"_id":"themes/next-gux/bower.json","hash":"057ec1580f78a7adae66bd26fe9e9f924621174f","modified":1529475725783},{"_id":"source/_posts/Spark-源码阅读计划-伪-第三部分-图解-shuffle-write.md","hash":"9fba0917b9c65919a4b28035998a9bdaa9ec8c15","modified":1529475725779},{"_id":"source/_posts/Spark优化-减少分区.md","hash":"154052f1180f8854af49bbb6c001132efb0c76f0","modified":1529475725783},{"_id":"source/_posts/Spark源码阅读计划-第四部分-shuffle-read.md","hash":"23c5f9a5f2d2b50ddd4467be9846a4b493210765","modified":1529475725783},{"_id":"source/_posts/使用git+hexo建立个人博客.md","hash":"8ea88d51903561b09c5dc4a6048d16151603c5f7","modified":1529475725783},{"_id":"source/_posts/Spark-源码阅读计划-第一部分-迭代计算.md","hash":"1e2c475b21077ccad1516bcc4fcb343ab6cdbbe7","modified":1529475725783},{"_id":"source/_posts/使用travis自动部署hexo到github.md","hash":"7d231be537f78b74a5494a3a3f85bd208d4705ce","modified":1530583504180},{"_id":"source/_posts/利用idea搭建spark-shell开发环境.md","hash":"26aaa1855aab7713c18d80f996a34efc048f599f","modified":1529652422810},{"_id":"source/_posts/在hexo中书写latex公式.md","hash":"94e3c231361bff3f02b3652bbf1e080734fce5b1","modified":1530582823735},{"_id":"source/_posts/常用-linux-命令系列.md","hash":"ada7baa32728dfd75eb82e481cb859d28436f0d8","modified":1529475725783},{"_id":"source/_posts/异常检测-Mass-estimation.md","hash":"872380205d27ea419ef42faabe4f0ac0cef036a6","modified":1530603206145},{"_id":"source/_posts/异常检测-体验孤立森林.md","hash":"0b7703ddf563e0c22c207c3bb1f0b7fd7f9284f4","modified":1531379846531},{"_id":"source/_posts/异常检测-孤立森林.md","hash":"ebb271c4288a2855ccd00c4f2e97b141ee6e5b9d","modified":1530598635457},{"_id":"source/_posts/异常检测-流检测.md","hash":"9b932fc402a4ac00f0ecdbdda67ab424fffe73ab","modified":1530612981422},{"_id":"source/_posts/数据挖掘基础-决策树.md","hash":"54743b0bd7105bdc565cdff70b5ffaef4face735","modified":1530582823735},{"_id":"source/_posts/数据挖掘基础-最大似然估计与最大后验估计.md","hash":"87f1413853d8fd5296779690481d30db572c2589","modified":1530582823739},{"_id":"source/_posts/数据挖掘基础-熵.md","hash":"9f99409afeb79249c940f4216c4c245525340fc5","modified":1530582823739},{"_id":"source/about/index.md","hash":"e9ca55c635160ccd3b41404fc77c6d4595573222","modified":1529475725783},{"_id":"source/_posts/源码阅读计划-第二部分-shuffle-write.md","hash":"a0fa36faf8445b4d86c549890ddf15aa15e28f11","modified":1529475725783},{"_id":"source/tags/index.md","hash":"7ccafacf77c140dbb2f677b91362795c96f3ea77","modified":1529475725783},{"_id":"source/categories/index.md","hash":"cccec3d72c25ca05444985af5a716760c86570d6","modified":1529475725783},{"_id":"themes/next-gux/.idea/compiler.xml","hash":"bff5196ea91a033d64bb5c4d6647a2e3b71bb548","modified":1529475725783},{"_id":"themes/next-gux/.idea/misc.xml","hash":"318a7276f979d7822e16d305da5f7c939a00852d","modified":1529475725783},{"_id":"themes/next-gux/.idea/modules.xml","hash":"64ff052d1dae024fe94ea8e2b1d7f704609f22a6","modified":1529475725783},{"_id":"themes/next-gux/.idea/next-guxiangfly.iml","hash":"980957b57c4f1eae5e85d664d8375f83d47d3e5a","modified":1529475725783},{"_id":"themes/next-gux/.idea/vcs.xml","hash":"6f94fc1df9e8721673d47588ac444667dc9ded06","modified":1529475725783},{"_id":"themes/next-gux/.idea/workspace.xml","hash":"a73aef09b7d8a062a38f2a2f6f0242f91844846b","modified":1529475725783},{"_id":"themes/next-gux/languages/default.yml","hash":"7e65ef918f16d0189055deb5f1616b9dedcb1920","modified":1529475725783},{"_id":"themes/next-gux/languages/fr-FR.yml","hash":"6d097445342a9fb5235afea35d65bf5271b772f0","modified":1529475725783},{"_id":"themes/next-gux/languages/ru.yml","hash":"b4a827b9ddac9d5f6dca096fe513aeafb46a3e93","modified":1529475725783},{"_id":"themes/next-gux/languages/de.yml","hash":"7a8de0e5665c52a1bf168c1e7dd222c8a74fb0ab","modified":1529475725783},{"_id":"themes/next-gux/languages/en.yml","hash":"7e65ef918f16d0189055deb5f1616b9dedcb1920","modified":1529475725783},{"_id":"themes/next-gux/languages/zh-Hans.yml","hash":"8af76df5557561050a950bdd7091d3bb3939c5c0","modified":1529475725783},{"_id":"themes/next-gux/languages/zh-hk.yml","hash":"3fc38103c9efa6f6c37149adbddb014ff85ec849","modified":1529475725783},{"_id":"themes/next-gux/languages/zh-tw.yml","hash":"8897a06e521b36c7a1226c72057c8357611eded8","modified":1529475725783},{"_id":"themes/next-gux/layout/_layout.swig","hash":"cbf2a45d0aca11965f083a732e482141f3f0b1c0","modified":1529475725783},{"_id":"themes/next-gux/layout/archive.swig","hash":"0c3ce594759f347ea90a4ce592a7a18e2ae4cc5c","modified":1529475725787},{"_id":"themes/next-gux/layout/category.swig","hash":"d6b3e1dc5e0b8deade9a084c463126e70188ee9b","modified":1529475725787},{"_id":"themes/next-gux/layout/index.swig","hash":"fdc801f0da71a2eb205ce9c0b12f156b219fdc9c","modified":1529475725787},{"_id":"themes/next-gux/scripts/merge-configs.js","hash":"dfd147d1317e56d283f5e779f00608e913603b51","modified":1529475725787},{"_id":"themes/next-gux/layout/post.swig","hash":"d1fe5e273d5852bbc5009cc1df629248fee54df1","modified":1529475725787},{"_id":"themes/next-gux/layout/page.swig","hash":"beb1fc9a4e35b602a18b59f895544c6a838a67f2","modified":1529475725787},{"_id":"themes/next-gux/layout/tag.swig","hash":"aab44af54fcbc66fea4ad12b2767ffca3eadd451","modified":1529475725787},{"_id":"themes/next-gux/test/.jshintrc","hash":"096ed6df627373edd820f24d46b8baf528dee61d","modified":1529475725795},{"_id":"themes/next-gux/test/helpers.js","hash":"7c8b0c7213ae06ec4e7948971f9b12842207b5c7","modified":1529475725799},{"_id":"themes/next-gux/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1529475725799},{"_id":"source/_posts/Spark-源码阅读计划-伪-第三部分-图解-shuffle-read/part.png","hash":"b2f702f44036d20b43eedec75b1cbbe0a06d5877","modified":1529475725779},{"_id":"source/_posts/Spark-源码阅读计划-伪-第三部分-图解-shuffle-read/sort.png","hash":"2fc1a5957fc0bfc7cf3027e38759fa3c65077768","modified":1529475725779},{"_id":"source/_posts/使用travis自动部署hexo到github/author-code.png","hash":"ca07f759bb39a3fe8ec9a5f78209c7043881051f","modified":1529650407384},{"_id":"source/_posts/使用travis自动部署hexo到github/author.png","hash":"ae2cf78724ebf56138d98e26c609905206634f81","modified":1529650407384},{"_id":"source/_posts/异常检测-体验孤立森林/normal_data.png","hash":"3f68151891f9cc8637fbd1b5107abec3708bc8c8","modified":1531362778294},{"_id":"source/_posts/数据挖掘基础-决策树/continue.png","hash":"fb39ceab7e300c825e2aff1a4d50f1f2141d2c24","modified":1530582823735},{"_id":"source/_posts/数据挖掘基础-决策树/cut.png","hash":"5180d8520aed5724fc6ced161742956443c1a450","modified":1530582823739},{"_id":"source/_posts/数据挖掘基础-决策树/discrete.png","hash":"e6880fb406a41b5331680a0ebd39d72851d3f713","modified":1530582823739},{"_id":"source/_posts/数据挖掘基础-决策树/tree.png","hash":"d60b9ed461b80f1cbaa9e8ee16e72281ded47f49","modified":1530582823739},{"_id":"source/_posts/数据挖掘基础-最大似然估计与最大后验估计/func_img.png","hash":"374f722fb3fde7054d7e6264e2dfd8a62fbff710","modified":1530582823739},{"_id":"source/_posts/数据挖掘基础-决策树/未命名文件 (1).png","hash":"fb39ceab7e300c825e2aff1a4d50f1f2141d2c24","modified":1530582823739},{"_id":"themes/next-gux/.idea/inspectionProfiles/Project_Default.xml","hash":"e1a86ce90b80bedfa05ec86db802b187d973f133","modified":1529475725783},{"_id":"themes/next-gux/layout/_partials/footer.swig","hash":"f25e080de405dd5db5b85366eb9257eee5997dce","modified":1529475725783},{"_id":"themes/next-gux/layout/_partials/footer.swig.bak","hash":"5cbf9456977c2087238bfcc47623327fb0e57f20","modified":1529475725783},{"_id":"themes/next-gux/layout/_partials/head.swig","hash":"0187e02567897c42027803f80f54fe3b92b949d3","modified":1530582823739},{"_id":"themes/next-gux/layout/_partials/header.swig","hash":"e66b8fca801d5daba31496d4b00bac3018b7c29b","modified":1529475725783},{"_id":"themes/next-gux/layout/_partials/old-browsers.swig","hash":"dbbfea810bf3a2ed9c83b9a6683037175aacfc67","modified":1529475725783},{"_id":"themes/next-gux/layout/_partials/pagination.swig","hash":"d6c7f04eee4388d8f133eb5526b7c0875c321a30","modified":1529475725783},{"_id":"themes/next-gux/layout/_partials/search.swig","hash":"64f14da26792a17bc27836c4e9d83190175f36e6","modified":1529475725783},{"_id":"themes/next-gux/layout/_scripts/analytics.swig","hash":"0ebbf76c2317faa8ba31365adba59331c2e0262c","modified":1529475725787},{"_id":"themes/next-gux/layout/_scripts/bootstrap.scrollspy.swig","hash":"85295f126836b95f0837d03e58228bb3cf8c4490","modified":1529475725787},{"_id":"themes/next-gux/layout/_scripts/fancy-box.swig","hash":"41b4ff1446060c88c33bf666a32277dcf12129f0","modified":1529475725787},{"_id":"themes/next-gux/layout/_scripts/helpers.swig","hash":"4d2cbfca0aaf546a2b5813288073e824c1498fdf","modified":1529475725787},{"_id":"themes/next-gux/layout/_scripts/motion.swig","hash":"817705bfd1a1282cb6bf59094afe507e11455aa0","modified":1529475725787},{"_id":"themes/next-gux/layout/_scripts/mathjax.swig","hash":"abc52fefb276c52cbb19de5c214521dfcf2a10fd","modified":1529475725787},{"_id":"themes/next-gux/layout/_macro/post-collapse.swig","hash":"42927bdde998cefd3cf4f19b0476d69bd9e5116a","modified":1529475725783},{"_id":"themes/next-gux/layout/_macro/post.swig","hash":"770c6ed5562205ed2b64d4c02f5418893144a9ff","modified":1529475725783},{"_id":"themes/next-gux/layout/_macro/sidebar.swig","hash":"7af60c855c060c5318df7264eb6860a7fbb7c3ce","modified":1529475725783},{"_id":"themes/next-gux/scripts/tags/center-quote.js","hash":"37274f743c2054244dcbbde56fba9ff353414281","modified":1529475725787},{"_id":"themes/next-gux/scripts/tags/full-image.js","hash":"0d69739d1bad5861a4a6ff2db511c3669783e438","modified":1529475725787},{"_id":"themes/next-gux/source/css/main.styl","hash":"151dccbe683e6a858d8a6ea09df913a2344b417f","modified":1529475725791},{"_id":"themes/next-gux/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1529475725791},{"_id":"themes/next-gux/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1529475725791},{"_id":"themes/next-gux/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1529475725791},{"_id":"themes/next-gux/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1529475725791},{"_id":"themes/next-gux/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1529475725791},{"_id":"themes/next-gux/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1529475725791},{"_id":"themes/next-gux/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1529475725791},{"_id":"themes/next-gux/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1529475725791},{"_id":"themes/next-gux/source/images/default_avatar.jpg","hash":"930cf7a3be73cd08cbd2ba3f2ae8bee564bad227","modified":1529475725791},{"_id":"themes/next-gux/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1529475725791},{"_id":"themes/next-gux/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1529475725791},{"_id":"themes/next-gux/source/js/bootstrap.scrollspy.js","hash":"ae7bdce88b515aade4eea8bf7407eec458bcd625","modified":1529475725795},{"_id":"themes/next-gux/source/js/fancy-box.js","hash":"b382ba746f4566682948ce92f2588ee940cd1755","modified":1529475725795},{"_id":"themes/next-gux/source/js/helpers.js","hash":"7499b413242a2e75a9308444aade5b72a12cce7d","modified":1529475725795},{"_id":"themes/next-gux/source/js/hook-duoshuo.js","hash":"9881b19132ad90dffd82c53947e4c356e30353e7","modified":1529475725795},{"_id":"themes/next-gux/source/js/lazyload.js","hash":"b92e9acdc7afc15468314c03f4a643b0c93944cf","modified":1529475725795},{"_id":"themes/next-gux/source/js/motion_fallback.js","hash":"a767d522c65a8b2fbad49135c9332135c6785c3e","modified":1529475725795},{"_id":"themes/next-gux/source/js/nav-toggle.js","hash":"78b59f1beb12adea0d7f9bcf4377cb699963f220","modified":1529475725795},{"_id":"themes/next-gux/source/js/motion_global.js","hash":"fea8cbb854601b7aee14e51079b3e3f80a1de261","modified":1529475725795},{"_id":"themes/next-gux/source/js/ua-parser.min.js","hash":"acf0ee6a47ffb7231472b56e43996e3f947c258a","modified":1529475725795},{"_id":"source/_posts/使用travis自动部署hexo到github/token-add.png","hash":"cf6aa24ed57ef160636765d5cc04a39b64e56154","modified":1529650407388},{"_id":"source/_posts/使用travis自动部署hexo到github/travis-add.png","hash":"790573634ebe578cd737ae099a4c93e06deee237","modified":1529650407388},{"_id":"source/_posts/使用travis自动部署hexo到github/travis-mainpage.png","hash":"667243094bd3f95e95285e58a26dd881ed7a0b6c","modified":1529650407388},{"_id":"themes/next-gux/source/css/_mixins/Mala.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1529475725791},{"_id":"themes/next-gux/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1529475725791},{"_id":"themes/next-gux/source/css/_mixins/default.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1529475725791},{"_id":"themes/next-gux/source/css/_variables/default.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1529475725791},{"_id":"themes/next-gux/source/images/background.jpg","hash":"6d3eac15433a762c0dd07b58fe81d555ceb62422","modified":1529475725791},{"_id":"themes/next-gux/layout/_partials/search/swiftype.swig","hash":"00c2b49f6289198b0b2b4e157e4ee783277f32a7","modified":1529475725783},{"_id":"themes/next-gux/layout/_partials/search/tinysou.swig","hash":"2f92046e0b50ebd65abb7045b1cbbfc50abbb034","modified":1529475725783},{"_id":"themes/next-gux/layout/_partials/share/baidu_share.swig","hash":"b4506174e385ee5fb1c94122b45732e3413a0ba2","modified":1529475725787},{"_id":"themes/next-gux/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1529475725787},{"_id":"themes/next-gux/layout/_scripts/analytics/baidu-analytics.swig","hash":"7c43d66da93cde65b473a7d6db2a86f9a42647d6","modified":1529475725787},{"_id":"themes/next-gux/layout/_partials/share/jiathis.swig","hash":"63315fcf210799f894208c9f512737096df84962","modified":1529475725787},{"_id":"themes/next-gux/layout/_scripts/analytics/busuanzi.swig","hash":"dee5f8ce80fc34fa2b0c914a45465c79da80612b","modified":1529475725787},{"_id":"themes/next-gux/layout/_scripts/analytics/google-analytics.swig","hash":"30a23fa7e816496fdec0e932aa42e2d13098a9c2","modified":1529475725787},{"_id":"themes/next-gux/layout/_partials/suprise/assist.swig","hash":"6b8a25353dbfe9f92e0d48388a6f46996e03b7cb","modified":1529475725787},{"_id":"themes/next-gux/layout/_partials/suprise/donate.swig","hash":"25f196afc193a7b192a49cb7d84db7d727a9e8c2","modified":1529475725787},{"_id":"themes/next-gux/layout/_scripts/comments/disqus.swig","hash":"3491d3cebabc8a28857200db28a1be65aad6adc2","modified":1529475725787},{"_id":"themes/next-gux/layout/_scripts/comments/duoshuo.swig","hash":"3351ea62225933f8045d036a79654e19e84d19a7","modified":1529475725787},{"_id":"themes/next-gux/layout/_scripts/pages/post-details.swig","hash":"b63ef233886538f30ced60344ac15d25e5f3e0af","modified":1529475725787},{"_id":"themes/next-gux/source/css/_mixins/base.styl","hash":"66985fe77bd323f7f8f634908e17166f51e96e95","modified":1529475725791},{"_id":"themes/next-gux/source/css/_custom/Mala.styl","hash":"ec81ad093b68bbb121e45d054a646f32397e137d","modified":1529475725791},{"_id":"themes/next-gux/source/css/_custom/Mala.styl.bak","hash":"4ec83e3e4ef02e67dca86615aa013c0b8a4f4b18","modified":1529475725791},{"_id":"themes/next-gux/source/css/_variables/Mala.styl","hash":"360aaa1746bc4032079493ff6027f8431f65b6df","modified":1529475725791},{"_id":"themes/next-gux/source/css/_variables/base.styl","hash":"fb85e5d8e37661c4c333b8aa08a7619cd7b3d046","modified":1529475725791},{"_id":"themes/next-gux/source/css/_variables/custom.styl","hash":"1a3e002602b0dff287b2463d2cd25c22f349a145","modified":1529475725791},{"_id":"themes/next-gux/source/fonts/icon-default/icomoon.eot","hash":"90763e97be18be78e65749075225cceeddc6fa8a","modified":1529475725791},{"_id":"themes/next-gux/source/fonts/icon-default/icomoon.ttf","hash":"c093408e6030221cafc1f79d897f1fb5283c1178","modified":1529475725791},{"_id":"themes/next-gux/source/fonts/icon-default/icomoon.svg","hash":"f92ad8cddc250f0bb5ca466fca95d321987e127e","modified":1529475725791},{"_id":"themes/next-gux/source/fonts/icon-default/icomoon.woff","hash":"dbe0368f2a65d87b13234cfea29d9783892fc7a8","modified":1529475725791},{"_id":"themes/next-gux/source/fonts/icon-default/selection.json","hash":"dc07c29f687315f9458f6b251c214768af865fb2","modified":1529475725791},{"_id":"themes/next-gux/source/fonts/icon-feather/icomoon.eot","hash":"11554b9e9d5b9f535ba96cbb27d45d8c8f1689fd","modified":1529475725791},{"_id":"themes/next-gux/source/fonts/icon-feather/icomoon.svg","hash":"d5eb756eefda9b454dcb23c2b1cefd4051d18d41","modified":1529475725791},{"_id":"themes/next-gux/source/fonts/icon-feather/icomoon.ttf","hash":"b2bbae4b613403cf61ad25037913378da1c07b8f","modified":1529475725791},{"_id":"themes/next-gux/source/fonts/icon-feather/icomoon.woff","hash":"2ea1c59c17422798e64ee6f4e9ce1f7aff1a06a5","modified":1529475725791},{"_id":"themes/next-gux/source/fonts/icon-feather/selection.json","hash":"06ea91e3f98ebe1080087acad4356802bc5b6ebf","modified":1529475725791},{"_id":"themes/next-gux/source/fonts/icon-fifty-shades/icomoon.eot","hash":"da86ba5df72d1288de9e9633e5f528062dd427d5","modified":1529475725791},{"_id":"themes/next-gux/source/fonts/icon-fifty-shades/icomoon.svg","hash":"1a4afd739e1f8eb8d430dbdd29e36a9999802e8d","modified":1529475725791},{"_id":"themes/next-gux/source/fonts/icon-fifty-shades/icomoon.ttf","hash":"72fe82e1f3db52414eed706952d385af241cb196","modified":1529475725791},{"_id":"themes/next-gux/source/fonts/icon-fifty-shades/icomoon.woff","hash":"4de6a74f523dee33d95dde61caae5809f9a5d448","modified":1529475725791},{"_id":"themes/next-gux/source/fonts/icon-fifty-shades/selection.json","hash":"fdd09098d1c3688e2c88cf33fd51e76b383b6d7f","modified":1529475725791},{"_id":"themes/next-gux/source/fonts/icon-icomoon/icomoon.eot","hash":"301fcf00c24750dddf1c529f944ca62c7f1a217d","modified":1529475725791},{"_id":"themes/next-gux/source/fonts/icon-icomoon/icomoon.svg","hash":"e316347805eb93425faa678611c5e42a7152da8f","modified":1529475725791},{"_id":"themes/next-gux/source/fonts/icon-icomoon/icomoon.woff","hash":"05f1ec0bd307da5e731a86eb4961589a6042aebb","modified":1529475725791},{"_id":"themes/next-gux/source/fonts/icon-icomoon/icomoon.ttf","hash":"f399713d1c9400d4d3373e38991a7e362a754a94","modified":1529475725791},{"_id":"themes/next-gux/source/vendors/fastclick/.bower.json","hash":"93ebd5b35e632f714dcf1753e1f6db77ec74449b","modified":1529475725795},{"_id":"themes/next-gux/source/vendors/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1529475725795},{"_id":"themes/next-gux/source/vendors/fastclick/README.md","hash":"1decd8e1adad2cd6db0ab50cf56de6035156f4ea","modified":1529475725795},{"_id":"themes/next-gux/source/vendors/fastclick/bower.json","hash":"13379463c7463b4b96d13556b46faa4cc38d81e6","modified":1529475725795},{"_id":"themes/next-gux/source/fonts/icon-linecons/icomoon.eot","hash":"e2d7f040428a632f3c50bfa94083b759936effc2","modified":1529475725791},{"_id":"themes/next-gux/source/fonts/icon-linecons/icomoon.svg","hash":"808eaf7d61f7e67c76976265c885e79c36920f0b","modified":1529475725791},{"_id":"themes/next-gux/source/fonts/icon-linecons/icomoon.ttf","hash":"078068206684e4f185b0187ad3cee16f54a287d7","modified":1529475725791},{"_id":"themes/next-gux/source/vendors/jquery/.bower.json","hash":"91745c2cc6c946c7275f952b2b0760b880cea69e","modified":1529475725795},{"_id":"themes/next-gux/source/fonts/icon-linecons/selection.json","hash":"db4ce25d31449ecc6685b32e145252103967bb5c","modified":1529475725791},{"_id":"themes/next-gux/source/fonts/icon-linecons/icomoon.woff","hash":"0b07ee6ceda3b1bceb40c1e7379b3aa48dcc15a8","modified":1529475725791},{"_id":"themes/next-gux/source/vendors/velocity/.bower.json","hash":"05f960846f1c7a93dab1d3f9a1121e86812e8c88","modified":1529475725795},{"_id":"themes/next-gux/source/vendors/velocity/bower.json","hash":"2ec99573e84c7117368beccb9e94b6bf35d2db03","modified":1529475725795},{"_id":"themes/next-gux/source/vendors/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1529475725795},{"_id":"themes/next-gux/source/vendors/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1529475725795},{"_id":"themes/next-gux/source/vendors/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1529475725795},{"_id":"themes/next-gux/source/css/_common/_page/home.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1529475725791},{"_id":"themes/next-gux/source/vendors/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1529475725795},{"_id":"themes/next-gux/source/css/_common/_component/back-to-top.styl","hash":"88cd66910260006aa8e9e795df4948d4b67bfa11","modified":1529475725787},{"_id":"themes/next-gux/source/css/_common/_component/buttons.styl","hash":"81063e0979f04a0f9af37f321d7321dda9abf593","modified":1529475725787},{"_id":"themes/next-gux/source/css/_common/_component/duoshuo.styl","hash":"c307f1e4827d7cb82816a5f9de109ae14ed4199c","modified":1529475725787},{"_id":"themes/next-gux/source/css/_common/_component/comments.styl","hash":"b468e452f29df359957731ee8846e165aef13b3d","modified":1529475725787},{"_id":"themes/next-gux/source/css/_common/_component/gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1529475725787},{"_id":"themes/next-gux/source/css/_common/_component/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1529475725787},{"_id":"themes/next-gux/source/css/_common/_component/pagination.styl","hash":"711c8830886619d4f4a0598b0cde5499dce50c62","modified":1529475725787},{"_id":"themes/next-gux/source/css/_common/_component/posts-collapse.styl","hash":"8f9e8f5f65956ccf1d52ff8526392803dff579d3","modified":1529475725787},{"_id":"themes/next-gux/source/css/_common/_component/posts-type.styl","hash":"40b593134bf96d1d6095b3439d47820659d7f10b","modified":1529475725787},{"_id":"themes/next-gux/source/css/_common/_component/posts-expand.styl","hash":"e5d24cc3b5486d1c24080161f2ea1d44e6bbcbb9","modified":1529475725787},{"_id":"themes/next-gux/source/css/_common/_component/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1529475725787},{"_id":"themes/next-gux/source/css/_common/_core/base.styl","hash":"e79a08484b191dca14ccfc005053eb95786dafae","modified":1529475725787},{"_id":"themes/next-gux/source/css/_common/_core/helpers.styl","hash":"41a31d651b60b4f458fc56a1d191dfbbdcb6d794","modified":1529475725787},{"_id":"themes/next-gux/source/css/_common/_core/scaffolding.styl","hash":"584c636707e0c8bfd6efc936c1b3a0d35d14f29d","modified":1529475725791},{"_id":"themes/next-gux/source/css/_common/_core/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1529475725787},{"_id":"themes/next-gux/source/css/_common/_page/archive.styl","hash":"dff879f55ca65fa79c07e9098719e53eeea7ac88","modified":1529475725791},{"_id":"themes/next-gux/source/css/_common/_core/tables.styl","hash":"f142a185fda68bc579e89ead9a31bc8fa0f3ca8c","modified":1529475725791},{"_id":"themes/next-gux/source/css/_common/_page/categories.styl","hash":"4f696a2eaeee2f214adcf273eab25c62a398077a","modified":1529475725791},{"_id":"themes/next-gux/source/css/_common/_page/post-detail.styl","hash":"73796f6f13caa7151a2ee8e55755627e0d189f55","modified":1529475725791},{"_id":"themes/next-gux/source/css/_common/_fonts/icon-default.styl","hash":"8b809aef383bebaeb3f282b47675f3a364ce3569","modified":1529475725791},{"_id":"themes/next-gux/source/css/_common/_fonts/icon-fifty-shades.styl","hash":"249f75bafa26b99d272352c0646e7497ea680b39","modified":1529475725791},{"_id":"themes/next-gux/source/css/_common/_fonts/icon-feather.styl","hash":"80413afacfa656322100ce1900fed1ebcd8f8f44","modified":1529475725791},{"_id":"themes/next-gux/source/css/_common/_fonts/icon-font.styl","hash":"ec3f86739bede393cafcd3e31052c01115ae20d6","modified":1529475725791},{"_id":"themes/next-gux/source/css/_common/_fonts/icon-linecons.styl","hash":"9cdbedb3627ac941cfb063b152abe5a75c3c699a","modified":1529475725791},{"_id":"themes/next-gux/source/css/_common/_section/body.styl","hash":"ca1a4766cbe25baac757c6b47a4858d221afdc40","modified":1529475725791},{"_id":"themes/next-gux/source/css/_common/_section/footer.styl","hash":"8994ffcce84deac0471532f270f97c44fea54dc0","modified":1529475725791},{"_id":"themes/next-gux/source/css/_common/_section/layout.styl","hash":"4daaadd156ece64ae05908ad6bb0159c8a27c071","modified":1529475725791},{"_id":"themes/next-gux/source/css/_common/_section/header.styl","hash":"ba501332fb111bd72dc0777f2e1c8a29ad538ff9","modified":1529475725791},{"_id":"themes/next-gux/source/css/_common/_section/media.styl","hash":"fa9809d2ecc753cf32f70803c1d0821c405211f4","modified":1529475725791},{"_id":"themes/next-gux/source/css/_common/_section/sidebar.styl","hash":"19ba3653e45187c064bbaf8142c2596a83ae7b08","modified":1529475725791},{"_id":"themes/next-gux/source/css/_schemes/Mala/index.styl","hash":"b2c5e70968c381ba9af79247aac4ef2891b0015c","modified":1529475725791},{"_id":"themes/next-gux/source/css/_schemes/default/_menu.styl","hash":"4bba29cece65ffc5122f4e052063dea4439fe4ae","modified":1529475725791},{"_id":"themes/next-gux/source/css/_schemes/default/_search.styl","hash":"c524bccdc554349106d1c8be9c3f275d4c0d4281","modified":1529475725791},{"_id":"themes/next-gux/source/css/_schemes/default/index.styl","hash":"2588e55132e10d82c0608f03c2c72a2bace8fa4e","modified":1529475725791},{"_id":"themes/next-gux/source/vendors/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1529475725795},{"_id":"themes/next-gux/source/vendors/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1529475725795},{"_id":"themes/next-gux/source/vendors/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1529475725795},{"_id":"themes/next-gux/source/vendors/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1529475725795},{"_id":"themes/next-gux/source/vendors/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1529475725795},{"_id":"themes/next-gux/source/vendors/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1529475725795},{"_id":"themes/next-gux/source/vendors/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1529475725795},{"_id":"themes/next-gux/source/vendors/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1529475725795},{"_id":"themes/next-gux/source/vendors/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1529475725795},{"_id":"themes/next-gux/source/vendors/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1529475725795},{"_id":"themes/next-gux/source/vendors/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1529475725795},{"_id":"themes/next-gux/layout/_partials/suprise/ball.swig","hash":"2c18d2cb89a054068fd04a9cf81c28fe3ac48120","modified":1529475725787},{"_id":"themes/next-gux/source/vendors/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1529475725795},{"_id":"themes/next-gux/source/css/_common/_vendor/highlight/highlight.styl","hash":"f3529b7da284c4b859429573c9b1004d32937e40","modified":1529475725791},{"_id":"themes/next-gux/source/css/_common/_vendor/highlight/theme.styl","hash":"ae19721ceee5ba460e131cb2427dae3c1ff39d6f","modified":1529475725791},{"_id":"themes/next-gux/source/vendors/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1529475725795},{"_id":"themes/next-gux/source/vendors/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1529475725795},{"_id":"themes/next-gux/source/vendors/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1529475725795},{"_id":"themes/next-gux/source/vendors/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1529475725795},{"_id":"themes/next-gux/source/vendors/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1529475725795},{"_id":"themes/next-gux/source/vendors/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1529475725795},{"_id":"source/_posts/异常检测-体验孤立森林/normal_scores_dis.png","hash":"5b64bf84eabf69144ba4ab21ca146c757303a23e","modified":1531366570311},{"_id":"source/_posts/absolute_uniform.png","hash":"93cf87872196401d4f32289fa14f4eb38b6eaec5","modified":1531376103385},{"_id":"source/_posts/异常检测-体验孤立森林/absolute_data.png","hash":"93cf87872196401d4f32289fa14f4eb38b6eaec5","modified":1531376523440},{"_id":"source/_posts/异常检测-体验孤立森林/absolute_score_dis.png","hash":"0cad04940129b49eea42df5990776640e43f7583","modified":1531376464142},{"_id":"source/_posts/异常检测-体验孤立森林/compare.png","hash":"f5cf77facaeb4096040fc44cf26e646bf827fd6d","modified":1531374596258},{"_id":"source/_posts/异常检测-体验孤立森林/normal_result.png","hash":"5389dda9f7204b2107dcf6a096d958f75affe8a3","modified":1531366886211},{"_id":"source/_posts/异常检测-体验孤立森林/uniform_scores_dis.png","hash":"5f5c34bfdc5d5f743daa921b338c9c78e0f30842","modified":1531371412630},{"_id":"source/_posts/异常检测-体验孤立森林/uniform_data.png","hash":"26cdeaae2b7ab7d7a4b614df932f6a6b115bb76c","modified":1531371497079}],"Category":[{"name":"Spark源码阅读计划","_id":"cjjhx8v2y0002b8tgb058bzb6"},{"name":"Spark调优","_id":"cjjhx8v360007b8tg9l2zh3gw"},{"name":"工具","_id":"cjjhx8v3a000cb8tgyftr6vto"},{"name":"笔记","_id":"cjjhx8v3p000ub8tgq59y1ruy"},{"name":"异常检测","_id":"cjjhx8v3r0010b8tgnolrjl6q"},{"name":"数据挖掘基础","_id":"cjjhx8v3w001eb8tg3hf98jhz"}],"Data":[],"Page":[{"_content":"## hello Im lishion\ngithhub: lishion\nwechat: lishion-me \n","source":"about/index.md","raw":"## hello Im lishion\ngithhub: lishion\nwechat: lishion-me \n","date":"2018-06-20T06:22:05.783Z","updated":"2018-06-20T06:22:05.783Z","path":"about/index.html","title":"","comments":1,"layout":"page","_id":"cjjhx8v7a002sb8tg5nebzr0x","content":"<h2 id=\"hello-Im-lishion\"><a href=\"#hello-Im-lishion\" class=\"headerlink\" title=\"hello Im lishion\"></a>hello Im lishion</h2><p>githhub: lishion<br>wechat: lishion-me </p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"hello-Im-lishion\"><a href=\"#hello-Im-lishion\" class=\"headerlink\" title=\"hello Im lishion\"></a>hello Im lishion</h2><p>githhub: lishion<br>wechat: lishion-me </p>\n"},{"title":"tags","date":"2018-06-14T09:20:42.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2018-06-14 17:20:42\ntype: \"tags\"\n---\n","updated":"2018-06-20T06:22:05.783Z","path":"tags/index.html","comments":1,"layout":"page","_id":"cjjhx8v7d002ub8tg3l39uyhl","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"categories","date":"2018-06-14T09:23:48.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2018-06-14 17:23:48\ntype: \"categories\"\n---\n","updated":"2018-06-20T06:22:05.783Z","path":"categories/index.html","comments":1,"layout":"page","_id":"cjjhx8v7f002wb8tg3y4cmny7","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"Spark 源码阅读计划 - 第三部分 - 图解 shuffle read","date":"2018-06-11T02:42:45.000Z","author":"lishion","toc":true,"_content":"\n你也许注意到了，今天的源码阅读计划后面有一个伪。没错，我们今天不读源码了，主要是因为我~~懒，想玩王者荣耀~~觉得前一章的 shuff write 源码的篇幅太长，害怕大家看完之后摸不着头脑。于是今天制作了 shuffle read 的图解版，顺便解决学习 spark 以来困扰我许久的几个问题。\n\n## 问题\n\n1. key 与如何与分区对应的\n\n   主要取决于分区器\n\n2. 分区发生在什么时候\n\n   建立初始RDD 以及 shuffle的时候\n\n3. key 与分区是一对一映射吗\n\n   不是。一个分区可以包含多个key，但同一个 key 只能存在一个分区中(如果使用hash 分区器)\n\n4. 如果同一个分区会存在多个key，那么在使用聚合函数的时候会不会把多个key的value聚合到一起?\n\n   当然不会，如果这样聚合就出错了。虽然聚合函数的最小处理对象是一个分区，但是实际上还是对 key 进行聚合。\n\n5. combineByKeyWithClassTag 中的 mergeValue 和 mergeCombiners 是否前者作用与分区内，后者作用于分区外(网上大部分都是这么说的，但其实是错误的)\n\n   不是，具体作用下面会讲解\n\n## 图解 shuffle write\n\n### 步骤一、分区 聚合\n\n这里是使用了 hash 表存放了聚合后的数据。与普通的 map 不同，数据的存储在一个 data 数组中。规则为索引为偶数的元素存放 KEY ，而对应的 value 存放在 KEY 的下一个元素，注意存放的 KEY 已经不是原始数据的 key，而是 key 以及其对的分区。由于在插入时使用二次探测法，数组中可能存在空值。用k1,v1表示原始数据键值对;`key=>p(key)`表示分区函数。数据表示流程图如下:\n\n{% asset_img part.png 分区并聚合 %}\n\n\n### 步骤二、排序并写入临时文件\n\n如果判断到 data 数组所占用的内存达到一定的阈值，就会对其中的数据进行排序。排序的方式为先对分区进行排序，分区内按照 key 的 hash 值进行排序。然后将排序好的数据写入临时文件，并产生一个空的 data 数组。这里可能发生多次写入临时文件的操作，也有可能最后仍有数据驻留在内存中而没有写入临时文件。例如，本分区一共有 10M 数据，内存阈值为 3M，则一共会发生 3 次写临时文件的操作以及最后会驻留1M的数据在内存中。需要注意的是这里并没有写入分区信息，只是写入原始的 key 以及对应聚合好的数据。\n\n### 步骤三、对所有的临时文件以及内存中驻留的文件进行排序\n\n由于存在多个临时文件并且内存中还有驻留的数据，因此需要对所有的数据进行排序并写入一个大文件中以减少临时文件的个数。这里的排序算法使用的方法大概是这样的:\n\n{% asset_img sort.png 排序 %}\n\n每次取出第一行的第一个元素就能得到排序结果，图中每一个竖向的数组就可以看做一个排序完成的临时文件。只不过在 shuffle write 并不是取前一行，而是取前一个分区。虽然文件中只存在 k c。但是在写临时文件时记录了每个分区对应的数据条数，因此仍然可以按分区取数据。\n\n这里还需要注意的一个地方是对于同一个 key 可能分布在不同的临时文件中，因此在排序的时候仍然需要对相同的 key 进行聚合。聚合方法其实很简单，例如获取到一条数据 (k1,c1)，如果下一条数据 (key,c) key == k1 就将 mergeCombiners 将 c1 与 c聚合，否则 key == k1 的数据已经全部读取完成，也就是剩下的数据中不会存在 key == k1 的数据。\n\n按照上面的方法不断的读取，聚合数据，写入文件就能将临时文件和内存中的所有数据进行聚合，排序并输出到一个大文件中。随后将每一个分区对应数据条数写入索引文件中，以便于记录大文件中某一条记录属于哪一个分区。完成以上步骤便完成了 shuffle write。\n\n## 总结\n\n读者: 等等，我要先吐个槽。说好的图解呢，怎么就两个图?\n\n作者: 本来有很多图的，但是我~~室友找我开黑~~要去跑步就只画了两个图。\n\n其实搞清楚 shuffle write 的大致步骤还是很简单的，但是具体到每一个细节就需要仔细的阅读源码了。这里推荐一波作者的[spark源码阅读计划](https://lishion.github.io/categories/Spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E8%AE%A1%E5%88%92/)。很~~混乱~~详细的解析了关于 shuffle write 每一部分代码，配合今天的~~图解~~食用更佳。\n\n\n\n\n\n\n\n","source":"_posts/Spark-源码阅读计划-伪-第三部分-图解-shuffle-write.md","raw":"---\ntitle: Spark 源码阅读计划 - 第三部分 - 图解 shuffle read\ndate: 2018-06-11 10:42:45\ntags:\n  - Spark\n  - 编程\ncategories: Spark源码阅读计划\nauthor: lishion\ntoc: true\n---\n\n你也许注意到了，今天的源码阅读计划后面有一个伪。没错，我们今天不读源码了，主要是因为我~~懒，想玩王者荣耀~~觉得前一章的 shuff write 源码的篇幅太长，害怕大家看完之后摸不着头脑。于是今天制作了 shuffle read 的图解版，顺便解决学习 spark 以来困扰我许久的几个问题。\n\n## 问题\n\n1. key 与如何与分区对应的\n\n   主要取决于分区器\n\n2. 分区发生在什么时候\n\n   建立初始RDD 以及 shuffle的时候\n\n3. key 与分区是一对一映射吗\n\n   不是。一个分区可以包含多个key，但同一个 key 只能存在一个分区中(如果使用hash 分区器)\n\n4. 如果同一个分区会存在多个key，那么在使用聚合函数的时候会不会把多个key的value聚合到一起?\n\n   当然不会，如果这样聚合就出错了。虽然聚合函数的最小处理对象是一个分区，但是实际上还是对 key 进行聚合。\n\n5. combineByKeyWithClassTag 中的 mergeValue 和 mergeCombiners 是否前者作用与分区内，后者作用于分区外(网上大部分都是这么说的，但其实是错误的)\n\n   不是，具体作用下面会讲解\n\n## 图解 shuffle write\n\n### 步骤一、分区 聚合\n\n这里是使用了 hash 表存放了聚合后的数据。与普通的 map 不同，数据的存储在一个 data 数组中。规则为索引为偶数的元素存放 KEY ，而对应的 value 存放在 KEY 的下一个元素，注意存放的 KEY 已经不是原始数据的 key，而是 key 以及其对的分区。由于在插入时使用二次探测法，数组中可能存在空值。用k1,v1表示原始数据键值对;`key=>p(key)`表示分区函数。数据表示流程图如下:\n\n{% asset_img part.png 分区并聚合 %}\n\n\n### 步骤二、排序并写入临时文件\n\n如果判断到 data 数组所占用的内存达到一定的阈值，就会对其中的数据进行排序。排序的方式为先对分区进行排序，分区内按照 key 的 hash 值进行排序。然后将排序好的数据写入临时文件，并产生一个空的 data 数组。这里可能发生多次写入临时文件的操作，也有可能最后仍有数据驻留在内存中而没有写入临时文件。例如，本分区一共有 10M 数据，内存阈值为 3M，则一共会发生 3 次写临时文件的操作以及最后会驻留1M的数据在内存中。需要注意的是这里并没有写入分区信息，只是写入原始的 key 以及对应聚合好的数据。\n\n### 步骤三、对所有的临时文件以及内存中驻留的文件进行排序\n\n由于存在多个临时文件并且内存中还有驻留的数据，因此需要对所有的数据进行排序并写入一个大文件中以减少临时文件的个数。这里的排序算法使用的方法大概是这样的:\n\n{% asset_img sort.png 排序 %}\n\n每次取出第一行的第一个元素就能得到排序结果，图中每一个竖向的数组就可以看做一个排序完成的临时文件。只不过在 shuffle write 并不是取前一行，而是取前一个分区。虽然文件中只存在 k c。但是在写临时文件时记录了每个分区对应的数据条数，因此仍然可以按分区取数据。\n\n这里还需要注意的一个地方是对于同一个 key 可能分布在不同的临时文件中，因此在排序的时候仍然需要对相同的 key 进行聚合。聚合方法其实很简单，例如获取到一条数据 (k1,c1)，如果下一条数据 (key,c) key == k1 就将 mergeCombiners 将 c1 与 c聚合，否则 key == k1 的数据已经全部读取完成，也就是剩下的数据中不会存在 key == k1 的数据。\n\n按照上面的方法不断的读取，聚合数据，写入文件就能将临时文件和内存中的所有数据进行聚合，排序并输出到一个大文件中。随后将每一个分区对应数据条数写入索引文件中，以便于记录大文件中某一条记录属于哪一个分区。完成以上步骤便完成了 shuffle write。\n\n## 总结\n\n读者: 等等，我要先吐个槽。说好的图解呢，怎么就两个图?\n\n作者: 本来有很多图的，但是我~~室友找我开黑~~要去跑步就只画了两个图。\n\n其实搞清楚 shuffle write 的大致步骤还是很简单的，但是具体到每一个细节就需要仔细的阅读源码了。这里推荐一波作者的[spark源码阅读计划](https://lishion.github.io/categories/Spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E8%AE%A1%E5%88%92/)。很~~混乱~~详细的解析了关于 shuffle write 每一部分代码，配合今天的~~图解~~食用更佳。\n\n\n\n\n\n\n\n","slug":"Spark-源码阅读计划-伪-第三部分-图解-shuffle-write","published":1,"updated":"2018-06-20T06:22:05.779Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjjhx8v2p0000b8tgrunuz6oj","content":"<p>你也许注意到了，今天的源码阅读计划后面有一个伪。没错，我们今天不读源码了，主要是因为我<del>懒，想玩王者荣耀</del>觉得前一章的 shuff write 源码的篇幅太长，害怕大家看完之后摸不着头脑。于是今天制作了 shuffle read 的图解版，顺便解决学习 spark 以来困扰我许久的几个问题。</p>\n<h2 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h2><ol>\n<li><p>key 与如何与分区对应的</p>\n<p>主要取决于分区器</p>\n</li>\n<li><p>分区发生在什么时候</p>\n<p>建立初始RDD 以及 shuffle的时候</p>\n</li>\n<li><p>key 与分区是一对一映射吗</p>\n<p>不是。一个分区可以包含多个key，但同一个 key 只能存在一个分区中(如果使用hash 分区器)</p>\n</li>\n<li><p>如果同一个分区会存在多个key，那么在使用聚合函数的时候会不会把多个key的value聚合到一起?</p>\n<p>当然不会，如果这样聚合就出错了。虽然聚合函数的最小处理对象是一个分区，但是实际上还是对 key 进行聚合。</p>\n</li>\n<li><p>combineByKeyWithClassTag 中的 mergeValue 和 mergeCombiners 是否前者作用与分区内，后者作用于分区外(网上大部分都是这么说的，但其实是错误的)</p>\n<p>不是，具体作用下面会讲解</p>\n</li>\n</ol>\n<h2 id=\"图解-shuffle-write\"><a href=\"#图解-shuffle-write\" class=\"headerlink\" title=\"图解 shuffle write\"></a>图解 shuffle write</h2><h3 id=\"步骤一、分区-聚合\"><a href=\"#步骤一、分区-聚合\" class=\"headerlink\" title=\"步骤一、分区 聚合\"></a>步骤一、分区 聚合</h3><p>这里是使用了 hash 表存放了聚合后的数据。与普通的 map 不同，数据的存储在一个 data 数组中。规则为索引为偶数的元素存放 KEY ，而对应的 value 存放在 KEY 的下一个元素，注意存放的 KEY 已经不是原始数据的 key，而是 key 以及其对的分区。由于在插入时使用二次探测法，数组中可能存在空值。用k1,v1表示原始数据键值对;<code>key=&gt;p(key)</code>表示分区函数。数据表示流程图如下:</p>\n\n<h3 id=\"步骤二、排序并写入临时文件\"><a href=\"#步骤二、排序并写入临时文件\" class=\"headerlink\" title=\"步骤二、排序并写入临时文件\"></a>步骤二、排序并写入临时文件</h3><p>如果判断到 data 数组所占用的内存达到一定的阈值，就会对其中的数据进行排序。排序的方式为先对分区进行排序，分区内按照 key 的 hash 值进行排序。然后将排序好的数据写入临时文件，并产生一个空的 data 数组。这里可能发生多次写入临时文件的操作，也有可能最后仍有数据驻留在内存中而没有写入临时文件。例如，本分区一共有 10M 数据，内存阈值为 3M，则一共会发生 3 次写临时文件的操作以及最后会驻留1M的数据在内存中。需要注意的是这里并没有写入分区信息，只是写入原始的 key 以及对应聚合好的数据。</p>\n<h3 id=\"步骤三、对所有的临时文件以及内存中驻留的文件进行排序\"><a href=\"#步骤三、对所有的临时文件以及内存中驻留的文件进行排序\" class=\"headerlink\" title=\"步骤三、对所有的临时文件以及内存中驻留的文件进行排序\"></a>步骤三、对所有的临时文件以及内存中驻留的文件进行排序</h3><p>由于存在多个临时文件并且内存中还有驻留的数据，因此需要对所有的数据进行排序并写入一个大文件中以减少临时文件的个数。这里的排序算法使用的方法大概是这样的:</p>\n\n<p>每次取出第一行的第一个元素就能得到排序结果，图中每一个竖向的数组就可以看做一个排序完成的临时文件。只不过在 shuffle write 并不是取前一行，而是取前一个分区。虽然文件中只存在 k c。但是在写临时文件时记录了每个分区对应的数据条数，因此仍然可以按分区取数据。</p>\n<p>这里还需要注意的一个地方是对于同一个 key 可能分布在不同的临时文件中，因此在排序的时候仍然需要对相同的 key 进行聚合。聚合方法其实很简单，例如获取到一条数据 (k1,c1)，如果下一条数据 (key,c) key == k1 就将 mergeCombiners 将 c1 与 c聚合，否则 key == k1 的数据已经全部读取完成，也就是剩下的数据中不会存在 key == k1 的数据。</p>\n<p>按照上面的方法不断的读取，聚合数据，写入文件就能将临时文件和内存中的所有数据进行聚合，排序并输出到一个大文件中。随后将每一个分区对应数据条数写入索引文件中，以便于记录大文件中某一条记录属于哪一个分区。完成以上步骤便完成了 shuffle write。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>读者: 等等，我要先吐个槽。说好的图解呢，怎么就两个图?</p>\n<p>作者: 本来有很多图的，但是我<del>室友找我开黑</del>要去跑步就只画了两个图。</p>\n<p>其实搞清楚 shuffle write 的大致步骤还是很简单的，但是具体到每一个细节就需要仔细的阅读源码了。这里推荐一波作者的<a href=\"https://lishion.github.io/categories/Spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E8%AE%A1%E5%88%92/\" target=\"_blank\" rel=\"noopener\">spark源码阅读计划</a>。很<del>混乱</del>详细的解析了关于 shuffle write 每一部分代码，配合今天的<del>图解</del>食用更佳。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>你也许注意到了，今天的源码阅读计划后面有一个伪。没错，我们今天不读源码了，主要是因为我<del>懒，想玩王者荣耀</del>觉得前一章的 shuff write 源码的篇幅太长，害怕大家看完之后摸不着头脑。于是今天制作了 shuffle read 的图解版，顺便解决学习 spark 以来困扰我许久的几个问题。</p>\n<h2 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h2><ol>\n<li><p>key 与如何与分区对应的</p>\n<p>主要取决于分区器</p>\n</li>\n<li><p>分区发生在什么时候</p>\n<p>建立初始RDD 以及 shuffle的时候</p>\n</li>\n<li><p>key 与分区是一对一映射吗</p>\n<p>不是。一个分区可以包含多个key，但同一个 key 只能存在一个分区中(如果使用hash 分区器)</p>\n</li>\n<li><p>如果同一个分区会存在多个key，那么在使用聚合函数的时候会不会把多个key的value聚合到一起?</p>\n<p>当然不会，如果这样聚合就出错了。虽然聚合函数的最小处理对象是一个分区，但是实际上还是对 key 进行聚合。</p>\n</li>\n<li><p>combineByKeyWithClassTag 中的 mergeValue 和 mergeCombiners 是否前者作用与分区内，后者作用于分区外(网上大部分都是这么说的，但其实是错误的)</p>\n<p>不是，具体作用下面会讲解</p>\n</li>\n</ol>\n<h2 id=\"图解-shuffle-write\"><a href=\"#图解-shuffle-write\" class=\"headerlink\" title=\"图解 shuffle write\"></a>图解 shuffle write</h2><h3 id=\"步骤一、分区-聚合\"><a href=\"#步骤一、分区-聚合\" class=\"headerlink\" title=\"步骤一、分区 聚合\"></a>步骤一、分区 聚合</h3><p>这里是使用了 hash 表存放了聚合后的数据。与普通的 map 不同，数据的存储在一个 data 数组中。规则为索引为偶数的元素存放 KEY ，而对应的 value 存放在 KEY 的下一个元素，注意存放的 KEY 已经不是原始数据的 key，而是 key 以及其对的分区。由于在插入时使用二次探测法，数组中可能存在空值。用k1,v1表示原始数据键值对;<code>key=&gt;p(key)</code>表示分区函数。数据表示流程图如下:</p>\n\n<h3 id=\"步骤二、排序并写入临时文件\"><a href=\"#步骤二、排序并写入临时文件\" class=\"headerlink\" title=\"步骤二、排序并写入临时文件\"></a>步骤二、排序并写入临时文件</h3><p>如果判断到 data 数组所占用的内存达到一定的阈值，就会对其中的数据进行排序。排序的方式为先对分区进行排序，分区内按照 key 的 hash 值进行排序。然后将排序好的数据写入临时文件，并产生一个空的 data 数组。这里可能发生多次写入临时文件的操作，也有可能最后仍有数据驻留在内存中而没有写入临时文件。例如，本分区一共有 10M 数据，内存阈值为 3M，则一共会发生 3 次写临时文件的操作以及最后会驻留1M的数据在内存中。需要注意的是这里并没有写入分区信息，只是写入原始的 key 以及对应聚合好的数据。</p>\n<h3 id=\"步骤三、对所有的临时文件以及内存中驻留的文件进行排序\"><a href=\"#步骤三、对所有的临时文件以及内存中驻留的文件进行排序\" class=\"headerlink\" title=\"步骤三、对所有的临时文件以及内存中驻留的文件进行排序\"></a>步骤三、对所有的临时文件以及内存中驻留的文件进行排序</h3><p>由于存在多个临时文件并且内存中还有驻留的数据，因此需要对所有的数据进行排序并写入一个大文件中以减少临时文件的个数。这里的排序算法使用的方法大概是这样的:</p>\n\n<p>每次取出第一行的第一个元素就能得到排序结果，图中每一个竖向的数组就可以看做一个排序完成的临时文件。只不过在 shuffle write 并不是取前一行，而是取前一个分区。虽然文件中只存在 k c。但是在写临时文件时记录了每个分区对应的数据条数，因此仍然可以按分区取数据。</p>\n<p>这里还需要注意的一个地方是对于同一个 key 可能分布在不同的临时文件中，因此在排序的时候仍然需要对相同的 key 进行聚合。聚合方法其实很简单，例如获取到一条数据 (k1,c1)，如果下一条数据 (key,c) key == k1 就将 mergeCombiners 将 c1 与 c聚合，否则 key == k1 的数据已经全部读取完成，也就是剩下的数据中不会存在 key == k1 的数据。</p>\n<p>按照上面的方法不断的读取，聚合数据，写入文件就能将临时文件和内存中的所有数据进行聚合，排序并输出到一个大文件中。随后将每一个分区对应数据条数写入索引文件中，以便于记录大文件中某一条记录属于哪一个分区。完成以上步骤便完成了 shuffle write。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>读者: 等等，我要先吐个槽。说好的图解呢，怎么就两个图?</p>\n<p>作者: 本来有很多图的，但是我<del>室友找我开黑</del>要去跑步就只画了两个图。</p>\n<p>其实搞清楚 shuffle write 的大致步骤还是很简单的，但是具体到每一个细节就需要仔细的阅读源码了。这里推荐一波作者的<a href=\"https://lishion.github.io/categories/Spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E8%AE%A1%E5%88%92/\" target=\"_blank\" rel=\"noopener\">spark源码阅读计划</a>。很<del>混乱</del>详细的解析了关于 shuffle write 每一部分代码，配合今天的<del>图解</del>食用更佳。</p>\n"},{"title":"Spark调优-选择合适的分区","date":"2018-06-15T03:48:57.000Z","_content":"\n昨天在利用 Spark 进行统计分析的时候大佬提醒我可以适当的减少分区数量来加快作业的执行速度。于是今天测试了一下，利用`sc.textFile(\"xxx\")`读取数据，采用默认分区(6657个)时执行时间为 190  秒。当手动使用 coalesce 进行重新分至 120 个分区时执行时间减少到 112 秒。当然这里的 120 并不是随便设置的，而是正好等于作业时候的并行度。\n\n## 原因\n\n我们知道分区的个数是等于 task 的个数的。如果分区数量远大于并行度，那么就会造成大量的时间浪费在 task 的切换中。而如果分区数量小于并行度，那么就会存在一些核心无法分配到需要的数据而闲置，造成资源浪费。因此在设置分区的时候尽量等于并行度。\n\n## 方法\n\n可以使用 coalesce 和 repartition 可以对 RDD 重新设置分区的个数。具体区别会在[Spark源码阅读计划](https://lishion.github.io/categories/Spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E8%AE%A1%E5%88%92/)中讲解。\n\n","source":"_posts/Spark优化-减少分区.md","raw":"---\ntitle: Spark调优-选择合适的分区\ndate: 2018-06-15 11:48:57\ntags: Spark\ncategories: Spark调优\n---\n\n昨天在利用 Spark 进行统计分析的时候大佬提醒我可以适当的减少分区数量来加快作业的执行速度。于是今天测试了一下，利用`sc.textFile(\"xxx\")`读取数据，采用默认分区(6657个)时执行时间为 190  秒。当手动使用 coalesce 进行重新分至 120 个分区时执行时间减少到 112 秒。当然这里的 120 并不是随便设置的，而是正好等于作业时候的并行度。\n\n## 原因\n\n我们知道分区的个数是等于 task 的个数的。如果分区数量远大于并行度，那么就会造成大量的时间浪费在 task 的切换中。而如果分区数量小于并行度，那么就会存在一些核心无法分配到需要的数据而闲置，造成资源浪费。因此在设置分区的时候尽量等于并行度。\n\n## 方法\n\n可以使用 coalesce 和 repartition 可以对 RDD 重新设置分区的个数。具体区别会在[Spark源码阅读计划](https://lishion.github.io/categories/Spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E8%AE%A1%E5%88%92/)中讲解。\n\n","slug":"Spark优化-减少分区","published":1,"updated":"2018-06-20T06:22:05.783Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjjhx8v2v0001b8tguq1mby4v","content":"<p>昨天在利用 Spark 进行统计分析的时候大佬提醒我可以适当的减少分区数量来加快作业的执行速度。于是今天测试了一下，利用<code>sc.textFile(&quot;xxx&quot;)</code>读取数据，采用默认分区(6657个)时执行时间为 190  秒。当手动使用 coalesce 进行重新分至 120 个分区时执行时间减少到 112 秒。当然这里的 120 并不是随便设置的，而是正好等于作业时候的并行度。</p>\n<h2 id=\"原因\"><a href=\"#原因\" class=\"headerlink\" title=\"原因\"></a>原因</h2><p>我们知道分区的个数是等于 task 的个数的。如果分区数量远大于并行度，那么就会造成大量的时间浪费在 task 的切换中。而如果分区数量小于并行度，那么就会存在一些核心无法分配到需要的数据而闲置，造成资源浪费。因此在设置分区的时候尽量等于并行度。</p>\n<h2 id=\"方法\"><a href=\"#方法\" class=\"headerlink\" title=\"方法\"></a>方法</h2><p>可以使用 coalesce 和 repartition 可以对 RDD 重新设置分区的个数。具体区别会在<a href=\"https://lishion.github.io/categories/Spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E8%AE%A1%E5%88%92/\" target=\"_blank\" rel=\"noopener\">Spark源码阅读计划</a>中讲解。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>昨天在利用 Spark 进行统计分析的时候大佬提醒我可以适当的减少分区数量来加快作业的执行速度。于是今天测试了一下，利用<code>sc.textFile(&quot;xxx&quot;)</code>读取数据，采用默认分区(6657个)时执行时间为 190  秒。当手动使用 coalesce 进行重新分至 120 个分区时执行时间减少到 112 秒。当然这里的 120 并不是随便设置的，而是正好等于作业时候的并行度。</p>\n<h2 id=\"原因\"><a href=\"#原因\" class=\"headerlink\" title=\"原因\"></a>原因</h2><p>我们知道分区的个数是等于 task 的个数的。如果分区数量远大于并行度，那么就会造成大量的时间浪费在 task 的切换中。而如果分区数量小于并行度，那么就会存在一些核心无法分配到需要的数据而闲置，造成资源浪费。因此在设置分区的时候尽量等于并行度。</p>\n<h2 id=\"方法\"><a href=\"#方法\" class=\"headerlink\" title=\"方法\"></a>方法</h2><p>可以使用 coalesce 和 repartition 可以对 RDD 重新设置分区的个数。具体区别会在<a href=\"https://lishion.github.io/categories/Spark%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E8%AE%A1%E5%88%92/\" target=\"_blank\" rel=\"noopener\">Spark源码阅读计划</a>中讲解。</p>\n"},{"title":"使用git + hexo 建立个人博客","date":"2018-06-07T02:42:45.000Z","_content":"\n## 安装 hexo\n\n1. 安装`node`\n2. 安装`npm`\n3. 安装`hexo: npm install -g hexo-cli`\n\n\n## 建立一个新博客\n\n1. `hexo init blog`:使用该命令会在blog目录下建立一个博客并且在`source/_posts/hello-world.md`生成一篇名为`hello world`的文章，随后你可以选择删除它并新建自己的文章。\n2. `cd blog`\n3. `hexo g`:使用该命令将 `source/_posts/hello-world.md` 渲染为`html、css、js`静态资源\n4. `hexo s`:开启服务器。然后http://localhost:4000/\n\n## 关联至github\n\n1. 新建仓库`xxx.github.io`，这里 xxx 可以是你想要取的名字，但是必须以`github.io`结尾\n\n2. 此时可以访问`https://xxx.github.io`，但是没有内容\n\n3. 修改`blog`目录下配置文件`_config.yml`，找到`deploy`选项，修改(新增)为:\n\n   ```yaml\n   deploy:\n     type: git\n     repository: git@github.com:xxx/xxx.github.io.git \n     branch: master\n   ```\n\n4. 安装插件`npm install hexo-deployer-git --save`\n\n5. `hexo d -g` 生成内容后部署\n\n6. 访问`https://xxx.github.io`，应该要延迟一段时间才能看到效果\n\n## 更换主题\n\n由于初始的主题不怎么好看，可以选择更换一下主题。官方主题地址为[hexo-themes](https://hexo.io/themes/)。本教程中采用[maupassant-hexo](https://www.haomwei.com/technology/maupassant-hexo.html#%E6%94%AF%E6%8C%81%E8%AF%AD%E8%A8%80)为主题\n\n1. 由于大部分的主题都托管在`github`上，在`blog`目录下运行:\n\n   `git clone https://github.com/tufu9441/maupassant-hexo.git themes/maupassant` \n\n   `themes/xxx`是`hexo`存放`xxx`主题的目录\n\n2. `npm install hexo-renderer-pug --save`\n\n3. `npm install hexo-renderer-sass --save --registry=https://registry.npm.taobao.org`\n\n4. 修改`_config.yml`中主题为`theme: maupassant`\n\n5. `hexo g`重新生成\n\n6. `hexo s`开启服务器\n\n主题还有许多可用的配置，请参照上面给出的链接进行设置\n\n## 目录、tag\n\n需要`归档`和`tag`只需要在`markdown`上加上一些`YAML`头部信息:\n\n```\n---\ntitle: hello\ncategories: 杂谈\ntag: 杂七杂八\n---\n```\n\n即可\n\n","source":"_posts/使用git+hexo建立个人博客.md","raw":"---\ntitle: 使用git + hexo 建立个人博客\ncategories: 工具\ndate: 2018-06-07 10:42:45\ntag: \n  - 工具\n  - 教程\n---\n\n## 安装 hexo\n\n1. 安装`node`\n2. 安装`npm`\n3. 安装`hexo: npm install -g hexo-cli`\n\n\n## 建立一个新博客\n\n1. `hexo init blog`:使用该命令会在blog目录下建立一个博客并且在`source/_posts/hello-world.md`生成一篇名为`hello world`的文章，随后你可以选择删除它并新建自己的文章。\n2. `cd blog`\n3. `hexo g`:使用该命令将 `source/_posts/hello-world.md` 渲染为`html、css、js`静态资源\n4. `hexo s`:开启服务器。然后http://localhost:4000/\n\n## 关联至github\n\n1. 新建仓库`xxx.github.io`，这里 xxx 可以是你想要取的名字，但是必须以`github.io`结尾\n\n2. 此时可以访问`https://xxx.github.io`，但是没有内容\n\n3. 修改`blog`目录下配置文件`_config.yml`，找到`deploy`选项，修改(新增)为:\n\n   ```yaml\n   deploy:\n     type: git\n     repository: git@github.com:xxx/xxx.github.io.git \n     branch: master\n   ```\n\n4. 安装插件`npm install hexo-deployer-git --save`\n\n5. `hexo d -g` 生成内容后部署\n\n6. 访问`https://xxx.github.io`，应该要延迟一段时间才能看到效果\n\n## 更换主题\n\n由于初始的主题不怎么好看，可以选择更换一下主题。官方主题地址为[hexo-themes](https://hexo.io/themes/)。本教程中采用[maupassant-hexo](https://www.haomwei.com/technology/maupassant-hexo.html#%E6%94%AF%E6%8C%81%E8%AF%AD%E8%A8%80)为主题\n\n1. 由于大部分的主题都托管在`github`上，在`blog`目录下运行:\n\n   `git clone https://github.com/tufu9441/maupassant-hexo.git themes/maupassant` \n\n   `themes/xxx`是`hexo`存放`xxx`主题的目录\n\n2. `npm install hexo-renderer-pug --save`\n\n3. `npm install hexo-renderer-sass --save --registry=https://registry.npm.taobao.org`\n\n4. 修改`_config.yml`中主题为`theme: maupassant`\n\n5. `hexo g`重新生成\n\n6. `hexo s`开启服务器\n\n主题还有许多可用的配置，请参照上面给出的链接进行设置\n\n## 目录、tag\n\n需要`归档`和`tag`只需要在`markdown`上加上一些`YAML`头部信息:\n\n```\n---\ntitle: hello\ncategories: 杂谈\ntag: 杂七杂八\n---\n```\n\n即可\n\n","slug":"使用git+hexo建立个人博客","published":1,"updated":"2018-06-20T06:22:05.783Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjjhx8v310004b8tgipujopgx","content":"<h2 id=\"安装-hexo\"><a href=\"#安装-hexo\" class=\"headerlink\" title=\"安装 hexo\"></a>安装 hexo</h2><ol>\n<li>安装<code>node</code></li>\n<li>安装<code>npm</code></li>\n<li>安装<code>hexo: npm install -g hexo-cli</code></li>\n</ol>\n<h2 id=\"建立一个新博客\"><a href=\"#建立一个新博客\" class=\"headerlink\" title=\"建立一个新博客\"></a>建立一个新博客</h2><ol>\n<li><code>hexo init blog</code>:使用该命令会在blog目录下建立一个博客并且在<code>source/_posts/hello-world.md</code>生成一篇名为<code>hello world</code>的文章，随后你可以选择删除它并新建自己的文章。</li>\n<li><code>cd blog</code></li>\n<li><code>hexo g</code>:使用该命令将 <code>source/_posts/hello-world.md</code> 渲染为<code>html、css、js</code>静态资源</li>\n<li><code>hexo s</code>:开启服务器。然后<a href=\"http://localhost:4000/\" target=\"_blank\" rel=\"noopener\">http://localhost:4000/</a></li>\n</ol>\n<h2 id=\"关联至github\"><a href=\"#关联至github\" class=\"headerlink\" title=\"关联至github\"></a>关联至github</h2><ol>\n<li><p>新建仓库<code>xxx.github.io</code>，这里 xxx 可以是你想要取的名字，但是必须以<code>github.io</code>结尾</p>\n</li>\n<li><p>此时可以访问<code>https://xxx.github.io</code>，但是没有内容</p>\n</li>\n<li><p>修改<code>blog</code>目录下配置文件<code>_config.yml</code>，找到<code>deploy</code>选项，修改(新增)为:</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">deploy:</span></span><br><span class=\"line\"><span class=\"attr\">  type:</span> <span class=\"string\">git</span></span><br><span class=\"line\"><span class=\"attr\">  repository:</span> <span class=\"string\">git@github.com:xxx/xxx.github.io.git</span> </span><br><span class=\"line\"><span class=\"attr\">  branch:</span> <span class=\"string\">master</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>安装插件<code>npm install hexo-deployer-git --save</code></p>\n</li>\n<li><p><code>hexo d -g</code> 生成内容后部署</p>\n</li>\n<li><p>访问<code>https://xxx.github.io</code>，应该要延迟一段时间才能看到效果</p>\n</li>\n</ol>\n<h2 id=\"更换主题\"><a href=\"#更换主题\" class=\"headerlink\" title=\"更换主题\"></a>更换主题</h2><p>由于初始的主题不怎么好看，可以选择更换一下主题。官方主题地址为<a href=\"https://hexo.io/themes/\" target=\"_blank\" rel=\"noopener\">hexo-themes</a>。本教程中采用<a href=\"https://www.haomwei.com/technology/maupassant-hexo.html#%E6%94%AF%E6%8C%81%E8%AF%AD%E8%A8%80\" target=\"_blank\" rel=\"noopener\">maupassant-hexo</a>为主题</p>\n<ol>\n<li><p>由于大部分的主题都托管在<code>github</code>上，在<code>blog</code>目录下运行:</p>\n<p><code>git clone https://github.com/tufu9441/maupassant-hexo.git themes/maupassant</code> </p>\n<p><code>themes/xxx</code>是<code>hexo</code>存放<code>xxx</code>主题的目录</p>\n</li>\n<li><p><code>npm install hexo-renderer-pug --save</code></p>\n</li>\n<li><p><code>npm install hexo-renderer-sass --save --registry=https://registry.npm.taobao.org</code></p>\n</li>\n<li><p>修改<code>_config.yml</code>中主题为<code>theme: maupassant</code></p>\n</li>\n<li><p><code>hexo g</code>重新生成</p>\n</li>\n<li><p><code>hexo s</code>开启服务器</p>\n</li>\n</ol>\n<p>主题还有许多可用的配置，请参照上面给出的链接进行设置</p>\n<h2 id=\"目录、tag\"><a href=\"#目录、tag\" class=\"headerlink\" title=\"目录、tag\"></a>目录、tag</h2><p>需要<code>归档</code>和<code>tag</code>只需要在<code>markdown</code>上加上一些<code>YAML</code>头部信息:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">---</span><br><span class=\"line\">title: hello</span><br><span class=\"line\">categories: 杂谈</span><br><span class=\"line\">tag: 杂七杂八</span><br><span class=\"line\">---</span><br></pre></td></tr></table></figure>\n<p>即可</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"安装-hexo\"><a href=\"#安装-hexo\" class=\"headerlink\" title=\"安装 hexo\"></a>安装 hexo</h2><ol>\n<li>安装<code>node</code></li>\n<li>安装<code>npm</code></li>\n<li>安装<code>hexo: npm install -g hexo-cli</code></li>\n</ol>\n<h2 id=\"建立一个新博客\"><a href=\"#建立一个新博客\" class=\"headerlink\" title=\"建立一个新博客\"></a>建立一个新博客</h2><ol>\n<li><code>hexo init blog</code>:使用该命令会在blog目录下建立一个博客并且在<code>source/_posts/hello-world.md</code>生成一篇名为<code>hello world</code>的文章，随后你可以选择删除它并新建自己的文章。</li>\n<li><code>cd blog</code></li>\n<li><code>hexo g</code>:使用该命令将 <code>source/_posts/hello-world.md</code> 渲染为<code>html、css、js</code>静态资源</li>\n<li><code>hexo s</code>:开启服务器。然后<a href=\"http://localhost:4000/\" target=\"_blank\" rel=\"noopener\">http://localhost:4000/</a></li>\n</ol>\n<h2 id=\"关联至github\"><a href=\"#关联至github\" class=\"headerlink\" title=\"关联至github\"></a>关联至github</h2><ol>\n<li><p>新建仓库<code>xxx.github.io</code>，这里 xxx 可以是你想要取的名字，但是必须以<code>github.io</code>结尾</p>\n</li>\n<li><p>此时可以访问<code>https://xxx.github.io</code>，但是没有内容</p>\n</li>\n<li><p>修改<code>blog</code>目录下配置文件<code>_config.yml</code>，找到<code>deploy</code>选项，修改(新增)为:</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">deploy:</span></span><br><span class=\"line\"><span class=\"attr\">  type:</span> <span class=\"string\">git</span></span><br><span class=\"line\"><span class=\"attr\">  repository:</span> <span class=\"string\">git@github.com:xxx/xxx.github.io.git</span> </span><br><span class=\"line\"><span class=\"attr\">  branch:</span> <span class=\"string\">master</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>安装插件<code>npm install hexo-deployer-git --save</code></p>\n</li>\n<li><p><code>hexo d -g</code> 生成内容后部署</p>\n</li>\n<li><p>访问<code>https://xxx.github.io</code>，应该要延迟一段时间才能看到效果</p>\n</li>\n</ol>\n<h2 id=\"更换主题\"><a href=\"#更换主题\" class=\"headerlink\" title=\"更换主题\"></a>更换主题</h2><p>由于初始的主题不怎么好看，可以选择更换一下主题。官方主题地址为<a href=\"https://hexo.io/themes/\" target=\"_blank\" rel=\"noopener\">hexo-themes</a>。本教程中采用<a href=\"https://www.haomwei.com/technology/maupassant-hexo.html#%E6%94%AF%E6%8C%81%E8%AF%AD%E8%A8%80\" target=\"_blank\" rel=\"noopener\">maupassant-hexo</a>为主题</p>\n<ol>\n<li><p>由于大部分的主题都托管在<code>github</code>上，在<code>blog</code>目录下运行:</p>\n<p><code>git clone https://github.com/tufu9441/maupassant-hexo.git themes/maupassant</code> </p>\n<p><code>themes/xxx</code>是<code>hexo</code>存放<code>xxx</code>主题的目录</p>\n</li>\n<li><p><code>npm install hexo-renderer-pug --save</code></p>\n</li>\n<li><p><code>npm install hexo-renderer-sass --save --registry=https://registry.npm.taobao.org</code></p>\n</li>\n<li><p>修改<code>_config.yml</code>中主题为<code>theme: maupassant</code></p>\n</li>\n<li><p><code>hexo g</code>重新生成</p>\n</li>\n<li><p><code>hexo s</code>开启服务器</p>\n</li>\n</ol>\n<p>主题还有许多可用的配置，请参照上面给出的链接进行设置</p>\n<h2 id=\"目录、tag\"><a href=\"#目录、tag\" class=\"headerlink\" title=\"目录、tag\"></a>目录、tag</h2><p>需要<code>归档</code>和<code>tag</code>只需要在<code>markdown</code>上加上一些<code>YAML</code>头部信息:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">---</span><br><span class=\"line\">title: hello</span><br><span class=\"line\">categories: 杂谈</span><br><span class=\"line\">tag: 杂七杂八</span><br><span class=\"line\">---</span><br></pre></td></tr></table></figure>\n<p>即可</p>\n"},{"title":"使用travis自动部署hexo到github","date":"2018-06-20T12:37:50.000Z","author":"lishion","toc":true,"_content":"\n在上一章[使用hexo+github搭建自己的博客](https://lishion.github.io/2018/06/07/%E4%BD%BF%E7%94%A8git+hexo%E5%BB%BA%E7%AB%8B%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/)介绍了如何使用 hexo 以及 github 搭建自己的博客。但是随后我们就遇到了一个问题: 如何我们想在多台电脑上编辑我们的博客应该怎么办。当然，我们可以选择新建一个仓库|分支利用 github 在多台电脑上同步。但是这种方式有一点繁琐，每次写作之前我们要保证源文件的同步 还要保证生成新的资源并部署到 github。例如一次典型的写作我们需要:\n\n```bash\ngit pull origin resource:resource # 假设我们的资源文件存储在 origin 分支\nhexo new xxx\ngit push origin resource\nhexo g\nhexo d\n```\n\n使用 travis 就能让我们省去渲染和部署的命令，变成:\n\n```shell~~\ngit pull origin resource:resource # 假设我们的资源文件存储在 origin 分支\nhexo new xxx\ngit push origin resource\n```\n\n~~难道就只能节省两个命令吗?~~\n\n~~哇，一共就五个命令。节省了两个等于节省了百分之四十啊!你说赛高不赛高。~~\n\n## 使用 travis 进行自动部署\n\n​\t看看廖雪峰大佬对 [travis](http://www.ruanyifeng.com/blog/2017/12/travis_ci_tutorial.html) 的介绍:\n\n> Travis CI 提供的是持续集成服务（Continuous Integration，简称 CI）。它绑定 Github 上面的项目，只要有新的代码，就会自动抓取。然后，提供一个运行环境，执行测试，完成构建，还能部署到服务器。\n\n简单的说 travis 提供了一个平台，这个平台可以监听你 Github 上仓库的变化，如果发生改变。则会将你的代码拉取到 travis 的服务器上，并执行你事先定义好的操作。\n\n### 初识 travis\n\n只要你拥有 github 的账户就可以直接登录 [travis官网](https://www.travis-ci.org/)使用。如果没有。。。。那就去注册一个。登录之后的页面大概是这样的:\n\n{% asset_img travis-mainpage.png travis主页 %}\n\n当然如果你没有使用过那左边的仓库栏和右边的信息栏都是空吧的，这个时候点击仓库栏上面的+号，添加一个仓库。\n\n{% asset_img travis-add.png 新增关联仓库 %}\n\n当然这里我们要选择的是 hexo 部署到的仓库。\n\n到这里，关联我们的仓库到 trvias 就完成了。但是我们还需要创建一个分支来保存我们的资源文件，因此，在我们的博客的根目录运行:\n\n```bash\ngit check out -b resource\ngit add *\ngit commit -m \"add resource\" \ngit add remote xxx # 这里的xxx就是博客对应的仓库\ngit push -u origin resource\n```\n\n这样把我们的资源文件同步到 github 上。\n\n### 配置文件\n\n当然，一个仓库有很多分支，我们还需要在根目录下配置一个 .travis.yml 文件来让 travis 知道我们需要构建哪一个分支。._config.yml 同时还告诉了 travis 在构建前需要做什么，构建后需要做什么。例如，我的 .travis.yml 如下:\n\n```yaml\nlanguage: node_js　# 使用什么语言\nnode_js:\n  - \"10\" # 语言版本\nbranches:\n  only:\n  - resource # 只构建 resource 分支\nscript:\n  - hexo g # 构建时执行的命令\ninstall:\n  - npm install  hexo # 由于 travis 的服务器默认是不带 hexo 所以需要安装\n  - npm install  hexo-cli\n  - npm install hexo-deployer-git\n  - sed -i'' \"s~git@github.com:~https://${TOKEN}@github.com/~\" _config.yml\nafter_success:\n  - hexo d # 构建完成后执行的命令 这里就是进行部署\nnotifications: # 设置通知项\n  email:\n    - 544670411@qq.com\n~                       \n```\n\n### 配置授权码\n\n现在还存在一个问题。travis 是需要从 github 拉取到他的服务器上，并且还需要通过 hexo 将构建后的文件 push 到 github，然而在 travis 的服务器上是不存在我们的 ssh 公钥的。也就会说目前 travis 还对无法操作我们的 github 仓库。所以我们还需要生成一个 github 授权码:\n\n{% asset_img author.png 新增授权码 %}\n\n然后在 travis 中配置这个授权码:\n\n{% asset_img token-add.png 配置授权码 %}\n\n其中名字可以自定义。但是配置了这个授权码如何使用呢?答案就是在 .travis.yml 中配置:\n\n```yaml\ninstall:\n  ...\n  - sed -i'' \"s~git@github.com:~https://${TOKEN}@github.com/~\" _config.yml\n```\n\n这里的 ${TOKEN} 需要将 TOKEN 替换为你刚才设置的名字。\n\n### 开始使用 \n\n到这里，整个设置部分就完成了。现在只需要 git pull origin resource 。然后登录 travis 就能看到项目正在构建了。从日志可以看到构建成功还是失败。以后每次写文章就只需要:\n\n```\ngit pull origin resource:resource\nhexo new xxx\ngit push orgin resource\n```\n\ntravis 就能帮你完成构建和部署的功能，是不是方便多了?\n\n\n\n\n\n\n","source":"_posts/使用travis自动部署hexo到github.md","raw":"---\ntitle: 使用travis自动部署hexo到github\ndate: 2018-06-20 20:37:50\ntags:\n  - 工具\n  - 教程\ncategories: 工具\nauthor: lishion\ntoc: true\n\n---\n\n在上一章[使用hexo+github搭建自己的博客](https://lishion.github.io/2018/06/07/%E4%BD%BF%E7%94%A8git+hexo%E5%BB%BA%E7%AB%8B%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/)介绍了如何使用 hexo 以及 github 搭建自己的博客。但是随后我们就遇到了一个问题: 如何我们想在多台电脑上编辑我们的博客应该怎么办。当然，我们可以选择新建一个仓库|分支利用 github 在多台电脑上同步。但是这种方式有一点繁琐，每次写作之前我们要保证源文件的同步 还要保证生成新的资源并部署到 github。例如一次典型的写作我们需要:\n\n```bash\ngit pull origin resource:resource # 假设我们的资源文件存储在 origin 分支\nhexo new xxx\ngit push origin resource\nhexo g\nhexo d\n```\n\n使用 travis 就能让我们省去渲染和部署的命令，变成:\n\n```shell~~\ngit pull origin resource:resource # 假设我们的资源文件存储在 origin 分支\nhexo new xxx\ngit push origin resource\n```\n\n~~难道就只能节省两个命令吗?~~\n\n~~哇，一共就五个命令。节省了两个等于节省了百分之四十啊!你说赛高不赛高。~~\n\n## 使用 travis 进行自动部署\n\n​\t看看廖雪峰大佬对 [travis](http://www.ruanyifeng.com/blog/2017/12/travis_ci_tutorial.html) 的介绍:\n\n> Travis CI 提供的是持续集成服务（Continuous Integration，简称 CI）。它绑定 Github 上面的项目，只要有新的代码，就会自动抓取。然后，提供一个运行环境，执行测试，完成构建，还能部署到服务器。\n\n简单的说 travis 提供了一个平台，这个平台可以监听你 Github 上仓库的变化，如果发生改变。则会将你的代码拉取到 travis 的服务器上，并执行你事先定义好的操作。\n\n### 初识 travis\n\n只要你拥有 github 的账户就可以直接登录 [travis官网](https://www.travis-ci.org/)使用。如果没有。。。。那就去注册一个。登录之后的页面大概是这样的:\n\n{% asset_img travis-mainpage.png travis主页 %}\n\n当然如果你没有使用过那左边的仓库栏和右边的信息栏都是空吧的，这个时候点击仓库栏上面的+号，添加一个仓库。\n\n{% asset_img travis-add.png 新增关联仓库 %}\n\n当然这里我们要选择的是 hexo 部署到的仓库。\n\n到这里，关联我们的仓库到 trvias 就完成了。但是我们还需要创建一个分支来保存我们的资源文件，因此，在我们的博客的根目录运行:\n\n```bash\ngit check out -b resource\ngit add *\ngit commit -m \"add resource\" \ngit add remote xxx # 这里的xxx就是博客对应的仓库\ngit push -u origin resource\n```\n\n这样把我们的资源文件同步到 github 上。\n\n### 配置文件\n\n当然，一个仓库有很多分支，我们还需要在根目录下配置一个 .travis.yml 文件来让 travis 知道我们需要构建哪一个分支。._config.yml 同时还告诉了 travis 在构建前需要做什么，构建后需要做什么。例如，我的 .travis.yml 如下:\n\n```yaml\nlanguage: node_js　# 使用什么语言\nnode_js:\n  - \"10\" # 语言版本\nbranches:\n  only:\n  - resource # 只构建 resource 分支\nscript:\n  - hexo g # 构建时执行的命令\ninstall:\n  - npm install  hexo # 由于 travis 的服务器默认是不带 hexo 所以需要安装\n  - npm install  hexo-cli\n  - npm install hexo-deployer-git\n  - sed -i'' \"s~git@github.com:~https://${TOKEN}@github.com/~\" _config.yml\nafter_success:\n  - hexo d # 构建完成后执行的命令 这里就是进行部署\nnotifications: # 设置通知项\n  email:\n    - 544670411@qq.com\n~                       \n```\n\n### 配置授权码\n\n现在还存在一个问题。travis 是需要从 github 拉取到他的服务器上，并且还需要通过 hexo 将构建后的文件 push 到 github，然而在 travis 的服务器上是不存在我们的 ssh 公钥的。也就会说目前 travis 还对无法操作我们的 github 仓库。所以我们还需要生成一个 github 授权码:\n\n{% asset_img author.png 新增授权码 %}\n\n然后在 travis 中配置这个授权码:\n\n{% asset_img token-add.png 配置授权码 %}\n\n其中名字可以自定义。但是配置了这个授权码如何使用呢?答案就是在 .travis.yml 中配置:\n\n```yaml\ninstall:\n  ...\n  - sed -i'' \"s~git@github.com:~https://${TOKEN}@github.com/~\" _config.yml\n```\n\n这里的 ${TOKEN} 需要将 TOKEN 替换为你刚才设置的名字。\n\n### 开始使用 \n\n到这里，整个设置部分就完成了。现在只需要 git pull origin resource 。然后登录 travis 就能看到项目正在构建了。从日志可以看到构建成功还是失败。以后每次写文章就只需要:\n\n```\ngit pull origin resource:resource\nhexo new xxx\ngit push orgin resource\n```\n\ntravis 就能帮你完成构建和部署的功能，是不是方便多了?\n\n\n\n\n\n\n","slug":"使用travis自动部署hexo到github","published":1,"updated":"2018-07-03T02:05:04.180Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjjhx8v330005b8tgzoedeabn","content":"<p>在上一章<a href=\"https://lishion.github.io/2018/06/07/%E4%BD%BF%E7%94%A8git+hexo%E5%BB%BA%E7%AB%8B%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/\" target=\"_blank\" rel=\"noopener\">使用hexo+github搭建自己的博客</a>介绍了如何使用 hexo 以及 github 搭建自己的博客。但是随后我们就遇到了一个问题: 如何我们想在多台电脑上编辑我们的博客应该怎么办。当然，我们可以选择新建一个仓库|分支利用 github 在多台电脑上同步。但是这种方式有一点繁琐，每次写作之前我们要保证源文件的同步 还要保证生成新的资源并部署到 github。例如一次典型的写作我们需要:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git pull origin resource:resource <span class=\"comment\"># 假设我们的资源文件存储在 origin 分支</span></span><br><span class=\"line\">hexo new xxx</span><br><span class=\"line\">git push origin resource</span><br><span class=\"line\">hexo g</span><br><span class=\"line\">hexo d</span><br></pre></td></tr></table></figure>\n<p>使用 travis 就能让我们省去渲染和部署的命令，变成:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git pull origin resource:resource # 假设我们的资源文件存储在 origin 分支</span><br><span class=\"line\">hexo new xxx</span><br><span class=\"line\">git push origin resource</span><br></pre></td></tr></table></figure>\n<p><del>难道就只能节省两个命令吗?</del></p>\n<p><del>哇，一共就五个命令。节省了两个等于节省了百分之四十啊!你说赛高不赛高。</del></p>\n<h2 id=\"使用-travis-进行自动部署\"><a href=\"#使用-travis-进行自动部署\" class=\"headerlink\" title=\"使用 travis 进行自动部署\"></a>使用 travis 进行自动部署</h2><p>​    看看廖雪峰大佬对 <a href=\"http://www.ruanyifeng.com/blog/2017/12/travis_ci_tutorial.html\" target=\"_blank\" rel=\"noopener\">travis</a> 的介绍:</p>\n<blockquote>\n<p>Travis CI 提供的是持续集成服务（Continuous Integration，简称 CI）。它绑定 Github 上面的项目，只要有新的代码，就会自动抓取。然后，提供一个运行环境，执行测试，完成构建，还能部署到服务器。</p>\n</blockquote>\n<p>简单的说 travis 提供了一个平台，这个平台可以监听你 Github 上仓库的变化，如果发生改变。则会将你的代码拉取到 travis 的服务器上，并执行你事先定义好的操作。</p>\n<h3 id=\"初识-travis\"><a href=\"#初识-travis\" class=\"headerlink\" title=\"初识 travis\"></a>初识 travis</h3><p>只要你拥有 github 的账户就可以直接登录 <a href=\"https://www.travis-ci.org/\" target=\"_blank\" rel=\"noopener\">travis官网</a>使用。如果没有。。。。那就去注册一个。登录之后的页面大概是这样的:</p>\n<img src=\"/2018/06/20/使用travis自动部署hexo到github/travis-mainpage.png\" title=\"travis主页\">\n<p>当然如果你没有使用过那左边的仓库栏和右边的信息栏都是空吧的，这个时候点击仓库栏上面的+号，添加一个仓库。</p>\n<img src=\"/2018/06/20/使用travis自动部署hexo到github/travis-add.png\" title=\"新增关联仓库\">\n<p>当然这里我们要选择的是 hexo 部署到的仓库。</p>\n<p>到这里，关联我们的仓库到 trvias 就完成了。但是我们还需要创建一个分支来保存我们的资源文件，因此，在我们的博客的根目录运行:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git check out -b resource</span><br><span class=\"line\">git add *</span><br><span class=\"line\">git commit -m <span class=\"string\">\"add resource\"</span> </span><br><span class=\"line\">git add remote xxx <span class=\"comment\"># 这里的xxx就是博客对应的仓库</span></span><br><span class=\"line\">git push -u origin resource</span><br></pre></td></tr></table></figure>\n<p>这样把我们的资源文件同步到 github 上。</p>\n<h3 id=\"配置文件\"><a href=\"#配置文件\" class=\"headerlink\" title=\"配置文件\"></a>配置文件</h3><p>当然，一个仓库有很多分支，我们还需要在根目录下配置一个 .travis.yml 文件来让 travis 知道我们需要构建哪一个分支。._config.yml 同时还告诉了 travis 在构建前需要做什么，构建后需要做什么。例如，我的 .travis.yml 如下:</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">language:</span> <span class=\"string\">node_js</span>　<span class=\"comment\"># 使用什么语言</span></span><br><span class=\"line\"><span class=\"attr\">node_js:</span></span><br><span class=\"line\"><span class=\"bullet\">  -</span> <span class=\"string\">\"10\"</span> <span class=\"comment\"># 语言版本</span></span><br><span class=\"line\"><span class=\"attr\">branches:</span></span><br><span class=\"line\"><span class=\"attr\">  only:</span></span><br><span class=\"line\"><span class=\"bullet\">  -</span> <span class=\"string\">resource</span> <span class=\"comment\"># 只构建 resource 分支</span></span><br><span class=\"line\"><span class=\"attr\">script:</span></span><br><span class=\"line\"><span class=\"bullet\">  -</span> <span class=\"string\">hexo</span> <span class=\"string\">g</span> <span class=\"comment\"># 构建时执行的命令</span></span><br><span class=\"line\"><span class=\"attr\">install:</span></span><br><span class=\"line\"><span class=\"bullet\">  -</span> <span class=\"string\">npm</span> <span class=\"string\">install</span>  <span class=\"string\">hexo</span> <span class=\"comment\"># 由于 travis 的服务器默认是不带 hexo 所以需要安装</span></span><br><span class=\"line\"><span class=\"bullet\">  -</span> <span class=\"string\">npm</span> <span class=\"string\">install</span>  <span class=\"string\">hexo-cli</span></span><br><span class=\"line\"><span class=\"bullet\">  -</span> <span class=\"string\">npm</span> <span class=\"string\">install</span> <span class=\"string\">hexo-deployer-git</span></span><br><span class=\"line\"><span class=\"bullet\">  -</span> <span class=\"string\">sed</span> <span class=\"bullet\">-i''</span> <span class=\"string\">\"s~git@github.com:~https://$&#123;TOKEN&#125;@github.com/~\"</span> <span class=\"string\">_config.yml</span></span><br><span class=\"line\"><span class=\"attr\">after_success:</span></span><br><span class=\"line\"><span class=\"bullet\">  -</span> <span class=\"string\">hexo</span> <span class=\"string\">d</span> <span class=\"comment\"># 构建完成后执行的命令 这里就是进行部署</span></span><br><span class=\"line\"><span class=\"attr\">notifications:</span> <span class=\"comment\"># 设置通知项</span></span><br><span class=\"line\"><span class=\"attr\">  email:</span></span><br><span class=\"line\"><span class=\"bullet\">    -</span> <span class=\"number\">544670411</span><span class=\"string\">@qq.com</span></span><br><span class=\"line\"><span class=\"string\">~</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"配置授权码\"><a href=\"#配置授权码\" class=\"headerlink\" title=\"配置授权码\"></a>配置授权码</h3><p>现在还存在一个问题。travis 是需要从 github 拉取到他的服务器上，并且还需要通过 hexo 将构建后的文件 push 到 github，然而在 travis 的服务器上是不存在我们的 ssh 公钥的。也就会说目前 travis 还对无法操作我们的 github 仓库。所以我们还需要生成一个 github 授权码:</p>\n<img src=\"/2018/06/20/使用travis自动部署hexo到github/author.png\" title=\"新增授权码\">\n<p>然后在 travis 中配置这个授权码:</p>\n<img src=\"/2018/06/20/使用travis自动部署hexo到github/token-add.png\" title=\"配置授权码\">\n<p>其中名字可以自定义。但是配置了这个授权码如何使用呢?答案就是在 .travis.yml 中配置:</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">install:</span></span><br><span class=\"line\">  <span class=\"string\">...</span></span><br><span class=\"line\"><span class=\"bullet\">  -</span> <span class=\"string\">sed</span> <span class=\"bullet\">-i''</span> <span class=\"string\">\"s~git@github.com:~https://$&#123;TOKEN&#125;@github.com/~\"</span> <span class=\"string\">_config.yml</span></span><br></pre></td></tr></table></figure>\n<p>这里的 ${TOKEN} 需要将 TOKEN 替换为你刚才设置的名字。</p>\n<h3 id=\"开始使用\"><a href=\"#开始使用\" class=\"headerlink\" title=\"开始使用\"></a>开始使用</h3><p>到这里，整个设置部分就完成了。现在只需要 git pull origin resource 。然后登录 travis 就能看到项目正在构建了。从日志可以看到构建成功还是失败。以后每次写文章就只需要:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git pull origin resource:resource</span><br><span class=\"line\">hexo new xxx</span><br><span class=\"line\">git push orgin resource</span><br></pre></td></tr></table></figure>\n<p>travis 就能帮你完成构建和部署的功能，是不是方便多了?</p>\n","site":{"data":{}},"excerpt":"","more":"<p>在上一章<a href=\"https://lishion.github.io/2018/06/07/%E4%BD%BF%E7%94%A8git+hexo%E5%BB%BA%E7%AB%8B%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/\" target=\"_blank\" rel=\"noopener\">使用hexo+github搭建自己的博客</a>介绍了如何使用 hexo 以及 github 搭建自己的博客。但是随后我们就遇到了一个问题: 如何我们想在多台电脑上编辑我们的博客应该怎么办。当然，我们可以选择新建一个仓库|分支利用 github 在多台电脑上同步。但是这种方式有一点繁琐，每次写作之前我们要保证源文件的同步 还要保证生成新的资源并部署到 github。例如一次典型的写作我们需要:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git pull origin resource:resource <span class=\"comment\"># 假设我们的资源文件存储在 origin 分支</span></span><br><span class=\"line\">hexo new xxx</span><br><span class=\"line\">git push origin resource</span><br><span class=\"line\">hexo g</span><br><span class=\"line\">hexo d</span><br></pre></td></tr></table></figure>\n<p>使用 travis 就能让我们省去渲染和部署的命令，变成:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git pull origin resource:resource # 假设我们的资源文件存储在 origin 分支</span><br><span class=\"line\">hexo new xxx</span><br><span class=\"line\">git push origin resource</span><br></pre></td></tr></table></figure>\n<p><del>难道就只能节省两个命令吗?</del></p>\n<p><del>哇，一共就五个命令。节省了两个等于节省了百分之四十啊!你说赛高不赛高。</del></p>\n<h2 id=\"使用-travis-进行自动部署\"><a href=\"#使用-travis-进行自动部署\" class=\"headerlink\" title=\"使用 travis 进行自动部署\"></a>使用 travis 进行自动部署</h2><p>​    看看廖雪峰大佬对 <a href=\"http://www.ruanyifeng.com/blog/2017/12/travis_ci_tutorial.html\" target=\"_blank\" rel=\"noopener\">travis</a> 的介绍:</p>\n<blockquote>\n<p>Travis CI 提供的是持续集成服务（Continuous Integration，简称 CI）。它绑定 Github 上面的项目，只要有新的代码，就会自动抓取。然后，提供一个运行环境，执行测试，完成构建，还能部署到服务器。</p>\n</blockquote>\n<p>简单的说 travis 提供了一个平台，这个平台可以监听你 Github 上仓库的变化，如果发生改变。则会将你的代码拉取到 travis 的服务器上，并执行你事先定义好的操作。</p>\n<h3 id=\"初识-travis\"><a href=\"#初识-travis\" class=\"headerlink\" title=\"初识 travis\"></a>初识 travis</h3><p>只要你拥有 github 的账户就可以直接登录 <a href=\"https://www.travis-ci.org/\" target=\"_blank\" rel=\"noopener\">travis官网</a>使用。如果没有。。。。那就去注册一个。登录之后的页面大概是这样的:</p>\n<img src=\"/2018/06/20/使用travis自动部署hexo到github/travis-mainpage.png\" title=\"travis主页\">\n<p>当然如果你没有使用过那左边的仓库栏和右边的信息栏都是空吧的，这个时候点击仓库栏上面的+号，添加一个仓库。</p>\n<img src=\"/2018/06/20/使用travis自动部署hexo到github/travis-add.png\" title=\"新增关联仓库\">\n<p>当然这里我们要选择的是 hexo 部署到的仓库。</p>\n<p>到这里，关联我们的仓库到 trvias 就完成了。但是我们还需要创建一个分支来保存我们的资源文件，因此，在我们的博客的根目录运行:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git check out -b resource</span><br><span class=\"line\">git add *</span><br><span class=\"line\">git commit -m <span class=\"string\">\"add resource\"</span> </span><br><span class=\"line\">git add remote xxx <span class=\"comment\"># 这里的xxx就是博客对应的仓库</span></span><br><span class=\"line\">git push -u origin resource</span><br></pre></td></tr></table></figure>\n<p>这样把我们的资源文件同步到 github 上。</p>\n<h3 id=\"配置文件\"><a href=\"#配置文件\" class=\"headerlink\" title=\"配置文件\"></a>配置文件</h3><p>当然，一个仓库有很多分支，我们还需要在根目录下配置一个 .travis.yml 文件来让 travis 知道我们需要构建哪一个分支。._config.yml 同时还告诉了 travis 在构建前需要做什么，构建后需要做什么。例如，我的 .travis.yml 如下:</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">language:</span> <span class=\"string\">node_js</span>　<span class=\"comment\"># 使用什么语言</span></span><br><span class=\"line\"><span class=\"attr\">node_js:</span></span><br><span class=\"line\"><span class=\"bullet\">  -</span> <span class=\"string\">\"10\"</span> <span class=\"comment\"># 语言版本</span></span><br><span class=\"line\"><span class=\"attr\">branches:</span></span><br><span class=\"line\"><span class=\"attr\">  only:</span></span><br><span class=\"line\"><span class=\"bullet\">  -</span> <span class=\"string\">resource</span> <span class=\"comment\"># 只构建 resource 分支</span></span><br><span class=\"line\"><span class=\"attr\">script:</span></span><br><span class=\"line\"><span class=\"bullet\">  -</span> <span class=\"string\">hexo</span> <span class=\"string\">g</span> <span class=\"comment\"># 构建时执行的命令</span></span><br><span class=\"line\"><span class=\"attr\">install:</span></span><br><span class=\"line\"><span class=\"bullet\">  -</span> <span class=\"string\">npm</span> <span class=\"string\">install</span>  <span class=\"string\">hexo</span> <span class=\"comment\"># 由于 travis 的服务器默认是不带 hexo 所以需要安装</span></span><br><span class=\"line\"><span class=\"bullet\">  -</span> <span class=\"string\">npm</span> <span class=\"string\">install</span>  <span class=\"string\">hexo-cli</span></span><br><span class=\"line\"><span class=\"bullet\">  -</span> <span class=\"string\">npm</span> <span class=\"string\">install</span> <span class=\"string\">hexo-deployer-git</span></span><br><span class=\"line\"><span class=\"bullet\">  -</span> <span class=\"string\">sed</span> <span class=\"bullet\">-i''</span> <span class=\"string\">\"s~git@github.com:~https://$&#123;TOKEN&#125;@github.com/~\"</span> <span class=\"string\">_config.yml</span></span><br><span class=\"line\"><span class=\"attr\">after_success:</span></span><br><span class=\"line\"><span class=\"bullet\">  -</span> <span class=\"string\">hexo</span> <span class=\"string\">d</span> <span class=\"comment\"># 构建完成后执行的命令 这里就是进行部署</span></span><br><span class=\"line\"><span class=\"attr\">notifications:</span> <span class=\"comment\"># 设置通知项</span></span><br><span class=\"line\"><span class=\"attr\">  email:</span></span><br><span class=\"line\"><span class=\"bullet\">    -</span> <span class=\"number\">544670411</span><span class=\"string\">@qq.com</span></span><br><span class=\"line\"><span class=\"string\">~</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"配置授权码\"><a href=\"#配置授权码\" class=\"headerlink\" title=\"配置授权码\"></a>配置授权码</h3><p>现在还存在一个问题。travis 是需要从 github 拉取到他的服务器上，并且还需要通过 hexo 将构建后的文件 push 到 github，然而在 travis 的服务器上是不存在我们的 ssh 公钥的。也就会说目前 travis 还对无法操作我们的 github 仓库。所以我们还需要生成一个 github 授权码:</p>\n<img src=\"/2018/06/20/使用travis自动部署hexo到github/author.png\" title=\"新增授权码\">\n<p>然后在 travis 中配置这个授权码:</p>\n<img src=\"/2018/06/20/使用travis自动部署hexo到github/token-add.png\" title=\"配置授权码\">\n<p>其中名字可以自定义。但是配置了这个授权码如何使用呢?答案就是在 .travis.yml 中配置:</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">install:</span></span><br><span class=\"line\">  <span class=\"string\">...</span></span><br><span class=\"line\"><span class=\"bullet\">  -</span> <span class=\"string\">sed</span> <span class=\"bullet\">-i''</span> <span class=\"string\">\"s~git@github.com:~https://$&#123;TOKEN&#125;@github.com/~\"</span> <span class=\"string\">_config.yml</span></span><br></pre></td></tr></table></figure>\n<p>这里的 ${TOKEN} 需要将 TOKEN 替换为你刚才设置的名字。</p>\n<h3 id=\"开始使用\"><a href=\"#开始使用\" class=\"headerlink\" title=\"开始使用\"></a>开始使用</h3><p>到这里，整个设置部分就完成了。现在只需要 git pull origin resource 。然后登录 travis 就能看到项目正在构建了。从日志可以看到构建成功还是失败。以后每次写文章就只需要:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git pull origin resource:resource</span><br><span class=\"line\">hexo new xxx</span><br><span class=\"line\">git push orgin resource</span><br></pre></td></tr></table></figure>\n<p>travis 就能帮你完成构建和部署的功能，是不是方便多了?</p>\n"},{"title":"利用idea搭建spark-shell开发环境","date":"2018-06-22T06:54:33.000Z","author":"lishion","toc":true,"_content":"\n之前一直使用 vscode 写的 spark-shell 脚本，然后上传到服务器跑。但是 vscode 对 scala 支持实在是太差了，连基本的自动补全和语法检查都不支持。后来实在是忍受不了了，决定使用 idea 搭建一个编写 spark-shell 脚本的环境。**注意搭建这个环境主要是为了自动补全和语法检测，并不是为了编写可以运行的代码**。当然你按照这个方法搭建只需要配置一下调试器应该就可以运行了。\n\n## 用 maven 搭建 scala 运行环境\n\n既然要写 spark-shell 脚本。事先肯定得搭建好 scala 的运行环境。至于像 JDK 这一类的依赖可以装也可以不装，可以使用 idea 自带的。scala项目可以使用 sbt 搭建也可以使用 maven 搭建。由于对 sbt 不是很熟悉，再加上 sbt 在国内慢如蜗牛，多疑还是选择 maven。\n\n步骤如下:\n\n1. 安装 scala 插件，建议直接下载离线版本，直接安装下载好的 zip 文件\n\n2. 新建一个 maven 项目，稍后添加支持 scala\n\n3. 新增 scala sdk：\n\n   选择 `FIle -> setting -> Libraries -> 点击+ ->Scala SDK `\n\n   应该会有多个选项，选择一个合适的版本就可以\n\n到此如果可以新建 scala class 就说明安装成功了\n\n##引入 spark 依赖\n\n引入 spark 依赖其实很简单，只需在 pom.xml 中添加就可以 。一般来说只需要引入 spark-core 就行。如果还需要使用 spark sql 或则其他的依赖可以直接在[spark-maven](http://mvnrepository.com/artifact/org.apache.spark)中查找。\n\n但是，例如像 sc spark 这种在 spark-shell 准备好的对象在我们搭建的环境中是不存在的。为了避免每次都手动生成这些对象，我们可以在根目录新建一个 Env 类。然后在类中准备好 sc spark 对象。然后写脚本的时候只需要引入这个类就可以。例如:\n\n```scala\n// Env.scala\nimport org.apache.spark.{SparkConf,SparkContext}\nimport org.apache.spark.sql.SparkSession\n// 注意一定是 object\nobject Env {\n    val sc = new SparkContext(new SparkConf().setMaster(\"123\").setAppName(\"123\"))\n    val spark = new SparkSession()\n}\n```\n\n然后需要写不同功能的脚本只需要新建一个包，然后引入:\n\n```scala\nimport Env._\nsc.xxx\nspark.sql.xx\n```\n\n就可以获取到 sc 以及 spark 对象。","source":"_posts/利用idea搭建spark-shell开发环境.md","raw":"---\ntitle: 利用idea搭建spark-shell开发环境\ndate: 2018-06-22 14:54:33\ntags:\n  - 工具\n  - 教程\n  - 编程\n  - Spark\ncategories: 工具\nauthor: lishion\ntoc: true\n---\n\n之前一直使用 vscode 写的 spark-shell 脚本，然后上传到服务器跑。但是 vscode 对 scala 支持实在是太差了，连基本的自动补全和语法检查都不支持。后来实在是忍受不了了，决定使用 idea 搭建一个编写 spark-shell 脚本的环境。**注意搭建这个环境主要是为了自动补全和语法检测，并不是为了编写可以运行的代码**。当然你按照这个方法搭建只需要配置一下调试器应该就可以运行了。\n\n## 用 maven 搭建 scala 运行环境\n\n既然要写 spark-shell 脚本。事先肯定得搭建好 scala 的运行环境。至于像 JDK 这一类的依赖可以装也可以不装，可以使用 idea 自带的。scala项目可以使用 sbt 搭建也可以使用 maven 搭建。由于对 sbt 不是很熟悉，再加上 sbt 在国内慢如蜗牛，多疑还是选择 maven。\n\n步骤如下:\n\n1. 安装 scala 插件，建议直接下载离线版本，直接安装下载好的 zip 文件\n\n2. 新建一个 maven 项目，稍后添加支持 scala\n\n3. 新增 scala sdk：\n\n   选择 `FIle -> setting -> Libraries -> 点击+ ->Scala SDK `\n\n   应该会有多个选项，选择一个合适的版本就可以\n\n到此如果可以新建 scala class 就说明安装成功了\n\n##引入 spark 依赖\n\n引入 spark 依赖其实很简单，只需在 pom.xml 中添加就可以 。一般来说只需要引入 spark-core 就行。如果还需要使用 spark sql 或则其他的依赖可以直接在[spark-maven](http://mvnrepository.com/artifact/org.apache.spark)中查找。\n\n但是，例如像 sc spark 这种在 spark-shell 准备好的对象在我们搭建的环境中是不存在的。为了避免每次都手动生成这些对象，我们可以在根目录新建一个 Env 类。然后在类中准备好 sc spark 对象。然后写脚本的时候只需要引入这个类就可以。例如:\n\n```scala\n// Env.scala\nimport org.apache.spark.{SparkConf,SparkContext}\nimport org.apache.spark.sql.SparkSession\n// 注意一定是 object\nobject Env {\n    val sc = new SparkContext(new SparkConf().setMaster(\"123\").setAppName(\"123\"))\n    val spark = new SparkSession()\n}\n```\n\n然后需要写不同功能的脚本只需要新建一个包，然后引入:\n\n```scala\nimport Env._\nsc.xxx\nspark.sql.xx\n```\n\n就可以获取到 sc 以及 spark 对象。","slug":"利用idea搭建spark-shell开发环境","published":1,"updated":"2018-06-22T07:27:02.810Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjjhx8v340006b8tgq16c9fkl","content":"<p>之前一直使用 vscode 写的 spark-shell 脚本，然后上传到服务器跑。但是 vscode 对 scala 支持实在是太差了，连基本的自动补全和语法检查都不支持。后来实在是忍受不了了，决定使用 idea 搭建一个编写 spark-shell 脚本的环境。<strong>注意搭建这个环境主要是为了自动补全和语法检测，并不是为了编写可以运行的代码</strong>。当然你按照这个方法搭建只需要配置一下调试器应该就可以运行了。</p>\n<h2 id=\"用-maven-搭建-scala-运行环境\"><a href=\"#用-maven-搭建-scala-运行环境\" class=\"headerlink\" title=\"用 maven 搭建 scala 运行环境\"></a>用 maven 搭建 scala 运行环境</h2><p>既然要写 spark-shell 脚本。事先肯定得搭建好 scala 的运行环境。至于像 JDK 这一类的依赖可以装也可以不装，可以使用 idea 自带的。scala项目可以使用 sbt 搭建也可以使用 maven 搭建。由于对 sbt 不是很熟悉，再加上 sbt 在国内慢如蜗牛，多疑还是选择 maven。</p>\n<p>步骤如下:</p>\n<ol>\n<li><p>安装 scala 插件，建议直接下载离线版本，直接安装下载好的 zip 文件</p>\n</li>\n<li><p>新建一个 maven 项目，稍后添加支持 scala</p>\n</li>\n<li><p>新增 scala sdk：</p>\n<p>选择 <code>FIle -&gt; setting -&gt; Libraries -&gt; 点击+ -&gt;Scala SDK</code></p>\n<p>应该会有多个选项，选择一个合适的版本就可以</p>\n</li>\n</ol>\n<p>到此如果可以新建 scala class 就说明安装成功了</p>\n<p>##引入 spark 依赖</p>\n<p>引入 spark 依赖其实很简单，只需在 pom.xml 中添加就可以 。一般来说只需要引入 spark-core 就行。如果还需要使用 spark sql 或则其他的依赖可以直接在<a href=\"http://mvnrepository.com/artifact/org.apache.spark\" target=\"_blank\" rel=\"noopener\">spark-maven</a>中查找。</p>\n<p>但是，例如像 sc spark 这种在 spark-shell 准备好的对象在我们搭建的环境中是不存在的。为了避免每次都手动生成这些对象，我们可以在根目录新建一个 Env 类。然后在类中准备好 sc spark 对象。然后写脚本的时候只需要引入这个类就可以。例如:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Env.scala</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.&#123;<span class=\"type\">SparkConf</span>,<span class=\"type\">SparkContext</span>&#125;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.sql.<span class=\"type\">SparkSession</span></span><br><span class=\"line\"><span class=\"comment\">// 注意一定是 object</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Env</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> sc = <span class=\"keyword\">new</span> <span class=\"type\">SparkContext</span>(<span class=\"keyword\">new</span> <span class=\"type\">SparkConf</span>().setMaster(<span class=\"string\">\"123\"</span>).setAppName(<span class=\"string\">\"123\"</span>))</span><br><span class=\"line\">    <span class=\"keyword\">val</span> spark = <span class=\"keyword\">new</span> <span class=\"type\">SparkSession</span>()</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>然后需要写不同功能的脚本只需要新建一个包，然后引入:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> <span class=\"type\">Env</span>._</span><br><span class=\"line\">sc.xxx</span><br><span class=\"line\">spark.sql.xx</span><br></pre></td></tr></table></figure>\n<p>就可以获取到 sc 以及 spark 对象。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>之前一直使用 vscode 写的 spark-shell 脚本，然后上传到服务器跑。但是 vscode 对 scala 支持实在是太差了，连基本的自动补全和语法检查都不支持。后来实在是忍受不了了，决定使用 idea 搭建一个编写 spark-shell 脚本的环境。<strong>注意搭建这个环境主要是为了自动补全和语法检测，并不是为了编写可以运行的代码</strong>。当然你按照这个方法搭建只需要配置一下调试器应该就可以运行了。</p>\n<h2 id=\"用-maven-搭建-scala-运行环境\"><a href=\"#用-maven-搭建-scala-运行环境\" class=\"headerlink\" title=\"用 maven 搭建 scala 运行环境\"></a>用 maven 搭建 scala 运行环境</h2><p>既然要写 spark-shell 脚本。事先肯定得搭建好 scala 的运行环境。至于像 JDK 这一类的依赖可以装也可以不装，可以使用 idea 自带的。scala项目可以使用 sbt 搭建也可以使用 maven 搭建。由于对 sbt 不是很熟悉，再加上 sbt 在国内慢如蜗牛，多疑还是选择 maven。</p>\n<p>步骤如下:</p>\n<ol>\n<li><p>安装 scala 插件，建议直接下载离线版本，直接安装下载好的 zip 文件</p>\n</li>\n<li><p>新建一个 maven 项目，稍后添加支持 scala</p>\n</li>\n<li><p>新增 scala sdk：</p>\n<p>选择 <code>FIle -&gt; setting -&gt; Libraries -&gt; 点击+ -&gt;Scala SDK</code></p>\n<p>应该会有多个选项，选择一个合适的版本就可以</p>\n</li>\n</ol>\n<p>到此如果可以新建 scala class 就说明安装成功了</p>\n<p>##引入 spark 依赖</p>\n<p>引入 spark 依赖其实很简单，只需在 pom.xml 中添加就可以 。一般来说只需要引入 spark-core 就行。如果还需要使用 spark sql 或则其他的依赖可以直接在<a href=\"http://mvnrepository.com/artifact/org.apache.spark\" target=\"_blank\" rel=\"noopener\">spark-maven</a>中查找。</p>\n<p>但是，例如像 sc spark 这种在 spark-shell 准备好的对象在我们搭建的环境中是不存在的。为了避免每次都手动生成这些对象，我们可以在根目录新建一个 Env 类。然后在类中准备好 sc spark 对象。然后写脚本的时候只需要引入这个类就可以。例如:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Env.scala</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.&#123;<span class=\"type\">SparkConf</span>,<span class=\"type\">SparkContext</span>&#125;</span><br><span class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.sql.<span class=\"type\">SparkSession</span></span><br><span class=\"line\"><span class=\"comment\">// 注意一定是 object</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Env</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> sc = <span class=\"keyword\">new</span> <span class=\"type\">SparkContext</span>(<span class=\"keyword\">new</span> <span class=\"type\">SparkConf</span>().setMaster(<span class=\"string\">\"123\"</span>).setAppName(<span class=\"string\">\"123\"</span>))</span><br><span class=\"line\">    <span class=\"keyword\">val</span> spark = <span class=\"keyword\">new</span> <span class=\"type\">SparkSession</span>()</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>然后需要写不同功能的脚本只需要新建一个包，然后引入:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> <span class=\"type\">Env</span>._</span><br><span class=\"line\">sc.xxx</span><br><span class=\"line\">spark.sql.xx</span><br></pre></td></tr></table></figure>\n<p>就可以获取到 sc 以及 spark 对象。</p>\n"},{"title":"在hexo中书写latex公式","date":"2018-06-28T05:27:26.000Z","author":"lishion","toc":true,"_content":"\n昨天在写笔记的时候发现在 typora 中可以正确显示的 latex 公式 hexo 无法正确渲染。hexo 默认渲染器不支持 latex公式。需要安装插件才能使用。这里我采用了[hexo-renderer-markdown-it-plus](https://github.com/CHENXCHEN/hexo-renderer-markdown-it-plus) 替换默认的渲染器。其中公式渲染采用 [katex](https://khan.github.io/KaTeX/) 使用步骤如下:\n\n1. 安装依赖:\n\n   `npm uninstall hexo-renderer-marked`\n\n   `npm install hexo-renderer-markdown-it-plus`\n\n   `npm install markdown-it-katex`\n\n2. 修改配置文件\n\n   在`_config.yml`中添加:\n\n   ```yaml\n   markdown_it_plus:\n       highlight: true\n       html: true\n       xhtmlOut: true\n       breaks: true\n       langPrefix:\n       linkify: true\n       typographer:\n       quotes: “”‘’\n       pre_class: highlight\n   ```\n\n3. 修改主题模板:\n\n      主题的 head 模板位于: `themes/next-gux/layout/_partials/head.swig` 中。将\n\n      ```\n      <link href=\"https://cdn.bootcss.com/KaTeX/0.7.1/katex.min.css\" rel=\"stylesheet\">\n      ```\n\n      添加到`head.swig`文件末尾。\n\n   完成以上步骤即可 hexo 即可正确的渲染 latex 公式。\n\n","source":"_posts/在hexo中书写latex公式.md","raw":"---\ntitle: 在hexo中书写latex公式\ndate: 2018-06-28 13:27:26\ntags:\n  - 教程\n  - 工具\ncategories: Spark源码阅读计划\nauthor: lishion\ntoc: true\n---\n\n昨天在写笔记的时候发现在 typora 中可以正确显示的 latex 公式 hexo 无法正确渲染。hexo 默认渲染器不支持 latex公式。需要安装插件才能使用。这里我采用了[hexo-renderer-markdown-it-plus](https://github.com/CHENXCHEN/hexo-renderer-markdown-it-plus) 替换默认的渲染器。其中公式渲染采用 [katex](https://khan.github.io/KaTeX/) 使用步骤如下:\n\n1. 安装依赖:\n\n   `npm uninstall hexo-renderer-marked`\n\n   `npm install hexo-renderer-markdown-it-plus`\n\n   `npm install markdown-it-katex`\n\n2. 修改配置文件\n\n   在`_config.yml`中添加:\n\n   ```yaml\n   markdown_it_plus:\n       highlight: true\n       html: true\n       xhtmlOut: true\n       breaks: true\n       langPrefix:\n       linkify: true\n       typographer:\n       quotes: “”‘’\n       pre_class: highlight\n   ```\n\n3. 修改主题模板:\n\n      主题的 head 模板位于: `themes/next-gux/layout/_partials/head.swig` 中。将\n\n      ```\n      <link href=\"https://cdn.bootcss.com/KaTeX/0.7.1/katex.min.css\" rel=\"stylesheet\">\n      ```\n\n      添加到`head.swig`文件末尾。\n\n   完成以上步骤即可 hexo 即可正确的渲染 latex 公式。\n\n","slug":"在hexo中书写latex公式","published":1,"updated":"2018-07-03T01:53:43.735Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjjhx8v370009b8tgd7hyxjer","content":"<p>昨天在写笔记的时候发现在 typora 中可以正确显示的 latex 公式 hexo 无法正确渲染。hexo 默认渲染器不支持 latex公式。需要安装插件才能使用。这里我采用了<a href=\"https://github.com/CHENXCHEN/hexo-renderer-markdown-it-plus\" target=\"_blank\" rel=\"noopener\">hexo-renderer-markdown-it-plus</a> 替换默认的渲染器。其中公式渲染采用 <a href=\"https://khan.github.io/KaTeX/\" target=\"_blank\" rel=\"noopener\">katex</a> 使用步骤如下:</p>\n<ol>\n<li><p>安装依赖:</p>\n<p><code>npm uninstall hexo-renderer-marked</code></p>\n<p><code>npm install hexo-renderer-markdown-it-plus</code></p>\n<p><code>npm install markdown-it-katex</code></p>\n</li>\n<li><p>修改配置文件</p>\n<p>在<code>_config.yml</code>中添加:</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">markdown_it_plus:</span></span><br><span class=\"line\"><span class=\"attr\">    highlight:</span> <span class=\"literal\">true</span></span><br><span class=\"line\"><span class=\"attr\">    html:</span> <span class=\"literal\">true</span></span><br><span class=\"line\"><span class=\"attr\">    xhtmlOut:</span> <span class=\"literal\">true</span></span><br><span class=\"line\"><span class=\"attr\">    breaks:</span> <span class=\"literal\">true</span></span><br><span class=\"line\"><span class=\"attr\">    langPrefix:</span></span><br><span class=\"line\"><span class=\"attr\">    linkify:</span> <span class=\"literal\">true</span></span><br><span class=\"line\"><span class=\"attr\">    typographer:</span></span><br><span class=\"line\"><span class=\"attr\">    quotes:</span> <span class=\"string\">“”‘’</span></span><br><span class=\"line\"><span class=\"attr\">    pre_class:</span> <span class=\"string\">highlight</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>修改主题模板:</p>\n<p>   主题的 head 模板位于: <code>themes/next-gux/layout/_partials/head.swig</code> 中。将</p>\n   <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;link href=&quot;https://cdn.bootcss.com/KaTeX/0.7.1/katex.min.css&quot; rel=&quot;stylesheet&quot;&gt;</span><br></pre></td></tr></table></figure>\n<p>   添加到<code>head.swig</code>文件末尾。</p>\n<p>完成以上步骤即可 hexo 即可正确的渲染 latex 公式。</p>\n</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<p>昨天在写笔记的时候发现在 typora 中可以正确显示的 latex 公式 hexo 无法正确渲染。hexo 默认渲染器不支持 latex公式。需要安装插件才能使用。这里我采用了<a href=\"https://github.com/CHENXCHEN/hexo-renderer-markdown-it-plus\" target=\"_blank\" rel=\"noopener\">hexo-renderer-markdown-it-plus</a> 替换默认的渲染器。其中公式渲染采用 <a href=\"https://khan.github.io/KaTeX/\" target=\"_blank\" rel=\"noopener\">katex</a> 使用步骤如下:</p>\n<ol>\n<li><p>安装依赖:</p>\n<p><code>npm uninstall hexo-renderer-marked</code></p>\n<p><code>npm install hexo-renderer-markdown-it-plus</code></p>\n<p><code>npm install markdown-it-katex</code></p>\n</li>\n<li><p>修改配置文件</p>\n<p>在<code>_config.yml</code>中添加:</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">markdown_it_plus:</span></span><br><span class=\"line\"><span class=\"attr\">    highlight:</span> <span class=\"literal\">true</span></span><br><span class=\"line\"><span class=\"attr\">    html:</span> <span class=\"literal\">true</span></span><br><span class=\"line\"><span class=\"attr\">    xhtmlOut:</span> <span class=\"literal\">true</span></span><br><span class=\"line\"><span class=\"attr\">    breaks:</span> <span class=\"literal\">true</span></span><br><span class=\"line\"><span class=\"attr\">    langPrefix:</span></span><br><span class=\"line\"><span class=\"attr\">    linkify:</span> <span class=\"literal\">true</span></span><br><span class=\"line\"><span class=\"attr\">    typographer:</span></span><br><span class=\"line\"><span class=\"attr\">    quotes:</span> <span class=\"string\">“”‘’</span></span><br><span class=\"line\"><span class=\"attr\">    pre_class:</span> <span class=\"string\">highlight</span></span><br></pre></td></tr></table></figure>\n</li>\n<li><p>修改主题模板:</p>\n<p>   主题的 head 模板位于: <code>themes/next-gux/layout/_partials/head.swig</code> 中。将</p>\n   <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;link href=&quot;https://cdn.bootcss.com/KaTeX/0.7.1/katex.min.css&quot; rel=&quot;stylesheet&quot;&gt;</span><br></pre></td></tr></table></figure>\n<p>   添加到<code>head.swig</code>文件末尾。</p>\n<p>完成以上步骤即可 hexo 即可正确的渲染 latex 公式。</p>\n</li>\n</ol>\n"},{"title":"常用 Linux 命令系列","author":"lishion","toc":true,"date":"2018-06-09T02:42:45.000Z","_content":"\n记录一下用到的 Linux 命令\n\n## sort\n\nsort 可以用于对文本进行排序，常用的参数有:\n\n> * -t : 指定分隔符，默认为空格\n> * -n : 按照数字规则排序\n> * -k : 按空格分隔后的排序列数\n\n例如:\n\n```bash\n# sort_test\n1:2:3\n3:2:1\n2:3:4\n5:6:11\nsort -nk -t : sort_test # 按数字排序\n3:2:1\n1:2:3\n2:3:4\n5:6:11\nsort -k -t : sort_test # 按首字母进行排序\n3:2:1\n5:6:11\n1:2:3\n2:3:4\n\n```\n\n","source":"_posts/常用-linux-命令系列.md","raw":"---\ntitle: 常用 Linux 命令系列\ncategories: 笔记\nauthor: lishion\ntoc: true\ndate: 2018-06-09 10:42:45\ntags: \n  - Linux\n  - 编程\n---\n\n记录一下用到的 Linux 命令\n\n## sort\n\nsort 可以用于对文本进行排序，常用的参数有:\n\n> * -t : 指定分隔符，默认为空格\n> * -n : 按照数字规则排序\n> * -k : 按空格分隔后的排序列数\n\n例如:\n\n```bash\n# sort_test\n1:2:3\n3:2:1\n2:3:4\n5:6:11\nsort -nk -t : sort_test # 按数字排序\n3:2:1\n1:2:3\n2:3:4\n5:6:11\nsort -k -t : sort_test # 按首字母进行排序\n3:2:1\n5:6:11\n1:2:3\n2:3:4\n\n```\n\n","slug":"常用-linux-命令系列","published":1,"updated":"2018-06-20T06:22:05.783Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjjhx8v38000ab8tg5zm33gv8","content":"<p>记录一下用到的 Linux 命令</p>\n<h2 id=\"sort\"><a href=\"#sort\" class=\"headerlink\" title=\"sort\"></a>sort</h2><p>sort 可以用于对文本进行排序，常用的参数有:</p>\n<blockquote>\n<ul>\n<li>-t : 指定分隔符，默认为空格</li>\n<li>-n : 按照数字规则排序</li>\n<li>-k : 按空格分隔后的排序列数</li>\n</ul>\n</blockquote>\n<p>例如:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># sort_test</span></span><br><span class=\"line\">1:2:3</span><br><span class=\"line\">3:2:1</span><br><span class=\"line\">2:3:4</span><br><span class=\"line\">5:6:11</span><br><span class=\"line\">sort -nk -t : sort_test <span class=\"comment\"># 按数字排序</span></span><br><span class=\"line\">3:2:1</span><br><span class=\"line\">1:2:3</span><br><span class=\"line\">2:3:4</span><br><span class=\"line\">5:6:11</span><br><span class=\"line\">sort -k -t : sort_test <span class=\"comment\"># 按首字母进行排序</span></span><br><span class=\"line\">3:2:1</span><br><span class=\"line\">5:6:11</span><br><span class=\"line\">1:2:3</span><br><span class=\"line\">2:3:4</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<p>记录一下用到的 Linux 命令</p>\n<h2 id=\"sort\"><a href=\"#sort\" class=\"headerlink\" title=\"sort\"></a>sort</h2><p>sort 可以用于对文本进行排序，常用的参数有:</p>\n<blockquote>\n<ul>\n<li>-t : 指定分隔符，默认为空格</li>\n<li>-n : 按照数字规则排序</li>\n<li>-k : 按空格分隔后的排序列数</li>\n</ul>\n</blockquote>\n<p>例如:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># sort_test</span></span><br><span class=\"line\">1:2:3</span><br><span class=\"line\">3:2:1</span><br><span class=\"line\">2:3:4</span><br><span class=\"line\">5:6:11</span><br><span class=\"line\">sort -nk -t : sort_test <span class=\"comment\"># 按数字排序</span></span><br><span class=\"line\">3:2:1</span><br><span class=\"line\">1:2:3</span><br><span class=\"line\">2:3:4</span><br><span class=\"line\">5:6:11</span><br><span class=\"line\">sort -k -t : sort_test <span class=\"comment\"># 按首字母进行排序</span></span><br><span class=\"line\">3:2:1</span><br><span class=\"line\">5:6:11</span><br><span class=\"line\">1:2:3</span><br><span class=\"line\">2:3:4</span><br></pre></td></tr></table></figure>\n"},{"title":"异常检测-Mass estimation与孤立森林","date":"2018-07-03T06:17:08.000Z","author":"lishion","toc":true,"_content":"\nMass Estimation 是一直种全新的评估数据分布的方法。与之前讲到过的孤立森林有很大的联系。\n\n## Mass Estimation\n\n在论文 *Mass Estimation and Its Applications* 一开始提到了现有的许多机器学习的方法都是基于**密度**估计的。这里的密度应该是指的**概率密度**，不知道与实际的数据密度有没有联系。看一下其中的一段话:\n\n>Density estimation has been the base modelling mechanism used in many techniques designed for tasks such as classiflcation, clustering, anomaly detection and information retrieval. For example in classiflcation, density estimation is employed to estimate class-conditional density function (or likelihood function) p(xjj) or posterior probability  p(jjx)|the principal function underlying many classiflcation methods e.g., mixture models, Bayesian networks, and Naive Bayes. Examples of density estimation include 。\n>\n>kernel density estimation, k-nearest neighbours density estimation, maximum likelihood procedures or Bayesian methods\n\n文中给出的几种方法都是基于概率密度的，而不是实际空间上的密度。有趣的是，Mass Estimation 虽然翻译过来应该是质量估计，但是实际它的性质与密度很类似。Mass Estimation 实际上是要先求得一个 mass distribution 。论文给出对质量分布描述如下:\n\n>A mass distribution stipulates an ordering from core points to fringe points in a data cloud. In addition, this ordering accentuates the fringe points with a concave function|fringe points have markedly smaller mass than points close to the core points。\n\n大致意思是质量分布给出了从数据中心到数据边缘的顺序。也就是对于处于中心的数据，质量分布函数的值越大，处于数据边缘的数据，质量分布函数值越小。这里和概率分布可以做一个对比：概率分布是出现次数多的数据，分布函数的值越大，反之越小。可见质量分布更注重数据分布的实际空间。这点对于异常检测非常重要，因为异常往往就是在实际空间中处于边缘部分的数据。\n\n最开始的论文 *Mass Estimation and Its Applications* 只给出了在一维空间中 mass distribution 的定义。而后来 *Multi-Dimensional Mass Estimation and Mass-based Clustering* 以及 *Mass estimation* 给出了从一维空间推广到多维空间的方法，并且给出了利用**二分空间树(Half-Space Tree)** 构造  mass distribution 的方法。也就是利用二叉树将数据空间分为$2^h$个，这个和之前讲过的决策树和孤立树都差不多。其中从一维推广到多维的过程比较难理解，这里直接给出在多维空间中的定义:\n\n设 $x$ 为 $R^d$ 空间中的一个点。$T^h(x)$表示$x$所在的空间。$T^h(.)$表示被划分的数据为$D$。$T^h(.|D_S)$表示被划分的数据为$D_s \\in D$。$m$为落在$T^h(x)$或$T^h(.|D_S)$中训练样本的个数。因此质量函数(mass base function)可以定义为:\n$$\nm(T^h(x)) = m 如果 x 落在的空间有m个训练样本 否则 0\n$$\n而对数据进行抽样得到多个子集后的平均质量为:\n$$\n\\overline{mass(x,h)} ≈\\frac 1 c \\sum_{k=1}^{c}m(T^h(x|D_{sk}))\n$$\n而最终，使用二分空间树求得的测试样本的 (augmented) mass 为:\n$$\ns(x) = m[l] * 2^l\n$$\n这里$m[l] $指的是样本落在 $l$ 层的叶子节点含有的训练样本数。而二分空间树在构造的时候叶子节点只有一个元素(不限制生长高度的情况)，因此最中的到:\n$$\ns(x) \\approx 2^l \\backsim l\n$$\n可以看出，最终求得的值正比于孤立森林的层数。论文中也提到了孤立森林的本质就是一种 mess estimation。因此这两种方法进行异常检测的性能相当。唯一不同，是二分空间树在构造的时候书中选择$\\frac {q_{max}-q_{min}} {2}$来作为划分点 $q$ 表示用来划分的特征，而孤立森林随机选择数据作为划分点。\n\n## 参考\n\nmess estimation 目前还没有大规模应用的趋势，对其进行研究的论文也不多。因此我也是看得云里雾里，所以这篇文章也就是大概对比一下 mess estimation 和孤立森林的关系。如果想详细研究，可以参照:\n\n>[1] Kai M T, Zhou G T, Liu F T, et al. Mass estimation and its applications[C]// ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2010:989-998.\n>\n>[2] Kai M T, Wells J R. Multi-dimensional Mass Estimation and Mass-based Clustering[J]. 2010:511-520.\n>\n>[3] Kai M T, Zhou G T, Liu F T, et al. Mass estimation[J]. Machine Learning, 2013, 90(1):127-160.\n\n其中 [1] 最开始提出了一维空间的 mass estimation。[2] 对其进行了推广到了多维空间。[3] 在 [1]的基础上增加了一些论文，并对比了其与孤立森林的关系。 ","source":"_posts/异常检测-Mass-estimation.md","raw":"---\ntitle: 异常检测-Mass estimation与孤立森林\ndate: 2018-07-03 14:17:08\ntags:\n  - 数据挖掘\n  - 机器学习\n  - 异常检测\ncategories: 异常检测\nauthor: lishion\ntoc: true\n\n---\n\nMass Estimation 是一直种全新的评估数据分布的方法。与之前讲到过的孤立森林有很大的联系。\n\n## Mass Estimation\n\n在论文 *Mass Estimation and Its Applications* 一开始提到了现有的许多机器学习的方法都是基于**密度**估计的。这里的密度应该是指的**概率密度**，不知道与实际的数据密度有没有联系。看一下其中的一段话:\n\n>Density estimation has been the base modelling mechanism used in many techniques designed for tasks such as classiflcation, clustering, anomaly detection and information retrieval. For example in classiflcation, density estimation is employed to estimate class-conditional density function (or likelihood function) p(xjj) or posterior probability  p(jjx)|the principal function underlying many classiflcation methods e.g., mixture models, Bayesian networks, and Naive Bayes. Examples of density estimation include 。\n>\n>kernel density estimation, k-nearest neighbours density estimation, maximum likelihood procedures or Bayesian methods\n\n文中给出的几种方法都是基于概率密度的，而不是实际空间上的密度。有趣的是，Mass Estimation 虽然翻译过来应该是质量估计，但是实际它的性质与密度很类似。Mass Estimation 实际上是要先求得一个 mass distribution 。论文给出对质量分布描述如下:\n\n>A mass distribution stipulates an ordering from core points to fringe points in a data cloud. In addition, this ordering accentuates the fringe points with a concave function|fringe points have markedly smaller mass than points close to the core points。\n\n大致意思是质量分布给出了从数据中心到数据边缘的顺序。也就是对于处于中心的数据，质量分布函数的值越大，处于数据边缘的数据，质量分布函数值越小。这里和概率分布可以做一个对比：概率分布是出现次数多的数据，分布函数的值越大，反之越小。可见质量分布更注重数据分布的实际空间。这点对于异常检测非常重要，因为异常往往就是在实际空间中处于边缘部分的数据。\n\n最开始的论文 *Mass Estimation and Its Applications* 只给出了在一维空间中 mass distribution 的定义。而后来 *Multi-Dimensional Mass Estimation and Mass-based Clustering* 以及 *Mass estimation* 给出了从一维空间推广到多维空间的方法，并且给出了利用**二分空间树(Half-Space Tree)** 构造  mass distribution 的方法。也就是利用二叉树将数据空间分为$2^h$个，这个和之前讲过的决策树和孤立树都差不多。其中从一维推广到多维的过程比较难理解，这里直接给出在多维空间中的定义:\n\n设 $x$ 为 $R^d$ 空间中的一个点。$T^h(x)$表示$x$所在的空间。$T^h(.)$表示被划分的数据为$D$。$T^h(.|D_S)$表示被划分的数据为$D_s \\in D$。$m$为落在$T^h(x)$或$T^h(.|D_S)$中训练样本的个数。因此质量函数(mass base function)可以定义为:\n$$\nm(T^h(x)) = m 如果 x 落在的空间有m个训练样本 否则 0\n$$\n而对数据进行抽样得到多个子集后的平均质量为:\n$$\n\\overline{mass(x,h)} ≈\\frac 1 c \\sum_{k=1}^{c}m(T^h(x|D_{sk}))\n$$\n而最终，使用二分空间树求得的测试样本的 (augmented) mass 为:\n$$\ns(x) = m[l] * 2^l\n$$\n这里$m[l] $指的是样本落在 $l$ 层的叶子节点含有的训练样本数。而二分空间树在构造的时候叶子节点只有一个元素(不限制生长高度的情况)，因此最中的到:\n$$\ns(x) \\approx 2^l \\backsim l\n$$\n可以看出，最终求得的值正比于孤立森林的层数。论文中也提到了孤立森林的本质就是一种 mess estimation。因此这两种方法进行异常检测的性能相当。唯一不同，是二分空间树在构造的时候书中选择$\\frac {q_{max}-q_{min}} {2}$来作为划分点 $q$ 表示用来划分的特征，而孤立森林随机选择数据作为划分点。\n\n## 参考\n\nmess estimation 目前还没有大规模应用的趋势，对其进行研究的论文也不多。因此我也是看得云里雾里，所以这篇文章也就是大概对比一下 mess estimation 和孤立森林的关系。如果想详细研究，可以参照:\n\n>[1] Kai M T, Zhou G T, Liu F T, et al. Mass estimation and its applications[C]// ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2010:989-998.\n>\n>[2] Kai M T, Wells J R. Multi-dimensional Mass Estimation and Mass-based Clustering[J]. 2010:511-520.\n>\n>[3] Kai M T, Zhou G T, Liu F T, et al. Mass estimation[J]. Machine Learning, 2013, 90(1):127-160.\n\n其中 [1] 最开始提出了一维空间的 mass estimation。[2] 对其进行了推广到了多维空间。[3] 在 [1]的基础上增加了一些论文，并对比了其与孤立森林的关系。 ","slug":"异常检测-Mass-estimation","published":1,"updated":"2018-07-03T07:33:26.145Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjjhx8v3b000eb8tg79yrbxlx","content":"<p>Mass Estimation 是一直种全新的评估数据分布的方法。与之前讲到过的孤立森林有很大的联系。</p>\n<h2 id=\"Mass-Estimation\"><a href=\"#Mass-Estimation\" class=\"headerlink\" title=\"Mass Estimation\"></a>Mass Estimation</h2><p>在论文 <em>Mass Estimation and Its Applications</em> 一开始提到了现有的许多机器学习的方法都是基于<strong>密度</strong>估计的。这里的密度应该是指的<strong>概率密度</strong>，不知道与实际的数据密度有没有联系。看一下其中的一段话:</p>\n<blockquote>\n<p>Density estimation has been the base modelling mechanism used in many techniques designed for tasks such as classiflcation, clustering, anomaly detection and information retrieval. For example in classiflcation, density estimation is employed to estimate class-conditional density function (or likelihood function) p(xjj) or posterior probability  p(jjx)|the principal function underlying many classiflcation methods e.g., mixture models, Bayesian networks, and Naive Bayes. Examples of density estimation include 。</p>\n<p>kernel density estimation, k-nearest neighbours density estimation, maximum likelihood procedures or Bayesian methods</p>\n</blockquote>\n<p>文中给出的几种方法都是基于概率密度的，而不是实际空间上的密度。有趣的是，Mass Estimation 虽然翻译过来应该是质量估计，但是实际它的性质与密度很类似。Mass Estimation 实际上是要先求得一个 mass distribution 。论文给出对质量分布描述如下:</p>\n<blockquote>\n<p>A mass distribution stipulates an ordering from core points to fringe points in a data cloud. In addition, this ordering accentuates the fringe points with a concave function|fringe points have markedly smaller mass than points close to the core points。</p>\n</blockquote>\n<p>大致意思是质量分布给出了从数据中心到数据边缘的顺序。也就是对于处于中心的数据，质量分布函数的值越大，处于数据边缘的数据，质量分布函数值越小。这里和概率分布可以做一个对比：概率分布是出现次数多的数据，分布函数的值越大，反之越小。可见质量分布更注重数据分布的实际空间。这点对于异常检测非常重要，因为异常往往就是在实际空间中处于边缘部分的数据。</p>\n<p>最开始的论文 <em>Mass Estimation and Its Applications</em> 只给出了在一维空间中 mass distribution 的定义。而后来 <em>Multi-Dimensional Mass Estimation and Mass-based Clustering</em> 以及 <em>Mass estimation</em> 给出了从一维空间推广到多维空间的方法，并且给出了利用<strong>二分空间树(Half-Space Tree)</strong> 构造  mass distribution 的方法。也就是利用二叉树将数据空间分为$2^h$个，这个和之前讲过的决策树和孤立树都差不多。其中从一维推广到多维的过程比较难理解，这里直接给出在多维空间中的定义:</p>\n<p>设 $x$ 为 $R^d$ 空间中的一个点。$T^h(x)$表示$x$所在的空间。$T^h(.)$表示被划分的数据为$D$。$T^h(.|D_S)$表示被划分的数据为$D_s \\in D$。$m$为落在$T^h(x)$或$T^h(.|D_S)$中训练样本的个数。因此质量函数(mass base function)可以定义为:<br>$$<br>m(T^h(x)) = m 如果 x 落在的空间有m个训练样本 否则 0<br>$$<br>而对数据进行抽样得到多个子集后的平均质量为:<br>$$<br>\\overline{mass(x,h)} ≈\\frac 1 c \\sum_{k=1}^{c}m(T^h(x|D_{sk}))<br>$$<br>而最终，使用二分空间树求得的测试样本的 (augmented) mass 为:<br>$$<br>s(x) = m[l] * 2^l<br>$$<br>这里$m[l] $指的是样本落在 $l$ 层的叶子节点含有的训练样本数。而二分空间树在构造的时候叶子节点只有一个元素(不限制生长高度的情况)，因此最中的到:<br>$$<br>s(x) \\approx 2^l \\backsim l<br>$$<br>可以看出，最终求得的值正比于孤立森林的层数。论文中也提到了孤立森林的本质就是一种 mess estimation。因此这两种方法进行异常检测的性能相当。唯一不同，是二分空间树在构造的时候书中选择$\\frac {q_{max}-q_{min}} {2}$来作为划分点 $q$ 表示用来划分的特征，而孤立森林随机选择数据作为划分点。</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p>mess estimation 目前还没有大规模应用的趋势，对其进行研究的论文也不多。因此我也是看得云里雾里，所以这篇文章也就是大概对比一下 mess estimation 和孤立森林的关系。如果想详细研究，可以参照:</p>\n<blockquote>\n<p>[1] Kai M T, Zhou G T, Liu F T, et al. Mass estimation and its applications[C]// ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2010:989-998.</p>\n<p>[2] Kai M T, Wells J R. Multi-dimensional Mass Estimation and Mass-based Clustering[J]. 2010:511-520.</p>\n<p>[3] Kai M T, Zhou G T, Liu F T, et al. Mass estimation[J]. Machine Learning, 2013, 90(1):127-160.</p>\n</blockquote>\n<p>其中 [1] 最开始提出了一维空间的 mass estimation。[2] 对其进行了推广到了多维空间。[3] 在 [1]的基础上增加了一些论文，并对比了其与孤立森林的关系。 </p>\n","site":{"data":{}},"excerpt":"","more":"<p>Mass Estimation 是一直种全新的评估数据分布的方法。与之前讲到过的孤立森林有很大的联系。</p>\n<h2 id=\"Mass-Estimation\"><a href=\"#Mass-Estimation\" class=\"headerlink\" title=\"Mass Estimation\"></a>Mass Estimation</h2><p>在论文 <em>Mass Estimation and Its Applications</em> 一开始提到了现有的许多机器学习的方法都是基于<strong>密度</strong>估计的。这里的密度应该是指的<strong>概率密度</strong>，不知道与实际的数据密度有没有联系。看一下其中的一段话:</p>\n<blockquote>\n<p>Density estimation has been the base modelling mechanism used in many techniques designed for tasks such as classiflcation, clustering, anomaly detection and information retrieval. For example in classiflcation, density estimation is employed to estimate class-conditional density function (or likelihood function) p(xjj) or posterior probability  p(jjx)|the principal function underlying many classiflcation methods e.g., mixture models, Bayesian networks, and Naive Bayes. Examples of density estimation include 。</p>\n<p>kernel density estimation, k-nearest neighbours density estimation, maximum likelihood procedures or Bayesian methods</p>\n</blockquote>\n<p>文中给出的几种方法都是基于概率密度的，而不是实际空间上的密度。有趣的是，Mass Estimation 虽然翻译过来应该是质量估计，但是实际它的性质与密度很类似。Mass Estimation 实际上是要先求得一个 mass distribution 。论文给出对质量分布描述如下:</p>\n<blockquote>\n<p>A mass distribution stipulates an ordering from core points to fringe points in a data cloud. In addition, this ordering accentuates the fringe points with a concave function|fringe points have markedly smaller mass than points close to the core points。</p>\n</blockquote>\n<p>大致意思是质量分布给出了从数据中心到数据边缘的顺序。也就是对于处于中心的数据，质量分布函数的值越大，处于数据边缘的数据，质量分布函数值越小。这里和概率分布可以做一个对比：概率分布是出现次数多的数据，分布函数的值越大，反之越小。可见质量分布更注重数据分布的实际空间。这点对于异常检测非常重要，因为异常往往就是在实际空间中处于边缘部分的数据。</p>\n<p>最开始的论文 <em>Mass Estimation and Its Applications</em> 只给出了在一维空间中 mass distribution 的定义。而后来 <em>Multi-Dimensional Mass Estimation and Mass-based Clustering</em> 以及 <em>Mass estimation</em> 给出了从一维空间推广到多维空间的方法，并且给出了利用<strong>二分空间树(Half-Space Tree)</strong> 构造  mass distribution 的方法。也就是利用二叉树将数据空间分为$2^h$个，这个和之前讲过的决策树和孤立树都差不多。其中从一维推广到多维的过程比较难理解，这里直接给出在多维空间中的定义:</p>\n<p>设 $x$ 为 $R^d$ 空间中的一个点。$T^h(x)$表示$x$所在的空间。$T^h(.)$表示被划分的数据为$D$。$T^h(.|D_S)$表示被划分的数据为$D_s \\in D$。$m$为落在$T^h(x)$或$T^h(.|D_S)$中训练样本的个数。因此质量函数(mass base function)可以定义为:<br>$$<br>m(T^h(x)) = m 如果 x 落在的空间有m个训练样本 否则 0<br>$$<br>而对数据进行抽样得到多个子集后的平均质量为:<br>$$<br>\\overline{mass(x,h)} ≈\\frac 1 c \\sum_{k=1}^{c}m(T^h(x|D_{sk}))<br>$$<br>而最终，使用二分空间树求得的测试样本的 (augmented) mass 为:<br>$$<br>s(x) = m[l] * 2^l<br>$$<br>这里$m[l] $指的是样本落在 $l$ 层的叶子节点含有的训练样本数。而二分空间树在构造的时候叶子节点只有一个元素(不限制生长高度的情况)，因此最中的到:<br>$$<br>s(x) \\approx 2^l \\backsim l<br>$$<br>可以看出，最终求得的值正比于孤立森林的层数。论文中也提到了孤立森林的本质就是一种 mess estimation。因此这两种方法进行异常检测的性能相当。唯一不同，是二分空间树在构造的时候书中选择$\\frac {q_{max}-q_{min}} {2}$来作为划分点 $q$ 表示用来划分的特征，而孤立森林随机选择数据作为划分点。</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p>mess estimation 目前还没有大规模应用的趋势，对其进行研究的论文也不多。因此我也是看得云里雾里，所以这篇文章也就是大概对比一下 mess estimation 和孤立森林的关系。如果想详细研究，可以参照:</p>\n<blockquote>\n<p>[1] Kai M T, Zhou G T, Liu F T, et al. Mass estimation and its applications[C]// ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2010:989-998.</p>\n<p>[2] Kai M T, Wells J R. Multi-dimensional Mass Estimation and Mass-based Clustering[J]. 2010:511-520.</p>\n<p>[3] Kai M T, Zhou G T, Liu F T, et al. Mass estimation[J]. Machine Learning, 2013, 90(1):127-160.</p>\n</blockquote>\n<p>其中 [1] 最开始提出了一维空间的 mass estimation。[2] 对其进行了推广到了多维空间。[3] 在 [1]的基础上增加了一些论文，并对比了其与孤立森林的关系。 </p>\n"},{"title":"异常检测-体验异常森林","date":"2018-07-12T01:58:08.000Z","author":"lishion","toc":true,"_content":"\n在[异常检测-孤立森林](https://lishion.github.io/2018/07/03/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-%E5%AD%A4%E7%AB%8B%E6%A3%AE%E6%9E%97/)中讲到了**孤立森林**的原理和详细算法。这篇文章主要是使用一下python 上的机器学习框架`sklearn`中的孤立森林模块，体验一下检测效果。\n\n## 检测高斯分布数据\n\n首先用产生1000个服从正太分布的数据,并画出散点图:\n\n```python\ndata = np.random.normal(50,10,(2000,2)) # 均值50 标准差10 1000个点\nplt.scatter(data[:,0],data[:,1]) \nplt.show()\n```\n\n如下:\n\n{% asset_img normal_data.png 正太分布数据采样图%}\n\n然后使用数据对孤立森林进行训练并获得异常得分:\n\n```\nfrom sklearn.ensemble import IsolationForest\nforest = IsolationForest(n_jobs=4,n_estimators=100)\nmodel =  forest.fit(data)\nscores = 0.5 - model.decision_function(data)\n```\n\n注意 sklearn 异常得分对论文中的得分进行了一点转换，用0.5 - 实际得分 =最终得分。因此获得的得分可能为负值。这里**再进行了一次处理**转换回了符合原论文的形式，也就是得分为 [0-1]。在论文中提到，如果一个样本的得分接近与1,那么这个样本一定是异常点。但是论文中并没有给出明确的阈值来划分异常样本和正常样本。我们先看一下样本异常得分的分布图:\n\n{% asset_img normal_scores_dis.png 样本得分频率值方图%}\n\n可以看出大部分的样本都在0.6以下，选择0.6作为阈值，用红点表示异常点:\n\n{% asset_img normal_result.png 异常检测结果%}\n\n在边缘部分的点被检测为了异常点，这与我们利用概率模型进行检验的结果相同。但是由于是无监督训练，无法给出检测率和虚警率。\n\n## 检测均匀分布数据 \n\n为了对比，我们需要对均匀分布的数据进行数据也进行一些分析。同样先产生样本:\n\n```\ndata = np.random.uniform(0,100,(1000,2)) # 0-1000 均匀分布的1000个点\nplt.scatter(data[:,0],data[:,1]) \nplt.show()\n```\n\n样本如下:\n\n{% asset_img uniform_data.png 异常检测结果%}\n\n获取异常得分并画出得分分布直方图:\n\n{% asset_img uniform_scores_dis.png 异常检测结果%}\n\n由于均匀分布产生的数据集应该没有明显的异常，这里就不对数据进行检测。而\n\n## 结果对比\n\n为了对比均匀分布的数据与正太分布数据的异常得分区别，利用同样的尺度画出分布曲线:\n\n{% asset_img compare.png 结果对比%}\n\n其中上图为正太分布频数直方图，下图为均匀分布频数直方图。\n\n可以看出，正太分布的异常得分从低分到高分逐渐减少，类似与是一种长尾分布。这是可以理解的，因为最中心部分的元素最正常，得分也最低，然后边缘的样本异常得分逐渐增长。因此形成了这种左往右逐渐降低的分布。\n\n从均匀分布的定义来看，样本出现在每一个位置的概率相同。但是由于采样误差和模型误差，导致样本的得分与理想情况不同。因此形成了一个类正太分布的情况。这也是比较合理的。为了证明这个猜想，我们手动生成一个完全理想的均匀分布数据如下:\n\n{% asset_img absolute_data.png 理想均匀分布数据%}\n\n得到的频数分布为:\n\n{% asset_img absolute_score_dis.png 理想均匀分布数据%}\n\n可以看出比不理想的均匀分布更作为训练数据时更接近于正太分布了。但是数据的结果仍然不够集中，因为理论上来说，均匀分布中每个样本的得分应该相同。所以异常得分频书分布应该是一条平行于y轴的直线，或者是方差非常小的正太分布图像。因此可以看出，至少在数据为均匀分布的情况，**模型仍有较大误差**。\n\n## 总结与讨论\n\n通过以上两个例子我们可以引出异常检测的一些难点与面临的问题:\n\n1. 由于需要已经异常检测的情况往往是无法获取到样本的标签，因此我们无法判断如何选取一个合适的阈值来将数据划分为正常数据和异常数据。\n2. 假如我们通过某种手段确定了一个阈值，将数据划分为正常、异常两部分。那么此时如何判断这个阈值的好坏？也就是说划分的异常样本是否据有参考价值？我们应该如何判断结果的可信度？\n3. 通过异常得分我们是否能够确定数据集中是否具有明显的异常样本？\n\n总之一句话就是**结论可信度**的问题，我们无法判断模型在我们需要的数据集上的表现如何，当然这也不只是是孤立森林面临的问题。\n\n此外，如果无法准确的判断每一个样本是否属于异常或正常情况。那么是否可以通过异常得分来判断某个数据集中总体异常水平？如果能判断，这也是一个非常有用的信息。\n\n","source":"_posts/异常检测-体验孤立森林.md","raw":"---\ntitle: 异常检测-体验异常森林\ndate: 2018-07-12 09:58:08\ntags:\n  - 数据挖掘\n  - 机器学习\n  - 异常检测\n  - 编程\ncategories: 异常检测\nauthor: lishion\ntoc: true\n\n---\n\n在[异常检测-孤立森林](https://lishion.github.io/2018/07/03/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-%E5%AD%A4%E7%AB%8B%E6%A3%AE%E6%9E%97/)中讲到了**孤立森林**的原理和详细算法。这篇文章主要是使用一下python 上的机器学习框架`sklearn`中的孤立森林模块，体验一下检测效果。\n\n## 检测高斯分布数据\n\n首先用产生1000个服从正太分布的数据,并画出散点图:\n\n```python\ndata = np.random.normal(50,10,(2000,2)) # 均值50 标准差10 1000个点\nplt.scatter(data[:,0],data[:,1]) \nplt.show()\n```\n\n如下:\n\n{% asset_img normal_data.png 正太分布数据采样图%}\n\n然后使用数据对孤立森林进行训练并获得异常得分:\n\n```\nfrom sklearn.ensemble import IsolationForest\nforest = IsolationForest(n_jobs=4,n_estimators=100)\nmodel =  forest.fit(data)\nscores = 0.5 - model.decision_function(data)\n```\n\n注意 sklearn 异常得分对论文中的得分进行了一点转换，用0.5 - 实际得分 =最终得分。因此获得的得分可能为负值。这里**再进行了一次处理**转换回了符合原论文的形式，也就是得分为 [0-1]。在论文中提到，如果一个样本的得分接近与1,那么这个样本一定是异常点。但是论文中并没有给出明确的阈值来划分异常样本和正常样本。我们先看一下样本异常得分的分布图:\n\n{% asset_img normal_scores_dis.png 样本得分频率值方图%}\n\n可以看出大部分的样本都在0.6以下，选择0.6作为阈值，用红点表示异常点:\n\n{% asset_img normal_result.png 异常检测结果%}\n\n在边缘部分的点被检测为了异常点，这与我们利用概率模型进行检验的结果相同。但是由于是无监督训练，无法给出检测率和虚警率。\n\n## 检测均匀分布数据 \n\n为了对比，我们需要对均匀分布的数据进行数据也进行一些分析。同样先产生样本:\n\n```\ndata = np.random.uniform(0,100,(1000,2)) # 0-1000 均匀分布的1000个点\nplt.scatter(data[:,0],data[:,1]) \nplt.show()\n```\n\n样本如下:\n\n{% asset_img uniform_data.png 异常检测结果%}\n\n获取异常得分并画出得分分布直方图:\n\n{% asset_img uniform_scores_dis.png 异常检测结果%}\n\n由于均匀分布产生的数据集应该没有明显的异常，这里就不对数据进行检测。而\n\n## 结果对比\n\n为了对比均匀分布的数据与正太分布数据的异常得分区别，利用同样的尺度画出分布曲线:\n\n{% asset_img compare.png 结果对比%}\n\n其中上图为正太分布频数直方图，下图为均匀分布频数直方图。\n\n可以看出，正太分布的异常得分从低分到高分逐渐减少，类似与是一种长尾分布。这是可以理解的，因为最中心部分的元素最正常，得分也最低，然后边缘的样本异常得分逐渐增长。因此形成了这种左往右逐渐降低的分布。\n\n从均匀分布的定义来看，样本出现在每一个位置的概率相同。但是由于采样误差和模型误差，导致样本的得分与理想情况不同。因此形成了一个类正太分布的情况。这也是比较合理的。为了证明这个猜想，我们手动生成一个完全理想的均匀分布数据如下:\n\n{% asset_img absolute_data.png 理想均匀分布数据%}\n\n得到的频数分布为:\n\n{% asset_img absolute_score_dis.png 理想均匀分布数据%}\n\n可以看出比不理想的均匀分布更作为训练数据时更接近于正太分布了。但是数据的结果仍然不够集中，因为理论上来说，均匀分布中每个样本的得分应该相同。所以异常得分频书分布应该是一条平行于y轴的直线，或者是方差非常小的正太分布图像。因此可以看出，至少在数据为均匀分布的情况，**模型仍有较大误差**。\n\n## 总结与讨论\n\n通过以上两个例子我们可以引出异常检测的一些难点与面临的问题:\n\n1. 由于需要已经异常检测的情况往往是无法获取到样本的标签，因此我们无法判断如何选取一个合适的阈值来将数据划分为正常数据和异常数据。\n2. 假如我们通过某种手段确定了一个阈值，将数据划分为正常、异常两部分。那么此时如何判断这个阈值的好坏？也就是说划分的异常样本是否据有参考价值？我们应该如何判断结果的可信度？\n3. 通过异常得分我们是否能够确定数据集中是否具有明显的异常样本？\n\n总之一句话就是**结论可信度**的问题，我们无法判断模型在我们需要的数据集上的表现如何，当然这也不只是是孤立森林面临的问题。\n\n此外，如果无法准确的判断每一个样本是否属于异常或正常情况。那么是否可以通过异常得分来判断某个数据集中总体异常水平？如果能判断，这也是一个非常有用的信息。\n\n","slug":"异常检测-体验孤立森林","published":1,"updated":"2018-07-12T07:22:41.984Z","_id":"cjjhx8v3e000gb8tgnh9dvicw","comments":1,"layout":"post","photos":[],"link":"","content":"<p>在<a href=\"https://lishion.github.io/2018/07/03/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-%E5%AD%A4%E7%AB%8B%E6%A3%AE%E6%9E%97/\" target=\"_blank\" rel=\"noopener\">异常检测-孤立森林</a>中讲到了<strong>孤立森林</strong>的原理和详细算法。这篇文章主要是使用一下python 上的机器学习框架<code>sklearn</code>中的孤立森林模块，体验一下检测效果。</p>\n<h2 id=\"检测高斯分布数据\"><a href=\"#检测高斯分布数据\" class=\"headerlink\" title=\"检测高斯分布数据\"></a>检测高斯分布数据</h2><p>首先用产生1000个服从正太分布的数据,并画出散点图:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">data = np.random.normal(<span class=\"number\">50</span>,<span class=\"number\">10</span>,(<span class=\"number\">2000</span>,<span class=\"number\">2</span>)) <span class=\"comment\"># 均值50 标准差10 1000个点</span></span><br><span class=\"line\">plt.scatter(data[:,<span class=\"number\">0</span>],data[:,<span class=\"number\">1</span>]) </span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p>如下:</p>\n<img src=\"/2018/07/12/异常检测-体验孤立森林/normal_data.png\" title=\"正太分布数据采样图\">\n<p>然后使用数据对孤立森林进行训练并获得异常得分:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from sklearn.ensemble import IsolationForest</span><br><span class=\"line\">forest = IsolationForest(n_jobs=4,n_estimators=100)</span><br><span class=\"line\">model =  forest.fit(data)</span><br><span class=\"line\">scores = 0.5 - model.decision_function(data)</span><br></pre></td></tr></table></figure>\n<p>注意 sklearn 异常得分对论文中的得分进行了一点转换，用0.5 - 实际得分 =最终得分。因此获得的得分可能为负值。这里<strong>再进行了一次处理</strong>转换回了符合原论文的形式，也就是得分为 [0-1]。在论文中提到，如果一个样本的得分接近与1,那么这个样本一定是异常点。但是论文中并没有给出明确的阈值来划分异常样本和正常样本。我们先看一下样本异常得分的分布图:</p>\n<img src=\"/2018/07/12/异常检测-体验孤立森林/normal_scores_dis.png\" title=\"样本得分频率值方图\">\n<p>可以看出大部分的样本都在0.6以下，选择0.6作为阈值，用红点表示异常点:</p>\n<img src=\"/2018/07/12/异常检测-体验孤立森林/normal_result.png\" title=\"异常检测结果\">\n<p>在边缘部分的点被检测为了异常点，这与我们利用概率模型进行检验的结果相同。但是由于是无监督训练，无法给出检测率和虚警率。</p>\n<h2 id=\"检测均匀分布数据\"><a href=\"#检测均匀分布数据\" class=\"headerlink\" title=\"检测均匀分布数据\"></a>检测均匀分布数据</h2><p>为了对比，我们需要对均匀分布的数据进行数据也进行一些分析。同样先产生样本:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">data = np.random.uniform(0,100,(1000,2)) # 0-1000 均匀分布的1000个点</span><br><span class=\"line\">plt.scatter(data[:,0],data[:,1]) </span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p>样本如下:</p>\n<img src=\"/2018/07/12/异常检测-体验孤立森林/uniform_data.png\" title=\"异常检测结果\">\n<p>获取异常得分并画出得分分布直方图:</p>\n<img src=\"/2018/07/12/异常检测-体验孤立森林/uniform_scores_dis.png\" title=\"异常检测结果\">\n<p>由于均匀分布产生的数据集应该没有明显的异常，这里就不对数据进行检测。而</p>\n<h2 id=\"结果对比\"><a href=\"#结果对比\" class=\"headerlink\" title=\"结果对比\"></a>结果对比</h2><p>为了对比均匀分布的数据与正太分布数据的异常得分区别，利用同样的尺度画出分布曲线:</p>\n<img src=\"/2018/07/12/异常检测-体验孤立森林/compare.png\" title=\"结果对比\">\n<p>其中上图为正太分布频数直方图，下图为均匀分布频数直方图。</p>\n<p>可以看出，正太分布的异常得分从低分到高分逐渐减少，类似与是一种长尾分布。这是可以理解的，因为最中心部分的元素最正常，得分也最低，然后边缘的样本异常得分逐渐增长。因此形成了这种左往右逐渐降低的分布。</p>\n<p>从均匀分布的定义来看，样本出现在每一个位置的概率相同。但是由于采样误差和模型误差，导致样本的得分与理想情况不同。因此形成了一个类正太分布的情况。这也是比较合理的。为了证明这个猜想，我们手动生成一个完全理想的均匀分布数据如下:</p>\n<img src=\"/2018/07/12/异常检测-体验孤立森林/absolute_data.png\" title=\"理想均匀分布数据\">\n<p>得到的频数分布为:</p>\n<img src=\"/2018/07/12/异常检测-体验孤立森林/absolute_score_dis.png\" title=\"理想均匀分布数据\">\n<p>可以看出比不理想的均匀分布更作为训练数据时更接近于正太分布了。但是数据的结果仍然不够集中，因为理论上来说，均匀分布中每个样本的得分应该相同。所以异常得分频书分布应该是一条平行于y轴的直线，或者是方差非常小的正太分布图像。因此可以看出，至少在数据为均匀分布的情况，<strong>模型仍有较大误差</strong>。</p>\n<h2 id=\"总结与讨论\"><a href=\"#总结与讨论\" class=\"headerlink\" title=\"总结与讨论\"></a>总结与讨论</h2><p>通过以上两个例子我们可以引出异常检测的一些难点与面临的问题:</p>\n<ol>\n<li>由于需要已经异常检测的情况往往是无法获取到样本的标签，因此我们无法判断如何选取一个合适的阈值来将数据划分为正常数据和异常数据。</li>\n<li>假如我们通过某种手段确定了一个阈值，将数据划分为正常、异常两部分。那么此时如何判断这个阈值的好坏？也就是说划分的异常样本是否据有参考价值？我们应该如何判断结果的可信度？</li>\n<li>通过异常得分我们是否能够确定数据集中是否具有明显的异常样本？</li>\n</ol>\n<p>总之一句话就是<strong>结论可信度</strong>的问题，我们无法判断模型在我们需要的数据集上的表现如何，当然这也不只是是孤立森林面临的问题。</p>\n<p>此外，如果无法准确的判断每一个样本是否属于异常或正常情况。那么是否可以通过异常得分来判断某个数据集中总体异常水平？如果能判断，这也是一个非常有用的信息。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>在<a href=\"https://lishion.github.io/2018/07/03/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-%E5%AD%A4%E7%AB%8B%E6%A3%AE%E6%9E%97/\" target=\"_blank\" rel=\"noopener\">异常检测-孤立森林</a>中讲到了<strong>孤立森林</strong>的原理和详细算法。这篇文章主要是使用一下python 上的机器学习框架<code>sklearn</code>中的孤立森林模块，体验一下检测效果。</p>\n<h2 id=\"检测高斯分布数据\"><a href=\"#检测高斯分布数据\" class=\"headerlink\" title=\"检测高斯分布数据\"></a>检测高斯分布数据</h2><p>首先用产生1000个服从正太分布的数据,并画出散点图:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">data = np.random.normal(<span class=\"number\">50</span>,<span class=\"number\">10</span>,(<span class=\"number\">2000</span>,<span class=\"number\">2</span>)) <span class=\"comment\"># 均值50 标准差10 1000个点</span></span><br><span class=\"line\">plt.scatter(data[:,<span class=\"number\">0</span>],data[:,<span class=\"number\">1</span>]) </span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p>如下:</p>\n<img src=\"/2018/07/12/异常检测-体验孤立森林/normal_data.png\" title=\"正太分布数据采样图\">\n<p>然后使用数据对孤立森林进行训练并获得异常得分:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from sklearn.ensemble import IsolationForest</span><br><span class=\"line\">forest = IsolationForest(n_jobs=4,n_estimators=100)</span><br><span class=\"line\">model =  forest.fit(data)</span><br><span class=\"line\">scores = 0.5 - model.decision_function(data)</span><br></pre></td></tr></table></figure>\n<p>注意 sklearn 异常得分对论文中的得分进行了一点转换，用0.5 - 实际得分 =最终得分。因此获得的得分可能为负值。这里<strong>再进行了一次处理</strong>转换回了符合原论文的形式，也就是得分为 [0-1]。在论文中提到，如果一个样本的得分接近与1,那么这个样本一定是异常点。但是论文中并没有给出明确的阈值来划分异常样本和正常样本。我们先看一下样本异常得分的分布图:</p>\n<img src=\"/2018/07/12/异常检测-体验孤立森林/normal_scores_dis.png\" title=\"样本得分频率值方图\">\n<p>可以看出大部分的样本都在0.6以下，选择0.6作为阈值，用红点表示异常点:</p>\n<img src=\"/2018/07/12/异常检测-体验孤立森林/normal_result.png\" title=\"异常检测结果\">\n<p>在边缘部分的点被检测为了异常点，这与我们利用概率模型进行检验的结果相同。但是由于是无监督训练，无法给出检测率和虚警率。</p>\n<h2 id=\"检测均匀分布数据\"><a href=\"#检测均匀分布数据\" class=\"headerlink\" title=\"检测均匀分布数据\"></a>检测均匀分布数据</h2><p>为了对比，我们需要对均匀分布的数据进行数据也进行一些分析。同样先产生样本:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">data = np.random.uniform(0,100,(1000,2)) # 0-1000 均匀分布的1000个点</span><br><span class=\"line\">plt.scatter(data[:,0],data[:,1]) </span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p>样本如下:</p>\n<img src=\"/2018/07/12/异常检测-体验孤立森林/uniform_data.png\" title=\"异常检测结果\">\n<p>获取异常得分并画出得分分布直方图:</p>\n<img src=\"/2018/07/12/异常检测-体验孤立森林/uniform_scores_dis.png\" title=\"异常检测结果\">\n<p>由于均匀分布产生的数据集应该没有明显的异常，这里就不对数据进行检测。而</p>\n<h2 id=\"结果对比\"><a href=\"#结果对比\" class=\"headerlink\" title=\"结果对比\"></a>结果对比</h2><p>为了对比均匀分布的数据与正太分布数据的异常得分区别，利用同样的尺度画出分布曲线:</p>\n<img src=\"/2018/07/12/异常检测-体验孤立森林/compare.png\" title=\"结果对比\">\n<p>其中上图为正太分布频数直方图，下图为均匀分布频数直方图。</p>\n<p>可以看出，正太分布的异常得分从低分到高分逐渐减少，类似与是一种长尾分布。这是可以理解的，因为最中心部分的元素最正常，得分也最低，然后边缘的样本异常得分逐渐增长。因此形成了这种左往右逐渐降低的分布。</p>\n<p>从均匀分布的定义来看，样本出现在每一个位置的概率相同。但是由于采样误差和模型误差，导致样本的得分与理想情况不同。因此形成了一个类正太分布的情况。这也是比较合理的。为了证明这个猜想，我们手动生成一个完全理想的均匀分布数据如下:</p>\n<img src=\"/2018/07/12/异常检测-体验孤立森林/absolute_data.png\" title=\"理想均匀分布数据\">\n<p>得到的频数分布为:</p>\n<img src=\"/2018/07/12/异常检测-体验孤立森林/absolute_score_dis.png\" title=\"理想均匀分布数据\">\n<p>可以看出比不理想的均匀分布更作为训练数据时更接近于正太分布了。但是数据的结果仍然不够集中，因为理论上来说，均匀分布中每个样本的得分应该相同。所以异常得分频书分布应该是一条平行于y轴的直线，或者是方差非常小的正太分布图像。因此可以看出，至少在数据为均匀分布的情况，<strong>模型仍有较大误差</strong>。</p>\n<h2 id=\"总结与讨论\"><a href=\"#总结与讨论\" class=\"headerlink\" title=\"总结与讨论\"></a>总结与讨论</h2><p>通过以上两个例子我们可以引出异常检测的一些难点与面临的问题:</p>\n<ol>\n<li>由于需要已经异常检测的情况往往是无法获取到样本的标签，因此我们无法判断如何选取一个合适的阈值来将数据划分为正常数据和异常数据。</li>\n<li>假如我们通过某种手段确定了一个阈值，将数据划分为正常、异常两部分。那么此时如何判断这个阈值的好坏？也就是说划分的异常样本是否据有参考价值？我们应该如何判断结果的可信度？</li>\n<li>通过异常得分我们是否能够确定数据集中是否具有明显的异常样本？</li>\n</ol>\n<p>总之一句话就是<strong>结论可信度</strong>的问题，我们无法判断模型在我们需要的数据集上的表现如何，当然这也不只是是孤立森林面临的问题。</p>\n<p>此外，如果无法准确的判断每一个样本是否属于异常或正常情况。那么是否可以通过异常得分来判断某个数据集中总体异常水平？如果能判断，这也是一个非常有用的信息。</p>\n"},{"title":"异常检测-孤立森林","date":"2018-07-03T02:03:42.000Z","author":"lishion","toc":true,"_content":"\n## iForest\n\niForest (Isolation Fores,孤立森林)是一种用于异常检测的模型，由周志华老师和他的学生等在2008年提出。孤立森林的基本思想是异常对象比起正常对象更容易被孤立。而孤立森林由多棵孤立树组成。孤立树来自于不断从样本数据中**随机选择特征**，并**随机选择**一个分割点进行二元划分生成子树，直到叶子节点中的每一个元素都被孤立，或者达到预先设置的最大高度。下面说一下孤立森林的特点以及算法的详细步骤。\n\n### 下采样\n\n与一般的异常检测方法需要大量的数据不同，孤立森林需要的数据量非常小。甚至需要通过下采样来增加检测的性能。在论文中，推荐的下采样值为 256 。并且论文中也提到，这个值可以用于很大范围的数据集。下采样值为 256 表示从原始样本中随机抽取 256 个数据(无放回)来训练一颗孤立树。每生成一棵树都需要重新进行采样。这里可以看出，孤立森林也是采样了**提升方法**中的**装袋**。即通过**自助**法从原始数据集中通过随机抽样的方式生成多个数据子集，并通过这些子集训练多个分类器以提高分类正确率。不同的是，在孤立森林中训练的是孤立树，而不是分类树\n\n### 随机选择\n\n在进行孤立树的生成时，用于进行分割的**特征**和**分割点**都是随机选择的。\n\n### 无监督\n\n孤立森林是**无监督**异常检测算法，因此训练样本与测试样本相同。\n\n\n### 详细算法\n\n孤立森林主要分为**训练**和**检测**两个阶段。\n\n#### 训练\n\n训练阶段主要是生成孤立树和孤立森林。孤立树与**二元分类树**结构相同，同样是使用超平面对样本空间不断划分。当对样本进行检测时，需要将样本输入孤立树，并判断样本最终落在哪个叶子节点。对于一个森林，论文中推荐由100 棵孤立树组成。孤立树对训练样本不断进行划分直到样本被孤立(叶子节点中只含有一个样本)，或者生长到最大高度。最大高度 $l$ 由 下采样数 $ψ$ 控制。公式为:\n$$\nl = ceiling(log2 ψ)\n$$\n生成一颗孤立树的算法如下:\n\n> 算法1: $iTree(X,e,l)$ \n>\n> 输入: $X$ - 输入数据，$e$ - 当前树的高度，$l$ - 最大高度 \n>\n> 输出: 一棵孤立树\n>\n> ```\n> if e >= l or |X|  <=1 then:\n>     return exNode{ size <- |X| }\n> else \n>     let Q be a list attributes in X\n>     randomly select an attribute q ∈ Q\n>     randomly select a split point p from max and min\n>     values of attribute q in X\n>     Xl ← filter(X,q < p)\n>     Xr ← filter(X,q ≥ p)\n>     return inNode{Left ← iTree(Xl,e + 1,l),\n>     Right ← iTree(Xr,e + 1,l),\n>     SplitAtt ← q,\n>     SplitValue ← p}\n> end if\n> ```\n\n\n\n其实就是不断随机选择特征和分割点对训练样本进行划分，划分的方式与二元分类树相同。直到:\n\n1. 该节点只包含一个元素\n2. 树达到了最大高度\n\n每生成一课孤立树就要对原始数据进行一次采样，并生成一个子集用于训练。生成的所有孤立树组合为一个孤立森林。\n\n#### 检测\n\n检测阶段使待检测的样本输入每一颗孤立树，并得到该样本经历的平均高度。样本最终的异常得分由以下公式给出:\n$$\nc(n) = 2H(n − 1) − (2(n − 1)/n)\n$$\n\n$$\ns(x, n) = 2^{-\\frac {E(h(x))} {c(n)}}\n$$\n\n其中 $n$ 该树接受的训练样本数量。平均高度 $h(x)$ 分两种情况进行计算:\n\n1. 样本 $x$ 最终落在 size 为 1 的叶子节点:\n\n   $h(x)=从根节点开始到叶子节点经过的边数$ \n\n2. 样本 $x$ 最终落在 size > 1 的叶子节点:\n\n   $h(x)=从根节点开始到叶子节点经过的边数 + c(该孤立树的样本大小)$\n\nsize 表示训练时落在该叶子节点的训练样本数量。\n\n通过计算每一颗孤立树的$h(x)$，求得平均$E(h(x))$，并计算异常得分$s(x,n)$。异常得分为一个归一化的值，代表的意义:\n\n1. 如果一个待检测样本的异常得分趋近于1，则该样本是一个异常点\n2. 如果一个待检测样本的异常得远小于0.5，则该样本应该被视为正常点\n3. 如果所有待检测的样本得分都约为0.5，则数据中没有十分异常的样本\n\n## SCiForest\n\nSCIForest(Isolation Forest with Split-selection Criterion) 是孤立森林的改进版。熟悉决策树的同学应该知道，孤立森林是使用平行于坐标轴的超平面对数据集进行划分。对于全局异常点来说，这种方法比较有用。但是对于局部异常点，平行坐标轴的划分方式往往无法将局部异常点较近的数据分开。因此 SCIForest 先将数据映射到一个随机的超平面(而不是孤立森林中映射到坐标轴在的平面)，对于映射后的数据，再使用一个最有超平面将其划分开，使得划分后的两组数据标准差最小。\n\nSCiForest 与 iForest 的最大区别是 SCiForest 用来对数据划分的超平面不是平行于坐标轴的，这一点类似于**斜决策树**。\n\nSCiForest 提升了 iForest 对`Scattered anomalies`和`Clustered anomalies`的检测能力，论文中对其定义如下:\n\n> Scattered anomalies are anomalies scattered outside the range of normal\n> points.\n>\n> Clustered anomalies are anomalies which form clusters outside the\n> range of normal points\n\n这也就是异常检测中经常遇到的 *masking(屏蔽)* 问题。\n\n## 参考\n\n> [1] Liu F T, Kai M T, Zhou Z H. Isolation Forest[C]// Eighth IEEE International Conference on Data Mining. IEEE, 2009:413-422.\n>\n> [2] Liu F T, Kai M T, Zhou Z H. On Detecting Clustered Anomalies Using SCiForest[C]// European Conference on Machine Learning and Knowledge Discovery in Databases. Springer-Verlag, 2010:274-290.\n\n\n\n","source":"_posts/异常检测-孤立森林.md","raw":"---\ntitle: 异常检测-孤立森林\ndate: 2018-07-03 10:03:42\ntags:\n  - 数据挖掘\n  - 机器学习\n  - 异常检测\ncategories: 异常检测\nauthor: lishion\ntoc: true\n\n---\n\n## iForest\n\niForest (Isolation Fores,孤立森林)是一种用于异常检测的模型，由周志华老师和他的学生等在2008年提出。孤立森林的基本思想是异常对象比起正常对象更容易被孤立。而孤立森林由多棵孤立树组成。孤立树来自于不断从样本数据中**随机选择特征**，并**随机选择**一个分割点进行二元划分生成子树，直到叶子节点中的每一个元素都被孤立，或者达到预先设置的最大高度。下面说一下孤立森林的特点以及算法的详细步骤。\n\n### 下采样\n\n与一般的异常检测方法需要大量的数据不同，孤立森林需要的数据量非常小。甚至需要通过下采样来增加检测的性能。在论文中，推荐的下采样值为 256 。并且论文中也提到，这个值可以用于很大范围的数据集。下采样值为 256 表示从原始样本中随机抽取 256 个数据(无放回)来训练一颗孤立树。每生成一棵树都需要重新进行采样。这里可以看出，孤立森林也是采样了**提升方法**中的**装袋**。即通过**自助**法从原始数据集中通过随机抽样的方式生成多个数据子集，并通过这些子集训练多个分类器以提高分类正确率。不同的是，在孤立森林中训练的是孤立树，而不是分类树\n\n### 随机选择\n\n在进行孤立树的生成时，用于进行分割的**特征**和**分割点**都是随机选择的。\n\n### 无监督\n\n孤立森林是**无监督**异常检测算法，因此训练样本与测试样本相同。\n\n\n### 详细算法\n\n孤立森林主要分为**训练**和**检测**两个阶段。\n\n#### 训练\n\n训练阶段主要是生成孤立树和孤立森林。孤立树与**二元分类树**结构相同，同样是使用超平面对样本空间不断划分。当对样本进行检测时，需要将样本输入孤立树，并判断样本最终落在哪个叶子节点。对于一个森林，论文中推荐由100 棵孤立树组成。孤立树对训练样本不断进行划分直到样本被孤立(叶子节点中只含有一个样本)，或者生长到最大高度。最大高度 $l$ 由 下采样数 $ψ$ 控制。公式为:\n$$\nl = ceiling(log2 ψ)\n$$\n生成一颗孤立树的算法如下:\n\n> 算法1: $iTree(X,e,l)$ \n>\n> 输入: $X$ - 输入数据，$e$ - 当前树的高度，$l$ - 最大高度 \n>\n> 输出: 一棵孤立树\n>\n> ```\n> if e >= l or |X|  <=1 then:\n>     return exNode{ size <- |X| }\n> else \n>     let Q be a list attributes in X\n>     randomly select an attribute q ∈ Q\n>     randomly select a split point p from max and min\n>     values of attribute q in X\n>     Xl ← filter(X,q < p)\n>     Xr ← filter(X,q ≥ p)\n>     return inNode{Left ← iTree(Xl,e + 1,l),\n>     Right ← iTree(Xr,e + 1,l),\n>     SplitAtt ← q,\n>     SplitValue ← p}\n> end if\n> ```\n\n\n\n其实就是不断随机选择特征和分割点对训练样本进行划分，划分的方式与二元分类树相同。直到:\n\n1. 该节点只包含一个元素\n2. 树达到了最大高度\n\n每生成一课孤立树就要对原始数据进行一次采样，并生成一个子集用于训练。生成的所有孤立树组合为一个孤立森林。\n\n#### 检测\n\n检测阶段使待检测的样本输入每一颗孤立树，并得到该样本经历的平均高度。样本最终的异常得分由以下公式给出:\n$$\nc(n) = 2H(n − 1) − (2(n − 1)/n)\n$$\n\n$$\ns(x, n) = 2^{-\\frac {E(h(x))} {c(n)}}\n$$\n\n其中 $n$ 该树接受的训练样本数量。平均高度 $h(x)$ 分两种情况进行计算:\n\n1. 样本 $x$ 最终落在 size 为 1 的叶子节点:\n\n   $h(x)=从根节点开始到叶子节点经过的边数$ \n\n2. 样本 $x$ 最终落在 size > 1 的叶子节点:\n\n   $h(x)=从根节点开始到叶子节点经过的边数 + c(该孤立树的样本大小)$\n\nsize 表示训练时落在该叶子节点的训练样本数量。\n\n通过计算每一颗孤立树的$h(x)$，求得平均$E(h(x))$，并计算异常得分$s(x,n)$。异常得分为一个归一化的值，代表的意义:\n\n1. 如果一个待检测样本的异常得分趋近于1，则该样本是一个异常点\n2. 如果一个待检测样本的异常得远小于0.5，则该样本应该被视为正常点\n3. 如果所有待检测的样本得分都约为0.5，则数据中没有十分异常的样本\n\n## SCiForest\n\nSCIForest(Isolation Forest with Split-selection Criterion) 是孤立森林的改进版。熟悉决策树的同学应该知道，孤立森林是使用平行于坐标轴的超平面对数据集进行划分。对于全局异常点来说，这种方法比较有用。但是对于局部异常点，平行坐标轴的划分方式往往无法将局部异常点较近的数据分开。因此 SCIForest 先将数据映射到一个随机的超平面(而不是孤立森林中映射到坐标轴在的平面)，对于映射后的数据，再使用一个最有超平面将其划分开，使得划分后的两组数据标准差最小。\n\nSCiForest 与 iForest 的最大区别是 SCiForest 用来对数据划分的超平面不是平行于坐标轴的，这一点类似于**斜决策树**。\n\nSCiForest 提升了 iForest 对`Scattered anomalies`和`Clustered anomalies`的检测能力，论文中对其定义如下:\n\n> Scattered anomalies are anomalies scattered outside the range of normal\n> points.\n>\n> Clustered anomalies are anomalies which form clusters outside the\n> range of normal points\n\n这也就是异常检测中经常遇到的 *masking(屏蔽)* 问题。\n\n## 参考\n\n> [1] Liu F T, Kai M T, Zhou Z H. Isolation Forest[C]// Eighth IEEE International Conference on Data Mining. IEEE, 2009:413-422.\n>\n> [2] Liu F T, Kai M T, Zhou Z H. On Detecting Clustered Anomalies Using SCiForest[C]// European Conference on Machine Learning and Knowledge Discovery in Databases. Springer-Verlag, 2010:274-290.\n\n\n\n","slug":"异常检测-孤立森林","published":1,"updated":"2018-07-03T06:17:15.457Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjjhx8v3i000lb8tgg2r8xyzb","content":"<h2 id=\"iForest\"><a href=\"#iForest\" class=\"headerlink\" title=\"iForest\"></a>iForest</h2><p>iForest (Isolation Fores,孤立森林)是一种用于异常检测的模型，由周志华老师和他的学生等在2008年提出。孤立森林的基本思想是异常对象比起正常对象更容易被孤立。而孤立森林由多棵孤立树组成。孤立树来自于不断从样本数据中<strong>随机选择特征</strong>，并<strong>随机选择</strong>一个分割点进行二元划分生成子树，直到叶子节点中的每一个元素都被孤立，或者达到预先设置的最大高度。下面说一下孤立森林的特点以及算法的详细步骤。</p>\n<h3 id=\"下采样\"><a href=\"#下采样\" class=\"headerlink\" title=\"下采样\"></a>下采样</h3><p>与一般的异常检测方法需要大量的数据不同，孤立森林需要的数据量非常小。甚至需要通过下采样来增加检测的性能。在论文中，推荐的下采样值为 256 。并且论文中也提到，这个值可以用于很大范围的数据集。下采样值为 256 表示从原始样本中随机抽取 256 个数据(无放回)来训练一颗孤立树。每生成一棵树都需要重新进行采样。这里可以看出，孤立森林也是采样了<strong>提升方法</strong>中的<strong>装袋</strong>。即通过<strong>自助</strong>法从原始数据集中通过随机抽样的方式生成多个数据子集，并通过这些子集训练多个分类器以提高分类正确率。不同的是，在孤立森林中训练的是孤立树，而不是分类树</p>\n<h3 id=\"随机选择\"><a href=\"#随机选择\" class=\"headerlink\" title=\"随机选择\"></a>随机选择</h3><p>在进行孤立树的生成时，用于进行分割的<strong>特征</strong>和<strong>分割点</strong>都是随机选择的。</p>\n<h3 id=\"无监督\"><a href=\"#无监督\" class=\"headerlink\" title=\"无监督\"></a>无监督</h3><p>孤立森林是<strong>无监督</strong>异常检测算法，因此训练样本与测试样本相同。</p>\n<h3 id=\"详细算法\"><a href=\"#详细算法\" class=\"headerlink\" title=\"详细算法\"></a>详细算法</h3><p>孤立森林主要分为<strong>训练</strong>和<strong>检测</strong>两个阶段。</p>\n<h4 id=\"训练\"><a href=\"#训练\" class=\"headerlink\" title=\"训练\"></a>训练</h4><p>训练阶段主要是生成孤立树和孤立森林。孤立树与<strong>二元分类树</strong>结构相同，同样是使用超平面对样本空间不断划分。当对样本进行检测时，需要将样本输入孤立树，并判断样本最终落在哪个叶子节点。对于一个森林，论文中推荐由100 棵孤立树组成。孤立树对训练样本不断进行划分直到样本被孤立(叶子节点中只含有一个样本)，或者生长到最大高度。最大高度 $l$ 由 下采样数 $ψ$ 控制。公式为:<br>$$<br>l = ceiling(log2 ψ)<br>$$<br>生成一颗孤立树的算法如下:</p>\n<blockquote>\n<p>算法1: $iTree(X,e,l)$ </p>\n<p>输入: $X$ - 输入数据，$e$ - 当前树的高度，$l$ - 最大高度 </p>\n<p>输出: 一棵孤立树</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt; if e &gt;= l or |X|  &lt;=1 then:</span><br><span class=\"line\">&gt;     return exNode&#123; size &lt;- |X| &#125;</span><br><span class=\"line\">&gt; else </span><br><span class=\"line\">&gt;     let Q be a list attributes in X</span><br><span class=\"line\">&gt;     randomly select an attribute q ∈ Q</span><br><span class=\"line\">&gt;     randomly select a split point p from max and min</span><br><span class=\"line\">&gt;     values of attribute q in X</span><br><span class=\"line\">&gt;     Xl ← filter(X,q &lt; p)</span><br><span class=\"line\">&gt;     Xr ← filter(X,q ≥ p)</span><br><span class=\"line\">&gt;     return inNode&#123;Left ← iTree(Xl,e + 1,l),</span><br><span class=\"line\">&gt;     Right ← iTree(Xr,e + 1,l),</span><br><span class=\"line\">&gt;     SplitAtt ← q,</span><br><span class=\"line\">&gt;     SplitValue ← p&#125;</span><br><span class=\"line\">&gt; end if</span><br><span class=\"line\">&gt;</span><br></pre></td></tr></table></figure>\n</blockquote>\n<p>其实就是不断随机选择特征和分割点对训练样本进行划分，划分的方式与二元分类树相同。直到:</p>\n<ol>\n<li>该节点只包含一个元素</li>\n<li>树达到了最大高度</li>\n</ol>\n<p>每生成一课孤立树就要对原始数据进行一次采样，并生成一个子集用于训练。生成的所有孤立树组合为一个孤立森林。</p>\n<h4 id=\"检测\"><a href=\"#检测\" class=\"headerlink\" title=\"检测\"></a>检测</h4><p>检测阶段使待检测的样本输入每一颗孤立树，并得到该样本经历的平均高度。样本最终的异常得分由以下公式给出:<br>$$<br>c(n) = 2H(n − 1) − (2(n − 1)/n)<br>$$</p>\n<p>$$<br>s(x, n) = 2^{-\\frac {E(h(x))} {c(n)}}<br>$$</p>\n<p>其中 $n$ 该树接受的训练样本数量。平均高度 $h(x)$ 分两种情况进行计算:</p>\n<ol>\n<li><p>样本 $x$ 最终落在 size 为 1 的叶子节点:</p>\n<p>$h(x)=从根节点开始到叶子节点经过的边数$ </p>\n</li>\n<li><p>样本 $x$ 最终落在 size &gt; 1 的叶子节点:</p>\n<p>$h(x)=从根节点开始到叶子节点经过的边数 + c(该孤立树的样本大小)$</p>\n</li>\n</ol>\n<p>size 表示训练时落在该叶子节点的训练样本数量。</p>\n<p>通过计算每一颗孤立树的$h(x)$，求得平均$E(h(x))$，并计算异常得分$s(x,n)$。异常得分为一个归一化的值，代表的意义:</p>\n<ol>\n<li>如果一个待检测样本的异常得分趋近于1，则该样本是一个异常点</li>\n<li>如果一个待检测样本的异常得远小于0.5，则该样本应该被视为正常点</li>\n<li>如果所有待检测的样本得分都约为0.5，则数据中没有十分异常的样本</li>\n</ol>\n<h2 id=\"SCiForest\"><a href=\"#SCiForest\" class=\"headerlink\" title=\"SCiForest\"></a>SCiForest</h2><p>SCIForest(Isolation Forest with Split-selection Criterion) 是孤立森林的改进版。熟悉决策树的同学应该知道，孤立森林是使用平行于坐标轴的超平面对数据集进行划分。对于全局异常点来说，这种方法比较有用。但是对于局部异常点，平行坐标轴的划分方式往往无法将局部异常点较近的数据分开。因此 SCIForest 先将数据映射到一个随机的超平面(而不是孤立森林中映射到坐标轴在的平面)，对于映射后的数据，再使用一个最有超平面将其划分开，使得划分后的两组数据标准差最小。</p>\n<p>SCiForest 与 iForest 的最大区别是 SCiForest 用来对数据划分的超平面不是平行于坐标轴的，这一点类似于<strong>斜决策树</strong>。</p>\n<p>SCiForest 提升了 iForest 对<code>Scattered anomalies</code>和<code>Clustered anomalies</code>的检测能力，论文中对其定义如下:</p>\n<blockquote>\n<p>Scattered anomalies are anomalies scattered outside the range of normal<br>points.</p>\n<p>Clustered anomalies are anomalies which form clusters outside the<br>range of normal points</p>\n</blockquote>\n<p>这也就是异常检测中经常遇到的 <em>masking(屏蔽)</em> 问题。</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><blockquote>\n<p>[1] Liu F T, Kai M T, Zhou Z H. Isolation Forest[C]// Eighth IEEE International Conference on Data Mining. IEEE, 2009:413-422.</p>\n<p>[2] Liu F T, Kai M T, Zhou Z H. On Detecting Clustered Anomalies Using SCiForest[C]// European Conference on Machine Learning and Knowledge Discovery in Databases. Springer-Verlag, 2010:274-290.</p>\n</blockquote>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"iForest\"><a href=\"#iForest\" class=\"headerlink\" title=\"iForest\"></a>iForest</h2><p>iForest (Isolation Fores,孤立森林)是一种用于异常检测的模型，由周志华老师和他的学生等在2008年提出。孤立森林的基本思想是异常对象比起正常对象更容易被孤立。而孤立森林由多棵孤立树组成。孤立树来自于不断从样本数据中<strong>随机选择特征</strong>，并<strong>随机选择</strong>一个分割点进行二元划分生成子树，直到叶子节点中的每一个元素都被孤立，或者达到预先设置的最大高度。下面说一下孤立森林的特点以及算法的详细步骤。</p>\n<h3 id=\"下采样\"><a href=\"#下采样\" class=\"headerlink\" title=\"下采样\"></a>下采样</h3><p>与一般的异常检测方法需要大量的数据不同，孤立森林需要的数据量非常小。甚至需要通过下采样来增加检测的性能。在论文中，推荐的下采样值为 256 。并且论文中也提到，这个值可以用于很大范围的数据集。下采样值为 256 表示从原始样本中随机抽取 256 个数据(无放回)来训练一颗孤立树。每生成一棵树都需要重新进行采样。这里可以看出，孤立森林也是采样了<strong>提升方法</strong>中的<strong>装袋</strong>。即通过<strong>自助</strong>法从原始数据集中通过随机抽样的方式生成多个数据子集，并通过这些子集训练多个分类器以提高分类正确率。不同的是，在孤立森林中训练的是孤立树，而不是分类树</p>\n<h3 id=\"随机选择\"><a href=\"#随机选择\" class=\"headerlink\" title=\"随机选择\"></a>随机选择</h3><p>在进行孤立树的生成时，用于进行分割的<strong>特征</strong>和<strong>分割点</strong>都是随机选择的。</p>\n<h3 id=\"无监督\"><a href=\"#无监督\" class=\"headerlink\" title=\"无监督\"></a>无监督</h3><p>孤立森林是<strong>无监督</strong>异常检测算法，因此训练样本与测试样本相同。</p>\n<h3 id=\"详细算法\"><a href=\"#详细算法\" class=\"headerlink\" title=\"详细算法\"></a>详细算法</h3><p>孤立森林主要分为<strong>训练</strong>和<strong>检测</strong>两个阶段。</p>\n<h4 id=\"训练\"><a href=\"#训练\" class=\"headerlink\" title=\"训练\"></a>训练</h4><p>训练阶段主要是生成孤立树和孤立森林。孤立树与<strong>二元分类树</strong>结构相同，同样是使用超平面对样本空间不断划分。当对样本进行检测时，需要将样本输入孤立树，并判断样本最终落在哪个叶子节点。对于一个森林，论文中推荐由100 棵孤立树组成。孤立树对训练样本不断进行划分直到样本被孤立(叶子节点中只含有一个样本)，或者生长到最大高度。最大高度 $l$ 由 下采样数 $ψ$ 控制。公式为:<br>$$<br>l = ceiling(log2 ψ)<br>$$<br>生成一颗孤立树的算法如下:</p>\n<blockquote>\n<p>算法1: $iTree(X,e,l)$ </p>\n<p>输入: $X$ - 输入数据，$e$ - 当前树的高度，$l$ - 最大高度 </p>\n<p>输出: 一棵孤立树</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt; if e &gt;= l or |X|  &lt;=1 then:</span><br><span class=\"line\">&gt;     return exNode&#123; size &lt;- |X| &#125;</span><br><span class=\"line\">&gt; else </span><br><span class=\"line\">&gt;     let Q be a list attributes in X</span><br><span class=\"line\">&gt;     randomly select an attribute q ∈ Q</span><br><span class=\"line\">&gt;     randomly select a split point p from max and min</span><br><span class=\"line\">&gt;     values of attribute q in X</span><br><span class=\"line\">&gt;     Xl ← filter(X,q &lt; p)</span><br><span class=\"line\">&gt;     Xr ← filter(X,q ≥ p)</span><br><span class=\"line\">&gt;     return inNode&#123;Left ← iTree(Xl,e + 1,l),</span><br><span class=\"line\">&gt;     Right ← iTree(Xr,e + 1,l),</span><br><span class=\"line\">&gt;     SplitAtt ← q,</span><br><span class=\"line\">&gt;     SplitValue ← p&#125;</span><br><span class=\"line\">&gt; end if</span><br><span class=\"line\">&gt;</span><br></pre></td></tr></table></figure>\n</blockquote>\n<p>其实就是不断随机选择特征和分割点对训练样本进行划分，划分的方式与二元分类树相同。直到:</p>\n<ol>\n<li>该节点只包含一个元素</li>\n<li>树达到了最大高度</li>\n</ol>\n<p>每生成一课孤立树就要对原始数据进行一次采样，并生成一个子集用于训练。生成的所有孤立树组合为一个孤立森林。</p>\n<h4 id=\"检测\"><a href=\"#检测\" class=\"headerlink\" title=\"检测\"></a>检测</h4><p>检测阶段使待检测的样本输入每一颗孤立树，并得到该样本经历的平均高度。样本最终的异常得分由以下公式给出:<br>$$<br>c(n) = 2H(n − 1) − (2(n − 1)/n)<br>$$</p>\n<p>$$<br>s(x, n) = 2^{-\\frac {E(h(x))} {c(n)}}<br>$$</p>\n<p>其中 $n$ 该树接受的训练样本数量。平均高度 $h(x)$ 分两种情况进行计算:</p>\n<ol>\n<li><p>样本 $x$ 最终落在 size 为 1 的叶子节点:</p>\n<p>$h(x)=从根节点开始到叶子节点经过的边数$ </p>\n</li>\n<li><p>样本 $x$ 最终落在 size &gt; 1 的叶子节点:</p>\n<p>$h(x)=从根节点开始到叶子节点经过的边数 + c(该孤立树的样本大小)$</p>\n</li>\n</ol>\n<p>size 表示训练时落在该叶子节点的训练样本数量。</p>\n<p>通过计算每一颗孤立树的$h(x)$，求得平均$E(h(x))$，并计算异常得分$s(x,n)$。异常得分为一个归一化的值，代表的意义:</p>\n<ol>\n<li>如果一个待检测样本的异常得分趋近于1，则该样本是一个异常点</li>\n<li>如果一个待检测样本的异常得远小于0.5，则该样本应该被视为正常点</li>\n<li>如果所有待检测的样本得分都约为0.5，则数据中没有十分异常的样本</li>\n</ol>\n<h2 id=\"SCiForest\"><a href=\"#SCiForest\" class=\"headerlink\" title=\"SCiForest\"></a>SCiForest</h2><p>SCIForest(Isolation Forest with Split-selection Criterion) 是孤立森林的改进版。熟悉决策树的同学应该知道，孤立森林是使用平行于坐标轴的超平面对数据集进行划分。对于全局异常点来说，这种方法比较有用。但是对于局部异常点，平行坐标轴的划分方式往往无法将局部异常点较近的数据分开。因此 SCIForest 先将数据映射到一个随机的超平面(而不是孤立森林中映射到坐标轴在的平面)，对于映射后的数据，再使用一个最有超平面将其划分开，使得划分后的两组数据标准差最小。</p>\n<p>SCiForest 与 iForest 的最大区别是 SCiForest 用来对数据划分的超平面不是平行于坐标轴的，这一点类似于<strong>斜决策树</strong>。</p>\n<p>SCiForest 提升了 iForest 对<code>Scattered anomalies</code>和<code>Clustered anomalies</code>的检测能力，论文中对其定义如下:</p>\n<blockquote>\n<p>Scattered anomalies are anomalies scattered outside the range of normal<br>points.</p>\n<p>Clustered anomalies are anomalies which form clusters outside the<br>range of normal points</p>\n</blockquote>\n<p>这也就是异常检测中经常遇到的 <em>masking(屏蔽)</em> 问题。</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><blockquote>\n<p>[1] Liu F T, Kai M T, Zhou Z H. Isolation Forest[C]// Eighth IEEE International Conference on Data Mining. IEEE, 2009:413-422.</p>\n<p>[2] Liu F T, Kai M T, Zhou Z H. On Detecting Clustered Anomalies Using SCiForest[C]// European Conference on Machine Learning and Knowledge Discovery in Databases. Springer-Verlag, 2010:274-290.</p>\n</blockquote>\n"},{"title":"异常检测-流检测","date":"2018-07-03T08:43:03.000Z","author":"lishion","toc":true,"_content":"之前提到的**孤立森林**是一种离线的数据挖掘挖掘|机器学习的方法。也就是说，**孤立森林**无法对实时的流数据进行检测。这里介绍一种使用 Half-Space Trees 构造的实时检测算法。Half-Space Trees 是一种构造方法和孤立森林极其相似的用于异常检测的模型，其理论是 Mass Estimation 。不熟悉的同学可以看:\n\n1. [异常检测-孤立森林](https://lishion.github.io/2018/07/03/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-%E5%AD%A4%E7%AB%8B%E6%A3%AE%E6%9E%97/)\n2. [异常检测-Mass estimation与孤立森林](https://lishion.github.io/2018/07/03/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-Mass-estimation/)\n\n具体的树和森林的构造方法就不在仔细讲。与**孤立森林**只有一点不同，孤立森林选择的划分点为随机的，而  Half-Space Trees 则是在用于划分的特征选取 (最大值+最小值)/2。这里主要讲一下其中在上面两篇文章中没有提到的概念以及如何进行事实检测。\n\n## 时间窗\n\n*Fast Anomaly Detection for Streaming Data* 文中直接用 window 来描述。这里为了好理解将其翻译为时间窗。在最开始阶段，使用 refer window 学习正常数据的质量特征(mass profile)。然后使用学得特征对流数据进行检测，而持续的流数据将放入一个 latest window 中。放入  latest window 中的数据也将对质量特征进行更新，当 latest window 中数据达到 window size，也就是一个 window 中容纳的最多样本的数目。就将 latest window 更新为 refer window。并将 refer window  学得质量特征更新为 latest window 学得的质量特征，如此反复。\n\n## 质量特征 \n\n质量特征用来计算需要检测数据的异常得分，最开始由正常数据计算，随后在检测的过程中不断更新。质量特征的计算方法如下:\n\n```\n对于数据x，将x通过每一个棵树。如果x经历了节点 n :\nif n in refer window:\n\tn.r ++\nelse if n in latest window:\n\tn.l ++\n```\n\n也就是说，对一个节点，如果有很多数据会进过这个节点，则这落在这个节点的数据就很多，而这个节点的正常度也就越高。反之则越低。\n\n最终异常得分的公式为:\n$$\nn.r × 2^{n.k}\n$$\n至于这里为什么要乘上一个系数 $2^{n.k}$ 可以看看论文 *Mass estimation* 中有详细的推导。对每一个棵树的异常得分取平均就能得到最终的异常得分。\n\n## 实时检测\n\n结合上面说的，可以得实时检测的算法如下:\n\n```\n1. 构建 Half-Space Trees //构建方法与构建异常森林基本相同\n2. 从训练样本中找到窗口大小的数据放入 refer window ，并构建质量特征\n3. 将从流中获取的数据输入 Half-Space Trees 获取异常值，并构建关于 latest window 的质量特征\n4. 如果 latest window 未满，则循环步骤3 否则进行步骤 5\n5. 将每一棵树的 n.r 替换为 n.l 并将 n.l 设置为 0。使用 latest window 替换 refer window。并跳到步骤 3\n```\n\n从上面的算法可以看出，实时检测并没有对树的结构进行更新，只是交换了质量特征。因此性能非常好。\n\n## 与孤立森林的区别\n\nHalf-Space Trees 在参数上与孤立森林有一点点差别:\n\n1. Half-Space Trees 落在在某一个节点样本数量 < 0.1 $\\cdot$ window size 会停止分裂。\n2. Half-Space Trees 树的最大高度并没有设置为$log2(训练样本数量)$\n3. Half-Space Trees 推荐使用 25 棵树集成一个森林。\n\n## 参考\n\n>Tan S C, Kai M T, Liu T F. Fast Anomaly Detection for Streaming Data.[C]// IJCAI 2011, Proceedings of the, International Joint Conference on Artificial Intelligence, Barcelona, Catalonia, Spain, July. DBLP, 2011:1511-1516.\n\n\n\n\n\n\n","source":"_posts/异常检测-流检测.md","raw":"---\ntitle: 异常检测-流检测\ndate: 2018-07-03 16:43:03\ntags:\n  - 数据挖掘\n  - 机器学习\n  - 异常检测\ncategories: 异常检测\nauthor: lishion\ntoc: true\n---\n之前提到的**孤立森林**是一种离线的数据挖掘挖掘|机器学习的方法。也就是说，**孤立森林**无法对实时的流数据进行检测。这里介绍一种使用 Half-Space Trees 构造的实时检测算法。Half-Space Trees 是一种构造方法和孤立森林极其相似的用于异常检测的模型，其理论是 Mass Estimation 。不熟悉的同学可以看:\n\n1. [异常检测-孤立森林](https://lishion.github.io/2018/07/03/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-%E5%AD%A4%E7%AB%8B%E6%A3%AE%E6%9E%97/)\n2. [异常检测-Mass estimation与孤立森林](https://lishion.github.io/2018/07/03/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-Mass-estimation/)\n\n具体的树和森林的构造方法就不在仔细讲。与**孤立森林**只有一点不同，孤立森林选择的划分点为随机的，而  Half-Space Trees 则是在用于划分的特征选取 (最大值+最小值)/2。这里主要讲一下其中在上面两篇文章中没有提到的概念以及如何进行事实检测。\n\n## 时间窗\n\n*Fast Anomaly Detection for Streaming Data* 文中直接用 window 来描述。这里为了好理解将其翻译为时间窗。在最开始阶段，使用 refer window 学习正常数据的质量特征(mass profile)。然后使用学得特征对流数据进行检测，而持续的流数据将放入一个 latest window 中。放入  latest window 中的数据也将对质量特征进行更新，当 latest window 中数据达到 window size，也就是一个 window 中容纳的最多样本的数目。就将 latest window 更新为 refer window。并将 refer window  学得质量特征更新为 latest window 学得的质量特征，如此反复。\n\n## 质量特征 \n\n质量特征用来计算需要检测数据的异常得分，最开始由正常数据计算，随后在检测的过程中不断更新。质量特征的计算方法如下:\n\n```\n对于数据x，将x通过每一个棵树。如果x经历了节点 n :\nif n in refer window:\n\tn.r ++\nelse if n in latest window:\n\tn.l ++\n```\n\n也就是说，对一个节点，如果有很多数据会进过这个节点，则这落在这个节点的数据就很多，而这个节点的正常度也就越高。反之则越低。\n\n最终异常得分的公式为:\n$$\nn.r × 2^{n.k}\n$$\n至于这里为什么要乘上一个系数 $2^{n.k}$ 可以看看论文 *Mass estimation* 中有详细的推导。对每一个棵树的异常得分取平均就能得到最终的异常得分。\n\n## 实时检测\n\n结合上面说的，可以得实时检测的算法如下:\n\n```\n1. 构建 Half-Space Trees //构建方法与构建异常森林基本相同\n2. 从训练样本中找到窗口大小的数据放入 refer window ，并构建质量特征\n3. 将从流中获取的数据输入 Half-Space Trees 获取异常值，并构建关于 latest window 的质量特征\n4. 如果 latest window 未满，则循环步骤3 否则进行步骤 5\n5. 将每一棵树的 n.r 替换为 n.l 并将 n.l 设置为 0。使用 latest window 替换 refer window。并跳到步骤 3\n```\n\n从上面的算法可以看出，实时检测并没有对树的结构进行更新，只是交换了质量特征。因此性能非常好。\n\n## 与孤立森林的区别\n\nHalf-Space Trees 在参数上与孤立森林有一点点差别:\n\n1. Half-Space Trees 落在在某一个节点样本数量 < 0.1 $\\cdot$ window size 会停止分裂。\n2. Half-Space Trees 树的最大高度并没有设置为$log2(训练样本数量)$\n3. Half-Space Trees 推荐使用 25 棵树集成一个森林。\n\n## 参考\n\n>Tan S C, Kai M T, Liu T F. Fast Anomaly Detection for Streaming Data.[C]// IJCAI 2011, Proceedings of the, International Joint Conference on Artificial Intelligence, Barcelona, Catalonia, Spain, July. DBLP, 2011:1511-1516.\n\n\n\n\n\n\n","slug":"异常检测-流检测","published":1,"updated":"2018-07-03T10:16:21.422Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjjhx8v3k000ob8tgeoi1qbba","content":"<p>之前提到的<strong>孤立森林</strong>是一种离线的数据挖掘挖掘|机器学习的方法。也就是说，<strong>孤立森林</strong>无法对实时的流数据进行检测。这里介绍一种使用 Half-Space Trees 构造的实时检测算法。Half-Space Trees 是一种构造方法和孤立森林极其相似的用于异常检测的模型，其理论是 Mass Estimation 。不熟悉的同学可以看:</p>\n<ol>\n<li><a href=\"https://lishion.github.io/2018/07/03/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-%E5%AD%A4%E7%AB%8B%E6%A3%AE%E6%9E%97/\" target=\"_blank\" rel=\"noopener\">异常检测-孤立森林</a></li>\n<li><a href=\"https://lishion.github.io/2018/07/03/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-Mass-estimation/\" target=\"_blank\" rel=\"noopener\">异常检测-Mass estimation与孤立森林</a></li>\n</ol>\n<p>具体的树和森林的构造方法就不在仔细讲。与<strong>孤立森林</strong>只有一点不同，孤立森林选择的划分点为随机的，而  Half-Space Trees 则是在用于划分的特征选取 (最大值+最小值)/2。这里主要讲一下其中在上面两篇文章中没有提到的概念以及如何进行事实检测。</p>\n<h2 id=\"时间窗\"><a href=\"#时间窗\" class=\"headerlink\" title=\"时间窗\"></a>时间窗</h2><p><em>Fast Anomaly Detection for Streaming Data</em> 文中直接用 window 来描述。这里为了好理解将其翻译为时间窗。在最开始阶段，使用 refer window 学习正常数据的质量特征(mass profile)。然后使用学得特征对流数据进行检测，而持续的流数据将放入一个 latest window 中。放入  latest window 中的数据也将对质量特征进行更新，当 latest window 中数据达到 window size，也就是一个 window 中容纳的最多样本的数目。就将 latest window 更新为 refer window。并将 refer window  学得质量特征更新为 latest window 学得的质量特征，如此反复。</p>\n<h2 id=\"质量特征\"><a href=\"#质量特征\" class=\"headerlink\" title=\"质量特征\"></a>质量特征</h2><p>质量特征用来计算需要检测数据的异常得分，最开始由正常数据计算，随后在检测的过程中不断更新。质量特征的计算方法如下:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">对于数据x，将x通过每一个棵树。如果x经历了节点 n :</span><br><span class=\"line\">if n in refer window:</span><br><span class=\"line\">\tn.r ++</span><br><span class=\"line\">else if n in latest window:</span><br><span class=\"line\">\tn.l ++</span><br></pre></td></tr></table></figure>\n<p>也就是说，对一个节点，如果有很多数据会进过这个节点，则这落在这个节点的数据就很多，而这个节点的正常度也就越高。反之则越低。</p>\n<p>最终异常得分的公式为:<br>$$<br>n.r × 2^{n.k}<br>$$<br>至于这里为什么要乘上一个系数 $2^{n.k}$ 可以看看论文 <em>Mass estimation</em> 中有详细的推导。对每一个棵树的异常得分取平均就能得到最终的异常得分。</p>\n<h2 id=\"实时检测\"><a href=\"#实时检测\" class=\"headerlink\" title=\"实时检测\"></a>实时检测</h2><p>结合上面说的，可以得实时检测的算法如下:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1. 构建 Half-Space Trees //构建方法与构建异常森林基本相同</span><br><span class=\"line\">2. 从训练样本中找到窗口大小的数据放入 refer window ，并构建质量特征</span><br><span class=\"line\">3. 将从流中获取的数据输入 Half-Space Trees 获取异常值，并构建关于 latest window 的质量特征</span><br><span class=\"line\">4. 如果 latest window 未满，则循环步骤3 否则进行步骤 5</span><br><span class=\"line\">5. 将每一棵树的 n.r 替换为 n.l 并将 n.l 设置为 0。使用 latest window 替换 refer window。并跳到步骤 3</span><br></pre></td></tr></table></figure>\n<p>从上面的算法可以看出，实时检测并没有对树的结构进行更新，只是交换了质量特征。因此性能非常好。</p>\n<h2 id=\"与孤立森林的区别\"><a href=\"#与孤立森林的区别\" class=\"headerlink\" title=\"与孤立森林的区别\"></a>与孤立森林的区别</h2><p>Half-Space Trees 在参数上与孤立森林有一点点差别:</p>\n<ol>\n<li>Half-Space Trees 落在在某一个节点样本数量 &lt; 0.1 $\\cdot$ window size 会停止分裂。</li>\n<li>Half-Space Trees 树的最大高度并没有设置为$log2(训练样本数量)$</li>\n<li>Half-Space Trees 推荐使用 25 棵树集成一个森林。</li>\n</ol>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><blockquote>\n<p>Tan S C, Kai M T, Liu T F. Fast Anomaly Detection for Streaming Data.[C]// IJCAI 2011, Proceedings of the, International Joint Conference on Artificial Intelligence, Barcelona, Catalonia, Spain, July. DBLP, 2011:1511-1516.</p>\n</blockquote>\n","site":{"data":{}},"excerpt":"","more":"<p>之前提到的<strong>孤立森林</strong>是一种离线的数据挖掘挖掘|机器学习的方法。也就是说，<strong>孤立森林</strong>无法对实时的流数据进行检测。这里介绍一种使用 Half-Space Trees 构造的实时检测算法。Half-Space Trees 是一种构造方法和孤立森林极其相似的用于异常检测的模型，其理论是 Mass Estimation 。不熟悉的同学可以看:</p>\n<ol>\n<li><a href=\"https://lishion.github.io/2018/07/03/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-%E5%AD%A4%E7%AB%8B%E6%A3%AE%E6%9E%97/\" target=\"_blank\" rel=\"noopener\">异常检测-孤立森林</a></li>\n<li><a href=\"https://lishion.github.io/2018/07/03/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-Mass-estimation/\" target=\"_blank\" rel=\"noopener\">异常检测-Mass estimation与孤立森林</a></li>\n</ol>\n<p>具体的树和森林的构造方法就不在仔细讲。与<strong>孤立森林</strong>只有一点不同，孤立森林选择的划分点为随机的，而  Half-Space Trees 则是在用于划分的特征选取 (最大值+最小值)/2。这里主要讲一下其中在上面两篇文章中没有提到的概念以及如何进行事实检测。</p>\n<h2 id=\"时间窗\"><a href=\"#时间窗\" class=\"headerlink\" title=\"时间窗\"></a>时间窗</h2><p><em>Fast Anomaly Detection for Streaming Data</em> 文中直接用 window 来描述。这里为了好理解将其翻译为时间窗。在最开始阶段，使用 refer window 学习正常数据的质量特征(mass profile)。然后使用学得特征对流数据进行检测，而持续的流数据将放入一个 latest window 中。放入  latest window 中的数据也将对质量特征进行更新，当 latest window 中数据达到 window size，也就是一个 window 中容纳的最多样本的数目。就将 latest window 更新为 refer window。并将 refer window  学得质量特征更新为 latest window 学得的质量特征，如此反复。</p>\n<h2 id=\"质量特征\"><a href=\"#质量特征\" class=\"headerlink\" title=\"质量特征\"></a>质量特征</h2><p>质量特征用来计算需要检测数据的异常得分，最开始由正常数据计算，随后在检测的过程中不断更新。质量特征的计算方法如下:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">对于数据x，将x通过每一个棵树。如果x经历了节点 n :</span><br><span class=\"line\">if n in refer window:</span><br><span class=\"line\">\tn.r ++</span><br><span class=\"line\">else if n in latest window:</span><br><span class=\"line\">\tn.l ++</span><br></pre></td></tr></table></figure>\n<p>也就是说，对一个节点，如果有很多数据会进过这个节点，则这落在这个节点的数据就很多，而这个节点的正常度也就越高。反之则越低。</p>\n<p>最终异常得分的公式为:<br>$$<br>n.r × 2^{n.k}<br>$$<br>至于这里为什么要乘上一个系数 $2^{n.k}$ 可以看看论文 <em>Mass estimation</em> 中有详细的推导。对每一个棵树的异常得分取平均就能得到最终的异常得分。</p>\n<h2 id=\"实时检测\"><a href=\"#实时检测\" class=\"headerlink\" title=\"实时检测\"></a>实时检测</h2><p>结合上面说的，可以得实时检测的算法如下:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1. 构建 Half-Space Trees //构建方法与构建异常森林基本相同</span><br><span class=\"line\">2. 从训练样本中找到窗口大小的数据放入 refer window ，并构建质量特征</span><br><span class=\"line\">3. 将从流中获取的数据输入 Half-Space Trees 获取异常值，并构建关于 latest window 的质量特征</span><br><span class=\"line\">4. 如果 latest window 未满，则循环步骤3 否则进行步骤 5</span><br><span class=\"line\">5. 将每一棵树的 n.r 替换为 n.l 并将 n.l 设置为 0。使用 latest window 替换 refer window。并跳到步骤 3</span><br></pre></td></tr></table></figure>\n<p>从上面的算法可以看出，实时检测并没有对树的结构进行更新，只是交换了质量特征。因此性能非常好。</p>\n<h2 id=\"与孤立森林的区别\"><a href=\"#与孤立森林的区别\" class=\"headerlink\" title=\"与孤立森林的区别\"></a>与孤立森林的区别</h2><p>Half-Space Trees 在参数上与孤立森林有一点点差别:</p>\n<ol>\n<li>Half-Space Trees 落在在某一个节点样本数量 &lt; 0.1 $\\cdot$ window size 会停止分裂。</li>\n<li>Half-Space Trees 树的最大高度并没有设置为$log2(训练样本数量)$</li>\n<li>Half-Space Trees 推荐使用 25 棵树集成一个森林。</li>\n</ol>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><blockquote>\n<p>Tan S C, Kai M T, Liu T F. Fast Anomaly Detection for Streaming Data.[C]// IJCAI 2011, Proceedings of the, International Joint Conference on Artificial Intelligence, Barcelona, Catalonia, Spain, July. DBLP, 2011:1511-1516.</p>\n</blockquote>\n"},{"title":"数据挖掘基础-最大似然估计与最大后验估计","date":"2018-06-28T06:47:50.000Z","author":"lishion","toc":true,"_content":"\n在某些时候，我们知道观测数据是由某个概率模型产生的，但是我们不知道模型的参数。而通过观测数据估计模型参数就称为参数估计。参数估计有很多种方法：最大释然估计、最大后验估计、采样等。这篇文章主要讨论最大释然估计与最大后验证估计。\n\n## 最大似然估计\n\n在给定模型参数$ \\theta $，观测到观测值 $X1$ 的概率为 $p(\\mathbf{X}|\\theta)$ ，这称之为似然函数。顾名思义，最大释然估计就是在已知观测值 $\\mathbf{X}$ 的情况下估计$\\theta$，使得 $p(\\mathbf{X}|\\theta)$ 最大。\n\n举一个例子，抛一枚硬币 6 次，假设这枚硬币正面向上的概率为$\\theta$，观测到的结果如下：\n\n```正 反 正 正 正 反```\n利用这个结果对 $\\theta$ 进行一个估计。首先写出似然函数:\n$$\np(\\mathbf{X}|\\theta)=(1-\\theta)\\theta^3(1-\\theta)\n$$\n令导数为 0 ，求得 $\\theta = \\frac 2 3$。\n\n有的同学看到这里嘴巴张成了 o 型。抛 6 次 4 次正面向上$\\theta=\\frac{2}{3}$，那抛 1 次正好有 1 次正面向上岂不是$\\theta=1$?其实这里你已经开始使用 $\\theta$ 的先验概率了。也就是说，在你以前的观察中，$\\theta=0.5$的概率是最高的。而$\\theta=1$是几乎不可能的。这也就是我们接下来要讲的**最大后验估计**。\n\n## 最大后验估计\n\n在上面提到，一般来说都会认为 $\\theta=0.5$ 概率较大，而 $\\theta=1$ 概率较小。这其实相当于我们认为 $\\theta$ 也具有一个分布，例如正太分布。也就是将$\\theta$看作一个随机变量。这里改变一下符号，令 $\\Theta$ 表示需要估计的参数，而 $\\theta$ 表示它一个具体的取值 。而最大后验估计的思想就是:\n$$\n\\mathop{max}_\\theta \\ p(\\Theta=\\theta|\\mathbf{X})\n$$\n\n也就是在观测到$\\mathbf{X}$之后，估计$\\Theta$并使得 $p(\\Theta|\\mathbf{X})$ 最大。利用贝叶斯公式：\n$$\np(\\Theta=\\theta|\\mathbf{X}) = \\frac{p(\\mathbf{X}|\\Theta=\\theta)P(\\Theta=\\theta)}{p(\\mathbf{X})}\n$$\n\n这里的分母是一个常数，与 $\\Theta$ 无关。于是我们只需要最大化分子就可以，于是最后为:\n$$\n\\mathop{max}_\\theta \\ p(\\mathbf{X}|\\Theta=\\theta)P(\\Theta=\\theta)\n$$\n可以看出，这里比最大释然估计多出了一个先验概率 $P(\\Theta=\\theta)$。\n$$\np(\\mathbf{X}|\\Theta=\\theta)\n$$\n例如这里我们假设$\\Theta \\backsim \\it{N(0.5,0.1)}$。由于计算比较复杂，这里直接画出函数图像为:\n\n{% asset_img func_img.png 后验函数图像 %}\n\n从图像可以估计出$\\theta=0.55$，这个结果就比最大似然估计更加贴近实际。\n\n## 总结\n\n一般来说，在知道参数正确分布的时候，最大后验估计要优于最大释然估计。而最大似然估计与最大后验估计的差别用一句话概括为:\n\n> 最大似然估计的估计量是一个**未知常量**，而最大后验估计的估计量是一个**未知随机变量**\n\n\n\n","source":"_posts/数据挖掘基础-最大似然估计与最大后验估计.md","raw":"---\ntitle: 数据挖掘基础-最大似然估计与最大后验估计\ndate: 2018-06-28 14:47:50\ntags:\n  - 数据挖掘\n  - 机器学习\ncategories: 数据挖掘基础\nauthor: lishion\ntoc: true\n---\n\n在某些时候，我们知道观测数据是由某个概率模型产生的，但是我们不知道模型的参数。而通过观测数据估计模型参数就称为参数估计。参数估计有很多种方法：最大释然估计、最大后验估计、采样等。这篇文章主要讨论最大释然估计与最大后验证估计。\n\n## 最大似然估计\n\n在给定模型参数$ \\theta $，观测到观测值 $X1$ 的概率为 $p(\\mathbf{X}|\\theta)$ ，这称之为似然函数。顾名思义，最大释然估计就是在已知观测值 $\\mathbf{X}$ 的情况下估计$\\theta$，使得 $p(\\mathbf{X}|\\theta)$ 最大。\n\n举一个例子，抛一枚硬币 6 次，假设这枚硬币正面向上的概率为$\\theta$，观测到的结果如下：\n\n```正 反 正 正 正 反```\n利用这个结果对 $\\theta$ 进行一个估计。首先写出似然函数:\n$$\np(\\mathbf{X}|\\theta)=(1-\\theta)\\theta^3(1-\\theta)\n$$\n令导数为 0 ，求得 $\\theta = \\frac 2 3$。\n\n有的同学看到这里嘴巴张成了 o 型。抛 6 次 4 次正面向上$\\theta=\\frac{2}{3}$，那抛 1 次正好有 1 次正面向上岂不是$\\theta=1$?其实这里你已经开始使用 $\\theta$ 的先验概率了。也就是说，在你以前的观察中，$\\theta=0.5$的概率是最高的。而$\\theta=1$是几乎不可能的。这也就是我们接下来要讲的**最大后验估计**。\n\n## 最大后验估计\n\n在上面提到，一般来说都会认为 $\\theta=0.5$ 概率较大，而 $\\theta=1$ 概率较小。这其实相当于我们认为 $\\theta$ 也具有一个分布，例如正太分布。也就是将$\\theta$看作一个随机变量。这里改变一下符号，令 $\\Theta$ 表示需要估计的参数，而 $\\theta$ 表示它一个具体的取值 。而最大后验估计的思想就是:\n$$\n\\mathop{max}_\\theta \\ p(\\Theta=\\theta|\\mathbf{X})\n$$\n\n也就是在观测到$\\mathbf{X}$之后，估计$\\Theta$并使得 $p(\\Theta|\\mathbf{X})$ 最大。利用贝叶斯公式：\n$$\np(\\Theta=\\theta|\\mathbf{X}) = \\frac{p(\\mathbf{X}|\\Theta=\\theta)P(\\Theta=\\theta)}{p(\\mathbf{X})}\n$$\n\n这里的分母是一个常数，与 $\\Theta$ 无关。于是我们只需要最大化分子就可以，于是最后为:\n$$\n\\mathop{max}_\\theta \\ p(\\mathbf{X}|\\Theta=\\theta)P(\\Theta=\\theta)\n$$\n可以看出，这里比最大释然估计多出了一个先验概率 $P(\\Theta=\\theta)$。\n$$\np(\\mathbf{X}|\\Theta=\\theta)\n$$\n例如这里我们假设$\\Theta \\backsim \\it{N(0.5,0.1)}$。由于计算比较复杂，这里直接画出函数图像为:\n\n{% asset_img func_img.png 后验函数图像 %}\n\n从图像可以估计出$\\theta=0.55$，这个结果就比最大似然估计更加贴近实际。\n\n## 总结\n\n一般来说，在知道参数正确分布的时候，最大后验估计要优于最大释然估计。而最大似然估计与最大后验估计的差别用一句话概括为:\n\n> 最大似然估计的估计量是一个**未知常量**，而最大后验估计的估计量是一个**未知随机变量**\n\n\n\n","slug":"数据挖掘基础-最大似然估计与最大后验估计","published":1,"updated":"2018-07-03T01:53:43.739Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjjhx8v3n000sb8tgkinolr95","content":"<p>在某些时候，我们知道观测数据是由某个概率模型产生的，但是我们不知道模型的参数。而通过观测数据估计模型参数就称为参数估计。参数估计有很多种方法：最大释然估计、最大后验估计、采样等。这篇文章主要讨论最大释然估计与最大后验证估计。</p>\n<h2 id=\"最大似然估计\"><a href=\"#最大似然估计\" class=\"headerlink\" title=\"最大似然估计\"></a>最大似然估计</h2><p>在给定模型参数$ \\theta $，观测到观测值 $X1$ 的概率为 $p(\\mathbf{X}|\\theta)$ ，这称之为似然函数。顾名思义，最大释然估计就是在已知观测值 $\\mathbf{X}$ 的情况下估计$\\theta$，使得 $p(\\mathbf{X}|\\theta)$ 最大。</p>\n<p>举一个例子，抛一枚硬币 6 次，假设这枚硬币正面向上的概率为$\\theta$，观测到的结果如下：</p>\n<p><code>正 反 正 正 正 反</code><br>利用这个结果对 $\\theta$ 进行一个估计。首先写出似然函数:<br>$$<br>p(\\mathbf{X}|\\theta)=(1-\\theta)\\theta^3(1-\\theta)<br>$$<br>令导数为 0 ，求得 $\\theta = \\frac 2 3$。</p>\n<p>有的同学看到这里嘴巴张成了 o 型。抛 6 次 4 次正面向上$\\theta=\\frac{2}{3}$，那抛 1 次正好有 1 次正面向上岂不是$\\theta=1$?其实这里你已经开始使用 $\\theta$ 的先验概率了。也就是说，在你以前的观察中，$\\theta=0.5$的概率是最高的。而$\\theta=1$是几乎不可能的。这也就是我们接下来要讲的<strong>最大后验估计</strong>。</p>\n<h2 id=\"最大后验估计\"><a href=\"#最大后验估计\" class=\"headerlink\" title=\"最大后验估计\"></a>最大后验估计</h2><p>在上面提到，一般来说都会认为 $\\theta=0.5$ 概率较大，而 $\\theta=1$ 概率较小。这其实相当于我们认为 $\\theta$ 也具有一个分布，例如正太分布。也就是将$\\theta$看作一个随机变量。这里改变一下符号，令 $\\Theta$ 表示需要估计的参数，而 $\\theta$ 表示它一个具体的取值 。而最大后验估计的思想就是:<br>$$<br>\\mathop{max}_\\theta \\ p(\\Theta=\\theta|\\mathbf{X})<br>$$</p>\n<p>也就是在观测到$\\mathbf{X}$之后，估计$\\Theta$并使得 $p(\\Theta|\\mathbf{X})$ 最大。利用贝叶斯公式：<br>$$<br>p(\\Theta=\\theta|\\mathbf{X}) = \\frac{p(\\mathbf{X}|\\Theta=\\theta)P(\\Theta=\\theta)}{p(\\mathbf{X})}<br>$$</p>\n<p>这里的分母是一个常数，与 $\\Theta$ 无关。于是我们只需要最大化分子就可以，于是最后为:<br>$$<br>\\mathop{max}_\\theta \\ p(\\mathbf{X}|\\Theta=\\theta)P(\\Theta=\\theta)<br>$$<br>可以看出，这里比最大释然估计多出了一个先验概率 $P(\\Theta=\\theta)$。<br>$$<br>p(\\mathbf{X}|\\Theta=\\theta)<br>$$<br>例如这里我们假设$\\Theta \\backsim \\it{N(0.5,0.1)}$。由于计算比较复杂，这里直接画出函数图像为:</p>\n<img src=\"/2018/06/28/数据挖掘基础-最大似然估计与最大后验估计/func_img.png\" title=\"后验函数图像\">\n<p>从图像可以估计出$\\theta=0.55$，这个结果就比最大似然估计更加贴近实际。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>一般来说，在知道参数正确分布的时候，最大后验估计要优于最大释然估计。而最大似然估计与最大后验估计的差别用一句话概括为:</p>\n<blockquote>\n<p>最大似然估计的估计量是一个<strong>未知常量</strong>，而最大后验估计的估计量是一个<strong>未知随机变量</strong></p>\n</blockquote>\n","site":{"data":{}},"excerpt":"","more":"<p>在某些时候，我们知道观测数据是由某个概率模型产生的，但是我们不知道模型的参数。而通过观测数据估计模型参数就称为参数估计。参数估计有很多种方法：最大释然估计、最大后验估计、采样等。这篇文章主要讨论最大释然估计与最大后验证估计。</p>\n<h2 id=\"最大似然估计\"><a href=\"#最大似然估计\" class=\"headerlink\" title=\"最大似然估计\"></a>最大似然估计</h2><p>在给定模型参数$ \\theta $，观测到观测值 $X1$ 的概率为 $p(\\mathbf{X}|\\theta)$ ，这称之为似然函数。顾名思义，最大释然估计就是在已知观测值 $\\mathbf{X}$ 的情况下估计$\\theta$，使得 $p(\\mathbf{X}|\\theta)$ 最大。</p>\n<p>举一个例子，抛一枚硬币 6 次，假设这枚硬币正面向上的概率为$\\theta$，观测到的结果如下：</p>\n<p><code>正 反 正 正 正 反</code><br>利用这个结果对 $\\theta$ 进行一个估计。首先写出似然函数:<br>$$<br>p(\\mathbf{X}|\\theta)=(1-\\theta)\\theta^3(1-\\theta)<br>$$<br>令导数为 0 ，求得 $\\theta = \\frac 2 3$。</p>\n<p>有的同学看到这里嘴巴张成了 o 型。抛 6 次 4 次正面向上$\\theta=\\frac{2}{3}$，那抛 1 次正好有 1 次正面向上岂不是$\\theta=1$?其实这里你已经开始使用 $\\theta$ 的先验概率了。也就是说，在你以前的观察中，$\\theta=0.5$的概率是最高的。而$\\theta=1$是几乎不可能的。这也就是我们接下来要讲的<strong>最大后验估计</strong>。</p>\n<h2 id=\"最大后验估计\"><a href=\"#最大后验估计\" class=\"headerlink\" title=\"最大后验估计\"></a>最大后验估计</h2><p>在上面提到，一般来说都会认为 $\\theta=0.5$ 概率较大，而 $\\theta=1$ 概率较小。这其实相当于我们认为 $\\theta$ 也具有一个分布，例如正太分布。也就是将$\\theta$看作一个随机变量。这里改变一下符号，令 $\\Theta$ 表示需要估计的参数，而 $\\theta$ 表示它一个具体的取值 。而最大后验估计的思想就是:<br>$$<br>\\mathop{max}_\\theta \\ p(\\Theta=\\theta|\\mathbf{X})<br>$$</p>\n<p>也就是在观测到$\\mathbf{X}$之后，估计$\\Theta$并使得 $p(\\Theta|\\mathbf{X})$ 最大。利用贝叶斯公式：<br>$$<br>p(\\Theta=\\theta|\\mathbf{X}) = \\frac{p(\\mathbf{X}|\\Theta=\\theta)P(\\Theta=\\theta)}{p(\\mathbf{X})}<br>$$</p>\n<p>这里的分母是一个常数，与 $\\Theta$ 无关。于是我们只需要最大化分子就可以，于是最后为:<br>$$<br>\\mathop{max}_\\theta \\ p(\\mathbf{X}|\\Theta=\\theta)P(\\Theta=\\theta)<br>$$<br>可以看出，这里比最大释然估计多出了一个先验概率 $P(\\Theta=\\theta)$。<br>$$<br>p(\\mathbf{X}|\\Theta=\\theta)<br>$$<br>例如这里我们假设$\\Theta \\backsim \\it{N(0.5,0.1)}$。由于计算比较复杂，这里直接画出函数图像为:</p>\n<img src=\"/2018/06/28/数据挖掘基础-最大似然估计与最大后验估计/func_img.png\" title=\"后验函数图像\">\n<p>从图像可以估计出$\\theta=0.55$，这个结果就比最大似然估计更加贴近实际。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>一般来说，在知道参数正确分布的时候，最大后验估计要优于最大释然估计。而最大似然估计与最大后验估计的差别用一句话概括为:</p>\n<blockquote>\n<p>最大似然估计的估计量是一个<strong>未知常量</strong>，而最大后验估计的估计量是一个<strong>未知随机变量</strong></p>\n</blockquote>\n"},{"title":"数据挖掘基础-熵","date":"2018-06-27T06:29:49.000Z","author":"lishion","toc":true,"_content":"\n熵最初是一个热力学中表征物质状态的参量，是体系混乱程度的度量。香农大佬在*通讯的数学原理*这篇论文中用来表示一个信源所发出的信息具有的平均信息量。可以说如果没有香农大佬这篇论文，就没有现在的现代通信体系。虽然信源熵这个概念出自通信，但是在其他的领域也有很广泛的应用。在数据挖掘和机器学习中也有所体现。由于作者专业本身就是通信，写今天这篇文章主要是想通过一些简单易懂的例子来描述一下熵的实际意义，帮助一些非通信的同学了解这个概念。\n\n## 信息有大小吗？如何定量描述?\n\n在香农大佬发表论文之前，人们没有办法对信息进行量化。例如**这个苹果是红的**这条消息，它到底包含了多少的信息呢，能够用数字来衡量吗？为了解决这个问题，香农大佬在他论文里描述到：**一条消息所包含的信息量，应该与这个消息所包含的事件发生的概率成反比**。这句话的意思是：如果一个事件发生的概率越大，那么这个事件发生时所包含的信息量就越少；如果一个事件发生的概率越小，那么这个事件包含的信息量就越大。\n\n就拿刚才那个红苹果的例子来举例：例如你昨天从超市买了一个红苹果放在桌子上，今天有一条消息告诉你**桌子上的苹果是红色的**。由于昨天购买的红苹果今天任然是红苹果的概率几乎是百分之百，因此这条信息的所包含的信息量几乎为0。如果有一条消息告诉你:**桌子上的苹果是金黄色的**，由于你购买一个红苹果放在桌上而变成金黄色的概率几乎为0，因此这条消息所包含的信息量就非常大。它可能意味着你购买了一个具有特异功能的苹果或者昨晚有小偷进入到你的家里把你桌子上苹果偷偷的涂成金黄色再悄悄离开。\n\n所以，香农大佬提出了自信息量的的概念:对于消息 x ，它发生的概率为 p。那么 x 的自信息量定为:\n$$\nI(x) = -log(p)\n$$\n这里 log 的底数可以是 2 或是 e。如果是 2 那么对应的单位为 bit 否则为 nat。   \n\n在很多时候，消息源都不止发出一条消息。那么如何衡量一个消息源的所包含的信息量呢?这里可以采用我们在统计学中**期望值**的思想：对于一个可以发出 n 条消息的消息源 X ，其中消息 $m_i$ 所发生的概率为 $p_i$ 。那么这个消息所包含的平均自信息量为:\n$$\nE(X)=-\\sum_{i=1}^n\\frac{1}{p_i}log(p)\n$$\n这也就是熵的定义。\n\n## 信息增益\n\n信息增益在数据挖掘中也有许多的应用，例如在决策树选择属性进行划分的时候要优先选择信息增益较大的属性。简答来说，信息增益就是在收到了一条消息后，信息源的不确定性的减少程度。再举一个例子。 \n\n有一天你的朋友问了你这样一个问题: 我在超市买了一个苹果，并把它藏在那大海的深处，请问你知道它是生的还是熟的吗？此时，你不知道关于这个苹果的任何信息，猜中这个苹果是生的还是熟的概率为50%。也就是说，苹果这个信息源有50%的可能会发出：我是生的这条消息。或者以50%的消息发出：我是熟的这条消息。那么我们计算一个这个苹果此时的熵为:\n$$\n-\\frac{1}{2}log\\frac{1}{2}-\\frac{1}{2}log\\frac{1}{2}=1(bit)\n$$\n此时你的朋友着你迷茫的双眼，他准备给你点提示: 我藏在大海深处的苹果是红色的。于是你到楼下超市看了一圈，并做出如下统计:\n\n|  颜色  | 成熟 | 非成熟 |\n| :----: | :--: | :----: |\n|  红色  |  90  |   10   |\n| 非红色 |  10  |   90   |\n\n\n\n这时你使用你在大学学到有关条件概率计算出。也就是说，当我们知道一个苹果是红色的，并且为熟的概率为 $\\frac{9}{10}$ 。那么此时苹果的熵为:\n$$\n-\\frac{9}{10}log\\frac{9}{10}-\\frac{1}{10}log\\frac{1}{10}=0.469(bit)\n$$\n\n\n可以看出，这个**苹果是红色的**这条消息使得信源的消息减少了 $1 - 0.469 = 0.531(bit)$\n\n\n\n","source":"_posts/数据挖掘基础-熵.md","raw":"---\ntitle: 数据挖掘基础-熵\ndate: 2018-06-27 14:29:49\ntags:\n  - 数据挖掘\n  - 机器学习\ncategories: 数据挖掘基础\nauthor: lishion\ntoc: true\n---\n\n熵最初是一个热力学中表征物质状态的参量，是体系混乱程度的度量。香农大佬在*通讯的数学原理*这篇论文中用来表示一个信源所发出的信息具有的平均信息量。可以说如果没有香农大佬这篇论文，就没有现在的现代通信体系。虽然信源熵这个概念出自通信，但是在其他的领域也有很广泛的应用。在数据挖掘和机器学习中也有所体现。由于作者专业本身就是通信，写今天这篇文章主要是想通过一些简单易懂的例子来描述一下熵的实际意义，帮助一些非通信的同学了解这个概念。\n\n## 信息有大小吗？如何定量描述?\n\n在香农大佬发表论文之前，人们没有办法对信息进行量化。例如**这个苹果是红的**这条消息，它到底包含了多少的信息呢，能够用数字来衡量吗？为了解决这个问题，香农大佬在他论文里描述到：**一条消息所包含的信息量，应该与这个消息所包含的事件发生的概率成反比**。这句话的意思是：如果一个事件发生的概率越大，那么这个事件发生时所包含的信息量就越少；如果一个事件发生的概率越小，那么这个事件包含的信息量就越大。\n\n就拿刚才那个红苹果的例子来举例：例如你昨天从超市买了一个红苹果放在桌子上，今天有一条消息告诉你**桌子上的苹果是红色的**。由于昨天购买的红苹果今天任然是红苹果的概率几乎是百分之百，因此这条信息的所包含的信息量几乎为0。如果有一条消息告诉你:**桌子上的苹果是金黄色的**，由于你购买一个红苹果放在桌上而变成金黄色的概率几乎为0，因此这条消息所包含的信息量就非常大。它可能意味着你购买了一个具有特异功能的苹果或者昨晚有小偷进入到你的家里把你桌子上苹果偷偷的涂成金黄色再悄悄离开。\n\n所以，香农大佬提出了自信息量的的概念:对于消息 x ，它发生的概率为 p。那么 x 的自信息量定为:\n$$\nI(x) = -log(p)\n$$\n这里 log 的底数可以是 2 或是 e。如果是 2 那么对应的单位为 bit 否则为 nat。   \n\n在很多时候，消息源都不止发出一条消息。那么如何衡量一个消息源的所包含的信息量呢?这里可以采用我们在统计学中**期望值**的思想：对于一个可以发出 n 条消息的消息源 X ，其中消息 $m_i$ 所发生的概率为 $p_i$ 。那么这个消息所包含的平均自信息量为:\n$$\nE(X)=-\\sum_{i=1}^n\\frac{1}{p_i}log(p)\n$$\n这也就是熵的定义。\n\n## 信息增益\n\n信息增益在数据挖掘中也有许多的应用，例如在决策树选择属性进行划分的时候要优先选择信息增益较大的属性。简答来说，信息增益就是在收到了一条消息后，信息源的不确定性的减少程度。再举一个例子。 \n\n有一天你的朋友问了你这样一个问题: 我在超市买了一个苹果，并把它藏在那大海的深处，请问你知道它是生的还是熟的吗？此时，你不知道关于这个苹果的任何信息，猜中这个苹果是生的还是熟的概率为50%。也就是说，苹果这个信息源有50%的可能会发出：我是生的这条消息。或者以50%的消息发出：我是熟的这条消息。那么我们计算一个这个苹果此时的熵为:\n$$\n-\\frac{1}{2}log\\frac{1}{2}-\\frac{1}{2}log\\frac{1}{2}=1(bit)\n$$\n此时你的朋友着你迷茫的双眼，他准备给你点提示: 我藏在大海深处的苹果是红色的。于是你到楼下超市看了一圈，并做出如下统计:\n\n|  颜色  | 成熟 | 非成熟 |\n| :----: | :--: | :----: |\n|  红色  |  90  |   10   |\n| 非红色 |  10  |   90   |\n\n\n\n这时你使用你在大学学到有关条件概率计算出。也就是说，当我们知道一个苹果是红色的，并且为熟的概率为 $\\frac{9}{10}$ 。那么此时苹果的熵为:\n$$\n-\\frac{9}{10}log\\frac{9}{10}-\\frac{1}{10}log\\frac{1}{10}=0.469(bit)\n$$\n\n\n可以看出，这个**苹果是红色的**这条消息使得信源的消息减少了 $1 - 0.469 = 0.531(bit)$\n\n\n\n","slug":"数据挖掘基础-熵","published":1,"updated":"2018-07-03T01:53:43.739Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjjhx8v3o000tb8tgfls8b2g2","content":"<p>熵最初是一个热力学中表征物质状态的参量，是体系混乱程度的度量。香农大佬在<em>通讯的数学原理</em>这篇论文中用来表示一个信源所发出的信息具有的平均信息量。可以说如果没有香农大佬这篇论文，就没有现在的现代通信体系。虽然信源熵这个概念出自通信，但是在其他的领域也有很广泛的应用。在数据挖掘和机器学习中也有所体现。由于作者专业本身就是通信，写今天这篇文章主要是想通过一些简单易懂的例子来描述一下熵的实际意义，帮助一些非通信的同学了解这个概念。</p>\n<h2 id=\"信息有大小吗？如何定量描述\"><a href=\"#信息有大小吗？如何定量描述\" class=\"headerlink\" title=\"信息有大小吗？如何定量描述?\"></a>信息有大小吗？如何定量描述?</h2><p>在香农大佬发表论文之前，人们没有办法对信息进行量化。例如<strong>这个苹果是红的</strong>这条消息，它到底包含了多少的信息呢，能够用数字来衡量吗？为了解决这个问题，香农大佬在他论文里描述到：<strong>一条消息所包含的信息量，应该与这个消息所包含的事件发生的概率成反比</strong>。这句话的意思是：如果一个事件发生的概率越大，那么这个事件发生时所包含的信息量就越少；如果一个事件发生的概率越小，那么这个事件包含的信息量就越大。</p>\n<p>就拿刚才那个红苹果的例子来举例：例如你昨天从超市买了一个红苹果放在桌子上，今天有一条消息告诉你<strong>桌子上的苹果是红色的</strong>。由于昨天购买的红苹果今天任然是红苹果的概率几乎是百分之百，因此这条信息的所包含的信息量几乎为0。如果有一条消息告诉你:<strong>桌子上的苹果是金黄色的</strong>，由于你购买一个红苹果放在桌上而变成金黄色的概率几乎为0，因此这条消息所包含的信息量就非常大。它可能意味着你购买了一个具有特异功能的苹果或者昨晚有小偷进入到你的家里把你桌子上苹果偷偷的涂成金黄色再悄悄离开。</p>\n<p>所以，香农大佬提出了自信息量的的概念:对于消息 x ，它发生的概率为 p。那么 x 的自信息量定为:<br>$$<br>I(x) = -log(p)<br>$$<br>这里 log 的底数可以是 2 或是 e。如果是 2 那么对应的单位为 bit 否则为 nat。   </p>\n<p>在很多时候，消息源都不止发出一条消息。那么如何衡量一个消息源的所包含的信息量呢?这里可以采用我们在统计学中<strong>期望值</strong>的思想：对于一个可以发出 n 条消息的消息源 X ，其中消息 $m_i$ 所发生的概率为 $p_i$ 。那么这个消息所包含的平均自信息量为:<br>$$<br>E(X)=-\\sum_{i=1}^n\\frac{1}{p_i}log(p)<br>$$<br>这也就是熵的定义。</p>\n<h2 id=\"信息增益\"><a href=\"#信息增益\" class=\"headerlink\" title=\"信息增益\"></a>信息增益</h2><p>信息增益在数据挖掘中也有许多的应用，例如在决策树选择属性进行划分的时候要优先选择信息增益较大的属性。简答来说，信息增益就是在收到了一条消息后，信息源的不确定性的减少程度。再举一个例子。 </p>\n<p>有一天你的朋友问了你这样一个问题: 我在超市买了一个苹果，并把它藏在那大海的深处，请问你知道它是生的还是熟的吗？此时，你不知道关于这个苹果的任何信息，猜中这个苹果是生的还是熟的概率为50%。也就是说，苹果这个信息源有50%的可能会发出：我是生的这条消息。或者以50%的消息发出：我是熟的这条消息。那么我们计算一个这个苹果此时的熵为:<br>$$<br>-\\frac{1}{2}log\\frac{1}{2}-\\frac{1}{2}log\\frac{1}{2}=1(bit)<br>$$<br>此时你的朋友着你迷茫的双眼，他准备给你点提示: 我藏在大海深处的苹果是红色的。于是你到楼下超市看了一圈，并做出如下统计:</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">颜色</th>\n<th style=\"text-align:center\">成熟</th>\n<th style=\"text-align:center\">非成熟</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">红色</td>\n<td style=\"text-align:center\">90</td>\n<td style=\"text-align:center\">10</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">非红色</td>\n<td style=\"text-align:center\">10</td>\n<td style=\"text-align:center\">90</td>\n</tr>\n</tbody>\n</table>\n<p>这时你使用你在大学学到有关条件概率计算出。也就是说，当我们知道一个苹果是红色的，并且为熟的概率为 $\\frac{9}{10}$ 。那么此时苹果的熵为:<br>$$<br>-\\frac{9}{10}log\\frac{9}{10}-\\frac{1}{10}log\\frac{1}{10}=0.469(bit)<br>$$</p>\n<p>可以看出，这个<strong>苹果是红色的</strong>这条消息使得信源的消息减少了 $1 - 0.469 = 0.531(bit)$</p>\n","site":{"data":{}},"excerpt":"","more":"<p>熵最初是一个热力学中表征物质状态的参量，是体系混乱程度的度量。香农大佬在<em>通讯的数学原理</em>这篇论文中用来表示一个信源所发出的信息具有的平均信息量。可以说如果没有香农大佬这篇论文，就没有现在的现代通信体系。虽然信源熵这个概念出自通信，但是在其他的领域也有很广泛的应用。在数据挖掘和机器学习中也有所体现。由于作者专业本身就是通信，写今天这篇文章主要是想通过一些简单易懂的例子来描述一下熵的实际意义，帮助一些非通信的同学了解这个概念。</p>\n<h2 id=\"信息有大小吗？如何定量描述\"><a href=\"#信息有大小吗？如何定量描述\" class=\"headerlink\" title=\"信息有大小吗？如何定量描述?\"></a>信息有大小吗？如何定量描述?</h2><p>在香农大佬发表论文之前，人们没有办法对信息进行量化。例如<strong>这个苹果是红的</strong>这条消息，它到底包含了多少的信息呢，能够用数字来衡量吗？为了解决这个问题，香农大佬在他论文里描述到：<strong>一条消息所包含的信息量，应该与这个消息所包含的事件发生的概率成反比</strong>。这句话的意思是：如果一个事件发生的概率越大，那么这个事件发生时所包含的信息量就越少；如果一个事件发生的概率越小，那么这个事件包含的信息量就越大。</p>\n<p>就拿刚才那个红苹果的例子来举例：例如你昨天从超市买了一个红苹果放在桌子上，今天有一条消息告诉你<strong>桌子上的苹果是红色的</strong>。由于昨天购买的红苹果今天任然是红苹果的概率几乎是百分之百，因此这条信息的所包含的信息量几乎为0。如果有一条消息告诉你:<strong>桌子上的苹果是金黄色的</strong>，由于你购买一个红苹果放在桌上而变成金黄色的概率几乎为0，因此这条消息所包含的信息量就非常大。它可能意味着你购买了一个具有特异功能的苹果或者昨晚有小偷进入到你的家里把你桌子上苹果偷偷的涂成金黄色再悄悄离开。</p>\n<p>所以，香农大佬提出了自信息量的的概念:对于消息 x ，它发生的概率为 p。那么 x 的自信息量定为:<br>$$<br>I(x) = -log(p)<br>$$<br>这里 log 的底数可以是 2 或是 e。如果是 2 那么对应的单位为 bit 否则为 nat。   </p>\n<p>在很多时候，消息源都不止发出一条消息。那么如何衡量一个消息源的所包含的信息量呢?这里可以采用我们在统计学中<strong>期望值</strong>的思想：对于一个可以发出 n 条消息的消息源 X ，其中消息 $m_i$ 所发生的概率为 $p_i$ 。那么这个消息所包含的平均自信息量为:<br>$$<br>E(X)=-\\sum_{i=1}^n\\frac{1}{p_i}log(p)<br>$$<br>这也就是熵的定义。</p>\n<h2 id=\"信息增益\"><a href=\"#信息增益\" class=\"headerlink\" title=\"信息增益\"></a>信息增益</h2><p>信息增益在数据挖掘中也有许多的应用，例如在决策树选择属性进行划分的时候要优先选择信息增益较大的属性。简答来说，信息增益就是在收到了一条消息后，信息源的不确定性的减少程度。再举一个例子。 </p>\n<p>有一天你的朋友问了你这样一个问题: 我在超市买了一个苹果，并把它藏在那大海的深处，请问你知道它是生的还是熟的吗？此时，你不知道关于这个苹果的任何信息，猜中这个苹果是生的还是熟的概率为50%。也就是说，苹果这个信息源有50%的可能会发出：我是生的这条消息。或者以50%的消息发出：我是熟的这条消息。那么我们计算一个这个苹果此时的熵为:<br>$$<br>-\\frac{1}{2}log\\frac{1}{2}-\\frac{1}{2}log\\frac{1}{2}=1(bit)<br>$$<br>此时你的朋友着你迷茫的双眼，他准备给你点提示: 我藏在大海深处的苹果是红色的。于是你到楼下超市看了一圈，并做出如下统计:</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">颜色</th>\n<th style=\"text-align:center\">成熟</th>\n<th style=\"text-align:center\">非成熟</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">红色</td>\n<td style=\"text-align:center\">90</td>\n<td style=\"text-align:center\">10</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">非红色</td>\n<td style=\"text-align:center\">10</td>\n<td style=\"text-align:center\">90</td>\n</tr>\n</tbody>\n</table>\n<p>这时你使用你在大学学到有关条件概率计算出。也就是说，当我们知道一个苹果是红色的，并且为熟的概率为 $\\frac{9}{10}$ 。那么此时苹果的熵为:<br>$$<br>-\\frac{9}{10}log\\frac{9}{10}-\\frac{1}{10}log\\frac{1}{10}=0.469(bit)<br>$$</p>\n<p>可以看出，这个<strong>苹果是红色的</strong>这条消息使得信源的消息减少了 $1 - 0.469 = 0.531(bit)$</p>\n"},{"title":"Spark源码阅读计划-第四部分-shuffle read","date":"2018-06-18T02:38:44.000Z","author":"lishion","toc":true,"_content":"\n拖了这么久终于把 shuffle read 部分的源码看了一遍了。虽然 shuffle read 再数据合并部分的逻辑要比 shuffle 简单，但是由于这个过程中 executor 要到 master 拉取 shuffle write 结果信息，就涉及到 spark 的 block manager 的一些东西，因此完整的 shuffle read 过程依然是很复杂的。但是由于我还是太菜了，对于 block manage 这一部分看的一知半解，因此关于 executor 与 master 进行元数据交互的部分也就不会写得很详细，当然还是会涉及到一些。有关 block manage 的这部分以后应该会写到，先立个 flag 在这里吧。\n\n## MapStatus\t\n\n在 shuffle write 完成之后会返回一个 MapStatus，MapStatus记录了 BlockManagerId 以及最终每个分区的大小。其中 BlockManagerId 包含了 BlockManager 所在的 host 以及 port 等信息。返回的 MapStatus 最终会被 Driver 端获并存储以便于 mapper 端获取。 这里要稍微提一下 BlockManager 的知识，Spark 利用 BlockManager 对数据进行读写，而 Block 就是其中的基本单位。每一个 Block 拥有一个 id。而通过BlockManager 则可以操作这些 Block 。因此 mapper 端只需要知道 BlockManager 的位置以及所需要的文件在哪些 Block 就能获取到对应的数据。这里通过 MapOutputTracker 通过 shuffleid 对一次 shuffle write 中所有的 mapper 产生的 MapStatus 进行了记录。\n\n在每一个 mapper 的 shuffle task 结束后，MapOutputTracker就会将其返回的 MapStatues 进行注册。在 DAGScheduler 中可以看到:\n\n```scala\n case smt: ShuffleMapTask =>\n              val shuffleStage = stage.asInstanceOf[ShuffleMapStage]\n              val status = event.result.asInstanceOf[MapStatus]\n              val execId = status.location.executorId\n\t\t\t  \n              mapOutputTracker.registerMapOutput(\n                shuffleStage.shuffleDep.shuffleId, smt.partitionId, status)\n```\n\n这里的 MapOutputTracker 实际的类型为 MapOutputTrackerMaster 。继续看关于注册的代码:\n\n```scala\n def registerMapOutput(shuffleId: Int, mapId: Int, status: MapStatus) {\n    shuffleStatuses(shuffleId).addMapOutput(mapId, status)\n  }\n```\n\nshuffleStatuses 实际上是一个 hashmap:\n\n```scala\nval shuffleStatuses = new ConcurrentHashMap[Int, ShuffleStatus]().asScala\n```\n\nShuffleStatus 是一个类，里面包含了一个 MapStatus 数组。也就是说，通过在 shuffle write 之前注册的 shuffle 所用的 shuffleid 作为索引存储了 shuffle write 过程产生的 MapStatus。而 MapOutputTrackerMaster 是用于 Driver 端的。用于 Executor 的则是 MapOutputTrackerWorker 。在 MapOutputTrackerWorker 中一开始是没有 shuffleStatuses 的。需要从 MapOutputTrackerMaster 中获取。\n\n## shuffle read\n\nshuffle read 代码开始于 Shuffled 中的 compute 方法:\n\n```scala\noverride def compute(split: Partition, context: TaskContext): Iterator[(K, C)] = {\n    val dep = dependencies.head.asInstanceOf[ShuffleDependency[K, V, C]]\n    SparkEnv.get.shuffleManager.getReader(dep.shuffleHandle, split.index, split.index + 1, context)\n      .read()\n      .asInstanceOf[Iterator[(K, C)]]\n  }\n```\n\n可以看到这里传入了需要计算的分区。代码中获取了一个 reader 并调用了其 read 方法。 getReader 代码如下:\n\n```scala\n  override def getReader[K, C](\n      handle: ShuffleHandle,\n      startPartition: Int,\n      endPartition: Int,\n      context: TaskContext): ShuffleReader[K, C] = {\n    new BlockStoreShuffleReader(\n      handle.asInstanceOf[BaseShuffleHandle[K, _, C]], startPartition, endPartition, context)\n  }\n```\n\n实际上是返回了一个 BlockStoreShuffleReader 。read 方法较长，准备一段一段的讲解:\n\n```scala\noverride def read(): Iterator[Product2[K, C]]\n```\n\n方法返回了一个 Iterator。方法的开始定义了一个:\n\n```scala\n val wrappedStreams = new ShuffleBlockFetcherIterator(\n      context,\n      blockManager.shuffleClient,\n      blockManager,\n      // 获取 blockManagerId 与　blockId\n      mapOutputTracker.getMapSizesByExecutorId(handle.shuffleId, startPartition, endPartition),\n      serializerManager.wrapStream,\n      // Note: we use getSizeAsMb when no suffix is provided for backwards compatibility\n      SparkEnv.get.conf.getSizeAsMb(\"spark.reducer.maxSizeInFlight\", \"48m\") * 1024 * 1024,\n      SparkEnv.get.conf.getInt(\"spark.reducer.maxReqsInFlight\", Int.MaxValue),\n      SparkEnv.get.conf.get(config.REDUCER_MAX_BLOCKS_IN_FLIGHT_PER_ADDRESS),\n      SparkEnv.get.conf.get(config.MAX_REMOTE_BLOCK_SIZE_FETCH_TO_MEM),\n      SparkEnv.get.conf.getBoolean(\"spark.shuffle.detectCorrupt\", true))\n```\n\n这个 wrappedStreams 实际上获取数据的输入流。看看:\n\n```scala\n override def getMapSizesByExecutorId(shuffleId: Int, startPartition: Int, endPartition: Int)\n      : Iterator[(BlockManagerId, Seq[(BlockId, Long)])] = {\n    logDebug(s\"Fetching outputs for shuffle $shuffleId, partitions $startPartition-$endPartition\")\n    // 通过本次的 shuffleId 先获取 mapper 端产生的 MapStatuses\n    val statuses = getStatuses(shuffleId)\n    try {\n       // \n      MapOutputTracker.convertMapStatuses(shuffleId, startPartition, endPartition, statuses)\n    } catch {\n      case e: MetadataFetchFailedException =>\n        // We experienced a fetch failure so our mapStatuses cache is outdated; clear it:\n        mapStatuses.clear()\n        throw e\n    }\n  }\n```\n\n随后调用了 convertMapStatuses:\n\n```scala\ndef convertMapStatuses(\n      shuffleId: Int,\n      startPartition: Int,\n      endPartition: Int,\n      statuses: Array[MapStatus]): Iterator[(BlockManagerId, Seq[(BlockId, Long)])] = {\n    assert (statuses != null)\n    val splitsByAddress = new HashMap[BlockManagerId, ListBuffer[(BlockId, Long)]]\n    for ((status, mapId) <- statuses.iterator.zipWithIndex) {\n      if (status == null) {\n        val errorMessage = s\"Missing an output location for shuffle $shuffleId\"\n        logError(errorMessage)\n        throw new MetadataFetchFailedException(shuffleId, startPartition, errorMessage)\n      } else {\n        for (part <- startPartition until endPartition) {\n          val size = status.getSizeForBlock(part) // 这里只获取了需要处理的分区对应的数据\n          if (size != 0) {\n            // 这里获取到需要计算的分区数据所在的 block。其实就是由 shuffleId, mapId, part\n            // 这三个组成的\n            splitsByAddress.getOrElseUpdate(status.location, ListBuffer()) +=\n                ((ShuffleBlockId(shuffleId, mapId, part), size))\n          }\n        }\n      }\n    }\n    splitsByAddress.iterator\n  }\n```\n\n这里涉及到 mapId 和 part 这两个变量。实际上，这两个都是分区的 Id 。只不过 mapId 是 mapper 端对应的分区Id，而 part 是经过 shuffle 之后 reducer 端对应的分区Id。通过`convertMapStatuses`就可以得到需要从哪些 Block 拉取数据。\n\n``` scala\n// 获取　block　对应的输入流\nval recordIter = wrappedStreams.flatMap { case (blockId, wrappedStream) =>\n      // Note: the asKeyValueIterator below wraps a key/value iterator inside of a\n      // NextIterator. The NextIterator makes sure that close() is called on the\n      // underlying InputStream when all records have been read.\n      serializerInstance.deserializeStream(wrappedStream).asKeyValueIterator\n    }\n\n    // Update the context task metrics for each record read.\n    val readMetrics = context.taskMetrics.createTempShuffleReadMetrics()\n    val metricIter = CompletionIterator[(Any, Any), Iterator[(Any, Any)]](\n      recordIter.map { record =>\n        readMetrics.incRecordsRead(1)\n        record\n      },\n      context.taskMetrics().mergeShuffleReadMetrics())\n\n    // An interruptible iterator must be used here in order to support task cancellation\n    //\n    val interruptibleIter = new InterruptibleIterator[(Any, Any)](context, metricIter)\n\t\n    val aggregatedIter: Iterator[Product2[K, C]] = if (dep.aggregator.isDefined) {\n      　// 如果需要聚合\n        if (dep.mapSideCombine) {\n        // We are reading values that are already combined\n        val combinedKeyValuesIterator = interruptibleIter.asInstanceOf[Iterator[(K, C)]]\n        dep.aggregator.get.combineCombinersByKey(combinedKeyValuesIterator, context)\n      } else {\n        // We don't know the value type, but also don't care -- the dependency *should*\n        // have made sure its compatible w/ this aggregator, which will convert the value\n        // type to the combined type C\n        val keyValuesIterator = interruptibleIter.asInstanceOf[Iterator[(K, Nothing)]]\n        dep.aggregator.get.combineValuesByKey(keyValuesIterator, context)\n      }\n    } else {\n      interruptibleIter.asInstanceOf[Iterator[Product2[K, C]]]\n    }\n```\n\n这里进行聚合的代码实际上在 combineCombinersByKey 中:\n\n```scala\n  def combineCombinersByKey(\n      iter: Iterator[_ <: Product2[K, C]],\n      context: TaskContext): Iterator[(K, C)] = {\n    val combiners = new ExternalAppendOnlyMap[K, C, C](identity, mergeCombiners, mergeCombiners)\n    // 这里的 insertAll 和　shuffle write 中的 insertAll 效果一样，会排序生成临时文件\n    combiners.insertAll(iter)\n    updateMetrics(context, combiners)\n    combiners.iterator\n  }\n```\n\n最终返回了 iterator:\n\n```scala\n override def iterator: Iterator[(K, C)] = {\n    if (currentMap == null) {\n      throw new IllegalStateException(\n        \"ExternalAppendOnlyMap.iterator is destructive and should only be called once.\")\n    }\n     //　如果没有生成临时文件\n    if (spilledMaps.isEmpty) {\n      CompletionIterator[(K, C), Iterator[(K, C)]](\n        destructiveIterator(currentMap.iterator), freeCurrentMap())\n    } else {\n      new ExternalIterator()\n    }\n```\n\n如果有零时文件生成会返回一个 ExternalIterator :\n\n```scala\nprivate val mergeHeap = new mutable.PriorityQueue[StreamBuffer]\n    private val sortedMap = CompletionIterator[(K, C), Iterator[(K, C)]](destructiveIterator(\n      currentMap.destructiveSortedIterator(keyComparator)), freeCurrentMap())\n    // 这里合并了临时文件与内存中的数据\n    private val inputStreams = (Seq(sortedMap) ++ spilledMaps).map(it => it.buffered)\n    inputStreams.foreach { it =>\n      val kcPairs = new ArrayBuffer[(K, C)]\n      readNextHashCode(it, kcPairs)\n      if (kcPairs.length > 0) {\n        mergeHeap.enqueue(new StreamBuffer(it, kcPairs))\n      }\n    }\n```\n\n这里涉及到了一个很重要的类 ArrayBuffer:\n\n```scala\n    private class StreamBuffer(\n        val iterator: BufferedIterator[(K, C)],\n        val pairs: ArrayBuffer[(K, C)])\n      extends Comparable[StreamBuffer] {\n      def isEmpty: Boolean = pairs.length == 0\n      def minKeyHash: Int = {\n        assert(pairs.length > 0)\n        hashKey(pairs.head)\n      }\n      override def compareTo(other: StreamBuffer): Int = {\n        if (other.minKeyHash < minKeyHash) -1 else if (other.minKeyHash == minKeyHash) 0 else 1\n      }\n```\n\nStreamBuffer 实际上维持了一个 iterator 与一个数组。并且重写了 compareTo 方法。是通过数据中第一个元素的 key 也就是minKeyHash来比较大小。而代码:\n\n```scala\n readNextHashCode(it, kcPairs)\n      if (kcPairs.length > 0) {\n        mergeHeap.enqueue(new StreamBuffer(it, kcPairs))\n      }\n```\n\n再看 next 方法:\n\n```scala\n    override def next(): (K, C) = {\n      if (mergeHeap.isEmpty) {\n        throw new NoSuchElementException\n      }\n      // Select a key from the StreamBuffer that holds the lowest key hash\n      //　这里需要注意的是每一个 StreamBuffer 中的数组的元素是按照 key 经过排序的，mergeHeap 中的 StreamBuffer 也是按照 minKeyHash 进行排序的。也就是从 mergeHeap 每取出一个StreamBuffer，其对应的数组中 key 的 hash 一定是目前 mergeHeap 中所有数组中 key 最小的，如果能理解这一点，那这里的 merge 就基本可以理解了。\n      val minBuffer = mergeHeap.dequeue()\n      val minPairs = minBuffer.pairs\n      val minHash = minBuffer.minKeyHash\n      val minPair = removeFromBuffer(minPairs, 0)\n      val minKey = minPair._1\n      var minCombiner = minPair._2\n      assert(hashKey(minPair) == minHash)\n\n      // For all other streams that may have this key (i.e. have the same minimum key hash),\n      // merge in the corresponding value (if any) from that stream\n      val mergedBuffers = ArrayBuffer[StreamBuffer](minBuffer)\n      // 如果下一个 StreamBuffer 中的 minKeyHash 相同，则可能会含有相同的 key，则需要合并。这里由于是经过排序的，所以不用遍历所有的 StreamBuffer。只要下一个不同，则后面一定不会有与当前 minKeyHash 相同的的 StreamBuffer。\n      while (mergeHeap.nonEmpty && mergeHeap.head.minKeyHash == minHash) {\n        val newBuffer = mergeHeap.dequeue()\n        // 这里如果有相同的 key 则进行合并\n        // 注意，hash 相同 key 不一定相同\n        minCombiner = mergeIfKeyExists(minKey, minCombiner, newBuffer)\n        mergedBuffers += newBuffer\n      }\n\n      // Repopulate each visited stream buffer and add it back to the queue if it is non-empty\n      \n      mergedBuffers.foreach { buffer =>\n        // 如果 key 被合并完了，就需要读取下一批hash相同的key的数据到ArrayBuffer的数组中\n        if (buffer.isEmpty) {\n          readNextHashCode(buffer.iterator, buffer.pairs)\n        }\n        if (!buffer.isEmpty) {\n          // 刚才dequeue的 ArrayBuffer的数组中可能有没合并完的数据 \n          // 或者有新读取的数据则需要继续放入 mergeHeap 中进行合并 \n          mergeHeap.enqueue(buffer)\n        }\n      }\n      // 返回合并完的数据\n      // 注意这里的 key 只是按照 hash 进行排序的，在 ExternalSorter 才是按照用户定义的排序方式进行排序\n      (minKey, minCombiner)\n    }\n```\n\n如果不需要排序，shuffle read 就算完成了。但是如果需要排序则:\n\n```scala\n val resultIter = dep.keyOrdering match {\n      case Some(keyOrd: Ordering[K]) =>\n        // Create an ExternalSorter to sort the data.\n        val sorter =\n          new ExternalSorter[K, C, C](context, ordering = Some(keyOrd), serializer = dep.serializer)\n        sorter.insertAll(aggregatedIter)\n        context.taskMetrics().incMemoryBytesSpilled(sorter.memoryBytesSpilled)\n        context.taskMetrics().incDiskBytesSpilled(sorter.diskBytesSpilled)\n        context.taskMetrics().incPeakExecutionMemory(sorter.peakMemoryUsedBytes)\n        // Use completion callback to stop sorter if task was finished/cancelled.\n        context.addTaskCompletionListener(_ => {\n          sorter.stop()\n        })\n        CompletionIterator[Product2[K, C], Iterator[Product2[K, C]]](sorter.iterator, sorter.stop())\n      case None =>\n        aggregatedIter\n    }\n\n    resultIter match {\n      case _: InterruptibleIterator[Product2[K, C]] => resultIter\n      case _ =>\n        // Use another interruptible iterator here to support task cancellation as aggregator\n        // or(and) sorter may have consumed previous interruptible iterator.\n        new InterruptibleIterator[Product2[K, C]](context, resultIter)\n    }\n```\n\n这里依然使用了 ExternalSorter 进行排序，而最终使用了 ExternalSorter 的 iterator 方法。\n\n```scala\n  def iterator: Iterator[Product2[K, C]] = {\n    isShuffleSort = false\n    partitionedIterator.flatMap(pair => pair._2)\n  }\n\n```\n\n其中 partitionedIterator 方法在 shuffle write 部分已经讲的很清楚了。有兴趣的可以去看看。这里 shuffle read 就基本结束了。其中从其他 Executor 拉取数据的部分由于涉及很多，这里就基本没有怎么讲解。以后可能会专门开一篇文章。","source":"_posts/Spark源码阅读计划-第四部分-shuffle-read.md","raw":"---\ntitle: Spark源码阅读计划-第四部分-shuffle read\ndate: 2018-06-18 10:38:44\ntags:\n  - Spark\n  - 编程\ncategories: Spark源码阅读计划\nauthor: lishion\ntoc: true\n---\n\n拖了这么久终于把 shuffle read 部分的源码看了一遍了。虽然 shuffle read 再数据合并部分的逻辑要比 shuffle 简单，但是由于这个过程中 executor 要到 master 拉取 shuffle write 结果信息，就涉及到 spark 的 block manager 的一些东西，因此完整的 shuffle read 过程依然是很复杂的。但是由于我还是太菜了，对于 block manage 这一部分看的一知半解，因此关于 executor 与 master 进行元数据交互的部分也就不会写得很详细，当然还是会涉及到一些。有关 block manage 的这部分以后应该会写到，先立个 flag 在这里吧。\n\n## MapStatus\t\n\n在 shuffle write 完成之后会返回一个 MapStatus，MapStatus记录了 BlockManagerId 以及最终每个分区的大小。其中 BlockManagerId 包含了 BlockManager 所在的 host 以及 port 等信息。返回的 MapStatus 最终会被 Driver 端获并存储以便于 mapper 端获取。 这里要稍微提一下 BlockManager 的知识，Spark 利用 BlockManager 对数据进行读写，而 Block 就是其中的基本单位。每一个 Block 拥有一个 id。而通过BlockManager 则可以操作这些 Block 。因此 mapper 端只需要知道 BlockManager 的位置以及所需要的文件在哪些 Block 就能获取到对应的数据。这里通过 MapOutputTracker 通过 shuffleid 对一次 shuffle write 中所有的 mapper 产生的 MapStatus 进行了记录。\n\n在每一个 mapper 的 shuffle task 结束后，MapOutputTracker就会将其返回的 MapStatues 进行注册。在 DAGScheduler 中可以看到:\n\n```scala\n case smt: ShuffleMapTask =>\n              val shuffleStage = stage.asInstanceOf[ShuffleMapStage]\n              val status = event.result.asInstanceOf[MapStatus]\n              val execId = status.location.executorId\n\t\t\t  \n              mapOutputTracker.registerMapOutput(\n                shuffleStage.shuffleDep.shuffleId, smt.partitionId, status)\n```\n\n这里的 MapOutputTracker 实际的类型为 MapOutputTrackerMaster 。继续看关于注册的代码:\n\n```scala\n def registerMapOutput(shuffleId: Int, mapId: Int, status: MapStatus) {\n    shuffleStatuses(shuffleId).addMapOutput(mapId, status)\n  }\n```\n\nshuffleStatuses 实际上是一个 hashmap:\n\n```scala\nval shuffleStatuses = new ConcurrentHashMap[Int, ShuffleStatus]().asScala\n```\n\nShuffleStatus 是一个类，里面包含了一个 MapStatus 数组。也就是说，通过在 shuffle write 之前注册的 shuffle 所用的 shuffleid 作为索引存储了 shuffle write 过程产生的 MapStatus。而 MapOutputTrackerMaster 是用于 Driver 端的。用于 Executor 的则是 MapOutputTrackerWorker 。在 MapOutputTrackerWorker 中一开始是没有 shuffleStatuses 的。需要从 MapOutputTrackerMaster 中获取。\n\n## shuffle read\n\nshuffle read 代码开始于 Shuffled 中的 compute 方法:\n\n```scala\noverride def compute(split: Partition, context: TaskContext): Iterator[(K, C)] = {\n    val dep = dependencies.head.asInstanceOf[ShuffleDependency[K, V, C]]\n    SparkEnv.get.shuffleManager.getReader(dep.shuffleHandle, split.index, split.index + 1, context)\n      .read()\n      .asInstanceOf[Iterator[(K, C)]]\n  }\n```\n\n可以看到这里传入了需要计算的分区。代码中获取了一个 reader 并调用了其 read 方法。 getReader 代码如下:\n\n```scala\n  override def getReader[K, C](\n      handle: ShuffleHandle,\n      startPartition: Int,\n      endPartition: Int,\n      context: TaskContext): ShuffleReader[K, C] = {\n    new BlockStoreShuffleReader(\n      handle.asInstanceOf[BaseShuffleHandle[K, _, C]], startPartition, endPartition, context)\n  }\n```\n\n实际上是返回了一个 BlockStoreShuffleReader 。read 方法较长，准备一段一段的讲解:\n\n```scala\noverride def read(): Iterator[Product2[K, C]]\n```\n\n方法返回了一个 Iterator。方法的开始定义了一个:\n\n```scala\n val wrappedStreams = new ShuffleBlockFetcherIterator(\n      context,\n      blockManager.shuffleClient,\n      blockManager,\n      // 获取 blockManagerId 与　blockId\n      mapOutputTracker.getMapSizesByExecutorId(handle.shuffleId, startPartition, endPartition),\n      serializerManager.wrapStream,\n      // Note: we use getSizeAsMb when no suffix is provided for backwards compatibility\n      SparkEnv.get.conf.getSizeAsMb(\"spark.reducer.maxSizeInFlight\", \"48m\") * 1024 * 1024,\n      SparkEnv.get.conf.getInt(\"spark.reducer.maxReqsInFlight\", Int.MaxValue),\n      SparkEnv.get.conf.get(config.REDUCER_MAX_BLOCKS_IN_FLIGHT_PER_ADDRESS),\n      SparkEnv.get.conf.get(config.MAX_REMOTE_BLOCK_SIZE_FETCH_TO_MEM),\n      SparkEnv.get.conf.getBoolean(\"spark.shuffle.detectCorrupt\", true))\n```\n\n这个 wrappedStreams 实际上获取数据的输入流。看看:\n\n```scala\n override def getMapSizesByExecutorId(shuffleId: Int, startPartition: Int, endPartition: Int)\n      : Iterator[(BlockManagerId, Seq[(BlockId, Long)])] = {\n    logDebug(s\"Fetching outputs for shuffle $shuffleId, partitions $startPartition-$endPartition\")\n    // 通过本次的 shuffleId 先获取 mapper 端产生的 MapStatuses\n    val statuses = getStatuses(shuffleId)\n    try {\n       // \n      MapOutputTracker.convertMapStatuses(shuffleId, startPartition, endPartition, statuses)\n    } catch {\n      case e: MetadataFetchFailedException =>\n        // We experienced a fetch failure so our mapStatuses cache is outdated; clear it:\n        mapStatuses.clear()\n        throw e\n    }\n  }\n```\n\n随后调用了 convertMapStatuses:\n\n```scala\ndef convertMapStatuses(\n      shuffleId: Int,\n      startPartition: Int,\n      endPartition: Int,\n      statuses: Array[MapStatus]): Iterator[(BlockManagerId, Seq[(BlockId, Long)])] = {\n    assert (statuses != null)\n    val splitsByAddress = new HashMap[BlockManagerId, ListBuffer[(BlockId, Long)]]\n    for ((status, mapId) <- statuses.iterator.zipWithIndex) {\n      if (status == null) {\n        val errorMessage = s\"Missing an output location for shuffle $shuffleId\"\n        logError(errorMessage)\n        throw new MetadataFetchFailedException(shuffleId, startPartition, errorMessage)\n      } else {\n        for (part <- startPartition until endPartition) {\n          val size = status.getSizeForBlock(part) // 这里只获取了需要处理的分区对应的数据\n          if (size != 0) {\n            // 这里获取到需要计算的分区数据所在的 block。其实就是由 shuffleId, mapId, part\n            // 这三个组成的\n            splitsByAddress.getOrElseUpdate(status.location, ListBuffer()) +=\n                ((ShuffleBlockId(shuffleId, mapId, part), size))\n          }\n        }\n      }\n    }\n    splitsByAddress.iterator\n  }\n```\n\n这里涉及到 mapId 和 part 这两个变量。实际上，这两个都是分区的 Id 。只不过 mapId 是 mapper 端对应的分区Id，而 part 是经过 shuffle 之后 reducer 端对应的分区Id。通过`convertMapStatuses`就可以得到需要从哪些 Block 拉取数据。\n\n``` scala\n// 获取　block　对应的输入流\nval recordIter = wrappedStreams.flatMap { case (blockId, wrappedStream) =>\n      // Note: the asKeyValueIterator below wraps a key/value iterator inside of a\n      // NextIterator. The NextIterator makes sure that close() is called on the\n      // underlying InputStream when all records have been read.\n      serializerInstance.deserializeStream(wrappedStream).asKeyValueIterator\n    }\n\n    // Update the context task metrics for each record read.\n    val readMetrics = context.taskMetrics.createTempShuffleReadMetrics()\n    val metricIter = CompletionIterator[(Any, Any), Iterator[(Any, Any)]](\n      recordIter.map { record =>\n        readMetrics.incRecordsRead(1)\n        record\n      },\n      context.taskMetrics().mergeShuffleReadMetrics())\n\n    // An interruptible iterator must be used here in order to support task cancellation\n    //\n    val interruptibleIter = new InterruptibleIterator[(Any, Any)](context, metricIter)\n\t\n    val aggregatedIter: Iterator[Product2[K, C]] = if (dep.aggregator.isDefined) {\n      　// 如果需要聚合\n        if (dep.mapSideCombine) {\n        // We are reading values that are already combined\n        val combinedKeyValuesIterator = interruptibleIter.asInstanceOf[Iterator[(K, C)]]\n        dep.aggregator.get.combineCombinersByKey(combinedKeyValuesIterator, context)\n      } else {\n        // We don't know the value type, but also don't care -- the dependency *should*\n        // have made sure its compatible w/ this aggregator, which will convert the value\n        // type to the combined type C\n        val keyValuesIterator = interruptibleIter.asInstanceOf[Iterator[(K, Nothing)]]\n        dep.aggregator.get.combineValuesByKey(keyValuesIterator, context)\n      }\n    } else {\n      interruptibleIter.asInstanceOf[Iterator[Product2[K, C]]]\n    }\n```\n\n这里进行聚合的代码实际上在 combineCombinersByKey 中:\n\n```scala\n  def combineCombinersByKey(\n      iter: Iterator[_ <: Product2[K, C]],\n      context: TaskContext): Iterator[(K, C)] = {\n    val combiners = new ExternalAppendOnlyMap[K, C, C](identity, mergeCombiners, mergeCombiners)\n    // 这里的 insertAll 和　shuffle write 中的 insertAll 效果一样，会排序生成临时文件\n    combiners.insertAll(iter)\n    updateMetrics(context, combiners)\n    combiners.iterator\n  }\n```\n\n最终返回了 iterator:\n\n```scala\n override def iterator: Iterator[(K, C)] = {\n    if (currentMap == null) {\n      throw new IllegalStateException(\n        \"ExternalAppendOnlyMap.iterator is destructive and should only be called once.\")\n    }\n     //　如果没有生成临时文件\n    if (spilledMaps.isEmpty) {\n      CompletionIterator[(K, C), Iterator[(K, C)]](\n        destructiveIterator(currentMap.iterator), freeCurrentMap())\n    } else {\n      new ExternalIterator()\n    }\n```\n\n如果有零时文件生成会返回一个 ExternalIterator :\n\n```scala\nprivate val mergeHeap = new mutable.PriorityQueue[StreamBuffer]\n    private val sortedMap = CompletionIterator[(K, C), Iterator[(K, C)]](destructiveIterator(\n      currentMap.destructiveSortedIterator(keyComparator)), freeCurrentMap())\n    // 这里合并了临时文件与内存中的数据\n    private val inputStreams = (Seq(sortedMap) ++ spilledMaps).map(it => it.buffered)\n    inputStreams.foreach { it =>\n      val kcPairs = new ArrayBuffer[(K, C)]\n      readNextHashCode(it, kcPairs)\n      if (kcPairs.length > 0) {\n        mergeHeap.enqueue(new StreamBuffer(it, kcPairs))\n      }\n    }\n```\n\n这里涉及到了一个很重要的类 ArrayBuffer:\n\n```scala\n    private class StreamBuffer(\n        val iterator: BufferedIterator[(K, C)],\n        val pairs: ArrayBuffer[(K, C)])\n      extends Comparable[StreamBuffer] {\n      def isEmpty: Boolean = pairs.length == 0\n      def minKeyHash: Int = {\n        assert(pairs.length > 0)\n        hashKey(pairs.head)\n      }\n      override def compareTo(other: StreamBuffer): Int = {\n        if (other.minKeyHash < minKeyHash) -1 else if (other.minKeyHash == minKeyHash) 0 else 1\n      }\n```\n\nStreamBuffer 实际上维持了一个 iterator 与一个数组。并且重写了 compareTo 方法。是通过数据中第一个元素的 key 也就是minKeyHash来比较大小。而代码:\n\n```scala\n readNextHashCode(it, kcPairs)\n      if (kcPairs.length > 0) {\n        mergeHeap.enqueue(new StreamBuffer(it, kcPairs))\n      }\n```\n\n再看 next 方法:\n\n```scala\n    override def next(): (K, C) = {\n      if (mergeHeap.isEmpty) {\n        throw new NoSuchElementException\n      }\n      // Select a key from the StreamBuffer that holds the lowest key hash\n      //　这里需要注意的是每一个 StreamBuffer 中的数组的元素是按照 key 经过排序的，mergeHeap 中的 StreamBuffer 也是按照 minKeyHash 进行排序的。也就是从 mergeHeap 每取出一个StreamBuffer，其对应的数组中 key 的 hash 一定是目前 mergeHeap 中所有数组中 key 最小的，如果能理解这一点，那这里的 merge 就基本可以理解了。\n      val minBuffer = mergeHeap.dequeue()\n      val minPairs = minBuffer.pairs\n      val minHash = minBuffer.minKeyHash\n      val minPair = removeFromBuffer(minPairs, 0)\n      val minKey = minPair._1\n      var minCombiner = minPair._2\n      assert(hashKey(minPair) == minHash)\n\n      // For all other streams that may have this key (i.e. have the same minimum key hash),\n      // merge in the corresponding value (if any) from that stream\n      val mergedBuffers = ArrayBuffer[StreamBuffer](minBuffer)\n      // 如果下一个 StreamBuffer 中的 minKeyHash 相同，则可能会含有相同的 key，则需要合并。这里由于是经过排序的，所以不用遍历所有的 StreamBuffer。只要下一个不同，则后面一定不会有与当前 minKeyHash 相同的的 StreamBuffer。\n      while (mergeHeap.nonEmpty && mergeHeap.head.minKeyHash == minHash) {\n        val newBuffer = mergeHeap.dequeue()\n        // 这里如果有相同的 key 则进行合并\n        // 注意，hash 相同 key 不一定相同\n        minCombiner = mergeIfKeyExists(minKey, minCombiner, newBuffer)\n        mergedBuffers += newBuffer\n      }\n\n      // Repopulate each visited stream buffer and add it back to the queue if it is non-empty\n      \n      mergedBuffers.foreach { buffer =>\n        // 如果 key 被合并完了，就需要读取下一批hash相同的key的数据到ArrayBuffer的数组中\n        if (buffer.isEmpty) {\n          readNextHashCode(buffer.iterator, buffer.pairs)\n        }\n        if (!buffer.isEmpty) {\n          // 刚才dequeue的 ArrayBuffer的数组中可能有没合并完的数据 \n          // 或者有新读取的数据则需要继续放入 mergeHeap 中进行合并 \n          mergeHeap.enqueue(buffer)\n        }\n      }\n      // 返回合并完的数据\n      // 注意这里的 key 只是按照 hash 进行排序的，在 ExternalSorter 才是按照用户定义的排序方式进行排序\n      (minKey, minCombiner)\n    }\n```\n\n如果不需要排序，shuffle read 就算完成了。但是如果需要排序则:\n\n```scala\n val resultIter = dep.keyOrdering match {\n      case Some(keyOrd: Ordering[K]) =>\n        // Create an ExternalSorter to sort the data.\n        val sorter =\n          new ExternalSorter[K, C, C](context, ordering = Some(keyOrd), serializer = dep.serializer)\n        sorter.insertAll(aggregatedIter)\n        context.taskMetrics().incMemoryBytesSpilled(sorter.memoryBytesSpilled)\n        context.taskMetrics().incDiskBytesSpilled(sorter.diskBytesSpilled)\n        context.taskMetrics().incPeakExecutionMemory(sorter.peakMemoryUsedBytes)\n        // Use completion callback to stop sorter if task was finished/cancelled.\n        context.addTaskCompletionListener(_ => {\n          sorter.stop()\n        })\n        CompletionIterator[Product2[K, C], Iterator[Product2[K, C]]](sorter.iterator, sorter.stop())\n      case None =>\n        aggregatedIter\n    }\n\n    resultIter match {\n      case _: InterruptibleIterator[Product2[K, C]] => resultIter\n      case _ =>\n        // Use another interruptible iterator here to support task cancellation as aggregator\n        // or(and) sorter may have consumed previous interruptible iterator.\n        new InterruptibleIterator[Product2[K, C]](context, resultIter)\n    }\n```\n\n这里依然使用了 ExternalSorter 进行排序，而最终使用了 ExternalSorter 的 iterator 方法。\n\n```scala\n  def iterator: Iterator[Product2[K, C]] = {\n    isShuffleSort = false\n    partitionedIterator.flatMap(pair => pair._2)\n  }\n\n```\n\n其中 partitionedIterator 方法在 shuffle write 部分已经讲的很清楚了。有兴趣的可以去看看。这里 shuffle read 就基本结束了。其中从其他 Executor 拉取数据的部分由于涉及很多，这里就基本没有怎么讲解。以后可能会专门开一篇文章。","slug":"Spark源码阅读计划-第四部分-shuffle-read","published":1,"updated":"2018-06-20T06:22:05.783Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjjhx8v79002rb8tg2feixeg6","content":"<p>拖了这么久终于把 shuffle read 部分的源码看了一遍了。虽然 shuffle read 再数据合并部分的逻辑要比 shuffle 简单，但是由于这个过程中 executor 要到 master 拉取 shuffle write 结果信息，就涉及到 spark 的 block manager 的一些东西，因此完整的 shuffle read 过程依然是很复杂的。但是由于我还是太菜了，对于 block manage 这一部分看的一知半解，因此关于 executor 与 master 进行元数据交互的部分也就不会写得很详细，当然还是会涉及到一些。有关 block manage 的这部分以后应该会写到，先立个 flag 在这里吧。</p>\n<h2 id=\"MapStatus\"><a href=\"#MapStatus\" class=\"headerlink\" title=\"MapStatus\"></a>MapStatus</h2><p>在 shuffle write 完成之后会返回一个 MapStatus，MapStatus记录了 BlockManagerId 以及最终每个分区的大小。其中 BlockManagerId 包含了 BlockManager 所在的 host 以及 port 等信息。返回的 MapStatus 最终会被 Driver 端获并存储以便于 mapper 端获取。 这里要稍微提一下 BlockManager 的知识，Spark 利用 BlockManager 对数据进行读写，而 Block 就是其中的基本单位。每一个 Block 拥有一个 id。而通过BlockManager 则可以操作这些 Block 。因此 mapper 端只需要知道 BlockManager 的位置以及所需要的文件在哪些 Block 就能获取到对应的数据。这里通过 MapOutputTracker 通过 shuffleid 对一次 shuffle write 中所有的 mapper 产生的 MapStatus 进行了记录。</p>\n<p>在每一个 mapper 的 shuffle task 结束后，MapOutputTracker就会将其返回的 MapStatues 进行注册。在 DAGScheduler 中可以看到:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">case</span> smt: <span class=\"type\">ShuffleMapTask</span> =&gt;</span><br><span class=\"line\">             <span class=\"keyword\">val</span> shuffleStage = stage.asInstanceOf[<span class=\"type\">ShuffleMapStage</span>]</span><br><span class=\"line\">             <span class=\"keyword\">val</span> status = event.result.asInstanceOf[<span class=\"type\">MapStatus</span>]</span><br><span class=\"line\">             <span class=\"keyword\">val</span> execId = status.location.executorId</span><br><span class=\"line\">\t\t  </span><br><span class=\"line\">             mapOutputTracker.registerMapOutput(</span><br><span class=\"line\">               shuffleStage.shuffleDep.shuffleId, smt.partitionId, status)</span><br></pre></td></tr></table></figure>\n<p>这里的 MapOutputTracker 实际的类型为 MapOutputTrackerMaster 。继续看关于注册的代码:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">registerMapOutput</span></span>(shuffleId: <span class=\"type\">Int</span>, mapId: <span class=\"type\">Int</span>, status: <span class=\"type\">MapStatus</span>) &#123;</span><br><span class=\"line\">   shuffleStatuses(shuffleId).addMapOutput(mapId, status)</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>shuffleStatuses 实际上是一个 hashmap:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> shuffleStatuses = <span class=\"keyword\">new</span> <span class=\"type\">ConcurrentHashMap</span>[<span class=\"type\">Int</span>, <span class=\"type\">ShuffleStatus</span>]().asScala</span><br></pre></td></tr></table></figure>\n<p>ShuffleStatus 是一个类，里面包含了一个 MapStatus 数组。也就是说，通过在 shuffle write 之前注册的 shuffle 所用的 shuffleid 作为索引存储了 shuffle write 过程产生的 MapStatus。而 MapOutputTrackerMaster 是用于 Driver 端的。用于 Executor 的则是 MapOutputTrackerWorker 。在 MapOutputTrackerWorker 中一开始是没有 shuffleStatuses 的。需要从 MapOutputTrackerMaster 中获取。</p>\n<h2 id=\"shuffle-read\"><a href=\"#shuffle-read\" class=\"headerlink\" title=\"shuffle read\"></a>shuffle read</h2><p>shuffle read 代码开始于 Shuffled 中的 compute 方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span></span>(split: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>)] = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> dep = dependencies.head.asInstanceOf[<span class=\"type\">ShuffleDependency</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>, <span class=\"type\">C</span>]]</span><br><span class=\"line\">    <span class=\"type\">SparkEnv</span>.get.shuffleManager.getReader(dep.shuffleHandle, split.index, split.index + <span class=\"number\">1</span>, context)</span><br><span class=\"line\">      .read()</span><br><span class=\"line\">      .asInstanceOf[<span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>)]]</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>可以看到这里传入了需要计算的分区。代码中获取了一个 reader 并调用了其 read 方法。 getReader 代码如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getReader</span></span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>](</span><br><span class=\"line\">    handle: <span class=\"type\">ShuffleHandle</span>,</span><br><span class=\"line\">    startPartition: <span class=\"type\">Int</span>,</span><br><span class=\"line\">    endPartition: <span class=\"type\">Int</span>,</span><br><span class=\"line\">    context: <span class=\"type\">TaskContext</span>): <span class=\"type\">ShuffleReader</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>] = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">new</span> <span class=\"type\">BlockStoreShuffleReader</span>(</span><br><span class=\"line\">    handle.asInstanceOf[<span class=\"type\">BaseShuffleHandle</span>[<span class=\"type\">K</span>, _, <span class=\"type\">C</span>]], startPartition, endPartition, context)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>实际上是返回了一个 BlockStoreShuffleReader 。read 方法较长，准备一段一段的讲解:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">read</span></span>(): <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]]</span><br></pre></td></tr></table></figure>\n<p>方法返回了一个 Iterator。方法的开始定义了一个:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> wrappedStreams = <span class=\"keyword\">new</span> <span class=\"type\">ShuffleBlockFetcherIterator</span>(</span><br><span class=\"line\">     context,</span><br><span class=\"line\">     blockManager.shuffleClient,</span><br><span class=\"line\">     blockManager,</span><br><span class=\"line\">     <span class=\"comment\">// 获取 blockManagerId 与　blockId</span></span><br><span class=\"line\">     mapOutputTracker.getMapSizesByExecutorId(handle.shuffleId, startPartition, endPartition),</span><br><span class=\"line\">     serializerManager.wrapStream,</span><br><span class=\"line\">     <span class=\"comment\">// Note: we use getSizeAsMb when no suffix is provided for backwards compatibility</span></span><br><span class=\"line\">     <span class=\"type\">SparkEnv</span>.get.conf.getSizeAsMb(<span class=\"string\">\"spark.reducer.maxSizeInFlight\"</span>, <span class=\"string\">\"48m\"</span>) * <span class=\"number\">1024</span> * <span class=\"number\">1024</span>,</span><br><span class=\"line\">     <span class=\"type\">SparkEnv</span>.get.conf.getInt(<span class=\"string\">\"spark.reducer.maxReqsInFlight\"</span>, <span class=\"type\">Int</span>.<span class=\"type\">MaxValue</span>),</span><br><span class=\"line\">     <span class=\"type\">SparkEnv</span>.get.conf.get(config.<span class=\"type\">REDUCER_MAX_BLOCKS_IN_FLIGHT_PER_ADDRESS</span>),</span><br><span class=\"line\">     <span class=\"type\">SparkEnv</span>.get.conf.get(config.<span class=\"type\">MAX_REMOTE_BLOCK_SIZE_FETCH_TO_MEM</span>),</span><br><span class=\"line\">     <span class=\"type\">SparkEnv</span>.get.conf.getBoolean(<span class=\"string\">\"spark.shuffle.detectCorrupt\"</span>, <span class=\"literal\">true</span>))</span><br></pre></td></tr></table></figure>\n<p>这个 wrappedStreams 实际上获取数据的输入流。看看:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getMapSizesByExecutorId</span></span>(shuffleId: <span class=\"type\">Int</span>, startPartition: <span class=\"type\">Int</span>, endPartition: <span class=\"type\">Int</span>)</span><br><span class=\"line\">     : <span class=\"type\">Iterator</span>[(<span class=\"type\">BlockManagerId</span>, <span class=\"type\">Seq</span>[(<span class=\"type\">BlockId</span>, <span class=\"type\">Long</span>)])] = &#123;</span><br><span class=\"line\">   logDebug(<span class=\"string\">s\"Fetching outputs for shuffle <span class=\"subst\">$shuffleId</span>, partitions <span class=\"subst\">$startPartition</span>-<span class=\"subst\">$endPartition</span>\"</span>)</span><br><span class=\"line\">   <span class=\"comment\">// 通过本次的 shuffleId 先获取 mapper 端产生的 MapStatuses</span></span><br><span class=\"line\">   <span class=\"keyword\">val</span> statuses = getStatuses(shuffleId)</span><br><span class=\"line\">   <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">      <span class=\"comment\">// </span></span><br><span class=\"line\">     <span class=\"type\">MapOutputTracker</span>.convertMapStatuses(shuffleId, startPartition, endPartition, statuses)</span><br><span class=\"line\">   &#125; <span class=\"keyword\">catch</span> &#123;</span><br><span class=\"line\">     <span class=\"keyword\">case</span> e: <span class=\"type\">MetadataFetchFailedException</span> =&gt;</span><br><span class=\"line\">       <span class=\"comment\">// We experienced a fetch failure so our mapStatuses cache is outdated; clear it:</span></span><br><span class=\"line\">       mapStatuses.clear()</span><br><span class=\"line\">       <span class=\"keyword\">throw</span> e</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>随后调用了 convertMapStatuses:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">convertMapStatuses</span></span>(</span><br><span class=\"line\">      shuffleId: <span class=\"type\">Int</span>,</span><br><span class=\"line\">      startPartition: <span class=\"type\">Int</span>,</span><br><span class=\"line\">      endPartition: <span class=\"type\">Int</span>,</span><br><span class=\"line\">      statuses: <span class=\"type\">Array</span>[<span class=\"type\">MapStatus</span>]): <span class=\"type\">Iterator</span>[(<span class=\"type\">BlockManagerId</span>, <span class=\"type\">Seq</span>[(<span class=\"type\">BlockId</span>, <span class=\"type\">Long</span>)])] = &#123;</span><br><span class=\"line\">    assert (statuses != <span class=\"literal\">null</span>)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> splitsByAddress = <span class=\"keyword\">new</span> <span class=\"type\">HashMap</span>[<span class=\"type\">BlockManagerId</span>, <span class=\"type\">ListBuffer</span>[(<span class=\"type\">BlockId</span>, <span class=\"type\">Long</span>)]]</span><br><span class=\"line\">    <span class=\"keyword\">for</span> ((status, mapId) &lt;- statuses.iterator.zipWithIndex) &#123;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (status == <span class=\"literal\">null</span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">val</span> errorMessage = <span class=\"string\">s\"Missing an output location for shuffle <span class=\"subst\">$shuffleId</span>\"</span></span><br><span class=\"line\">        logError(errorMessage)</span><br><span class=\"line\">        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">MetadataFetchFailedException</span>(shuffleId, startPartition, errorMessage)</span><br><span class=\"line\">      &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (part &lt;- startPartition until endPartition) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">val</span> size = status.getSizeForBlock(part) <span class=\"comment\">// 这里只获取了需要处理的分区对应的数据</span></span><br><span class=\"line\">          <span class=\"keyword\">if</span> (size != <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">            <span class=\"comment\">// 这里获取到需要计算的分区数据所在的 block。其实就是由 shuffleId, mapId, part</span></span><br><span class=\"line\">            <span class=\"comment\">// 这三个组成的</span></span><br><span class=\"line\">            splitsByAddress.getOrElseUpdate(status.location, <span class=\"type\">ListBuffer</span>()) +=</span><br><span class=\"line\">                ((<span class=\"type\">ShuffleBlockId</span>(shuffleId, mapId, part), size))</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    splitsByAddress.iterator</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>这里涉及到 mapId 和 part 这两个变量。实际上，这两个都是分区的 Id 。只不过 mapId 是 mapper 端对应的分区Id，而 part 是经过 shuffle 之后 reducer 端对应的分区Id。通过<code>convertMapStatuses</code>就可以得到需要从哪些 Block 拉取数据。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 获取　block　对应的输入流</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> recordIter = wrappedStreams.flatMap &#123; <span class=\"keyword\">case</span> (blockId, wrappedStream) =&gt;</span><br><span class=\"line\">      <span class=\"comment\">// Note: the asKeyValueIterator below wraps a key/value iterator inside of a</span></span><br><span class=\"line\">      <span class=\"comment\">// NextIterator. The NextIterator makes sure that close() is called on the</span></span><br><span class=\"line\">      <span class=\"comment\">// underlying InputStream when all records have been read.</span></span><br><span class=\"line\">      serializerInstance.deserializeStream(wrappedStream).asKeyValueIterator</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// Update the context task metrics for each record read.</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> readMetrics = context.taskMetrics.createTempShuffleReadMetrics()</span><br><span class=\"line\">    <span class=\"keyword\">val</span> metricIter = <span class=\"type\">CompletionIterator</span>[(<span class=\"type\">Any</span>, <span class=\"type\">Any</span>), <span class=\"type\">Iterator</span>[(<span class=\"type\">Any</span>, <span class=\"type\">Any</span>)]](</span><br><span class=\"line\">      recordIter.map &#123; record =&gt;</span><br><span class=\"line\">        readMetrics.incRecordsRead(<span class=\"number\">1</span>)</span><br><span class=\"line\">        record</span><br><span class=\"line\">      &#125;,</span><br><span class=\"line\">      context.taskMetrics().mergeShuffleReadMetrics())</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// An interruptible iterator must be used here in order to support task cancellation</span></span><br><span class=\"line\">    <span class=\"comment\">//</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> interruptibleIter = <span class=\"keyword\">new</span> <span class=\"type\">InterruptibleIterator</span>[(<span class=\"type\">Any</span>, <span class=\"type\">Any</span>)](context, metricIter)</span><br><span class=\"line\">\t</span><br><span class=\"line\">    <span class=\"keyword\">val</span> aggregatedIter: <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] = <span class=\"keyword\">if</span> (dep.aggregator.isDefined) &#123;</span><br><span class=\"line\">      　<span class=\"comment\">// 如果需要聚合</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (dep.mapSideCombine) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// We are reading values that are already combined</span></span><br><span class=\"line\">        <span class=\"keyword\">val</span> combinedKeyValuesIterator = interruptibleIter.asInstanceOf[<span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>)]]</span><br><span class=\"line\">        dep.aggregator.get.combineCombinersByKey(combinedKeyValuesIterator, context)</span><br><span class=\"line\">      &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// We don't know the value type, but also don't care -- the dependency *should*</span></span><br><span class=\"line\">        <span class=\"comment\">// have made sure its compatible w/ this aggregator, which will convert the value</span></span><br><span class=\"line\">        <span class=\"comment\">// type to the combined type C</span></span><br><span class=\"line\">        <span class=\"keyword\">val</span> keyValuesIterator = interruptibleIter.asInstanceOf[<span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">Nothing</span>)]]</span><br><span class=\"line\">        dep.aggregator.get.combineValuesByKey(keyValuesIterator, context)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">      interruptibleIter.asInstanceOf[<span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]]]</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>这里进行聚合的代码实际上在 combineCombinersByKey 中:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">combineCombinersByKey</span></span>(</span><br><span class=\"line\">    iter: <span class=\"type\">Iterator</span>[_ &lt;: <span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]],</span><br><span class=\"line\">    context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>)] = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">val</span> combiners = <span class=\"keyword\">new</span> <span class=\"type\">ExternalAppendOnlyMap</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>, <span class=\"type\">C</span>](identity, mergeCombiners, mergeCombiners)</span><br><span class=\"line\">  <span class=\"comment\">// 这里的 insertAll 和　shuffle write 中的 insertAll 效果一样，会排序生成临时文件</span></span><br><span class=\"line\">  combiners.insertAll(iter)</span><br><span class=\"line\">  updateMetrics(context, combiners)</span><br><span class=\"line\">  combiners.iterator</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>最终返回了 iterator:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">iterator</span></span>: <span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>)] = &#123;</span><br><span class=\"line\">   <span class=\"keyword\">if</span> (currentMap == <span class=\"literal\">null</span>) &#123;</span><br><span class=\"line\">     <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">IllegalStateException</span>(</span><br><span class=\"line\">       <span class=\"string\">\"ExternalAppendOnlyMap.iterator is destructive and should only be called once.\"</span>)</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">    <span class=\"comment\">//　如果没有生成临时文件</span></span><br><span class=\"line\">   <span class=\"keyword\">if</span> (spilledMaps.isEmpty) &#123;</span><br><span class=\"line\">     <span class=\"type\">CompletionIterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>), <span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>)]](</span><br><span class=\"line\">       destructiveIterator(currentMap.iterator), freeCurrentMap())</span><br><span class=\"line\">   &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">     <span class=\"keyword\">new</span> <span class=\"type\">ExternalIterator</span>()</span><br><span class=\"line\">   &#125;</span><br></pre></td></tr></table></figure>\n<p>如果有零时文件生成会返回一个 ExternalIterator :</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"keyword\">val</span> mergeHeap = <span class=\"keyword\">new</span> mutable.<span class=\"type\">PriorityQueue</span>[<span class=\"type\">StreamBuffer</span>]</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> sortedMap = <span class=\"type\">CompletionIterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>), <span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>)]](destructiveIterator(</span><br><span class=\"line\">      currentMap.destructiveSortedIterator(keyComparator)), freeCurrentMap())</span><br><span class=\"line\">    <span class=\"comment\">// 这里合并了临时文件与内存中的数据</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> inputStreams = (<span class=\"type\">Seq</span>(sortedMap) ++ spilledMaps).map(it =&gt; it.buffered)</span><br><span class=\"line\">    inputStreams.foreach &#123; it =&gt;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> kcPairs = <span class=\"keyword\">new</span> <span class=\"type\">ArrayBuffer</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>)]</span><br><span class=\"line\">      readNextHashCode(it, kcPairs)</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (kcPairs.length &gt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">        mergeHeap.enqueue(<span class=\"keyword\">new</span> <span class=\"type\">StreamBuffer</span>(it, kcPairs))</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>这里涉及到了一个很重要的类 ArrayBuffer:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">StreamBuffer</span>(<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val iterator: <span class=\"type\">BufferedIterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span></span>)],</span></span><br><span class=\"line\"><span class=\"class\">    <span class=\"title\">val</span> <span class=\"title\">pairs</span></span>: <span class=\"type\">ArrayBuffer</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>)])</span><br><span class=\"line\">  <span class=\"keyword\">extends</span> <span class=\"type\">Comparable</span>[<span class=\"type\">StreamBuffer</span>] &#123;</span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">isEmpty</span></span>: <span class=\"type\">Boolean</span> = pairs.length == <span class=\"number\">0</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">minKeyHash</span></span>: <span class=\"type\">Int</span> = &#123;</span><br><span class=\"line\">    assert(pairs.length &gt; <span class=\"number\">0</span>)</span><br><span class=\"line\">    hashKey(pairs.head)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compareTo</span></span>(other: <span class=\"type\">StreamBuffer</span>): <span class=\"type\">Int</span> = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (other.minKeyHash &lt; minKeyHash) <span class=\"number\">-1</span> <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (other.minKeyHash == minKeyHash) <span class=\"number\">0</span> <span class=\"keyword\">else</span> <span class=\"number\">1</span></span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>StreamBuffer 实际上维持了一个 iterator 与一个数组。并且重写了 compareTo 方法。是通过数据中第一个元素的 key 也就是minKeyHash来比较大小。而代码:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">readNextHashCode(it, kcPairs)</span><br><span class=\"line\">     <span class=\"keyword\">if</span> (kcPairs.length &gt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">       mergeHeap.enqueue(<span class=\"keyword\">new</span> <span class=\"type\">StreamBuffer</span>(it, kcPairs))</span><br><span class=\"line\">     &#125;</span><br></pre></td></tr></table></figure>\n<p>再看 next 方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">next</span></span>(): (<span class=\"type\">K</span>, <span class=\"type\">C</span>) = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (mergeHeap.isEmpty) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">NoSuchElementException</span></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"comment\">// Select a key from the StreamBuffer that holds the lowest key hash</span></span><br><span class=\"line\">  <span class=\"comment\">//　这里需要注意的是每一个 StreamBuffer 中的数组的元素是按照 key 经过排序的，mergeHeap 中的 StreamBuffer 也是按照 minKeyHash 进行排序的。也就是从 mergeHeap 每取出一个StreamBuffer，其对应的数组中 key 的 hash 一定是目前 mergeHeap 中所有数组中 key 最小的，如果能理解这一点，那这里的 merge 就基本可以理解了。</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> minBuffer = mergeHeap.dequeue()</span><br><span class=\"line\">  <span class=\"keyword\">val</span> minPairs = minBuffer.pairs</span><br><span class=\"line\">  <span class=\"keyword\">val</span> minHash = minBuffer.minKeyHash</span><br><span class=\"line\">  <span class=\"keyword\">val</span> minPair = removeFromBuffer(minPairs, <span class=\"number\">0</span>)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> minKey = minPair._1</span><br><span class=\"line\">  <span class=\"keyword\">var</span> minCombiner = minPair._2</span><br><span class=\"line\">  assert(hashKey(minPair) == minHash)</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">// For all other streams that may have this key (i.e. have the same minimum key hash),</span></span><br><span class=\"line\">  <span class=\"comment\">// merge in the corresponding value (if any) from that stream</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> mergedBuffers = <span class=\"type\">ArrayBuffer</span>[<span class=\"type\">StreamBuffer</span>](minBuffer)</span><br><span class=\"line\">  <span class=\"comment\">// 如果下一个 StreamBuffer 中的 minKeyHash 相同，则可能会含有相同的 key，则需要合并。这里由于是经过排序的，所以不用遍历所有的 StreamBuffer。只要下一个不同，则后面一定不会有与当前 minKeyHash 相同的的 StreamBuffer。</span></span><br><span class=\"line\">  <span class=\"keyword\">while</span> (mergeHeap.nonEmpty &amp;&amp; mergeHeap.head.minKeyHash == minHash) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> newBuffer = mergeHeap.dequeue()</span><br><span class=\"line\">    <span class=\"comment\">// 这里如果有相同的 key 则进行合并</span></span><br><span class=\"line\">    <span class=\"comment\">// 注意，hash 相同 key 不一定相同</span></span><br><span class=\"line\">    minCombiner = mergeIfKeyExists(minKey, minCombiner, newBuffer)</span><br><span class=\"line\">    mergedBuffers += newBuffer</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">// Repopulate each visited stream buffer and add it back to the queue if it is non-empty</span></span><br><span class=\"line\">  </span><br><span class=\"line\">  mergedBuffers.foreach &#123; buffer =&gt;</span><br><span class=\"line\">    <span class=\"comment\">// 如果 key 被合并完了，就需要读取下一批hash相同的key的数据到ArrayBuffer的数组中</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (buffer.isEmpty) &#123;</span><br><span class=\"line\">      readNextHashCode(buffer.iterator, buffer.pairs)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (!buffer.isEmpty) &#123;</span><br><span class=\"line\">      <span class=\"comment\">// 刚才dequeue的 ArrayBuffer的数组中可能有没合并完的数据 </span></span><br><span class=\"line\">      <span class=\"comment\">// 或者有新读取的数据则需要继续放入 mergeHeap 中进行合并 </span></span><br><span class=\"line\">      mergeHeap.enqueue(buffer)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"comment\">// 返回合并完的数据</span></span><br><span class=\"line\">  <span class=\"comment\">// 注意这里的 key 只是按照 hash 进行排序的，在 ExternalSorter 才是按照用户定义的排序方式进行排序</span></span><br><span class=\"line\">  (minKey, minCombiner)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>如果不需要排序，shuffle read 就算完成了。但是如果需要排序则:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> resultIter = dep.keyOrdering <span class=\"keyword\">match</span> &#123;</span><br><span class=\"line\">     <span class=\"keyword\">case</span> <span class=\"type\">Some</span>(keyOrd: <span class=\"type\">Ordering</span>[<span class=\"type\">K</span>]) =&gt;</span><br><span class=\"line\">       <span class=\"comment\">// Create an ExternalSorter to sort the data.</span></span><br><span class=\"line\">       <span class=\"keyword\">val</span> sorter =</span><br><span class=\"line\">         <span class=\"keyword\">new</span> <span class=\"type\">ExternalSorter</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>, <span class=\"type\">C</span>](context, ordering = <span class=\"type\">Some</span>(keyOrd), serializer = dep.serializer)</span><br><span class=\"line\">       sorter.insertAll(aggregatedIter)</span><br><span class=\"line\">       context.taskMetrics().incMemoryBytesSpilled(sorter.memoryBytesSpilled)</span><br><span class=\"line\">       context.taskMetrics().incDiskBytesSpilled(sorter.diskBytesSpilled)</span><br><span class=\"line\">       context.taskMetrics().incPeakExecutionMemory(sorter.peakMemoryUsedBytes)</span><br><span class=\"line\">       <span class=\"comment\">// Use completion callback to stop sorter if task was finished/cancelled.</span></span><br><span class=\"line\">       context.addTaskCompletionListener(_ =&gt; &#123;</span><br><span class=\"line\">         sorter.stop()</span><br><span class=\"line\">       &#125;)</span><br><span class=\"line\">       <span class=\"type\">CompletionIterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>], <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]]](sorter.iterator, sorter.stop())</span><br><span class=\"line\">     <span class=\"keyword\">case</span> <span class=\"type\">None</span> =&gt;</span><br><span class=\"line\">       aggregatedIter</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">   resultIter <span class=\"keyword\">match</span> &#123;</span><br><span class=\"line\">     <span class=\"keyword\">case</span> _: <span class=\"type\">InterruptibleIterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] =&gt; resultIter</span><br><span class=\"line\">     <span class=\"keyword\">case</span> _ =&gt;</span><br><span class=\"line\">       <span class=\"comment\">// Use another interruptible iterator here to support task cancellation as aggregator</span></span><br><span class=\"line\">       <span class=\"comment\">// or(and) sorter may have consumed previous interruptible iterator.</span></span><br><span class=\"line\">       <span class=\"keyword\">new</span> <span class=\"type\">InterruptibleIterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]](context, resultIter)</span><br><span class=\"line\">   &#125;</span><br></pre></td></tr></table></figure>\n<p>这里依然使用了 ExternalSorter 进行排序，而最终使用了 ExternalSorter 的 iterator 方法。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">iterator</span></span>: <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] = &#123;</span><br><span class=\"line\">  isShuffleSort = <span class=\"literal\">false</span></span><br><span class=\"line\">  partitionedIterator.flatMap(pair =&gt; pair._2)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>其中 partitionedIterator 方法在 shuffle write 部分已经讲的很清楚了。有兴趣的可以去看看。这里 shuffle read 就基本结束了。其中从其他 Executor 拉取数据的部分由于涉及很多，这里就基本没有怎么讲解。以后可能会专门开一篇文章。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>拖了这么久终于把 shuffle read 部分的源码看了一遍了。虽然 shuffle read 再数据合并部分的逻辑要比 shuffle 简单，但是由于这个过程中 executor 要到 master 拉取 shuffle write 结果信息，就涉及到 spark 的 block manager 的一些东西，因此完整的 shuffle read 过程依然是很复杂的。但是由于我还是太菜了，对于 block manage 这一部分看的一知半解，因此关于 executor 与 master 进行元数据交互的部分也就不会写得很详细，当然还是会涉及到一些。有关 block manage 的这部分以后应该会写到，先立个 flag 在这里吧。</p>\n<h2 id=\"MapStatus\"><a href=\"#MapStatus\" class=\"headerlink\" title=\"MapStatus\"></a>MapStatus</h2><p>在 shuffle write 完成之后会返回一个 MapStatus，MapStatus记录了 BlockManagerId 以及最终每个分区的大小。其中 BlockManagerId 包含了 BlockManager 所在的 host 以及 port 等信息。返回的 MapStatus 最终会被 Driver 端获并存储以便于 mapper 端获取。 这里要稍微提一下 BlockManager 的知识，Spark 利用 BlockManager 对数据进行读写，而 Block 就是其中的基本单位。每一个 Block 拥有一个 id。而通过BlockManager 则可以操作这些 Block 。因此 mapper 端只需要知道 BlockManager 的位置以及所需要的文件在哪些 Block 就能获取到对应的数据。这里通过 MapOutputTracker 通过 shuffleid 对一次 shuffle write 中所有的 mapper 产生的 MapStatus 进行了记录。</p>\n<p>在每一个 mapper 的 shuffle task 结束后，MapOutputTracker就会将其返回的 MapStatues 进行注册。在 DAGScheduler 中可以看到:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">case</span> smt: <span class=\"type\">ShuffleMapTask</span> =&gt;</span><br><span class=\"line\">             <span class=\"keyword\">val</span> shuffleStage = stage.asInstanceOf[<span class=\"type\">ShuffleMapStage</span>]</span><br><span class=\"line\">             <span class=\"keyword\">val</span> status = event.result.asInstanceOf[<span class=\"type\">MapStatus</span>]</span><br><span class=\"line\">             <span class=\"keyword\">val</span> execId = status.location.executorId</span><br><span class=\"line\">\t\t  </span><br><span class=\"line\">             mapOutputTracker.registerMapOutput(</span><br><span class=\"line\">               shuffleStage.shuffleDep.shuffleId, smt.partitionId, status)</span><br></pre></td></tr></table></figure>\n<p>这里的 MapOutputTracker 实际的类型为 MapOutputTrackerMaster 。继续看关于注册的代码:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">registerMapOutput</span></span>(shuffleId: <span class=\"type\">Int</span>, mapId: <span class=\"type\">Int</span>, status: <span class=\"type\">MapStatus</span>) &#123;</span><br><span class=\"line\">   shuffleStatuses(shuffleId).addMapOutput(mapId, status)</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>shuffleStatuses 实际上是一个 hashmap:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> shuffleStatuses = <span class=\"keyword\">new</span> <span class=\"type\">ConcurrentHashMap</span>[<span class=\"type\">Int</span>, <span class=\"type\">ShuffleStatus</span>]().asScala</span><br></pre></td></tr></table></figure>\n<p>ShuffleStatus 是一个类，里面包含了一个 MapStatus 数组。也就是说，通过在 shuffle write 之前注册的 shuffle 所用的 shuffleid 作为索引存储了 shuffle write 过程产生的 MapStatus。而 MapOutputTrackerMaster 是用于 Driver 端的。用于 Executor 的则是 MapOutputTrackerWorker 。在 MapOutputTrackerWorker 中一开始是没有 shuffleStatuses 的。需要从 MapOutputTrackerMaster 中获取。</p>\n<h2 id=\"shuffle-read\"><a href=\"#shuffle-read\" class=\"headerlink\" title=\"shuffle read\"></a>shuffle read</h2><p>shuffle read 代码开始于 Shuffled 中的 compute 方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span></span>(split: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>)] = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> dep = dependencies.head.asInstanceOf[<span class=\"type\">ShuffleDependency</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>, <span class=\"type\">C</span>]]</span><br><span class=\"line\">    <span class=\"type\">SparkEnv</span>.get.shuffleManager.getReader(dep.shuffleHandle, split.index, split.index + <span class=\"number\">1</span>, context)</span><br><span class=\"line\">      .read()</span><br><span class=\"line\">      .asInstanceOf[<span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>)]]</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>可以看到这里传入了需要计算的分区。代码中获取了一个 reader 并调用了其 read 方法。 getReader 代码如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getReader</span></span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>](</span><br><span class=\"line\">    handle: <span class=\"type\">ShuffleHandle</span>,</span><br><span class=\"line\">    startPartition: <span class=\"type\">Int</span>,</span><br><span class=\"line\">    endPartition: <span class=\"type\">Int</span>,</span><br><span class=\"line\">    context: <span class=\"type\">TaskContext</span>): <span class=\"type\">ShuffleReader</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>] = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">new</span> <span class=\"type\">BlockStoreShuffleReader</span>(</span><br><span class=\"line\">    handle.asInstanceOf[<span class=\"type\">BaseShuffleHandle</span>[<span class=\"type\">K</span>, _, <span class=\"type\">C</span>]], startPartition, endPartition, context)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>实际上是返回了一个 BlockStoreShuffleReader 。read 方法较长，准备一段一段的讲解:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">read</span></span>(): <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]]</span><br></pre></td></tr></table></figure>\n<p>方法返回了一个 Iterator。方法的开始定义了一个:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> wrappedStreams = <span class=\"keyword\">new</span> <span class=\"type\">ShuffleBlockFetcherIterator</span>(</span><br><span class=\"line\">     context,</span><br><span class=\"line\">     blockManager.shuffleClient,</span><br><span class=\"line\">     blockManager,</span><br><span class=\"line\">     <span class=\"comment\">// 获取 blockManagerId 与　blockId</span></span><br><span class=\"line\">     mapOutputTracker.getMapSizesByExecutorId(handle.shuffleId, startPartition, endPartition),</span><br><span class=\"line\">     serializerManager.wrapStream,</span><br><span class=\"line\">     <span class=\"comment\">// Note: we use getSizeAsMb when no suffix is provided for backwards compatibility</span></span><br><span class=\"line\">     <span class=\"type\">SparkEnv</span>.get.conf.getSizeAsMb(<span class=\"string\">\"spark.reducer.maxSizeInFlight\"</span>, <span class=\"string\">\"48m\"</span>) * <span class=\"number\">1024</span> * <span class=\"number\">1024</span>,</span><br><span class=\"line\">     <span class=\"type\">SparkEnv</span>.get.conf.getInt(<span class=\"string\">\"spark.reducer.maxReqsInFlight\"</span>, <span class=\"type\">Int</span>.<span class=\"type\">MaxValue</span>),</span><br><span class=\"line\">     <span class=\"type\">SparkEnv</span>.get.conf.get(config.<span class=\"type\">REDUCER_MAX_BLOCKS_IN_FLIGHT_PER_ADDRESS</span>),</span><br><span class=\"line\">     <span class=\"type\">SparkEnv</span>.get.conf.get(config.<span class=\"type\">MAX_REMOTE_BLOCK_SIZE_FETCH_TO_MEM</span>),</span><br><span class=\"line\">     <span class=\"type\">SparkEnv</span>.get.conf.getBoolean(<span class=\"string\">\"spark.shuffle.detectCorrupt\"</span>, <span class=\"literal\">true</span>))</span><br></pre></td></tr></table></figure>\n<p>这个 wrappedStreams 实际上获取数据的输入流。看看:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getMapSizesByExecutorId</span></span>(shuffleId: <span class=\"type\">Int</span>, startPartition: <span class=\"type\">Int</span>, endPartition: <span class=\"type\">Int</span>)</span><br><span class=\"line\">     : <span class=\"type\">Iterator</span>[(<span class=\"type\">BlockManagerId</span>, <span class=\"type\">Seq</span>[(<span class=\"type\">BlockId</span>, <span class=\"type\">Long</span>)])] = &#123;</span><br><span class=\"line\">   logDebug(<span class=\"string\">s\"Fetching outputs for shuffle <span class=\"subst\">$shuffleId</span>, partitions <span class=\"subst\">$startPartition</span>-<span class=\"subst\">$endPartition</span>\"</span>)</span><br><span class=\"line\">   <span class=\"comment\">// 通过本次的 shuffleId 先获取 mapper 端产生的 MapStatuses</span></span><br><span class=\"line\">   <span class=\"keyword\">val</span> statuses = getStatuses(shuffleId)</span><br><span class=\"line\">   <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">      <span class=\"comment\">// </span></span><br><span class=\"line\">     <span class=\"type\">MapOutputTracker</span>.convertMapStatuses(shuffleId, startPartition, endPartition, statuses)</span><br><span class=\"line\">   &#125; <span class=\"keyword\">catch</span> &#123;</span><br><span class=\"line\">     <span class=\"keyword\">case</span> e: <span class=\"type\">MetadataFetchFailedException</span> =&gt;</span><br><span class=\"line\">       <span class=\"comment\">// We experienced a fetch failure so our mapStatuses cache is outdated; clear it:</span></span><br><span class=\"line\">       mapStatuses.clear()</span><br><span class=\"line\">       <span class=\"keyword\">throw</span> e</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>随后调用了 convertMapStatuses:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">convertMapStatuses</span></span>(</span><br><span class=\"line\">      shuffleId: <span class=\"type\">Int</span>,</span><br><span class=\"line\">      startPartition: <span class=\"type\">Int</span>,</span><br><span class=\"line\">      endPartition: <span class=\"type\">Int</span>,</span><br><span class=\"line\">      statuses: <span class=\"type\">Array</span>[<span class=\"type\">MapStatus</span>]): <span class=\"type\">Iterator</span>[(<span class=\"type\">BlockManagerId</span>, <span class=\"type\">Seq</span>[(<span class=\"type\">BlockId</span>, <span class=\"type\">Long</span>)])] = &#123;</span><br><span class=\"line\">    assert (statuses != <span class=\"literal\">null</span>)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> splitsByAddress = <span class=\"keyword\">new</span> <span class=\"type\">HashMap</span>[<span class=\"type\">BlockManagerId</span>, <span class=\"type\">ListBuffer</span>[(<span class=\"type\">BlockId</span>, <span class=\"type\">Long</span>)]]</span><br><span class=\"line\">    <span class=\"keyword\">for</span> ((status, mapId) &lt;- statuses.iterator.zipWithIndex) &#123;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (status == <span class=\"literal\">null</span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">val</span> errorMessage = <span class=\"string\">s\"Missing an output location for shuffle <span class=\"subst\">$shuffleId</span>\"</span></span><br><span class=\"line\">        logError(errorMessage)</span><br><span class=\"line\">        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">MetadataFetchFailedException</span>(shuffleId, startPartition, errorMessage)</span><br><span class=\"line\">      &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (part &lt;- startPartition until endPartition) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">val</span> size = status.getSizeForBlock(part) <span class=\"comment\">// 这里只获取了需要处理的分区对应的数据</span></span><br><span class=\"line\">          <span class=\"keyword\">if</span> (size != <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">            <span class=\"comment\">// 这里获取到需要计算的分区数据所在的 block。其实就是由 shuffleId, mapId, part</span></span><br><span class=\"line\">            <span class=\"comment\">// 这三个组成的</span></span><br><span class=\"line\">            splitsByAddress.getOrElseUpdate(status.location, <span class=\"type\">ListBuffer</span>()) +=</span><br><span class=\"line\">                ((<span class=\"type\">ShuffleBlockId</span>(shuffleId, mapId, part), size))</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    splitsByAddress.iterator</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>这里涉及到 mapId 和 part 这两个变量。实际上，这两个都是分区的 Id 。只不过 mapId 是 mapper 端对应的分区Id，而 part 是经过 shuffle 之后 reducer 端对应的分区Id。通过<code>convertMapStatuses</code>就可以得到需要从哪些 Block 拉取数据。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 获取　block　对应的输入流</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> recordIter = wrappedStreams.flatMap &#123; <span class=\"keyword\">case</span> (blockId, wrappedStream) =&gt;</span><br><span class=\"line\">      <span class=\"comment\">// Note: the asKeyValueIterator below wraps a key/value iterator inside of a</span></span><br><span class=\"line\">      <span class=\"comment\">// NextIterator. The NextIterator makes sure that close() is called on the</span></span><br><span class=\"line\">      <span class=\"comment\">// underlying InputStream when all records have been read.</span></span><br><span class=\"line\">      serializerInstance.deserializeStream(wrappedStream).asKeyValueIterator</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// Update the context task metrics for each record read.</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> readMetrics = context.taskMetrics.createTempShuffleReadMetrics()</span><br><span class=\"line\">    <span class=\"keyword\">val</span> metricIter = <span class=\"type\">CompletionIterator</span>[(<span class=\"type\">Any</span>, <span class=\"type\">Any</span>), <span class=\"type\">Iterator</span>[(<span class=\"type\">Any</span>, <span class=\"type\">Any</span>)]](</span><br><span class=\"line\">      recordIter.map &#123; record =&gt;</span><br><span class=\"line\">        readMetrics.incRecordsRead(<span class=\"number\">1</span>)</span><br><span class=\"line\">        record</span><br><span class=\"line\">      &#125;,</span><br><span class=\"line\">      context.taskMetrics().mergeShuffleReadMetrics())</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// An interruptible iterator must be used here in order to support task cancellation</span></span><br><span class=\"line\">    <span class=\"comment\">//</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> interruptibleIter = <span class=\"keyword\">new</span> <span class=\"type\">InterruptibleIterator</span>[(<span class=\"type\">Any</span>, <span class=\"type\">Any</span>)](context, metricIter)</span><br><span class=\"line\">\t</span><br><span class=\"line\">    <span class=\"keyword\">val</span> aggregatedIter: <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] = <span class=\"keyword\">if</span> (dep.aggregator.isDefined) &#123;</span><br><span class=\"line\">      　<span class=\"comment\">// 如果需要聚合</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (dep.mapSideCombine) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// We are reading values that are already combined</span></span><br><span class=\"line\">        <span class=\"keyword\">val</span> combinedKeyValuesIterator = interruptibleIter.asInstanceOf[<span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>)]]</span><br><span class=\"line\">        dep.aggregator.get.combineCombinersByKey(combinedKeyValuesIterator, context)</span><br><span class=\"line\">      &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// We don't know the value type, but also don't care -- the dependency *should*</span></span><br><span class=\"line\">        <span class=\"comment\">// have made sure its compatible w/ this aggregator, which will convert the value</span></span><br><span class=\"line\">        <span class=\"comment\">// type to the combined type C</span></span><br><span class=\"line\">        <span class=\"keyword\">val</span> keyValuesIterator = interruptibleIter.asInstanceOf[<span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">Nothing</span>)]]</span><br><span class=\"line\">        dep.aggregator.get.combineValuesByKey(keyValuesIterator, context)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">      interruptibleIter.asInstanceOf[<span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]]]</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>这里进行聚合的代码实际上在 combineCombinersByKey 中:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">combineCombinersByKey</span></span>(</span><br><span class=\"line\">    iter: <span class=\"type\">Iterator</span>[_ &lt;: <span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]],</span><br><span class=\"line\">    context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>)] = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">val</span> combiners = <span class=\"keyword\">new</span> <span class=\"type\">ExternalAppendOnlyMap</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>, <span class=\"type\">C</span>](identity, mergeCombiners, mergeCombiners)</span><br><span class=\"line\">  <span class=\"comment\">// 这里的 insertAll 和　shuffle write 中的 insertAll 效果一样，会排序生成临时文件</span></span><br><span class=\"line\">  combiners.insertAll(iter)</span><br><span class=\"line\">  updateMetrics(context, combiners)</span><br><span class=\"line\">  combiners.iterator</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>最终返回了 iterator:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">iterator</span></span>: <span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>)] = &#123;</span><br><span class=\"line\">   <span class=\"keyword\">if</span> (currentMap == <span class=\"literal\">null</span>) &#123;</span><br><span class=\"line\">     <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">IllegalStateException</span>(</span><br><span class=\"line\">       <span class=\"string\">\"ExternalAppendOnlyMap.iterator is destructive and should only be called once.\"</span>)</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">    <span class=\"comment\">//　如果没有生成临时文件</span></span><br><span class=\"line\">   <span class=\"keyword\">if</span> (spilledMaps.isEmpty) &#123;</span><br><span class=\"line\">     <span class=\"type\">CompletionIterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>), <span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>)]](</span><br><span class=\"line\">       destructiveIterator(currentMap.iterator), freeCurrentMap())</span><br><span class=\"line\">   &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">     <span class=\"keyword\">new</span> <span class=\"type\">ExternalIterator</span>()</span><br><span class=\"line\">   &#125;</span><br></pre></td></tr></table></figure>\n<p>如果有零时文件生成会返回一个 ExternalIterator :</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"keyword\">val</span> mergeHeap = <span class=\"keyword\">new</span> mutable.<span class=\"type\">PriorityQueue</span>[<span class=\"type\">StreamBuffer</span>]</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> sortedMap = <span class=\"type\">CompletionIterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>), <span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>)]](destructiveIterator(</span><br><span class=\"line\">      currentMap.destructiveSortedIterator(keyComparator)), freeCurrentMap())</span><br><span class=\"line\">    <span class=\"comment\">// 这里合并了临时文件与内存中的数据</span></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">val</span> inputStreams = (<span class=\"type\">Seq</span>(sortedMap) ++ spilledMaps).map(it =&gt; it.buffered)</span><br><span class=\"line\">    inputStreams.foreach &#123; it =&gt;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> kcPairs = <span class=\"keyword\">new</span> <span class=\"type\">ArrayBuffer</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>)]</span><br><span class=\"line\">      readNextHashCode(it, kcPairs)</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (kcPairs.length &gt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">        mergeHeap.enqueue(<span class=\"keyword\">new</span> <span class=\"type\">StreamBuffer</span>(it, kcPairs))</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>这里涉及到了一个很重要的类 ArrayBuffer:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">StreamBuffer</span>(<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val iterator: <span class=\"type\">BufferedIterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span></span>)],</span></span><br><span class=\"line\"><span class=\"class\">    <span class=\"title\">val</span> <span class=\"title\">pairs</span></span>: <span class=\"type\">ArrayBuffer</span>[(<span class=\"type\">K</span>, <span class=\"type\">C</span>)])</span><br><span class=\"line\">  <span class=\"keyword\">extends</span> <span class=\"type\">Comparable</span>[<span class=\"type\">StreamBuffer</span>] &#123;</span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">isEmpty</span></span>: <span class=\"type\">Boolean</span> = pairs.length == <span class=\"number\">0</span></span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">minKeyHash</span></span>: <span class=\"type\">Int</span> = &#123;</span><br><span class=\"line\">    assert(pairs.length &gt; <span class=\"number\">0</span>)</span><br><span class=\"line\">    hashKey(pairs.head)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compareTo</span></span>(other: <span class=\"type\">StreamBuffer</span>): <span class=\"type\">Int</span> = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (other.minKeyHash &lt; minKeyHash) <span class=\"number\">-1</span> <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (other.minKeyHash == minKeyHash) <span class=\"number\">0</span> <span class=\"keyword\">else</span> <span class=\"number\">1</span></span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>StreamBuffer 实际上维持了一个 iterator 与一个数组。并且重写了 compareTo 方法。是通过数据中第一个元素的 key 也就是minKeyHash来比较大小。而代码:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">readNextHashCode(it, kcPairs)</span><br><span class=\"line\">     <span class=\"keyword\">if</span> (kcPairs.length &gt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">       mergeHeap.enqueue(<span class=\"keyword\">new</span> <span class=\"type\">StreamBuffer</span>(it, kcPairs))</span><br><span class=\"line\">     &#125;</span><br></pre></td></tr></table></figure>\n<p>再看 next 方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">next</span></span>(): (<span class=\"type\">K</span>, <span class=\"type\">C</span>) = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (mergeHeap.isEmpty) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">NoSuchElementException</span></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"comment\">// Select a key from the StreamBuffer that holds the lowest key hash</span></span><br><span class=\"line\">  <span class=\"comment\">//　这里需要注意的是每一个 StreamBuffer 中的数组的元素是按照 key 经过排序的，mergeHeap 中的 StreamBuffer 也是按照 minKeyHash 进行排序的。也就是从 mergeHeap 每取出一个StreamBuffer，其对应的数组中 key 的 hash 一定是目前 mergeHeap 中所有数组中 key 最小的，如果能理解这一点，那这里的 merge 就基本可以理解了。</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> minBuffer = mergeHeap.dequeue()</span><br><span class=\"line\">  <span class=\"keyword\">val</span> minPairs = minBuffer.pairs</span><br><span class=\"line\">  <span class=\"keyword\">val</span> minHash = minBuffer.minKeyHash</span><br><span class=\"line\">  <span class=\"keyword\">val</span> minPair = removeFromBuffer(minPairs, <span class=\"number\">0</span>)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> minKey = minPair._1</span><br><span class=\"line\">  <span class=\"keyword\">var</span> minCombiner = minPair._2</span><br><span class=\"line\">  assert(hashKey(minPair) == minHash)</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">// For all other streams that may have this key (i.e. have the same minimum key hash),</span></span><br><span class=\"line\">  <span class=\"comment\">// merge in the corresponding value (if any) from that stream</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> mergedBuffers = <span class=\"type\">ArrayBuffer</span>[<span class=\"type\">StreamBuffer</span>](minBuffer)</span><br><span class=\"line\">  <span class=\"comment\">// 如果下一个 StreamBuffer 中的 minKeyHash 相同，则可能会含有相同的 key，则需要合并。这里由于是经过排序的，所以不用遍历所有的 StreamBuffer。只要下一个不同，则后面一定不会有与当前 minKeyHash 相同的的 StreamBuffer。</span></span><br><span class=\"line\">  <span class=\"keyword\">while</span> (mergeHeap.nonEmpty &amp;&amp; mergeHeap.head.minKeyHash == minHash) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> newBuffer = mergeHeap.dequeue()</span><br><span class=\"line\">    <span class=\"comment\">// 这里如果有相同的 key 则进行合并</span></span><br><span class=\"line\">    <span class=\"comment\">// 注意，hash 相同 key 不一定相同</span></span><br><span class=\"line\">    minCombiner = mergeIfKeyExists(minKey, minCombiner, newBuffer)</span><br><span class=\"line\">    mergedBuffers += newBuffer</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"comment\">// Repopulate each visited stream buffer and add it back to the queue if it is non-empty</span></span><br><span class=\"line\">  </span><br><span class=\"line\">  mergedBuffers.foreach &#123; buffer =&gt;</span><br><span class=\"line\">    <span class=\"comment\">// 如果 key 被合并完了，就需要读取下一批hash相同的key的数据到ArrayBuffer的数组中</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (buffer.isEmpty) &#123;</span><br><span class=\"line\">      readNextHashCode(buffer.iterator, buffer.pairs)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (!buffer.isEmpty) &#123;</span><br><span class=\"line\">      <span class=\"comment\">// 刚才dequeue的 ArrayBuffer的数组中可能有没合并完的数据 </span></span><br><span class=\"line\">      <span class=\"comment\">// 或者有新读取的数据则需要继续放入 mergeHeap 中进行合并 </span></span><br><span class=\"line\">      mergeHeap.enqueue(buffer)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"comment\">// 返回合并完的数据</span></span><br><span class=\"line\">  <span class=\"comment\">// 注意这里的 key 只是按照 hash 进行排序的，在 ExternalSorter 才是按照用户定义的排序方式进行排序</span></span><br><span class=\"line\">  (minKey, minCombiner)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>如果不需要排序，shuffle read 就算完成了。但是如果需要排序则:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> resultIter = dep.keyOrdering <span class=\"keyword\">match</span> &#123;</span><br><span class=\"line\">     <span class=\"keyword\">case</span> <span class=\"type\">Some</span>(keyOrd: <span class=\"type\">Ordering</span>[<span class=\"type\">K</span>]) =&gt;</span><br><span class=\"line\">       <span class=\"comment\">// Create an ExternalSorter to sort the data.</span></span><br><span class=\"line\">       <span class=\"keyword\">val</span> sorter =</span><br><span class=\"line\">         <span class=\"keyword\">new</span> <span class=\"type\">ExternalSorter</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>, <span class=\"type\">C</span>](context, ordering = <span class=\"type\">Some</span>(keyOrd), serializer = dep.serializer)</span><br><span class=\"line\">       sorter.insertAll(aggregatedIter)</span><br><span class=\"line\">       context.taskMetrics().incMemoryBytesSpilled(sorter.memoryBytesSpilled)</span><br><span class=\"line\">       context.taskMetrics().incDiskBytesSpilled(sorter.diskBytesSpilled)</span><br><span class=\"line\">       context.taskMetrics().incPeakExecutionMemory(sorter.peakMemoryUsedBytes)</span><br><span class=\"line\">       <span class=\"comment\">// Use completion callback to stop sorter if task was finished/cancelled.</span></span><br><span class=\"line\">       context.addTaskCompletionListener(_ =&gt; &#123;</span><br><span class=\"line\">         sorter.stop()</span><br><span class=\"line\">       &#125;)</span><br><span class=\"line\">       <span class=\"type\">CompletionIterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>], <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]]](sorter.iterator, sorter.stop())</span><br><span class=\"line\">     <span class=\"keyword\">case</span> <span class=\"type\">None</span> =&gt;</span><br><span class=\"line\">       aggregatedIter</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">   resultIter <span class=\"keyword\">match</span> &#123;</span><br><span class=\"line\">     <span class=\"keyword\">case</span> _: <span class=\"type\">InterruptibleIterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] =&gt; resultIter</span><br><span class=\"line\">     <span class=\"keyword\">case</span> _ =&gt;</span><br><span class=\"line\">       <span class=\"comment\">// Use another interruptible iterator here to support task cancellation as aggregator</span></span><br><span class=\"line\">       <span class=\"comment\">// or(and) sorter may have consumed previous interruptible iterator.</span></span><br><span class=\"line\">       <span class=\"keyword\">new</span> <span class=\"type\">InterruptibleIterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]](context, resultIter)</span><br><span class=\"line\">   &#125;</span><br></pre></td></tr></table></figure>\n<p>这里依然使用了 ExternalSorter 进行排序，而最终使用了 ExternalSorter 的 iterator 方法。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">iterator</span></span>: <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] = &#123;</span><br><span class=\"line\">  isShuffleSort = <span class=\"literal\">false</span></span><br><span class=\"line\">  partitionedIterator.flatMap(pair =&gt; pair._2)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>其中 partitionedIterator 方法在 shuffle write 部分已经讲的很清楚了。有兴趣的可以去看看。这里 shuffle read 就基本结束了。其中从其他 Executor 拉取数据的部分由于涉及很多，这里就基本没有怎么讲解。以后可能会专门开一篇文章。</p>\n"},{"title":"数据挖掘基础-决策树","date":"2018-06-30T06:50:49.000Z","author":"lishion","toc":true,"_content":"\n决策树是一种简单但是应用广泛的分类器。最开始了解到决策树的时候认为它太简单了，就像代码里的 if 语句一样。事实上，决策树虽然简单，但是也是一种非常有效的分类方式。将决策树通过**袋装**的方式组合为随机深林更是在很多数据环境中拥有比 svm 等经典分类器更优异的性能。周志华老师还提出**孤立森利**和**深度深林**等基本单元为决策树等算法。本文主要对决策树的基本概念，决策树的生成剪枝算法进行简述。\n\n## 决策树的基本概念\n\n决策树一种分类算法，也可以用来进行回归。决策树应用树结构对输入结果进行判定，每一个内部节点代表对某一个属性进行测试，而每一条边作为一个测试结果，每一个叶子节点代表一中分类。通过不断的比较将数据分配到某一个叶子节点代表的类中。生成一个决策树有以下步骤:\n\n1. 特征选择: 选择一个特征作为当前节点的判断标准\n2. 生成子树: 利用选择的特征递归的分裂，直到数据不可分或达到停止条件\n3. 剪枝: 减掉一些子树避免过拟合\n\n以下将详细的介绍这三个步骤。\n\n## 特征选择\n\n### 划分方式\n\n特征选择的具体意义是选择一个特征，利用该特征将数据划分为 N 个子集。划分有**二元划分**和**多路划分两种方式**：\n\n离散特征的划分:\n\n{% asset_img discrete.png 离散属性划分 %}\n\n注意对于有顺序的离散属性，划分时不能打乱其顺序。例如对于产品质量有:不合格　合格　优良三种类型。作为二元划分可以为: {不合格} $\\cup$｛合格、优良｝但是不能为是{合格} $\\cup$ {不合格，优良}\n\n连续特征的换分:\n\n{% asset_img continue.png 连续属性划分 %}\n\n### 特征选择准则\n\n选择准则决定了在某个节点选择哪一个特征进行划分。一般有**信息增益**和**信息增益比**两种方式:\n\n**信息增益**\n\n数据的**纯度**可以用来表示数据含有的信息量大小，纯度越高，信息量越大。如果根据某特征划分后，子集的纯度减小，则说明通过该特征获得了未划分之前的信息。如果子集与原始数据的纯度差别越大，则划分的特征就是一个越优的特征。具体请参照[数据挖掘基础－熵](https://lishion.github.io/2018/06/27/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%9F%BA%E7%A1%80-%E7%86%B5/)。判断子节点不纯度的度量有**熵**和**基尼系数**。设特征值对样本数据进行了 N 个划分，其中每个划分为$D_1 D_2 ... D_N$。对每一个划分$D_m$中**熵**和**基尼系数**定义如下:\n$$\nEntropy(D_m) = -\\sum_{i=1}^{K}p(C_i)log(p(Ci))\n$$\n\n$$\nGini(D_m)=\\sum_{i=1}^{K}p(C_i)(1-p(C_i))\n$$\n\n其中 $p(C_i)$ 表示类 $C_i$ 属于划分 $D_m$ 的概率。即是:\n$$\np(C_i) = \\frac{|p(C_i)|}{|D_m|}\n$$\n$|.|$表示集合中元素的个数。\n\n对于下面个人情况和是否同意贷款的数据如下:\n\n|  ID  | 年龄 | 有工作 | 有自己的房子 | 信贷情况 | 类别 |\n| :--: | :--: | :----: | :----------: | :------: | :--: |\n|  1   | 青年 |   N    |      N       |   一般   |  N   |\n|  2   | 青年 |   N    |      N       |    好    |  N   |\n|  3   | 青年 |   Y    |      N       |    好    |  Y   |\n|  4   | 青年 |   Y    |      Y       |   一般   |  Y   |\n|  5   | 青年 |   N    |      N       |   一般   |  N   |\n|  6   | 中年 |   N    |      N       |   一般   |  N   |\n|  7   | 中年 |   N    |      N       |    好    |  N   |\n|  8   | 中年 |   Y    |      Y       |    好    |  Y   |\n|  9   | 中年 |   N    |      Y       |  非常好  |  Y   |\n|  10  | 中年 |   N    |      Y       |  非常好  |  Y   |\n|  11  | 老年 |   N    |      Y       |  非常好  |  Y   |\n|  12  | 老年 |   N    |      Y       |    好    |  Y   |\n|  13  | 老年 |   Y    |      N       |    好    |  Y   |\n|  14  | 老年 |   Y    |      N       |  非常好  |  Y   |\n|  15  | 老年 |   N    |      N       |   一般   |  N   |\n\n\n\n利用熵作为不纯度的度量，计算每一个特征划分后子节点的不纯度。特征**年龄**将数据进行三个划分，分别为:\n\n$D_1=\\{N N Y Y N \\} ,D2=\\{N N Y Y N \\},D3=\\{Y Y Y Y N\\} $\n\n对于每一个划分的熵为:\n\n$E(D_1)=-\\frac 2 5log_2(\\frac 2 5)-\\frac 3 5 log_2(\\frac 3 5)$\n\n$E(D_2)=-\\frac 2 5log_2(\\frac 2 5)-\\frac 3 5 log_2(\\frac 3 5)$\n\n$E(D_3)=-\\frac 1 5log_2(\\frac 1 5)-\\frac 4 5 log_2(\\frac 4 5)$\n\n利用每一个划分集合大小占总数据集合大小比例作为权值，最终的熵为:\n\n$E(D)=\\frac 1 3E(D_1)+\\frac 1 3 E(D_2)+\\frac 1 3 E(D_3)$\n\n 利用同样的方法，计算**有工作、有自己的房子、信贷情况**等特征的熵为:\n\n0.674 0.551 0.608\n\n未划分前数据熵相同，所以在当前节点应该选择特征**有自己的房子**作为划分的特征。\n\n**信息增益比**\n\n信息增益趋向于**选择取值较多的特征**。信息增益比则是使用**信息增益**/**划分后的熵**\n\n## 生成子树\n\n在选择了合适的特征进行换分之后，对通过划分得到的每一个集合继续递归的进行寻找特征，进行划分步骤，直到子节点的数据无法划分。\n\n例如，对于表中的数据，在根节点利用**有自己的房子**作为划分后，得到两个子集合编号为:\n\n$D_1=\\{4 ,8 ,9, 10, 11, 12\\} \\ D_2=\\{1,2,3,5,6,7,13,14,15\\}$\n\n对应的类别为:\n\n$D_1=\\{Y,Y,Y,Y,Y,Y\\} \\ D_2=\\{N,N,Y,N,N,N,Y,Y,N\\} $\n\n可以看出 $D_1$中的数据所有类别相同，无需再划分。对于 $D_2$ 中的数据，剩下**年龄、有工作、信贷情况**这几个特征，计算所有特征对应划分的不纯度，得到不纯度最高的特征为**有工作**。使用**有工作**对 $D_2$ 进行划分，得到:\n\n$D_3=\\{1,2,5,6,7,15\\} \\ D_4=\\{3,13,14\\}$\n\n对应的类别为:\n\n$D_3=\\{N,N,N,N,N,N\\} \\ D_4=\\{Y,Y,Y\\}$\n\n可以看出$D_3$和$D_4$中都有含有一种类别的数据，无需继续划分。因此最终学得的决策树为:\n\n{% asset_img tree.png 最终学得的决策树 %}\n\n一般来说，生成子树的停止条件有很多种。例如:\n\n1. 叶子节点中的数据都能被分为同一类\n2. 叶子节点中的不纯度小于一定的阈值\n3. 叶子节点包含样本的数据量小于一定的数量\n\n## 剪枝\n\n用上面方法学的决策树可以完全拟合训练数据，容易出现过拟合。将已经学得的决策树进行简化以避免过拟合的过程称之为剪枝。这里介绍一种简单的剪枝算法。\n\n对于决策树，可以通过极小化决策树整体的损失函数来进行实现。设树$T$的叶节点个数为$|T|$，$N$是其中的一个叶节点。该节点含有的样本数量为$|N|$，其中$k$类的样本数有$|N_k|$个。$E(N)$为节点$N$上的熵，$\\alpha \\ge 0 $表示模型复杂度对损失函数的影响。决策树模型的最终的损失函数可以表示为:\n$$\nC_a(T)=\\sum_{i=1}^{|T|}|N|E(N)+\\alpha|T|\n$$\n即表示为`每一个叶节点的熵 x 该节点的个数 + 模型的复杂度`。模型的复杂度用叶节点的个数表示。很明显，叶节点的个数越多，模型越复杂。而熵表示叶节点的纯度。熵越小，叶节点越不纯，拟合度就越高。因此损失函数有两部分组成，一部份表示模型对训练数据的拟合程度，一部分表示模型的复杂度。可以写为:\n$$\nC_a(T)=C(T)+\\alpha|T|\n$$\n模型越复杂，对训练数据拟合就越好，$C(T)$ 就越小，同时$\\alpha|T|$就越大。反正模型越简单，$\\alpha|T|$就越小，但是对模型的拟合度就低，$C(T)$也就越大。因此要想使得$C_a(T)$越小，就要在**模型拟合程度**和**模型复杂度**上取得一个权值。\n\n剪枝的一个示意图如下:\n\n{% asset_img cut.png 决策树剪枝 %}\n\n对整个决策树进行剪枝的步骤为:\n\n1. 计算$Ca(T)$\n2. 选择一个叶子节点，设减掉该节点得到的子树为$Ta$。若$Ca(T)>Ca(T_a)$，则需要减掉该叶子节点。\n3. 重复步骤1,2直到无法减掉任何一个叶子节点。\n\n## CART 回归与分类与分类树\n\n回归与分类树(classification and regression tree,CART)是一种应用广泛的的决策树模型。算法包括了决策树特征的选择，生成，剪枝操作。既可以用于回归，也可以用于分类。CART 决策树模型为**二叉树**，递归的二分每个特征。\n### 回归树\n\n回归树通过最小化平方误差来进行特征的选择以及划分点的选择。\n\n对于训练样本$(X,Y)$。$X=[x_1,x_2,...,x_n]$，其对应的输出$Y=[y_1,y_2,...,y_n]$。划分的原则是找到$x_i$第个j个特征$x^{(j)}$及其取值s。使得数据称为两个划分:\n$$\nD_1=x_i|x_i^{(j)}\\le s \\ \\ \\ \\ \\ D_2 =x_i|x_i^{(j)}>s\n$$\n划分的准则是使得根据该特征划分后的平方误差最小，即:\n$$\n\\sum_{i=1}^{N1}(y_{1i}-\\hat{y_1})^2 + \\sum_{i=1}^{N2} (y_{2i}-\\hat{y_2})\n$$\n其中 $N_j$表示第 j 个划分样本的个数。$y_{ji}$表示第 j 个划分中的第 i 个样本对应的输出。$\\hat{y_j}$表示第 j 个划分的期望输出，$j=1,2$。期望输出定义为:\n$$\n\\hat{y_j} =\\frac 1 N_j \\sum_{i=1}^{N_j} y_{ji}\n$$\n如我们想用回归树拟合函数$y=3a^2+2b^2$。样本为:\n\n|  id  |  a   |  b   |  c   |\n| :--: | :--: | :--: | :--: |\n|  1   |  0   | 1.5  | 4.5  |\n|  2   | 0.1  | 1.4  | 3.94 |\n|  3   | 0.2  | 1.3  | 3.5  |\n|  4   | 0.3  | 1.2  | 3.15 |\n|  5   | 0.4  | 1.1  | 2.9  |\n|  6   | 0.5  | 1.0  | 2.75 |\n|  7   | 0.6  | 0.9  | 2.7  |\n|  8   | 0.7  | 0.8  | 2.75 |\n|  9   | 0.8  | 0.7  | 2.9  |\n|  10  | 0.9  | 0.6  | 3.15 |\n\n\n\n由于这里的变量都是单调递增或者递减的，因此两个变量产生的划分点完全相同。不妨选择变量$a$，不同的划分点对应的平方误差:\n\n| 划分点(a) | 误差 |\n| :-------: | :--: |\n|     0     | 0.15 |\n|    0.1    | 0.14 |\n|    0.2    | 0.19 |\n|    0.3    | 0.27 |\n|    0.4    | 0.35 |\n|    0.5    | 0.40 |\n|    0.6    | 0.41 |\n|    0.7    | 0.39 |\n|    0.8    | 0.35 |\n|    0.9    | 0.31 |\n\n\n\n因此应该选择$a=0.1$作为划分点。划分后两个划分的 id 为:\n\n$D_1 = 1,2 \\ D2 = 3,4,5,6,7,8,9$\n\n不断的按照此种方法进行划分，直到满足停止条件。最后叶节点的输出就为该节点的期望输出。\n\n### 分类树\n\n分类树采用基尼系数作为特征选择的指标。具体的生成方式与普通的决策树类似。这里不再重复。\n\n### 剪枝\n\nCART 树的剪枝方式是从生成的完整的树的底端依次的减去一些子树，得到简化后的子树序列 $T_1,T_2,T_3,..,T_N​$。然后用交叉验证的方式检验其中的每一个子树，得到一个子树序列。\n\n对于某一个节点 $N$ ，进行剪枝后该节点变为变为叶子节点$N_t$。则以节点$Ｎ$为根节点的子树的损失函数:\n$$\nC_a(T) = C(T) + \\alpha(|T|)\n$$\n剪枝之后得到的以$N$为单节点的子树$T_t$的损失函数为:\n$$\nC_a(T_t)=C(T_t)+\\alpha\n$$\n当$\\alpha＝０$或足够小时，$C_a(Tt) > C_a(t)$。当$\\alpha$慢慢增大到某一个值时，此时有$C_a(Tt) = C_a(t)$。也就是说，只要:\n$$\n\\alpha = \\frac{C_a(T)-C_a(T_t)}{1-|T|}\n$$\n$T$ 和 $T_t$ 有相同的损失函数，而此时 $T_t$ 比 $T$ 更小的节点树。因此应该对$T$进行剪枝。\n\n因此 CART 树的剪枝方式为，对于完整树$T_0$的每一个内部节点，计算:\n$$\n\\alpha　＝　\\frac{C_a(T)-C_a(T_t)}{1-|T|}\n$$\n得到其中最小的节点，对该节点进行剪枝操作。对得到的子树重复这种剪枝操作，直到只剩下根节点。随后利用交叉验证的方式对子树序列进行测试，找到误差最小的子树。**这种剪枝方式的优点在于不断的增大**$\\alpha$**的值，即不断简化树的结构，得到最优的拟合误差与结构复杂度的均衡**。","source":"_posts/数据挖掘基础-决策树.md","raw":"---\ntitle: 数据挖掘基础-决策树\ndate: 2018-06-30 14:50:49\ntags:\n  - 数据挖掘\n  - 机器学习\ncategories: 数据挖掘基础\nauthor: lishion\ntoc: true\n---\n\n决策树是一种简单但是应用广泛的分类器。最开始了解到决策树的时候认为它太简单了，就像代码里的 if 语句一样。事实上，决策树虽然简单，但是也是一种非常有效的分类方式。将决策树通过**袋装**的方式组合为随机深林更是在很多数据环境中拥有比 svm 等经典分类器更优异的性能。周志华老师还提出**孤立森利**和**深度深林**等基本单元为决策树等算法。本文主要对决策树的基本概念，决策树的生成剪枝算法进行简述。\n\n## 决策树的基本概念\n\n决策树一种分类算法，也可以用来进行回归。决策树应用树结构对输入结果进行判定，每一个内部节点代表对某一个属性进行测试，而每一条边作为一个测试结果，每一个叶子节点代表一中分类。通过不断的比较将数据分配到某一个叶子节点代表的类中。生成一个决策树有以下步骤:\n\n1. 特征选择: 选择一个特征作为当前节点的判断标准\n2. 生成子树: 利用选择的特征递归的分裂，直到数据不可分或达到停止条件\n3. 剪枝: 减掉一些子树避免过拟合\n\n以下将详细的介绍这三个步骤。\n\n## 特征选择\n\n### 划分方式\n\n特征选择的具体意义是选择一个特征，利用该特征将数据划分为 N 个子集。划分有**二元划分**和**多路划分两种方式**：\n\n离散特征的划分:\n\n{% asset_img discrete.png 离散属性划分 %}\n\n注意对于有顺序的离散属性，划分时不能打乱其顺序。例如对于产品质量有:不合格　合格　优良三种类型。作为二元划分可以为: {不合格} $\\cup$｛合格、优良｝但是不能为是{合格} $\\cup$ {不合格，优良}\n\n连续特征的换分:\n\n{% asset_img continue.png 连续属性划分 %}\n\n### 特征选择准则\n\n选择准则决定了在某个节点选择哪一个特征进行划分。一般有**信息增益**和**信息增益比**两种方式:\n\n**信息增益**\n\n数据的**纯度**可以用来表示数据含有的信息量大小，纯度越高，信息量越大。如果根据某特征划分后，子集的纯度减小，则说明通过该特征获得了未划分之前的信息。如果子集与原始数据的纯度差别越大，则划分的特征就是一个越优的特征。具体请参照[数据挖掘基础－熵](https://lishion.github.io/2018/06/27/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%9F%BA%E7%A1%80-%E7%86%B5/)。判断子节点不纯度的度量有**熵**和**基尼系数**。设特征值对样本数据进行了 N 个划分，其中每个划分为$D_1 D_2 ... D_N$。对每一个划分$D_m$中**熵**和**基尼系数**定义如下:\n$$\nEntropy(D_m) = -\\sum_{i=1}^{K}p(C_i)log(p(Ci))\n$$\n\n$$\nGini(D_m)=\\sum_{i=1}^{K}p(C_i)(1-p(C_i))\n$$\n\n其中 $p(C_i)$ 表示类 $C_i$ 属于划分 $D_m$ 的概率。即是:\n$$\np(C_i) = \\frac{|p(C_i)|}{|D_m|}\n$$\n$|.|$表示集合中元素的个数。\n\n对于下面个人情况和是否同意贷款的数据如下:\n\n|  ID  | 年龄 | 有工作 | 有自己的房子 | 信贷情况 | 类别 |\n| :--: | :--: | :----: | :----------: | :------: | :--: |\n|  1   | 青年 |   N    |      N       |   一般   |  N   |\n|  2   | 青年 |   N    |      N       |    好    |  N   |\n|  3   | 青年 |   Y    |      N       |    好    |  Y   |\n|  4   | 青年 |   Y    |      Y       |   一般   |  Y   |\n|  5   | 青年 |   N    |      N       |   一般   |  N   |\n|  6   | 中年 |   N    |      N       |   一般   |  N   |\n|  7   | 中年 |   N    |      N       |    好    |  N   |\n|  8   | 中年 |   Y    |      Y       |    好    |  Y   |\n|  9   | 中年 |   N    |      Y       |  非常好  |  Y   |\n|  10  | 中年 |   N    |      Y       |  非常好  |  Y   |\n|  11  | 老年 |   N    |      Y       |  非常好  |  Y   |\n|  12  | 老年 |   N    |      Y       |    好    |  Y   |\n|  13  | 老年 |   Y    |      N       |    好    |  Y   |\n|  14  | 老年 |   Y    |      N       |  非常好  |  Y   |\n|  15  | 老年 |   N    |      N       |   一般   |  N   |\n\n\n\n利用熵作为不纯度的度量，计算每一个特征划分后子节点的不纯度。特征**年龄**将数据进行三个划分，分别为:\n\n$D_1=\\{N N Y Y N \\} ,D2=\\{N N Y Y N \\},D3=\\{Y Y Y Y N\\} $\n\n对于每一个划分的熵为:\n\n$E(D_1)=-\\frac 2 5log_2(\\frac 2 5)-\\frac 3 5 log_2(\\frac 3 5)$\n\n$E(D_2)=-\\frac 2 5log_2(\\frac 2 5)-\\frac 3 5 log_2(\\frac 3 5)$\n\n$E(D_3)=-\\frac 1 5log_2(\\frac 1 5)-\\frac 4 5 log_2(\\frac 4 5)$\n\n利用每一个划分集合大小占总数据集合大小比例作为权值，最终的熵为:\n\n$E(D)=\\frac 1 3E(D_1)+\\frac 1 3 E(D_2)+\\frac 1 3 E(D_3)$\n\n 利用同样的方法，计算**有工作、有自己的房子、信贷情况**等特征的熵为:\n\n0.674 0.551 0.608\n\n未划分前数据熵相同，所以在当前节点应该选择特征**有自己的房子**作为划分的特征。\n\n**信息增益比**\n\n信息增益趋向于**选择取值较多的特征**。信息增益比则是使用**信息增益**/**划分后的熵**\n\n## 生成子树\n\n在选择了合适的特征进行换分之后，对通过划分得到的每一个集合继续递归的进行寻找特征，进行划分步骤，直到子节点的数据无法划分。\n\n例如，对于表中的数据，在根节点利用**有自己的房子**作为划分后，得到两个子集合编号为:\n\n$D_1=\\{4 ,8 ,9, 10, 11, 12\\} \\ D_2=\\{1,2,3,5,6,7,13,14,15\\}$\n\n对应的类别为:\n\n$D_1=\\{Y,Y,Y,Y,Y,Y\\} \\ D_2=\\{N,N,Y,N,N,N,Y,Y,N\\} $\n\n可以看出 $D_1$中的数据所有类别相同，无需再划分。对于 $D_2$ 中的数据，剩下**年龄、有工作、信贷情况**这几个特征，计算所有特征对应划分的不纯度，得到不纯度最高的特征为**有工作**。使用**有工作**对 $D_2$ 进行划分，得到:\n\n$D_3=\\{1,2,5,6,7,15\\} \\ D_4=\\{3,13,14\\}$\n\n对应的类别为:\n\n$D_3=\\{N,N,N,N,N,N\\} \\ D_4=\\{Y,Y,Y\\}$\n\n可以看出$D_3$和$D_4$中都有含有一种类别的数据，无需继续划分。因此最终学得的决策树为:\n\n{% asset_img tree.png 最终学得的决策树 %}\n\n一般来说，生成子树的停止条件有很多种。例如:\n\n1. 叶子节点中的数据都能被分为同一类\n2. 叶子节点中的不纯度小于一定的阈值\n3. 叶子节点包含样本的数据量小于一定的数量\n\n## 剪枝\n\n用上面方法学的决策树可以完全拟合训练数据，容易出现过拟合。将已经学得的决策树进行简化以避免过拟合的过程称之为剪枝。这里介绍一种简单的剪枝算法。\n\n对于决策树，可以通过极小化决策树整体的损失函数来进行实现。设树$T$的叶节点个数为$|T|$，$N$是其中的一个叶节点。该节点含有的样本数量为$|N|$，其中$k$类的样本数有$|N_k|$个。$E(N)$为节点$N$上的熵，$\\alpha \\ge 0 $表示模型复杂度对损失函数的影响。决策树模型的最终的损失函数可以表示为:\n$$\nC_a(T)=\\sum_{i=1}^{|T|}|N|E(N)+\\alpha|T|\n$$\n即表示为`每一个叶节点的熵 x 该节点的个数 + 模型的复杂度`。模型的复杂度用叶节点的个数表示。很明显，叶节点的个数越多，模型越复杂。而熵表示叶节点的纯度。熵越小，叶节点越不纯，拟合度就越高。因此损失函数有两部分组成，一部份表示模型对训练数据的拟合程度，一部分表示模型的复杂度。可以写为:\n$$\nC_a(T)=C(T)+\\alpha|T|\n$$\n模型越复杂，对训练数据拟合就越好，$C(T)$ 就越小，同时$\\alpha|T|$就越大。反正模型越简单，$\\alpha|T|$就越小，但是对模型的拟合度就低，$C(T)$也就越大。因此要想使得$C_a(T)$越小，就要在**模型拟合程度**和**模型复杂度**上取得一个权值。\n\n剪枝的一个示意图如下:\n\n{% asset_img cut.png 决策树剪枝 %}\n\n对整个决策树进行剪枝的步骤为:\n\n1. 计算$Ca(T)$\n2. 选择一个叶子节点，设减掉该节点得到的子树为$Ta$。若$Ca(T)>Ca(T_a)$，则需要减掉该叶子节点。\n3. 重复步骤1,2直到无法减掉任何一个叶子节点。\n\n## CART 回归与分类与分类树\n\n回归与分类树(classification and regression tree,CART)是一种应用广泛的的决策树模型。算法包括了决策树特征的选择，生成，剪枝操作。既可以用于回归，也可以用于分类。CART 决策树模型为**二叉树**，递归的二分每个特征。\n### 回归树\n\n回归树通过最小化平方误差来进行特征的选择以及划分点的选择。\n\n对于训练样本$(X,Y)$。$X=[x_1,x_2,...,x_n]$，其对应的输出$Y=[y_1,y_2,...,y_n]$。划分的原则是找到$x_i$第个j个特征$x^{(j)}$及其取值s。使得数据称为两个划分:\n$$\nD_1=x_i|x_i^{(j)}\\le s \\ \\ \\ \\ \\ D_2 =x_i|x_i^{(j)}>s\n$$\n划分的准则是使得根据该特征划分后的平方误差最小，即:\n$$\n\\sum_{i=1}^{N1}(y_{1i}-\\hat{y_1})^2 + \\sum_{i=1}^{N2} (y_{2i}-\\hat{y_2})\n$$\n其中 $N_j$表示第 j 个划分样本的个数。$y_{ji}$表示第 j 个划分中的第 i 个样本对应的输出。$\\hat{y_j}$表示第 j 个划分的期望输出，$j=1,2$。期望输出定义为:\n$$\n\\hat{y_j} =\\frac 1 N_j \\sum_{i=1}^{N_j} y_{ji}\n$$\n如我们想用回归树拟合函数$y=3a^2+2b^2$。样本为:\n\n|  id  |  a   |  b   |  c   |\n| :--: | :--: | :--: | :--: |\n|  1   |  0   | 1.5  | 4.5  |\n|  2   | 0.1  | 1.4  | 3.94 |\n|  3   | 0.2  | 1.3  | 3.5  |\n|  4   | 0.3  | 1.2  | 3.15 |\n|  5   | 0.4  | 1.1  | 2.9  |\n|  6   | 0.5  | 1.0  | 2.75 |\n|  7   | 0.6  | 0.9  | 2.7  |\n|  8   | 0.7  | 0.8  | 2.75 |\n|  9   | 0.8  | 0.7  | 2.9  |\n|  10  | 0.9  | 0.6  | 3.15 |\n\n\n\n由于这里的变量都是单调递增或者递减的，因此两个变量产生的划分点完全相同。不妨选择变量$a$，不同的划分点对应的平方误差:\n\n| 划分点(a) | 误差 |\n| :-------: | :--: |\n|     0     | 0.15 |\n|    0.1    | 0.14 |\n|    0.2    | 0.19 |\n|    0.3    | 0.27 |\n|    0.4    | 0.35 |\n|    0.5    | 0.40 |\n|    0.6    | 0.41 |\n|    0.7    | 0.39 |\n|    0.8    | 0.35 |\n|    0.9    | 0.31 |\n\n\n\n因此应该选择$a=0.1$作为划分点。划分后两个划分的 id 为:\n\n$D_1 = 1,2 \\ D2 = 3,4,5,6,7,8,9$\n\n不断的按照此种方法进行划分，直到满足停止条件。最后叶节点的输出就为该节点的期望输出。\n\n### 分类树\n\n分类树采用基尼系数作为特征选择的指标。具体的生成方式与普通的决策树类似。这里不再重复。\n\n### 剪枝\n\nCART 树的剪枝方式是从生成的完整的树的底端依次的减去一些子树，得到简化后的子树序列 $T_1,T_2,T_3,..,T_N​$。然后用交叉验证的方式检验其中的每一个子树，得到一个子树序列。\n\n对于某一个节点 $N$ ，进行剪枝后该节点变为变为叶子节点$N_t$。则以节点$Ｎ$为根节点的子树的损失函数:\n$$\nC_a(T) = C(T) + \\alpha(|T|)\n$$\n剪枝之后得到的以$N$为单节点的子树$T_t$的损失函数为:\n$$\nC_a(T_t)=C(T_t)+\\alpha\n$$\n当$\\alpha＝０$或足够小时，$C_a(Tt) > C_a(t)$。当$\\alpha$慢慢增大到某一个值时，此时有$C_a(Tt) = C_a(t)$。也就是说，只要:\n$$\n\\alpha = \\frac{C_a(T)-C_a(T_t)}{1-|T|}\n$$\n$T$ 和 $T_t$ 有相同的损失函数，而此时 $T_t$ 比 $T$ 更小的节点树。因此应该对$T$进行剪枝。\n\n因此 CART 树的剪枝方式为，对于完整树$T_0$的每一个内部节点，计算:\n$$\n\\alpha　＝　\\frac{C_a(T)-C_a(T_t)}{1-|T|}\n$$\n得到其中最小的节点，对该节点进行剪枝操作。对得到的子树重复这种剪枝操作，直到只剩下根节点。随后利用交叉验证的方式对子树序列进行测试，找到误差最小的子树。**这种剪枝方式的优点在于不断的增大**$\\alpha$**的值，即不断简化树的结构，得到最优的拟合误差与结构复杂度的均衡**。","slug":"数据挖掘基础-决策树","published":1,"updated":"2018-07-03T01:53:43.735Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjjhx8v7c002tb8tgld0ujngr","content":"<p>决策树是一种简单但是应用广泛的分类器。最开始了解到决策树的时候认为它太简单了，就像代码里的 if 语句一样。事实上，决策树虽然简单，但是也是一种非常有效的分类方式。将决策树通过<strong>袋装</strong>的方式组合为随机深林更是在很多数据环境中拥有比 svm 等经典分类器更优异的性能。周志华老师还提出<strong>孤立森利</strong>和<strong>深度深林</strong>等基本单元为决策树等算法。本文主要对决策树的基本概念，决策树的生成剪枝算法进行简述。</p>\n<h2 id=\"决策树的基本概念\"><a href=\"#决策树的基本概念\" class=\"headerlink\" title=\"决策树的基本概念\"></a>决策树的基本概念</h2><p>决策树一种分类算法，也可以用来进行回归。决策树应用树结构对输入结果进行判定，每一个内部节点代表对某一个属性进行测试，而每一条边作为一个测试结果，每一个叶子节点代表一中分类。通过不断的比较将数据分配到某一个叶子节点代表的类中。生成一个决策树有以下步骤:</p>\n<ol>\n<li>特征选择: 选择一个特征作为当前节点的判断标准</li>\n<li>生成子树: 利用选择的特征递归的分裂，直到数据不可分或达到停止条件</li>\n<li>剪枝: 减掉一些子树避免过拟合</li>\n</ol>\n<p>以下将详细的介绍这三个步骤。</p>\n<h2 id=\"特征选择\"><a href=\"#特征选择\" class=\"headerlink\" title=\"特征选择\"></a>特征选择</h2><h3 id=\"划分方式\"><a href=\"#划分方式\" class=\"headerlink\" title=\"划分方式\"></a>划分方式</h3><p>特征选择的具体意义是选择一个特征，利用该特征将数据划分为 N 个子集。划分有<strong>二元划分</strong>和<strong>多路划分两种方式</strong>：</p>\n<p>离散特征的划分:</p>\n<img src=\"/2018/06/30/数据挖掘基础-决策树/discrete.png\" title=\"离散属性划分\">\n<p>注意对于有顺序的离散属性，划分时不能打乱其顺序。例如对于产品质量有:不合格　合格　优良三种类型。作为二元划分可以为: {不合格} $\\cup$｛合格、优良｝但是不能为是{合格} $\\cup$ {不合格，优良}</p>\n<p>连续特征的换分:</p>\n<img src=\"/2018/06/30/数据挖掘基础-决策树/continue.png\" title=\"连续属性划分\">\n<h3 id=\"特征选择准则\"><a href=\"#特征选择准则\" class=\"headerlink\" title=\"特征选择准则\"></a>特征选择准则</h3><p>选择准则决定了在某个节点选择哪一个特征进行划分。一般有<strong>信息增益</strong>和<strong>信息增益比</strong>两种方式:</p>\n<p><strong>信息增益</strong></p>\n<p>数据的<strong>纯度</strong>可以用来表示数据含有的信息量大小，纯度越高，信息量越大。如果根据某特征划分后，子集的纯度减小，则说明通过该特征获得了未划分之前的信息。如果子集与原始数据的纯度差别越大，则划分的特征就是一个越优的特征。具体请参照<a href=\"https://lishion.github.io/2018/06/27/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%9F%BA%E7%A1%80-%E7%86%B5/\" target=\"_blank\" rel=\"noopener\">数据挖掘基础－熵</a>。判断子节点不纯度的度量有<strong>熵</strong>和<strong>基尼系数</strong>。设特征值对样本数据进行了 N 个划分，其中每个划分为$D_1 D_2 … D_N$。对每一个划分$D_m$中<strong>熵</strong>和<strong>基尼系数</strong>定义如下:<br>$$<br>Entropy(D_m) = -\\sum_{i=1}^{K}p(C_i)log(p(Ci))<br>$$</p>\n<p>$$<br>Gini(D_m)=\\sum_{i=1}^{K}p(C_i)(1-p(C_i))<br>$$</p>\n<p>其中 $p(C_i)$ 表示类 $C_i$ 属于划分 $D_m$ 的概率。即是:<br>$$<br>p(C_i) = \\frac{|p(C_i)|}{|D_m|}<br>$$<br>$|.|$表示集合中元素的个数。</p>\n<p>对于下面个人情况和是否同意贷款的数据如下:</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">ID</th>\n<th style=\"text-align:center\">年龄</th>\n<th style=\"text-align:center\">有工作</th>\n<th style=\"text-align:center\">有自己的房子</th>\n<th style=\"text-align:center\">信贷情况</th>\n<th style=\"text-align:center\">类别</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">青年</td>\n<td style=\"text-align:center\">N</td>\n<td style=\"text-align:center\">N</td>\n<td style=\"text-align:center\">一般</td>\n<td style=\"text-align:center\">N</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">青年</td>\n<td style=\"text-align:center\">N</td>\n<td style=\"text-align:center\">N</td>\n<td style=\"text-align:center\">好</td>\n<td style=\"text-align:center\">N</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">青年</td>\n<td style=\"text-align:center\">Y</td>\n<td style=\"text-align:center\">N</td>\n<td style=\"text-align:center\">好</td>\n<td style=\"text-align:center\">Y</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">4</td>\n<td style=\"text-align:center\">青年</td>\n<td style=\"text-align:center\">Y</td>\n<td style=\"text-align:center\">Y</td>\n<td style=\"text-align:center\">一般</td>\n<td style=\"text-align:center\">Y</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">5</td>\n<td style=\"text-align:center\">青年</td>\n<td style=\"text-align:center\">N</td>\n<td style=\"text-align:center\">N</td>\n<td style=\"text-align:center\">一般</td>\n<td style=\"text-align:center\">N</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">6</td>\n<td style=\"text-align:center\">中年</td>\n<td style=\"text-align:center\">N</td>\n<td style=\"text-align:center\">N</td>\n<td style=\"text-align:center\">一般</td>\n<td style=\"text-align:center\">N</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">7</td>\n<td style=\"text-align:center\">中年</td>\n<td style=\"text-align:center\">N</td>\n<td style=\"text-align:center\">N</td>\n<td style=\"text-align:center\">好</td>\n<td style=\"text-align:center\">N</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">8</td>\n<td style=\"text-align:center\">中年</td>\n<td style=\"text-align:center\">Y</td>\n<td style=\"text-align:center\">Y</td>\n<td style=\"text-align:center\">好</td>\n<td style=\"text-align:center\">Y</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">9</td>\n<td style=\"text-align:center\">中年</td>\n<td style=\"text-align:center\">N</td>\n<td style=\"text-align:center\">Y</td>\n<td style=\"text-align:center\">非常好</td>\n<td style=\"text-align:center\">Y</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">10</td>\n<td style=\"text-align:center\">中年</td>\n<td style=\"text-align:center\">N</td>\n<td style=\"text-align:center\">Y</td>\n<td style=\"text-align:center\">非常好</td>\n<td style=\"text-align:center\">Y</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">11</td>\n<td style=\"text-align:center\">老年</td>\n<td style=\"text-align:center\">N</td>\n<td style=\"text-align:center\">Y</td>\n<td style=\"text-align:center\">非常好</td>\n<td style=\"text-align:center\">Y</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">12</td>\n<td style=\"text-align:center\">老年</td>\n<td style=\"text-align:center\">N</td>\n<td style=\"text-align:center\">Y</td>\n<td style=\"text-align:center\">好</td>\n<td style=\"text-align:center\">Y</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">13</td>\n<td style=\"text-align:center\">老年</td>\n<td style=\"text-align:center\">Y</td>\n<td style=\"text-align:center\">N</td>\n<td style=\"text-align:center\">好</td>\n<td style=\"text-align:center\">Y</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">14</td>\n<td style=\"text-align:center\">老年</td>\n<td style=\"text-align:center\">Y</td>\n<td style=\"text-align:center\">N</td>\n<td style=\"text-align:center\">非常好</td>\n<td style=\"text-align:center\">Y</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">15</td>\n<td style=\"text-align:center\">老年</td>\n<td style=\"text-align:center\">N</td>\n<td style=\"text-align:center\">N</td>\n<td style=\"text-align:center\">一般</td>\n<td style=\"text-align:center\">N</td>\n</tr>\n</tbody>\n</table>\n<p>利用熵作为不纯度的度量，计算每一个特征划分后子节点的不纯度。特征<strong>年龄</strong>将数据进行三个划分，分别为:</p>\n<p>$D_1={N N Y Y N } ,D2={N N Y Y N },D3={Y Y Y Y N} $</p>\n<p>对于每一个划分的熵为:</p>\n<p>$E(D_1)=-\\frac 2 5log_2(\\frac 2 5)-\\frac 3 5 log_2(\\frac 3 5)$</p>\n<p>$E(D_2)=-\\frac 2 5log_2(\\frac 2 5)-\\frac 3 5 log_2(\\frac 3 5)$</p>\n<p>$E(D_3)=-\\frac 1 5log_2(\\frac 1 5)-\\frac 4 5 log_2(\\frac 4 5)$</p>\n<p>利用每一个划分集合大小占总数据集合大小比例作为权值，最终的熵为:</p>\n<p>$E(D)=\\frac 1 3E(D_1)+\\frac 1 3 E(D_2)+\\frac 1 3 E(D_3)$</p>\n<p> 利用同样的方法，计算<strong>有工作、有自己的房子、信贷情况</strong>等特征的熵为:</p>\n<p>0.674 0.551 0.608</p>\n<p>未划分前数据熵相同，所以在当前节点应该选择特征<strong>有自己的房子</strong>作为划分的特征。</p>\n<p><strong>信息增益比</strong></p>\n<p>信息增益趋向于<strong>选择取值较多的特征</strong>。信息增益比则是使用<strong>信息增益</strong>/<strong>划分后的熵</strong></p>\n<h2 id=\"生成子树\"><a href=\"#生成子树\" class=\"headerlink\" title=\"生成子树\"></a>生成子树</h2><p>在选择了合适的特征进行换分之后，对通过划分得到的每一个集合继续递归的进行寻找特征，进行划分步骤，直到子节点的数据无法划分。</p>\n<p>例如，对于表中的数据，在根节点利用<strong>有自己的房子</strong>作为划分后，得到两个子集合编号为:</p>\n<p>$D_1={4 ,8 ,9, 10, 11, 12} \\ D_2={1,2,3,5,6,7,13,14,15}$</p>\n<p>对应的类别为:</p>\n<p>$D_1={Y,Y,Y,Y,Y,Y} \\ D_2={N,N,Y,N,N,N,Y,Y,N} $</p>\n<p>可以看出 $D_1$中的数据所有类别相同，无需再划分。对于 $D_2$ 中的数据，剩下<strong>年龄、有工作、信贷情况</strong>这几个特征，计算所有特征对应划分的不纯度，得到不纯度最高的特征为<strong>有工作</strong>。使用<strong>有工作</strong>对 $D_2$ 进行划分，得到:</p>\n<p>$D_3={1,2,5,6,7,15} \\ D_4={3,13,14}$</p>\n<p>对应的类别为:</p>\n<p>$D_3={N,N,N,N,N,N} \\ D_4={Y,Y,Y}$</p>\n<p>可以看出$D_3$和$D_4$中都有含有一种类别的数据，无需继续划分。因此最终学得的决策树为:</p>\n<img src=\"/2018/06/30/数据挖掘基础-决策树/tree.png\" title=\"最终学得的决策树\">\n<p>一般来说，生成子树的停止条件有很多种。例如:</p>\n<ol>\n<li>叶子节点中的数据都能被分为同一类</li>\n<li>叶子节点中的不纯度小于一定的阈值</li>\n<li>叶子节点包含样本的数据量小于一定的数量</li>\n</ol>\n<h2 id=\"剪枝\"><a href=\"#剪枝\" class=\"headerlink\" title=\"剪枝\"></a>剪枝</h2><p>用上面方法学的决策树可以完全拟合训练数据，容易出现过拟合。将已经学得的决策树进行简化以避免过拟合的过程称之为剪枝。这里介绍一种简单的剪枝算法。</p>\n<p>对于决策树，可以通过极小化决策树整体的损失函数来进行实现。设树$T$的叶节点个数为$|T|$，$N$是其中的一个叶节点。该节点含有的样本数量为$|N|$，其中$k$类的样本数有$|N_k|$个。$E(N)$为节点$N$上的熵，$\\alpha \\ge 0 $表示模型复杂度对损失函数的影响。决策树模型的最终的损失函数可以表示为:<br>$$<br>C_a(T)=\\sum_{i=1}^{|T|}|N|E(N)+\\alpha|T|<br>$$<br>即表示为<code>每一个叶节点的熵 x 该节点的个数 + 模型的复杂度</code>。模型的复杂度用叶节点的个数表示。很明显，叶节点的个数越多，模型越复杂。而熵表示叶节点的纯度。熵越小，叶节点越不纯，拟合度就越高。因此损失函数有两部分组成，一部份表示模型对训练数据的拟合程度，一部分表示模型的复杂度。可以写为:<br>$$<br>C_a(T)=C(T)+\\alpha|T|<br>$$<br>模型越复杂，对训练数据拟合就越好，$C(T)$ 就越小，同时$\\alpha|T|$就越大。反正模型越简单，$\\alpha|T|$就越小，但是对模型的拟合度就低，$C(T)$也就越大。因此要想使得$C_a(T)$越小，就要在<strong>模型拟合程度</strong>和<strong>模型复杂度</strong>上取得一个权值。</p>\n<p>剪枝的一个示意图如下:</p>\n<img src=\"/2018/06/30/数据挖掘基础-决策树/cut.png\" title=\"决策树剪枝\">\n<p>对整个决策树进行剪枝的步骤为:</p>\n<ol>\n<li>计算$Ca(T)$</li>\n<li>选择一个叶子节点，设减掉该节点得到的子树为$Ta$。若$Ca(T)&gt;Ca(T_a)$，则需要减掉该叶子节点。</li>\n<li>重复步骤1,2直到无法减掉任何一个叶子节点。</li>\n</ol>\n<h2 id=\"CART-回归与分类与分类树\"><a href=\"#CART-回归与分类与分类树\" class=\"headerlink\" title=\"CART 回归与分类与分类树\"></a>CART 回归与分类与分类树</h2><p>回归与分类树(classification and regression tree,CART)是一种应用广泛的的决策树模型。算法包括了决策树特征的选择，生成，剪枝操作。既可以用于回归，也可以用于分类。CART 决策树模型为<strong>二叉树</strong>，递归的二分每个特征。</p>\n<h3 id=\"回归树\"><a href=\"#回归树\" class=\"headerlink\" title=\"回归树\"></a>回归树</h3><p>回归树通过最小化平方误差来进行特征的选择以及划分点的选择。</p>\n<p>对于训练样本$(X,Y)$。$X=[x_1,x_2,…,x_n]$，其对应的输出$Y=[y_1,y_2,…,y_n]$。划分的原则是找到$x_i$第个j个特征$x^{(j)}$及其取值s。使得数据称为两个划分:<br>$$<br>D_1=x_i|x_i^{(j)}\\le s \\ \\ \\ \\ \\ D_2 =x_i|x_i^{(j)}&gt;s<br>$$<br>划分的准则是使得根据该特征划分后的平方误差最小，即:<br>$$<br>\\sum_{i=1}^{N1}(y_{1i}-\\hat{y_1})^2 + \\sum_{i=1}^{N2} (y_{2i}-\\hat{y_2})<br>$$<br>其中 $N_j$表示第 j 个划分样本的个数。$y_{ji}$表示第 j 个划分中的第 i 个样本对应的输出。$\\hat{y_j}$表示第 j 个划分的期望输出，$j=1,2$。期望输出定义为:<br>$$<br>\\hat{y_j} =\\frac 1 N_j \\sum_{i=1}^{N_j} y_{ji}<br>$$<br>如我们想用回归树拟合函数$y=3a^2+2b^2$。样本为:</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">id</th>\n<th style=\"text-align:center\">a</th>\n<th style=\"text-align:center\">b</th>\n<th style=\"text-align:center\">c</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">1.5</td>\n<td style=\"text-align:center\">4.5</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">0.1</td>\n<td style=\"text-align:center\">1.4</td>\n<td style=\"text-align:center\">3.94</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">0.2</td>\n<td style=\"text-align:center\">1.3</td>\n<td style=\"text-align:center\">3.5</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">4</td>\n<td style=\"text-align:center\">0.3</td>\n<td style=\"text-align:center\">1.2</td>\n<td style=\"text-align:center\">3.15</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">5</td>\n<td style=\"text-align:center\">0.4</td>\n<td style=\"text-align:center\">1.1</td>\n<td style=\"text-align:center\">2.9</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">6</td>\n<td style=\"text-align:center\">0.5</td>\n<td style=\"text-align:center\">1.0</td>\n<td style=\"text-align:center\">2.75</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">7</td>\n<td style=\"text-align:center\">0.6</td>\n<td style=\"text-align:center\">0.9</td>\n<td style=\"text-align:center\">2.7</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">8</td>\n<td style=\"text-align:center\">0.7</td>\n<td style=\"text-align:center\">0.8</td>\n<td style=\"text-align:center\">2.75</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">9</td>\n<td style=\"text-align:center\">0.8</td>\n<td style=\"text-align:center\">0.7</td>\n<td style=\"text-align:center\">2.9</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">10</td>\n<td style=\"text-align:center\">0.9</td>\n<td style=\"text-align:center\">0.6</td>\n<td style=\"text-align:center\">3.15</td>\n</tr>\n</tbody>\n</table>\n<p>由于这里的变量都是单调递增或者递减的，因此两个变量产生的划分点完全相同。不妨选择变量$a$，不同的划分点对应的平方误差:</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">划分点(a)</th>\n<th style=\"text-align:center\">误差</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">0.15</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">0.1</td>\n<td style=\"text-align:center\">0.14</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">0.2</td>\n<td style=\"text-align:center\">0.19</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">0.3</td>\n<td style=\"text-align:center\">0.27</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">0.4</td>\n<td style=\"text-align:center\">0.35</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">0.5</td>\n<td style=\"text-align:center\">0.40</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">0.6</td>\n<td style=\"text-align:center\">0.41</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">0.7</td>\n<td style=\"text-align:center\">0.39</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">0.8</td>\n<td style=\"text-align:center\">0.35</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">0.9</td>\n<td style=\"text-align:center\">0.31</td>\n</tr>\n</tbody>\n</table>\n<p>因此应该选择$a=0.1$作为划分点。划分后两个划分的 id 为:</p>\n<p>$D_1 = 1,2 \\ D2 = 3,4,5,6,7,8,9$</p>\n<p>不断的按照此种方法进行划分，直到满足停止条件。最后叶节点的输出就为该节点的期望输出。</p>\n<h3 id=\"分类树\"><a href=\"#分类树\" class=\"headerlink\" title=\"分类树\"></a>分类树</h3><p>分类树采用基尼系数作为特征选择的指标。具体的生成方式与普通的决策树类似。这里不再重复。</p>\n<h3 id=\"剪枝-1\"><a href=\"#剪枝-1\" class=\"headerlink\" title=\"剪枝\"></a>剪枝</h3><p>CART 树的剪枝方式是从生成的完整的树的底端依次的减去一些子树，得到简化后的子树序列 $T_1,T_2,T_3,..,T_N​$。然后用交叉验证的方式检验其中的每一个子树，得到一个子树序列。</p>\n<p>对于某一个节点 $N$ ，进行剪枝后该节点变为变为叶子节点$N_t$。则以节点$Ｎ$为根节点的子树的损失函数:<br>$$<br>C_a(T) = C(T) + \\alpha(|T|)<br>$$<br>剪枝之后得到的以$N$为单节点的子树$T_t$的损失函数为:<br>$$<br>C_a(T_t)=C(T_t)+\\alpha<br>$$<br>当$\\alpha＝０$或足够小时，$C_a(Tt) &gt; C_a(t)$。当$\\alpha$慢慢增大到某一个值时，此时有$C_a(Tt) = C_a(t)$。也就是说，只要:<br>$$<br>\\alpha = \\frac{C_a(T)-C_a(T_t)}{1-|T|}<br>$$<br>$T$ 和 $T_t$ 有相同的损失函数，而此时 $T_t$ 比 $T$ 更小的节点树。因此应该对$T$进行剪枝。</p>\n<p>因此 CART 树的剪枝方式为，对于完整树$T_0$的每一个内部节点，计算:<br>$$<br>\\alpha　＝　\\frac{C_a(T)-C_a(T_t)}{1-|T|}<br>$$<br>得到其中最小的节点，对该节点进行剪枝操作。对得到的子树重复这种剪枝操作，直到只剩下根节点。随后利用交叉验证的方式对子树序列进行测试，找到误差最小的子树。<strong>这种剪枝方式的优点在于不断的增大</strong>$\\alpha$<strong>的值，即不断简化树的结构，得到最优的拟合误差与结构复杂度的均衡</strong>。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>决策树是一种简单但是应用广泛的分类器。最开始了解到决策树的时候认为它太简单了，就像代码里的 if 语句一样。事实上，决策树虽然简单，但是也是一种非常有效的分类方式。将决策树通过<strong>袋装</strong>的方式组合为随机深林更是在很多数据环境中拥有比 svm 等经典分类器更优异的性能。周志华老师还提出<strong>孤立森利</strong>和<strong>深度深林</strong>等基本单元为决策树等算法。本文主要对决策树的基本概念，决策树的生成剪枝算法进行简述。</p>\n<h2 id=\"决策树的基本概念\"><a href=\"#决策树的基本概念\" class=\"headerlink\" title=\"决策树的基本概念\"></a>决策树的基本概念</h2><p>决策树一种分类算法，也可以用来进行回归。决策树应用树结构对输入结果进行判定，每一个内部节点代表对某一个属性进行测试，而每一条边作为一个测试结果，每一个叶子节点代表一中分类。通过不断的比较将数据分配到某一个叶子节点代表的类中。生成一个决策树有以下步骤:</p>\n<ol>\n<li>特征选择: 选择一个特征作为当前节点的判断标准</li>\n<li>生成子树: 利用选择的特征递归的分裂，直到数据不可分或达到停止条件</li>\n<li>剪枝: 减掉一些子树避免过拟合</li>\n</ol>\n<p>以下将详细的介绍这三个步骤。</p>\n<h2 id=\"特征选择\"><a href=\"#特征选择\" class=\"headerlink\" title=\"特征选择\"></a>特征选择</h2><h3 id=\"划分方式\"><a href=\"#划分方式\" class=\"headerlink\" title=\"划分方式\"></a>划分方式</h3><p>特征选择的具体意义是选择一个特征，利用该特征将数据划分为 N 个子集。划分有<strong>二元划分</strong>和<strong>多路划分两种方式</strong>：</p>\n<p>离散特征的划分:</p>\n<img src=\"/2018/06/30/数据挖掘基础-决策树/discrete.png\" title=\"离散属性划分\">\n<p>注意对于有顺序的离散属性，划分时不能打乱其顺序。例如对于产品质量有:不合格　合格　优良三种类型。作为二元划分可以为: {不合格} $\\cup$｛合格、优良｝但是不能为是{合格} $\\cup$ {不合格，优良}</p>\n<p>连续特征的换分:</p>\n<img src=\"/2018/06/30/数据挖掘基础-决策树/continue.png\" title=\"连续属性划分\">\n<h3 id=\"特征选择准则\"><a href=\"#特征选择准则\" class=\"headerlink\" title=\"特征选择准则\"></a>特征选择准则</h3><p>选择准则决定了在某个节点选择哪一个特征进行划分。一般有<strong>信息增益</strong>和<strong>信息增益比</strong>两种方式:</p>\n<p><strong>信息增益</strong></p>\n<p>数据的<strong>纯度</strong>可以用来表示数据含有的信息量大小，纯度越高，信息量越大。如果根据某特征划分后，子集的纯度减小，则说明通过该特征获得了未划分之前的信息。如果子集与原始数据的纯度差别越大，则划分的特征就是一个越优的特征。具体请参照<a href=\"https://lishion.github.io/2018/06/27/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%9F%BA%E7%A1%80-%E7%86%B5/\" target=\"_blank\" rel=\"noopener\">数据挖掘基础－熵</a>。判断子节点不纯度的度量有<strong>熵</strong>和<strong>基尼系数</strong>。设特征值对样本数据进行了 N 个划分，其中每个划分为$D_1 D_2 … D_N$。对每一个划分$D_m$中<strong>熵</strong>和<strong>基尼系数</strong>定义如下:<br>$$<br>Entropy(D_m) = -\\sum_{i=1}^{K}p(C_i)log(p(Ci))<br>$$</p>\n<p>$$<br>Gini(D_m)=\\sum_{i=1}^{K}p(C_i)(1-p(C_i))<br>$$</p>\n<p>其中 $p(C_i)$ 表示类 $C_i$ 属于划分 $D_m$ 的概率。即是:<br>$$<br>p(C_i) = \\frac{|p(C_i)|}{|D_m|}<br>$$<br>$|.|$表示集合中元素的个数。</p>\n<p>对于下面个人情况和是否同意贷款的数据如下:</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">ID</th>\n<th style=\"text-align:center\">年龄</th>\n<th style=\"text-align:center\">有工作</th>\n<th style=\"text-align:center\">有自己的房子</th>\n<th style=\"text-align:center\">信贷情况</th>\n<th style=\"text-align:center\">类别</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">青年</td>\n<td style=\"text-align:center\">N</td>\n<td style=\"text-align:center\">N</td>\n<td style=\"text-align:center\">一般</td>\n<td style=\"text-align:center\">N</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">青年</td>\n<td style=\"text-align:center\">N</td>\n<td style=\"text-align:center\">N</td>\n<td style=\"text-align:center\">好</td>\n<td style=\"text-align:center\">N</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">青年</td>\n<td style=\"text-align:center\">Y</td>\n<td style=\"text-align:center\">N</td>\n<td style=\"text-align:center\">好</td>\n<td style=\"text-align:center\">Y</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">4</td>\n<td style=\"text-align:center\">青年</td>\n<td style=\"text-align:center\">Y</td>\n<td style=\"text-align:center\">Y</td>\n<td style=\"text-align:center\">一般</td>\n<td style=\"text-align:center\">Y</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">5</td>\n<td style=\"text-align:center\">青年</td>\n<td style=\"text-align:center\">N</td>\n<td style=\"text-align:center\">N</td>\n<td style=\"text-align:center\">一般</td>\n<td style=\"text-align:center\">N</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">6</td>\n<td style=\"text-align:center\">中年</td>\n<td style=\"text-align:center\">N</td>\n<td style=\"text-align:center\">N</td>\n<td style=\"text-align:center\">一般</td>\n<td style=\"text-align:center\">N</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">7</td>\n<td style=\"text-align:center\">中年</td>\n<td style=\"text-align:center\">N</td>\n<td style=\"text-align:center\">N</td>\n<td style=\"text-align:center\">好</td>\n<td style=\"text-align:center\">N</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">8</td>\n<td style=\"text-align:center\">中年</td>\n<td style=\"text-align:center\">Y</td>\n<td style=\"text-align:center\">Y</td>\n<td style=\"text-align:center\">好</td>\n<td style=\"text-align:center\">Y</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">9</td>\n<td style=\"text-align:center\">中年</td>\n<td style=\"text-align:center\">N</td>\n<td style=\"text-align:center\">Y</td>\n<td style=\"text-align:center\">非常好</td>\n<td style=\"text-align:center\">Y</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">10</td>\n<td style=\"text-align:center\">中年</td>\n<td style=\"text-align:center\">N</td>\n<td style=\"text-align:center\">Y</td>\n<td style=\"text-align:center\">非常好</td>\n<td style=\"text-align:center\">Y</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">11</td>\n<td style=\"text-align:center\">老年</td>\n<td style=\"text-align:center\">N</td>\n<td style=\"text-align:center\">Y</td>\n<td style=\"text-align:center\">非常好</td>\n<td style=\"text-align:center\">Y</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">12</td>\n<td style=\"text-align:center\">老年</td>\n<td style=\"text-align:center\">N</td>\n<td style=\"text-align:center\">Y</td>\n<td style=\"text-align:center\">好</td>\n<td style=\"text-align:center\">Y</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">13</td>\n<td style=\"text-align:center\">老年</td>\n<td style=\"text-align:center\">Y</td>\n<td style=\"text-align:center\">N</td>\n<td style=\"text-align:center\">好</td>\n<td style=\"text-align:center\">Y</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">14</td>\n<td style=\"text-align:center\">老年</td>\n<td style=\"text-align:center\">Y</td>\n<td style=\"text-align:center\">N</td>\n<td style=\"text-align:center\">非常好</td>\n<td style=\"text-align:center\">Y</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">15</td>\n<td style=\"text-align:center\">老年</td>\n<td style=\"text-align:center\">N</td>\n<td style=\"text-align:center\">N</td>\n<td style=\"text-align:center\">一般</td>\n<td style=\"text-align:center\">N</td>\n</tr>\n</tbody>\n</table>\n<p>利用熵作为不纯度的度量，计算每一个特征划分后子节点的不纯度。特征<strong>年龄</strong>将数据进行三个划分，分别为:</p>\n<p>$D_1={N N Y Y N } ,D2={N N Y Y N },D3={Y Y Y Y N} $</p>\n<p>对于每一个划分的熵为:</p>\n<p>$E(D_1)=-\\frac 2 5log_2(\\frac 2 5)-\\frac 3 5 log_2(\\frac 3 5)$</p>\n<p>$E(D_2)=-\\frac 2 5log_2(\\frac 2 5)-\\frac 3 5 log_2(\\frac 3 5)$</p>\n<p>$E(D_3)=-\\frac 1 5log_2(\\frac 1 5)-\\frac 4 5 log_2(\\frac 4 5)$</p>\n<p>利用每一个划分集合大小占总数据集合大小比例作为权值，最终的熵为:</p>\n<p>$E(D)=\\frac 1 3E(D_1)+\\frac 1 3 E(D_2)+\\frac 1 3 E(D_3)$</p>\n<p> 利用同样的方法，计算<strong>有工作、有自己的房子、信贷情况</strong>等特征的熵为:</p>\n<p>0.674 0.551 0.608</p>\n<p>未划分前数据熵相同，所以在当前节点应该选择特征<strong>有自己的房子</strong>作为划分的特征。</p>\n<p><strong>信息增益比</strong></p>\n<p>信息增益趋向于<strong>选择取值较多的特征</strong>。信息增益比则是使用<strong>信息增益</strong>/<strong>划分后的熵</strong></p>\n<h2 id=\"生成子树\"><a href=\"#生成子树\" class=\"headerlink\" title=\"生成子树\"></a>生成子树</h2><p>在选择了合适的特征进行换分之后，对通过划分得到的每一个集合继续递归的进行寻找特征，进行划分步骤，直到子节点的数据无法划分。</p>\n<p>例如，对于表中的数据，在根节点利用<strong>有自己的房子</strong>作为划分后，得到两个子集合编号为:</p>\n<p>$D_1={4 ,8 ,9, 10, 11, 12} \\ D_2={1,2,3,5,6,7,13,14,15}$</p>\n<p>对应的类别为:</p>\n<p>$D_1={Y,Y,Y,Y,Y,Y} \\ D_2={N,N,Y,N,N,N,Y,Y,N} $</p>\n<p>可以看出 $D_1$中的数据所有类别相同，无需再划分。对于 $D_2$ 中的数据，剩下<strong>年龄、有工作、信贷情况</strong>这几个特征，计算所有特征对应划分的不纯度，得到不纯度最高的特征为<strong>有工作</strong>。使用<strong>有工作</strong>对 $D_2$ 进行划分，得到:</p>\n<p>$D_3={1,2,5,6,7,15} \\ D_4={3,13,14}$</p>\n<p>对应的类别为:</p>\n<p>$D_3={N,N,N,N,N,N} \\ D_4={Y,Y,Y}$</p>\n<p>可以看出$D_3$和$D_4$中都有含有一种类别的数据，无需继续划分。因此最终学得的决策树为:</p>\n<img src=\"/2018/06/30/数据挖掘基础-决策树/tree.png\" title=\"最终学得的决策树\">\n<p>一般来说，生成子树的停止条件有很多种。例如:</p>\n<ol>\n<li>叶子节点中的数据都能被分为同一类</li>\n<li>叶子节点中的不纯度小于一定的阈值</li>\n<li>叶子节点包含样本的数据量小于一定的数量</li>\n</ol>\n<h2 id=\"剪枝\"><a href=\"#剪枝\" class=\"headerlink\" title=\"剪枝\"></a>剪枝</h2><p>用上面方法学的决策树可以完全拟合训练数据，容易出现过拟合。将已经学得的决策树进行简化以避免过拟合的过程称之为剪枝。这里介绍一种简单的剪枝算法。</p>\n<p>对于决策树，可以通过极小化决策树整体的损失函数来进行实现。设树$T$的叶节点个数为$|T|$，$N$是其中的一个叶节点。该节点含有的样本数量为$|N|$，其中$k$类的样本数有$|N_k|$个。$E(N)$为节点$N$上的熵，$\\alpha \\ge 0 $表示模型复杂度对损失函数的影响。决策树模型的最终的损失函数可以表示为:<br>$$<br>C_a(T)=\\sum_{i=1}^{|T|}|N|E(N)+\\alpha|T|<br>$$<br>即表示为<code>每一个叶节点的熵 x 该节点的个数 + 模型的复杂度</code>。模型的复杂度用叶节点的个数表示。很明显，叶节点的个数越多，模型越复杂。而熵表示叶节点的纯度。熵越小，叶节点越不纯，拟合度就越高。因此损失函数有两部分组成，一部份表示模型对训练数据的拟合程度，一部分表示模型的复杂度。可以写为:<br>$$<br>C_a(T)=C(T)+\\alpha|T|<br>$$<br>模型越复杂，对训练数据拟合就越好，$C(T)$ 就越小，同时$\\alpha|T|$就越大。反正模型越简单，$\\alpha|T|$就越小，但是对模型的拟合度就低，$C(T)$也就越大。因此要想使得$C_a(T)$越小，就要在<strong>模型拟合程度</strong>和<strong>模型复杂度</strong>上取得一个权值。</p>\n<p>剪枝的一个示意图如下:</p>\n<img src=\"/2018/06/30/数据挖掘基础-决策树/cut.png\" title=\"决策树剪枝\">\n<p>对整个决策树进行剪枝的步骤为:</p>\n<ol>\n<li>计算$Ca(T)$</li>\n<li>选择一个叶子节点，设减掉该节点得到的子树为$Ta$。若$Ca(T)&gt;Ca(T_a)$，则需要减掉该叶子节点。</li>\n<li>重复步骤1,2直到无法减掉任何一个叶子节点。</li>\n</ol>\n<h2 id=\"CART-回归与分类与分类树\"><a href=\"#CART-回归与分类与分类树\" class=\"headerlink\" title=\"CART 回归与分类与分类树\"></a>CART 回归与分类与分类树</h2><p>回归与分类树(classification and regression tree,CART)是一种应用广泛的的决策树模型。算法包括了决策树特征的选择，生成，剪枝操作。既可以用于回归，也可以用于分类。CART 决策树模型为<strong>二叉树</strong>，递归的二分每个特征。</p>\n<h3 id=\"回归树\"><a href=\"#回归树\" class=\"headerlink\" title=\"回归树\"></a>回归树</h3><p>回归树通过最小化平方误差来进行特征的选择以及划分点的选择。</p>\n<p>对于训练样本$(X,Y)$。$X=[x_1,x_2,…,x_n]$，其对应的输出$Y=[y_1,y_2,…,y_n]$。划分的原则是找到$x_i$第个j个特征$x^{(j)}$及其取值s。使得数据称为两个划分:<br>$$<br>D_1=x_i|x_i^{(j)}\\le s \\ \\ \\ \\ \\ D_2 =x_i|x_i^{(j)}&gt;s<br>$$<br>划分的准则是使得根据该特征划分后的平方误差最小，即:<br>$$<br>\\sum_{i=1}^{N1}(y_{1i}-\\hat{y_1})^2 + \\sum_{i=1}^{N2} (y_{2i}-\\hat{y_2})<br>$$<br>其中 $N_j$表示第 j 个划分样本的个数。$y_{ji}$表示第 j 个划分中的第 i 个样本对应的输出。$\\hat{y_j}$表示第 j 个划分的期望输出，$j=1,2$。期望输出定义为:<br>$$<br>\\hat{y_j} =\\frac 1 N_j \\sum_{i=1}^{N_j} y_{ji}<br>$$<br>如我们想用回归树拟合函数$y=3a^2+2b^2$。样本为:</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">id</th>\n<th style=\"text-align:center\">a</th>\n<th style=\"text-align:center\">b</th>\n<th style=\"text-align:center\">c</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">1.5</td>\n<td style=\"text-align:center\">4.5</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">0.1</td>\n<td style=\"text-align:center\">1.4</td>\n<td style=\"text-align:center\">3.94</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\">0.2</td>\n<td style=\"text-align:center\">1.3</td>\n<td style=\"text-align:center\">3.5</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">4</td>\n<td style=\"text-align:center\">0.3</td>\n<td style=\"text-align:center\">1.2</td>\n<td style=\"text-align:center\">3.15</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">5</td>\n<td style=\"text-align:center\">0.4</td>\n<td style=\"text-align:center\">1.1</td>\n<td style=\"text-align:center\">2.9</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">6</td>\n<td style=\"text-align:center\">0.5</td>\n<td style=\"text-align:center\">1.0</td>\n<td style=\"text-align:center\">2.75</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">7</td>\n<td style=\"text-align:center\">0.6</td>\n<td style=\"text-align:center\">0.9</td>\n<td style=\"text-align:center\">2.7</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">8</td>\n<td style=\"text-align:center\">0.7</td>\n<td style=\"text-align:center\">0.8</td>\n<td style=\"text-align:center\">2.75</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">9</td>\n<td style=\"text-align:center\">0.8</td>\n<td style=\"text-align:center\">0.7</td>\n<td style=\"text-align:center\">2.9</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">10</td>\n<td style=\"text-align:center\">0.9</td>\n<td style=\"text-align:center\">0.6</td>\n<td style=\"text-align:center\">3.15</td>\n</tr>\n</tbody>\n</table>\n<p>由于这里的变量都是单调递增或者递减的，因此两个变量产生的划分点完全相同。不妨选择变量$a$，不同的划分点对应的平方误差:</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">划分点(a)</th>\n<th style=\"text-align:center\">误差</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">0</td>\n<td style=\"text-align:center\">0.15</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">0.1</td>\n<td style=\"text-align:center\">0.14</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">0.2</td>\n<td style=\"text-align:center\">0.19</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">0.3</td>\n<td style=\"text-align:center\">0.27</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">0.4</td>\n<td style=\"text-align:center\">0.35</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">0.5</td>\n<td style=\"text-align:center\">0.40</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">0.6</td>\n<td style=\"text-align:center\">0.41</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">0.7</td>\n<td style=\"text-align:center\">0.39</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">0.8</td>\n<td style=\"text-align:center\">0.35</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">0.9</td>\n<td style=\"text-align:center\">0.31</td>\n</tr>\n</tbody>\n</table>\n<p>因此应该选择$a=0.1$作为划分点。划分后两个划分的 id 为:</p>\n<p>$D_1 = 1,2 \\ D2 = 3,4,5,6,7,8,9$</p>\n<p>不断的按照此种方法进行划分，直到满足停止条件。最后叶节点的输出就为该节点的期望输出。</p>\n<h3 id=\"分类树\"><a href=\"#分类树\" class=\"headerlink\" title=\"分类树\"></a>分类树</h3><p>分类树采用基尼系数作为特征选择的指标。具体的生成方式与普通的决策树类似。这里不再重复。</p>\n<h3 id=\"剪枝-1\"><a href=\"#剪枝-1\" class=\"headerlink\" title=\"剪枝\"></a>剪枝</h3><p>CART 树的剪枝方式是从生成的完整的树的底端依次的减去一些子树，得到简化后的子树序列 $T_1,T_2,T_3,..,T_N​$。然后用交叉验证的方式检验其中的每一个子树，得到一个子树序列。</p>\n<p>对于某一个节点 $N$ ，进行剪枝后该节点变为变为叶子节点$N_t$。则以节点$Ｎ$为根节点的子树的损失函数:<br>$$<br>C_a(T) = C(T) + \\alpha(|T|)<br>$$<br>剪枝之后得到的以$N$为单节点的子树$T_t$的损失函数为:<br>$$<br>C_a(T_t)=C(T_t)+\\alpha<br>$$<br>当$\\alpha＝０$或足够小时，$C_a(Tt) &gt; C_a(t)$。当$\\alpha$慢慢增大到某一个值时，此时有$C_a(Tt) = C_a(t)$。也就是说，只要:<br>$$<br>\\alpha = \\frac{C_a(T)-C_a(T_t)}{1-|T|}<br>$$<br>$T$ 和 $T_t$ 有相同的损失函数，而此时 $T_t$ 比 $T$ 更小的节点树。因此应该对$T$进行剪枝。</p>\n<p>因此 CART 树的剪枝方式为，对于完整树$T_0$的每一个内部节点，计算:<br>$$<br>\\alpha　＝　\\frac{C_a(T)-C_a(T_t)}{1-|T|}<br>$$<br>得到其中最小的节点，对该节点进行剪枝操作。对得到的子树重复这种剪枝操作，直到只剩下根节点。随后利用交叉验证的方式对子树序列进行测试，找到误差最小的子树。<strong>这种剪枝方式的优点在于不断的增大</strong>$\\alpha$<strong>的值，即不断简化树的结构，得到最优的拟合误差与结构复杂度的均衡</strong>。</p>\n"},{"title":"Spark 源码阅读计划 - 第一部分 - 迭代计算","date":"2018-06-06T05:46:45.000Z","author":"lishion","toc":true,"_content":"首先立一个flag，这将是一个长期更新的版块。\n\n## 写在最开始\n\n在我使用`spark`进行日志分析的时候感受到了`spark`的便捷与强大。在学习`spark`初期，我阅读了许多与`spark`相关的文档，在这个过程中了解了`RDD`，`分区`，`shuffle`等概念，但是我并没有对这些概念有更多具体的认识。由于不了解`spark`的基本运作方式使得我在编写代码的时候十分困扰。由于这个原因，我准备阅读`spark`的源代码来解决我对基本概念的认识。\n\n## 本部分主要内容\n\n虽然该章节的名字叫做**迭代计算**，但是本章会讨论包括**RDD、分区、Job、迭代计算**等相关的内容，其中会重点讨论分区与迭代计算。因此在阅读本章之前你至少需要对这些提到的概念有一定的了解。\n\n## 准备工作\n\n你需要准备一份 Spark 的源代码以及一个便于全局搜索的编辑器|IDE。\n\n## 一个简单的例子\n\n假设现在我们有这样一个需求 : 寻找从 0 to 100 的偶数并输出其总数。利用 spark 编写代码如下:\n\n```scala\nsc.parallelize(0 to 100).filter(_%2 == 0).count\n//res0: Long = 51\n```\n\n这是一个非常简单的例子，但是里面却蕴涵了许多基础的知识。这一章的内容也是从这个例子展开的。\n\n## 迭代计算\n\n### comput 方法与 itertor \n\n在RDD.scala 定义的类 RDD中，有两个实现迭代计算的核心方法，分别是`compute`以及`itertor`方法。其中`itertor`方法如下:\n\n```scala\n  final def iterator(split: Partition, context: TaskContext): Iterator[T] = {\n    if (storageLevel != StorageLevel.NONE) {//如果有缓存\n      getOrCompute(split, context)\n    } else {\n      computeOrReadCheckpoint(split, context)\n    }\n  }\n```\n\n我们只关心这个方法的第一个参数**分区**，以及返回的迭代器。忽略有缓存的情况，我们继续看`computeOrReadCheckpoint`这个方法:\n\n```scala\n  private[spark] def computeOrReadCheckpoint(split: Partition, context: TaskContext): Iterator[T] =\n  {\n    if (isCheckpointedAndMaterialized) {//如果有checkpoint\n      firstParent[T].iterator(split, context)\n    } else {\n      compute(split, context)\n    }\n  }\n```\n\n可以看到，在没有缓存和 checkpoint 的情况下，itertor 方法直接调用了 compute 方法。而compute方法定义如下:\n\n```scala\n /**\n   * Implemented by subclasses to compute a given partition.\n   */\n  def compute(split: Partition, context: TaskContext): Iterator[T]\n```\n\n从注释可以看出，compute 方法应该由子类实现，用以计算给定分区并生成一个迭代器。如果不考虑缓存和 checkpoint 的情况下，简化一下代码:\n\n```scala\n final def iterator(split: Partition, context: TaskContext): Iterator[T] = {\n    compute(split,context)\n  }\n```\n\n那么实际上 iterator 的功能是: **接受一个分区，对这个分区进行计算，并返回计算结果的迭代器**。而之所以 compute 需要子类来实现，是因为只需要在子类中实现不同的 compute 方法，就能产生不同类型的 RDD。既然不同的RDD 有不同的功能，我们想知道之前的例子是如何进行迭代计算的，就需要知道这个例子中涉及到了哪些 RDD。\n\n首先查看`SparkContext`中与`parallelize`相关的部分代码:\n\n```scala\n  def parallelize[T: ClassTag](\n      seq: Seq[T],\n      numSlices: Int = defaultParallelism): RDD[T] = withScope {\n    assertNotStopped()\n    new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())\n  }\n```\n\n可以看到，`parallelize`实际返回了一个`ParallelCollectionRDD`。在`ParallelCollectionRDD`中并没有对`filter`方法进行重写，因此我们查看`RDD`中的`filter`方法:\n\n```scala\n def filter(f: T => Boolean): RDD[T] = withScope {\n    val cleanF = sc.clean(f)\n    new MapPartitionsRDD[T, T](\n      this,\n      (context, pid, iter) => iter.filter(cleanF),\n      preservesPartitioning = true)\n  }\n```\n\nfilter 方法返回了`MapPartitionsRDD`。为了方便起见，我将 ParallelCollectionRDD 类型的 RDD 称为 a，MapPartitionsRDD 类型的 RDD 成为B。那么先看一下 b 的 compute 方法:\n\n```scala\nprivate[spark] class MapPartitionsRDD[U: ClassTag, T: ClassTag](\n    var prev: RDD[T],\n    f: (TaskContext, Int, Iterator[T]) => Iterator[U],  // (TaskContext, partition index, iterator)\n    preservesPartitioning: Boolean = false)\n  extends RDD[U](prev) {\n  override val partitioner = if (preservesPartitioning) firstParent[T].partitioner else None\n  override def getPartitions: Array[Partition] = firstParent[T].partitions\n  override def compute(split: Partition, context: TaskContext): Iterator[U] =\n    f(context, split.index, firstParent[T].iterator(split, context))\n  override def clearDependencies() {\n    super.clearDependencies()\n    prev = null\n  }\n}\n\n```\n\n这里可以看到 compute 方法实际上调用 f 这个函数。而 f 这个函数是在 filter 方法中传入的`(context, pid, iter) => iter.filter(cleanF)` 。那么实际上 compute 进行的计算为:\n\n```scala\n(context, split.index, firstParent[T].iterator(split, context)) => firstParent[T].iterator(split, context).filter(cleanF)\n```\n\n前两个参数并没有用到，也就是最终的方法可以简化为:\n\n```scala\nfirstParent[T].iterator(split, context) => firstParent[T].iterator(split, context).filter(cleanF)\n```\n\n这里出现了一个`firstParent`，我们知道 RDD 之间存在依赖关系，如果rdd3 依赖 rdd2，rdd2 依赖 rdd1。rdd2 与 rdd3 都是rdd1 的一个父rdd, spark也将其称为血缘关系。而故名意思 rdd2 是 rdd3 的 firstParent。在这个例子中 a 是 b的 firstParent。那么在b 的 compute 中又调用了 a 的 iterator 方法。而 a 的 iterator 方法又会调用a 的 comput 方法。用箭头表示调用关系:\n\n```\nb.iterotr -> b.compute -> a.iterotr -> a.compute ->| 调用\nb.iterotr <- b.compute <- a.iterotr <- a.compute <-| 返回\n```\n\n而这里我们也可以看出来，b 调用 iterotr 返回的数据，是使用 b 中的方法对 a 返回数据进行处理。也就是b 的计算结果依赖于a的计算结果。如果将一个rdd比作一个函数，大概就是 `fb(fa())`，这样的。此时，我们已经得到了关于 spark 迭代的最简单的模型，假如我们有 n 个 rdd。那么之间的调用依赖关系便是:\n\n```\nn.iterotr -> n.compute -> (n-1).iterotr -> (n-1).compute ->...-> 1.iterotr -> 1.compute->| \nn.iterotr <- n.compute <- (n-1).iterotr <- (n-1).compute <-...<- 1.iterotr <- 1.compute<-|\n```\n\n那么细心的同学应该注意到了，所有的数据都是源自于第一个 RDD。这里把它称为源 RDD。例如本例中产生的 RDD 以及 textFile 方法产生的HadoopRDD都属于源RDD。既然属于源 RDD ，那么这个 RDD 一定会与内存或磁盘中的源数据产生交互，否则就没有真实的数据来源。这里的源数据是指内存的数组、本地文件、或者是HDFS上的分布式文件等。\n\n那么我们再继续看 a 的 compute 方法:\n\n```scala\n  override def compute(s: Partition, context: TaskContext): Iterator[T] = {\n    new InterruptibleIterator(context,s.asInstanceOf[ParallelCollectionPartition[T]].iterator)\n  }\n```\n\n可以看到 a 的 compute 根据 ParallelCollectionPartition 类的 iterator直接生成了一个迭代器。再看看ParallelCollectionPartition 的定义:\n\n```scala\nrivate[spark] class ParallelCollectionPartition[T: ClassTag](\n    var rddId: Long,\n    var slice: Int,\n    var values: Seq[T]\n  ) extends Partition with Serializable {\n  def iterator: Iterator[T] = values.iterator\n  ...\n```\n\niterator 是来自于 values 的一个迭代器，显然，values是内存中的数据。这说明 a 的 compute 方法直接返回了一个与源数据相关的迭代器。而这里 ParallelCollectionPartition 的数据是如何传入的，又是在什么时候被传入的呢?在**分区**小节将会提到。\n\n到这里，我们已经基本了解了 spark 的迭代计算的原理以及数据来源。但是仍然还有许多不了解的地方，例如 iterotr 会接受一个  Partition 作为参数。那么这个Partition参数到底起到了什么作用。其次，我们知道 rdd 的 iterotr 方法 是从后开始往前调用，那么又是谁调用的最后一个 rdd 的 itertor 方法呢。接下来我们就探索一下与分区相关的内容.\n\n### 分区\n\n提到分区，大部分都是类似于“RDD 中最小的数据单元”，“只是数据的抽象分区，而不是真正的持有数据”等等这样的描述。\n\n```scala\n/**\n * An identifier for a partition in an RDD.\n */\ntrait Partition extends Serializable {\n  def index: Int\n  override def hashCode(): Int = index\n  override def equals(other: Any): Boolean = super.equals(other)\n}\n```\n\n可以看到分区的类十分的简单，只有一个 index 属性。 当将分区这个参数传入 itertor 时表示我们需要 index 对应的分区的数据经过一系列计算的之后的迭代器。那么显然，在源 RDD 中一定会有更据分区的index获取对应数据的代码部分。而我们又知道，对于`ParallelCollectionPartition`这个分区类是直接持有数据的迭代器，因此我们只需要知道这个类如何被建立，便知道了是如何根据分区获取数据的。我们在整个工程中搜索 ParallelCollectionPartition，发现:\n\n```scala\n override def getPartitions: Array[Partition] = {\n    val slices = ParallelCollectionRDD.slice(data, numSlices).toArray\n    slices.indices.map(i => new ParallelCollectionPartition(id, i, slices(i))).toArray\n  }\n```\n\n中生成了一系列的分区对象。其中 ParallelCollectionRDD.slice() 方法根据总共有多少个分区来将 data  进行切分。而 data 我们在调用 parallelize() 方法时传入的。看到这里就应该明白了，**ParallelCollectionRDD 会将我们输入的数组进行切分，然后将利用分区对象持有数据的迭代器，当调用到 itertor 方法时就返回该分区的迭代器，供后续的RDD**进行计算。虽然这里我们只看了 ParallelCollectionRDD 的原代码，但是其他例如 HadoopRDD 的远离基本相同。 只不过一个是从内存中获取数据进行切分，另一个是从磁盘上获取数据进行切分。\n\n但是直到这里，我们仍然不知道 a 中的 itertor 方法的分区对象是如何传入的，因为这个方法间接被 b 的 itertor 方法调用。 而 b 的  itertor 方法同样也需要在外部被调用 ，因此要解决这个问题只需要找到 b 的 itertor 被调用的地方。不过我们首先可以根据代码猜测一下:\n\n```scala\n  override def compute(s: Partition, context: TaskContext): Iterator[T] = {\n    new InterruptibleIterator(context,s.asInstanceOf[ParallelCollectionPartition[T]].iterator)\n  }\n```\n\na 中的 compute 方法直接将传入的分区对象转为了 ParallelCollectionPartition 并获取了对应的迭代器。根据对象间强转的\n\n规则，传入的分区对象只能是 ParallelCollectionPartition 类型 ，因为其父类 RDD 并没有 iterator 方法。 并且传入的ParallelCollectionPartition 一定是持有源数据迭代器的对象，否则在 a 的 compute 中就无法向后返回迭代器。而且我们知道，spark 的计算是**惰性计算**，在调用 action 算子之后才会真正的开始计算。那么可以猜测，b 的 iterator 也是在调用了 count 方法之后才被调用的。为了证明这一点，我们继续查看代码:\n\n```scala\ndef count(): Long = sc.runJob(this, Utils.getIteratorSize _).sum\n```\n\n可以看到是调用了 SparkContext 中的runJob方法:\n\n```scala\n def runJob[T, U: ClassTag](\n     ...\n    dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)\n    ...\n  }\n```\n\n而 SparkContext 中的 runJob 又继续调用了 DAGScheduler 中的 runJob 方法， 继而调用了 submitJob 方法。之后由经历了 DAGSchedulerEventProcessLoop.post DAGSchedulerEventProcessLoop.doOnReceive 等方法，最后在 submitMissingTasks 看到建立了 ResultTask 对象。由于一个分区的数据会被一个task 处理，因此我们猜测在 ＲesultTask 中会有关于对应 rdd 调用 iterator 的信息，果然，在ResultTask中找到了 runTask 方法:\n\n```scala\noverride def runTask(context: TaskContext): U = {\n    // Deserialize the RDD and the func using the broadcast variables.\n    val threadMXBean = ManagementFactory.getThreadMXBean\n    val deserializeStartTime = System.currentTimeMillis()\n    val deserializeStartCpuTime = if (threadMXBean.isCurrentThreadCpuTimeSupported) {\n      threadMXBean.getCurrentThreadCpuTime\n    } else 0L\n    val ser = SparkEnv.get.closureSerializer.newInstance()\n    val (rdd, func) = ser.deserialize[(RDD[T], (TaskContext, Iterator[T]) => U)](\n      ByteBuffer.wrap(taskBinary.value), Thread.currentThread.getContextClassLoader)\n    _executorDeserializeTime = System.currentTimeMillis() - deserializeStartTime\n    _executorDeserializeCpuTime = if (threadMXBean.isCurrentThreadCpuTimeSupported) {\n      threadMXBean.getCurrentThreadCpuTime - deserializeStartCpuTime\n    } else 0L\n\n    func(context, rdd.iterator(partition, context))//这里就 b 的iterator被真正调用的地方\n  }\n\n\n```\n\n那么我们可以看到最后一行有一个rdd的调用，而这个rdd 是反徐序列化得的，很明显，这个rdd就是 b。此时，b 调用 iterator 方法的地方找到了，但是分区对象partition依然没有找到从哪里传入。由于 partition 是 ResultTask 构造时传入的，我们回到 submitMissingTasks 中，创建ResultTask时分区参数对应的为变量 part 。 \n\n```scala\n      //从需要计算的分区map中获取分区，并生成task\n       partitionsToCompute.map { id =>\n            val p: Int = stage.partitions(id)\n            val part = partitions(p)\n            val locs = taskIdToLocations(id)\n            new ResultTask(stage.id, stage.latestInfo.attemptNumber,\n              taskBinary, part, locs, id, properties, serializedTaskMetrics,\n              Option(jobId), Option(sc.applicationId), sc.applicationAttemptId)\n          }\n```\n\n继续往上看，可以看到 part 是 从 partitions 中获取的。这里我们也可以看出 **一个任务对应一个分区数据**。继续往上看，发现`partitions = stage.rdd.partitions`。实际上来自于 Stage 中的 rdd 中的 partitions。那么看Stage 对 rdd 这个属性的描述:\n\n```scala\n *\n * @param rdd RDD that this stage runs on: for a shuffle map stage, it's the RDD we run map tasks\n *   on, while for a result stage, it's the target RDD that we ran an action on\n *   这里的意思是 rdd 表示调用 action 算子的 rdd 也就是 b \n */\nprivate[scheduler] abstract class Stage(\n    val id: Int,\n    val rdd: RDD[_],\n    val numTasks: Int,\n    val parents: List[Stage],\n    val firstJobId: Int,\n    val callSite: CallSite)\n  extends Logging {\n```\n\n我们发现一个惊人的事实 ，这个 rdd 居然也是 b。也就是说，我们在 runTask函数中实际调用的方法是这样的:\n\n```scala\n  func(context, b.iterator(b.partitions(index), context))//这里就 b 的iterator被真正调用的地方\n```\n\nindex 表示该task对应需要执行的分区标号。随即我们继续看 b 中的 partitions 的定义:\n\n```scala\n override def getPartitions: Array[Partition] = firstParent[T].partitions\n```\n\n说明这个 partitions 来自于 a。 而 a 中的 partitions 实际上又来自于:\n\n```scala\n override def getPartitions: Array[Partition] = {\n    val slices = ParallelCollectionRDD.slice(data, numSlices).toArray\n    slices.indices.map(i => new ParallelCollectionPartition(id, i, slices(i))).toArray\n  }\n```\n\n这样最终 runTask 函数实际调用的方法为:\n\n```scala\n func(context, b.iterator(a.partitions(index), context))//这里就 b 的iterator被真正调用的地方\n```\n\n这和我们猜测的相同，这个分区确实为 ParallelCollectionPartition 对象，并且也持有了源数据的迭代器。这里其实有一个很巧妙的地方，虽然 RDD  的迭代计算是从后往前调用，但是传入源 RDD 的分区对象依然来自于源 RDD 自身。\n\n到了这里，我们也就明白分区对象是如何和数据关联起来的。ParallelCollectionPartition 对象中存在分区编号 index 以及 源数据的迭代器，通过分区编号就能获取到源数据的迭代器。\n\n## 总结\n\n经过如此大篇幅的讲解，终于对 spark 的迭代计算以及分区做了一个简单的分析。虽然还有很多地方不完善，例如还有其它类型的分区对象没有讲解，但是我相信你能完全看懂这篇文章所讲的内容，其他类型的分区对象对于你来说也很简单了。作者水平有限，并且这也是作者的第一篇关于 spark 源码阅读的博客，难免有遗漏和错误。如果想如给作者提建议和意见的话，请联系作者的微信或直接在 github 上提 issue 。\n\n \n\n","source":"_posts/Spark-源码阅读计划-第一部分-迭代计算.md","raw":"---\ntitle: Spark 源码阅读计划 - 第一部分 - 迭代计算\ndate: 2018-06-06 13:46:45\ntags:\n  - Spark\n  - 编程\ncategories: Spark源码阅读计划\nauthor: lishion\ntoc: true\n---\n首先立一个flag，这将是一个长期更新的版块。\n\n## 写在最开始\n\n在我使用`spark`进行日志分析的时候感受到了`spark`的便捷与强大。在学习`spark`初期，我阅读了许多与`spark`相关的文档，在这个过程中了解了`RDD`，`分区`，`shuffle`等概念，但是我并没有对这些概念有更多具体的认识。由于不了解`spark`的基本运作方式使得我在编写代码的时候十分困扰。由于这个原因，我准备阅读`spark`的源代码来解决我对基本概念的认识。\n\n## 本部分主要内容\n\n虽然该章节的名字叫做**迭代计算**，但是本章会讨论包括**RDD、分区、Job、迭代计算**等相关的内容，其中会重点讨论分区与迭代计算。因此在阅读本章之前你至少需要对这些提到的概念有一定的了解。\n\n## 准备工作\n\n你需要准备一份 Spark 的源代码以及一个便于全局搜索的编辑器|IDE。\n\n## 一个简单的例子\n\n假设现在我们有这样一个需求 : 寻找从 0 to 100 的偶数并输出其总数。利用 spark 编写代码如下:\n\n```scala\nsc.parallelize(0 to 100).filter(_%2 == 0).count\n//res0: Long = 51\n```\n\n这是一个非常简单的例子，但是里面却蕴涵了许多基础的知识。这一章的内容也是从这个例子展开的。\n\n## 迭代计算\n\n### comput 方法与 itertor \n\n在RDD.scala 定义的类 RDD中，有两个实现迭代计算的核心方法，分别是`compute`以及`itertor`方法。其中`itertor`方法如下:\n\n```scala\n  final def iterator(split: Partition, context: TaskContext): Iterator[T] = {\n    if (storageLevel != StorageLevel.NONE) {//如果有缓存\n      getOrCompute(split, context)\n    } else {\n      computeOrReadCheckpoint(split, context)\n    }\n  }\n```\n\n我们只关心这个方法的第一个参数**分区**，以及返回的迭代器。忽略有缓存的情况，我们继续看`computeOrReadCheckpoint`这个方法:\n\n```scala\n  private[spark] def computeOrReadCheckpoint(split: Partition, context: TaskContext): Iterator[T] =\n  {\n    if (isCheckpointedAndMaterialized) {//如果有checkpoint\n      firstParent[T].iterator(split, context)\n    } else {\n      compute(split, context)\n    }\n  }\n```\n\n可以看到，在没有缓存和 checkpoint 的情况下，itertor 方法直接调用了 compute 方法。而compute方法定义如下:\n\n```scala\n /**\n   * Implemented by subclasses to compute a given partition.\n   */\n  def compute(split: Partition, context: TaskContext): Iterator[T]\n```\n\n从注释可以看出，compute 方法应该由子类实现，用以计算给定分区并生成一个迭代器。如果不考虑缓存和 checkpoint 的情况下，简化一下代码:\n\n```scala\n final def iterator(split: Partition, context: TaskContext): Iterator[T] = {\n    compute(split,context)\n  }\n```\n\n那么实际上 iterator 的功能是: **接受一个分区，对这个分区进行计算，并返回计算结果的迭代器**。而之所以 compute 需要子类来实现，是因为只需要在子类中实现不同的 compute 方法，就能产生不同类型的 RDD。既然不同的RDD 有不同的功能，我们想知道之前的例子是如何进行迭代计算的，就需要知道这个例子中涉及到了哪些 RDD。\n\n首先查看`SparkContext`中与`parallelize`相关的部分代码:\n\n```scala\n  def parallelize[T: ClassTag](\n      seq: Seq[T],\n      numSlices: Int = defaultParallelism): RDD[T] = withScope {\n    assertNotStopped()\n    new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())\n  }\n```\n\n可以看到，`parallelize`实际返回了一个`ParallelCollectionRDD`。在`ParallelCollectionRDD`中并没有对`filter`方法进行重写，因此我们查看`RDD`中的`filter`方法:\n\n```scala\n def filter(f: T => Boolean): RDD[T] = withScope {\n    val cleanF = sc.clean(f)\n    new MapPartitionsRDD[T, T](\n      this,\n      (context, pid, iter) => iter.filter(cleanF),\n      preservesPartitioning = true)\n  }\n```\n\nfilter 方法返回了`MapPartitionsRDD`。为了方便起见，我将 ParallelCollectionRDD 类型的 RDD 称为 a，MapPartitionsRDD 类型的 RDD 成为B。那么先看一下 b 的 compute 方法:\n\n```scala\nprivate[spark] class MapPartitionsRDD[U: ClassTag, T: ClassTag](\n    var prev: RDD[T],\n    f: (TaskContext, Int, Iterator[T]) => Iterator[U],  // (TaskContext, partition index, iterator)\n    preservesPartitioning: Boolean = false)\n  extends RDD[U](prev) {\n  override val partitioner = if (preservesPartitioning) firstParent[T].partitioner else None\n  override def getPartitions: Array[Partition] = firstParent[T].partitions\n  override def compute(split: Partition, context: TaskContext): Iterator[U] =\n    f(context, split.index, firstParent[T].iterator(split, context))\n  override def clearDependencies() {\n    super.clearDependencies()\n    prev = null\n  }\n}\n\n```\n\n这里可以看到 compute 方法实际上调用 f 这个函数。而 f 这个函数是在 filter 方法中传入的`(context, pid, iter) => iter.filter(cleanF)` 。那么实际上 compute 进行的计算为:\n\n```scala\n(context, split.index, firstParent[T].iterator(split, context)) => firstParent[T].iterator(split, context).filter(cleanF)\n```\n\n前两个参数并没有用到，也就是最终的方法可以简化为:\n\n```scala\nfirstParent[T].iterator(split, context) => firstParent[T].iterator(split, context).filter(cleanF)\n```\n\n这里出现了一个`firstParent`，我们知道 RDD 之间存在依赖关系，如果rdd3 依赖 rdd2，rdd2 依赖 rdd1。rdd2 与 rdd3 都是rdd1 的一个父rdd, spark也将其称为血缘关系。而故名意思 rdd2 是 rdd3 的 firstParent。在这个例子中 a 是 b的 firstParent。那么在b 的 compute 中又调用了 a 的 iterator 方法。而 a 的 iterator 方法又会调用a 的 comput 方法。用箭头表示调用关系:\n\n```\nb.iterotr -> b.compute -> a.iterotr -> a.compute ->| 调用\nb.iterotr <- b.compute <- a.iterotr <- a.compute <-| 返回\n```\n\n而这里我们也可以看出来，b 调用 iterotr 返回的数据，是使用 b 中的方法对 a 返回数据进行处理。也就是b 的计算结果依赖于a的计算结果。如果将一个rdd比作一个函数，大概就是 `fb(fa())`，这样的。此时，我们已经得到了关于 spark 迭代的最简单的模型，假如我们有 n 个 rdd。那么之间的调用依赖关系便是:\n\n```\nn.iterotr -> n.compute -> (n-1).iterotr -> (n-1).compute ->...-> 1.iterotr -> 1.compute->| \nn.iterotr <- n.compute <- (n-1).iterotr <- (n-1).compute <-...<- 1.iterotr <- 1.compute<-|\n```\n\n那么细心的同学应该注意到了，所有的数据都是源自于第一个 RDD。这里把它称为源 RDD。例如本例中产生的 RDD 以及 textFile 方法产生的HadoopRDD都属于源RDD。既然属于源 RDD ，那么这个 RDD 一定会与内存或磁盘中的源数据产生交互，否则就没有真实的数据来源。这里的源数据是指内存的数组、本地文件、或者是HDFS上的分布式文件等。\n\n那么我们再继续看 a 的 compute 方法:\n\n```scala\n  override def compute(s: Partition, context: TaskContext): Iterator[T] = {\n    new InterruptibleIterator(context,s.asInstanceOf[ParallelCollectionPartition[T]].iterator)\n  }\n```\n\n可以看到 a 的 compute 根据 ParallelCollectionPartition 类的 iterator直接生成了一个迭代器。再看看ParallelCollectionPartition 的定义:\n\n```scala\nrivate[spark] class ParallelCollectionPartition[T: ClassTag](\n    var rddId: Long,\n    var slice: Int,\n    var values: Seq[T]\n  ) extends Partition with Serializable {\n  def iterator: Iterator[T] = values.iterator\n  ...\n```\n\niterator 是来自于 values 的一个迭代器，显然，values是内存中的数据。这说明 a 的 compute 方法直接返回了一个与源数据相关的迭代器。而这里 ParallelCollectionPartition 的数据是如何传入的，又是在什么时候被传入的呢?在**分区**小节将会提到。\n\n到这里，我们已经基本了解了 spark 的迭代计算的原理以及数据来源。但是仍然还有许多不了解的地方，例如 iterotr 会接受一个  Partition 作为参数。那么这个Partition参数到底起到了什么作用。其次，我们知道 rdd 的 iterotr 方法 是从后开始往前调用，那么又是谁调用的最后一个 rdd 的 itertor 方法呢。接下来我们就探索一下与分区相关的内容.\n\n### 分区\n\n提到分区，大部分都是类似于“RDD 中最小的数据单元”，“只是数据的抽象分区，而不是真正的持有数据”等等这样的描述。\n\n```scala\n/**\n * An identifier for a partition in an RDD.\n */\ntrait Partition extends Serializable {\n  def index: Int\n  override def hashCode(): Int = index\n  override def equals(other: Any): Boolean = super.equals(other)\n}\n```\n\n可以看到分区的类十分的简单，只有一个 index 属性。 当将分区这个参数传入 itertor 时表示我们需要 index 对应的分区的数据经过一系列计算的之后的迭代器。那么显然，在源 RDD 中一定会有更据分区的index获取对应数据的代码部分。而我们又知道，对于`ParallelCollectionPartition`这个分区类是直接持有数据的迭代器，因此我们只需要知道这个类如何被建立，便知道了是如何根据分区获取数据的。我们在整个工程中搜索 ParallelCollectionPartition，发现:\n\n```scala\n override def getPartitions: Array[Partition] = {\n    val slices = ParallelCollectionRDD.slice(data, numSlices).toArray\n    slices.indices.map(i => new ParallelCollectionPartition(id, i, slices(i))).toArray\n  }\n```\n\n中生成了一系列的分区对象。其中 ParallelCollectionRDD.slice() 方法根据总共有多少个分区来将 data  进行切分。而 data 我们在调用 parallelize() 方法时传入的。看到这里就应该明白了，**ParallelCollectionRDD 会将我们输入的数组进行切分，然后将利用分区对象持有数据的迭代器，当调用到 itertor 方法时就返回该分区的迭代器，供后续的RDD**进行计算。虽然这里我们只看了 ParallelCollectionRDD 的原代码，但是其他例如 HadoopRDD 的远离基本相同。 只不过一个是从内存中获取数据进行切分，另一个是从磁盘上获取数据进行切分。\n\n但是直到这里，我们仍然不知道 a 中的 itertor 方法的分区对象是如何传入的，因为这个方法间接被 b 的 itertor 方法调用。 而 b 的  itertor 方法同样也需要在外部被调用 ，因此要解决这个问题只需要找到 b 的 itertor 被调用的地方。不过我们首先可以根据代码猜测一下:\n\n```scala\n  override def compute(s: Partition, context: TaskContext): Iterator[T] = {\n    new InterruptibleIterator(context,s.asInstanceOf[ParallelCollectionPartition[T]].iterator)\n  }\n```\n\na 中的 compute 方法直接将传入的分区对象转为了 ParallelCollectionPartition 并获取了对应的迭代器。根据对象间强转的\n\n规则，传入的分区对象只能是 ParallelCollectionPartition 类型 ，因为其父类 RDD 并没有 iterator 方法。 并且传入的ParallelCollectionPartition 一定是持有源数据迭代器的对象，否则在 a 的 compute 中就无法向后返回迭代器。而且我们知道，spark 的计算是**惰性计算**，在调用 action 算子之后才会真正的开始计算。那么可以猜测，b 的 iterator 也是在调用了 count 方法之后才被调用的。为了证明这一点，我们继续查看代码:\n\n```scala\ndef count(): Long = sc.runJob(this, Utils.getIteratorSize _).sum\n```\n\n可以看到是调用了 SparkContext 中的runJob方法:\n\n```scala\n def runJob[T, U: ClassTag](\n     ...\n    dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)\n    ...\n  }\n```\n\n而 SparkContext 中的 runJob 又继续调用了 DAGScheduler 中的 runJob 方法， 继而调用了 submitJob 方法。之后由经历了 DAGSchedulerEventProcessLoop.post DAGSchedulerEventProcessLoop.doOnReceive 等方法，最后在 submitMissingTasks 看到建立了 ResultTask 对象。由于一个分区的数据会被一个task 处理，因此我们猜测在 ＲesultTask 中会有关于对应 rdd 调用 iterator 的信息，果然，在ResultTask中找到了 runTask 方法:\n\n```scala\noverride def runTask(context: TaskContext): U = {\n    // Deserialize the RDD and the func using the broadcast variables.\n    val threadMXBean = ManagementFactory.getThreadMXBean\n    val deserializeStartTime = System.currentTimeMillis()\n    val deserializeStartCpuTime = if (threadMXBean.isCurrentThreadCpuTimeSupported) {\n      threadMXBean.getCurrentThreadCpuTime\n    } else 0L\n    val ser = SparkEnv.get.closureSerializer.newInstance()\n    val (rdd, func) = ser.deserialize[(RDD[T], (TaskContext, Iterator[T]) => U)](\n      ByteBuffer.wrap(taskBinary.value), Thread.currentThread.getContextClassLoader)\n    _executorDeserializeTime = System.currentTimeMillis() - deserializeStartTime\n    _executorDeserializeCpuTime = if (threadMXBean.isCurrentThreadCpuTimeSupported) {\n      threadMXBean.getCurrentThreadCpuTime - deserializeStartCpuTime\n    } else 0L\n\n    func(context, rdd.iterator(partition, context))//这里就 b 的iterator被真正调用的地方\n  }\n\n\n```\n\n那么我们可以看到最后一行有一个rdd的调用，而这个rdd 是反徐序列化得的，很明显，这个rdd就是 b。此时，b 调用 iterator 方法的地方找到了，但是分区对象partition依然没有找到从哪里传入。由于 partition 是 ResultTask 构造时传入的，我们回到 submitMissingTasks 中，创建ResultTask时分区参数对应的为变量 part 。 \n\n```scala\n      //从需要计算的分区map中获取分区，并生成task\n       partitionsToCompute.map { id =>\n            val p: Int = stage.partitions(id)\n            val part = partitions(p)\n            val locs = taskIdToLocations(id)\n            new ResultTask(stage.id, stage.latestInfo.attemptNumber,\n              taskBinary, part, locs, id, properties, serializedTaskMetrics,\n              Option(jobId), Option(sc.applicationId), sc.applicationAttemptId)\n          }\n```\n\n继续往上看，可以看到 part 是 从 partitions 中获取的。这里我们也可以看出 **一个任务对应一个分区数据**。继续往上看，发现`partitions = stage.rdd.partitions`。实际上来自于 Stage 中的 rdd 中的 partitions。那么看Stage 对 rdd 这个属性的描述:\n\n```scala\n *\n * @param rdd RDD that this stage runs on: for a shuffle map stage, it's the RDD we run map tasks\n *   on, while for a result stage, it's the target RDD that we ran an action on\n *   这里的意思是 rdd 表示调用 action 算子的 rdd 也就是 b \n */\nprivate[scheduler] abstract class Stage(\n    val id: Int,\n    val rdd: RDD[_],\n    val numTasks: Int,\n    val parents: List[Stage],\n    val firstJobId: Int,\n    val callSite: CallSite)\n  extends Logging {\n```\n\n我们发现一个惊人的事实 ，这个 rdd 居然也是 b。也就是说，我们在 runTask函数中实际调用的方法是这样的:\n\n```scala\n  func(context, b.iterator(b.partitions(index), context))//这里就 b 的iterator被真正调用的地方\n```\n\nindex 表示该task对应需要执行的分区标号。随即我们继续看 b 中的 partitions 的定义:\n\n```scala\n override def getPartitions: Array[Partition] = firstParent[T].partitions\n```\n\n说明这个 partitions 来自于 a。 而 a 中的 partitions 实际上又来自于:\n\n```scala\n override def getPartitions: Array[Partition] = {\n    val slices = ParallelCollectionRDD.slice(data, numSlices).toArray\n    slices.indices.map(i => new ParallelCollectionPartition(id, i, slices(i))).toArray\n  }\n```\n\n这样最终 runTask 函数实际调用的方法为:\n\n```scala\n func(context, b.iterator(a.partitions(index), context))//这里就 b 的iterator被真正调用的地方\n```\n\n这和我们猜测的相同，这个分区确实为 ParallelCollectionPartition 对象，并且也持有了源数据的迭代器。这里其实有一个很巧妙的地方，虽然 RDD  的迭代计算是从后往前调用，但是传入源 RDD 的分区对象依然来自于源 RDD 自身。\n\n到了这里，我们也就明白分区对象是如何和数据关联起来的。ParallelCollectionPartition 对象中存在分区编号 index 以及 源数据的迭代器，通过分区编号就能获取到源数据的迭代器。\n\n## 总结\n\n经过如此大篇幅的讲解，终于对 spark 的迭代计算以及分区做了一个简单的分析。虽然还有很多地方不完善，例如还有其它类型的分区对象没有讲解，但是我相信你能完全看懂这篇文章所讲的内容，其他类型的分区对象对于你来说也很简单了。作者水平有限，并且这也是作者的第一篇关于 spark 源码阅读的博客，难免有遗漏和错误。如果想如给作者提建议和意见的话，请联系作者的微信或直接在 github 上提 issue 。\n\n \n\n","slug":"Spark-源码阅读计划-第一部分-迭代计算","published":1,"updated":"2018-06-20T06:22:05.783Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjjhx8v800032b8tg5hkbpul2","content":"<p>首先立一个flag，这将是一个长期更新的版块。</p>\n<h2 id=\"写在最开始\"><a href=\"#写在最开始\" class=\"headerlink\" title=\"写在最开始\"></a>写在最开始</h2><p>在我使用<code>spark</code>进行日志分析的时候感受到了<code>spark</code>的便捷与强大。在学习<code>spark</code>初期，我阅读了许多与<code>spark</code>相关的文档，在这个过程中了解了<code>RDD</code>，<code>分区</code>，<code>shuffle</code>等概念，但是我并没有对这些概念有更多具体的认识。由于不了解<code>spark</code>的基本运作方式使得我在编写代码的时候十分困扰。由于这个原因，我准备阅读<code>spark</code>的源代码来解决我对基本概念的认识。</p>\n<h2 id=\"本部分主要内容\"><a href=\"#本部分主要内容\" class=\"headerlink\" title=\"本部分主要内容\"></a>本部分主要内容</h2><p>虽然该章节的名字叫做<strong>迭代计算</strong>，但是本章会讨论包括<strong>RDD、分区、Job、迭代计算</strong>等相关的内容，其中会重点讨论分区与迭代计算。因此在阅读本章之前你至少需要对这些提到的概念有一定的了解。</p>\n<h2 id=\"准备工作\"><a href=\"#准备工作\" class=\"headerlink\" title=\"准备工作\"></a>准备工作</h2><p>你需要准备一份 Spark 的源代码以及一个便于全局搜索的编辑器|IDE。</p>\n<h2 id=\"一个简单的例子\"><a href=\"#一个简单的例子\" class=\"headerlink\" title=\"一个简单的例子\"></a>一个简单的例子</h2><p>假设现在我们有这样一个需求 : 寻找从 0 to 100 的偶数并输出其总数。利用 spark 编写代码如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sc.parallelize(<span class=\"number\">0</span> to <span class=\"number\">100</span>).filter(_%<span class=\"number\">2</span> == <span class=\"number\">0</span>).count</span><br><span class=\"line\"><span class=\"comment\">//res0: Long = 51</span></span><br></pre></td></tr></table></figure>\n<p>这是一个非常简单的例子，但是里面却蕴涵了许多基础的知识。这一章的内容也是从这个例子展开的。</p>\n<h2 id=\"迭代计算\"><a href=\"#迭代计算\" class=\"headerlink\" title=\"迭代计算\"></a>迭代计算</h2><h3 id=\"comput-方法与-itertor\"><a href=\"#comput-方法与-itertor\" class=\"headerlink\" title=\"comput 方法与 itertor\"></a>comput 方法与 itertor</h3><p>在RDD.scala 定义的类 RDD中，有两个实现迭代计算的核心方法，分别是<code>compute</code>以及<code>itertor</code>方法。其中<code>itertor</code>方法如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">final</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">iterator</span></span>(split: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>] = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (storageLevel != <span class=\"type\">StorageLevel</span>.<span class=\"type\">NONE</span>) &#123;<span class=\"comment\">//如果有缓存</span></span><br><span class=\"line\">    getOrCompute(split, context)</span><br><span class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">    computeOrReadCheckpoint(split, context)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>我们只关心这个方法的第一个参数<strong>分区</strong>，以及返回的迭代器。忽略有缓存的情况，我们继续看<code>computeOrReadCheckpoint</code>这个方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span>[spark] <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">computeOrReadCheckpoint</span></span>(split: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>] =</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (isCheckpointedAndMaterialized) &#123;<span class=\"comment\">//如果有checkpoint</span></span><br><span class=\"line\">    firstParent[<span class=\"type\">T</span>].iterator(split, context)</span><br><span class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">    compute(split, context)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可以看到，在没有缓存和 checkpoint 的情况下，itertor 方法直接调用了 compute 方法。而compute方法定义如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">  * Implemented by subclasses to compute a given partition.</span></span><br><span class=\"line\"><span class=\"comment\">  */</span></span><br><span class=\"line\"> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span></span>(split: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>]</span><br></pre></td></tr></table></figure>\n<p>从注释可以看出，compute 方法应该由子类实现，用以计算给定分区并生成一个迭代器。如果不考虑缓存和 checkpoint 的情况下，简化一下代码:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">final</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">iterator</span></span>(split: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>] = &#123;</span><br><span class=\"line\">   compute(split,context)</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>那么实际上 iterator 的功能是: <strong>接受一个分区，对这个分区进行计算，并返回计算结果的迭代器</strong>。而之所以 compute 需要子类来实现，是因为只需要在子类中实现不同的 compute 方法，就能产生不同类型的 RDD。既然不同的RDD 有不同的功能，我们想知道之前的例子是如何进行迭代计算的，就需要知道这个例子中涉及到了哪些 RDD。</p>\n<p>首先查看<code>SparkContext</code>中与<code>parallelize</code>相关的部分代码:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">parallelize</span></span>[<span class=\"type\">T</span>: <span class=\"type\">ClassTag</span>](</span><br><span class=\"line\">    seq: <span class=\"type\">Seq</span>[<span class=\"type\">T</span>],</span><br><span class=\"line\">    numSlices: <span class=\"type\">Int</span> = defaultParallelism): <span class=\"type\">RDD</span>[<span class=\"type\">T</span>] = withScope &#123;</span><br><span class=\"line\">  assertNotStopped()</span><br><span class=\"line\">  <span class=\"keyword\">new</span> <span class=\"type\">ParallelCollectionRDD</span>[<span class=\"type\">T</span>](<span class=\"keyword\">this</span>, seq, numSlices, <span class=\"type\">Map</span>[<span class=\"type\">Int</span>, <span class=\"type\">Seq</span>[<span class=\"type\">String</span>]]())</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可以看到，<code>parallelize</code>实际返回了一个<code>ParallelCollectionRDD</code>。在<code>ParallelCollectionRDD</code>中并没有对<code>filter</code>方法进行重写，因此我们查看<code>RDD</code>中的<code>filter</code>方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">filter</span></span>(f: <span class=\"type\">T</span> =&gt; <span class=\"type\">Boolean</span>): <span class=\"type\">RDD</span>[<span class=\"type\">T</span>] = withScope &#123;</span><br><span class=\"line\">   <span class=\"keyword\">val</span> cleanF = sc.clean(f)</span><br><span class=\"line\">   <span class=\"keyword\">new</span> <span class=\"type\">MapPartitionsRDD</span>[<span class=\"type\">T</span>, <span class=\"type\">T</span>](</span><br><span class=\"line\">     <span class=\"keyword\">this</span>,</span><br><span class=\"line\">     (context, pid, iter) =&gt; iter.filter(cleanF),</span><br><span class=\"line\">     preservesPartitioning = <span class=\"literal\">true</span>)</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>filter 方法返回了<code>MapPartitionsRDD</code>。为了方便起见，我将 ParallelCollectionRDD 类型的 RDD 称为 a，MapPartitionsRDD 类型的 RDD 成为B。那么先看一下 b 的 compute 方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span>[spark] <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MapPartitionsRDD</span>[<span class=\"type\">U</span>: <span class=\"type\">ClassTag</span>, <span class=\"type\">T</span>: <span class=\"type\">ClassTag</span>](<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    var prev: <span class=\"type\">RDD</span>[<span class=\"type\">T</span>],</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    f: (<span class=\"type\">TaskContext</span>, <span class=\"type\">Int</span>, <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>]</span>) <span class=\"title\">=&gt;</span> <span class=\"title\">Iterator</span>[<span class=\"type\">U</span>],  <span class=\"title\">//</span> (<span class=\"params\"><span class=\"type\">TaskContext</span>, partition index, iterator</span>)</span></span><br><span class=\"line\"><span class=\"class\">    <span class=\"title\">preservesPartitioning</span></span>: <span class=\"type\">Boolean</span> = <span class=\"literal\">false</span>)</span><br><span class=\"line\">  <span class=\"keyword\">extends</span> <span class=\"type\">RDD</span>[<span class=\"type\">U</span>](prev) &#123;</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"keyword\">val</span> partitioner = <span class=\"keyword\">if</span> (preservesPartitioning) firstParent[<span class=\"type\">T</span>].partitioner <span class=\"keyword\">else</span> <span class=\"type\">None</span></span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPartitions</span></span>: <span class=\"type\">Array</span>[<span class=\"type\">Partition</span>] = firstParent[<span class=\"type\">T</span>].partitions</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span></span>(split: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">U</span>] =</span><br><span class=\"line\">    f(context, split.index, firstParent[<span class=\"type\">T</span>].iterator(split, context))</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">clearDependencies</span></span>() &#123;</span><br><span class=\"line\">    <span class=\"keyword\">super</span>.clearDependencies()</span><br><span class=\"line\">    prev = <span class=\"literal\">null</span></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>这里可以看到 compute 方法实际上调用 f 这个函数。而 f 这个函数是在 filter 方法中传入的<code>(context, pid, iter) =&gt; iter.filter(cleanF)</code> 。那么实际上 compute 进行的计算为:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(context, split.index, firstParent[<span class=\"type\">T</span>].iterator(split, context)) =&gt; firstParent[<span class=\"type\">T</span>].iterator(split, context).filter(cleanF)</span><br></pre></td></tr></table></figure>\n<p>前两个参数并没有用到，也就是最终的方法可以简化为:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">firstParent[<span class=\"type\">T</span>].iterator(split, context) =&gt; firstParent[<span class=\"type\">T</span>].iterator(split, context).filter(cleanF)</span><br></pre></td></tr></table></figure>\n<p>这里出现了一个<code>firstParent</code>，我们知道 RDD 之间存在依赖关系，如果rdd3 依赖 rdd2，rdd2 依赖 rdd1。rdd2 与 rdd3 都是rdd1 的一个父rdd, spark也将其称为血缘关系。而故名意思 rdd2 是 rdd3 的 firstParent。在这个例子中 a 是 b的 firstParent。那么在b 的 compute 中又调用了 a 的 iterator 方法。而 a 的 iterator 方法又会调用a 的 comput 方法。用箭头表示调用关系:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">b.iterotr -&gt; b.compute -&gt; a.iterotr -&gt; a.compute -&gt;| 调用</span><br><span class=\"line\">b.iterotr &lt;- b.compute &lt;- a.iterotr &lt;- a.compute &lt;-| 返回</span><br></pre></td></tr></table></figure>\n<p>而这里我们也可以看出来，b 调用 iterotr 返回的数据，是使用 b 中的方法对 a 返回数据进行处理。也就是b 的计算结果依赖于a的计算结果。如果将一个rdd比作一个函数，大概就是 <code>fb(fa())</code>，这样的。此时，我们已经得到了关于 spark 迭代的最简单的模型，假如我们有 n 个 rdd。那么之间的调用依赖关系便是:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">n.iterotr -&gt; n.compute -&gt; (n-1).iterotr -&gt; (n-1).compute -&gt;...-&gt; 1.iterotr -&gt; 1.compute-&gt;| </span><br><span class=\"line\">n.iterotr &lt;- n.compute &lt;- (n-1).iterotr &lt;- (n-1).compute &lt;-...&lt;- 1.iterotr &lt;- 1.compute&lt;-|</span><br></pre></td></tr></table></figure>\n<p>那么细心的同学应该注意到了，所有的数据都是源自于第一个 RDD。这里把它称为源 RDD。例如本例中产生的 RDD 以及 textFile 方法产生的HadoopRDD都属于源RDD。既然属于源 RDD ，那么这个 RDD 一定会与内存或磁盘中的源数据产生交互，否则就没有真实的数据来源。这里的源数据是指内存的数组、本地文件、或者是HDFS上的分布式文件等。</p>\n<p>那么我们再继续看 a 的 compute 方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span></span>(s: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>] = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">new</span> <span class=\"type\">InterruptibleIterator</span>(context,s.asInstanceOf[<span class=\"type\">ParallelCollectionPartition</span>[<span class=\"type\">T</span>]].iterator)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可以看到 a 的 compute 根据 ParallelCollectionPartition 类的 iterator直接生成了一个迭代器。再看看ParallelCollectionPartition 的定义:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rivate[spark] <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ParallelCollectionPartition</span>[<span class=\"type\">T</span>: <span class=\"type\">ClassTag</span>](<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    var rddId: <span class=\"type\">Long</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    var slice: <span class=\"type\">Int</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    var values: <span class=\"type\">Seq</span>[<span class=\"type\">T</span>]</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">  </span>) <span class=\"keyword\">extends</span> <span class=\"title\">Partition</span> <span class=\"keyword\">with</span> <span class=\"title\">Serializable</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">iterator</span></span>: <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>] = values.iterator</span><br><span class=\"line\">  ...</span><br></pre></td></tr></table></figure>\n<p>iterator 是来自于 values 的一个迭代器，显然，values是内存中的数据。这说明 a 的 compute 方法直接返回了一个与源数据相关的迭代器。而这里 ParallelCollectionPartition 的数据是如何传入的，又是在什么时候被传入的呢?在<strong>分区</strong>小节将会提到。</p>\n<p>到这里，我们已经基本了解了 spark 的迭代计算的原理以及数据来源。但是仍然还有许多不了解的地方，例如 iterotr 会接受一个  Partition 作为参数。那么这个Partition参数到底起到了什么作用。其次，我们知道 rdd 的 iterotr 方法 是从后开始往前调用，那么又是谁调用的最后一个 rdd 的 itertor 方法呢。接下来我们就探索一下与分区相关的内容.</p>\n<h3 id=\"分区\"><a href=\"#分区\" class=\"headerlink\" title=\"分区\"></a>分区</h3><p>提到分区，大部分都是类似于“RDD 中最小的数据单元”，“只是数据的抽象分区，而不是真正的持有数据”等等这样的描述。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * An identifier for a partition in an RDD.</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">trait</span> <span class=\"title\">Partition</span> <span class=\"keyword\">extends</span> <span class=\"title\">Serializable</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">index</span></span>: <span class=\"type\">Int</span></span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hashCode</span></span>(): <span class=\"type\">Int</span> = index</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">equals</span></span>(other: <span class=\"type\">Any</span>): <span class=\"type\">Boolean</span> = <span class=\"keyword\">super</span>.equals(other)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可以看到分区的类十分的简单，只有一个 index 属性。 当将分区这个参数传入 itertor 时表示我们需要 index 对应的分区的数据经过一系列计算的之后的迭代器。那么显然，在源 RDD 中一定会有更据分区的index获取对应数据的代码部分。而我们又知道，对于<code>ParallelCollectionPartition</code>这个分区类是直接持有数据的迭代器，因此我们只需要知道这个类如何被建立，便知道了是如何根据分区获取数据的。我们在整个工程中搜索 ParallelCollectionPartition，发现:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPartitions</span></span>: <span class=\"type\">Array</span>[<span class=\"type\">Partition</span>] = &#123;</span><br><span class=\"line\">   <span class=\"keyword\">val</span> slices = <span class=\"type\">ParallelCollectionRDD</span>.slice(data, numSlices).toArray</span><br><span class=\"line\">   slices.indices.map(i =&gt; <span class=\"keyword\">new</span> <span class=\"type\">ParallelCollectionPartition</span>(id, i, slices(i))).toArray</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>中生成了一系列的分区对象。其中 ParallelCollectionRDD.slice() 方法根据总共有多少个分区来将 data  进行切分。而 data 我们在调用 parallelize() 方法时传入的。看到这里就应该明白了，<strong>ParallelCollectionRDD 会将我们输入的数组进行切分，然后将利用分区对象持有数据的迭代器，当调用到 itertor 方法时就返回该分区的迭代器，供后续的RDD</strong>进行计算。虽然这里我们只看了 ParallelCollectionRDD 的原代码，但是其他例如 HadoopRDD 的远离基本相同。 只不过一个是从内存中获取数据进行切分，另一个是从磁盘上获取数据进行切分。</p>\n<p>但是直到这里，我们仍然不知道 a 中的 itertor 方法的分区对象是如何传入的，因为这个方法间接被 b 的 itertor 方法调用。 而 b 的  itertor 方法同样也需要在外部被调用 ，因此要解决这个问题只需要找到 b 的 itertor 被调用的地方。不过我们首先可以根据代码猜测一下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span></span>(s: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>] = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">new</span> <span class=\"type\">InterruptibleIterator</span>(context,s.asInstanceOf[<span class=\"type\">ParallelCollectionPartition</span>[<span class=\"type\">T</span>]].iterator)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>a 中的 compute 方法直接将传入的分区对象转为了 ParallelCollectionPartition 并获取了对应的迭代器。根据对象间强转的</p>\n<p>规则，传入的分区对象只能是 ParallelCollectionPartition 类型 ，因为其父类 RDD 并没有 iterator 方法。 并且传入的ParallelCollectionPartition 一定是持有源数据迭代器的对象，否则在 a 的 compute 中就无法向后返回迭代器。而且我们知道，spark 的计算是<strong>惰性计算</strong>，在调用 action 算子之后才会真正的开始计算。那么可以猜测，b 的 iterator 也是在调用了 count 方法之后才被调用的。为了证明这一点，我们继续查看代码:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">count</span></span>(): <span class=\"type\">Long</span> = sc.runJob(<span class=\"keyword\">this</span>, <span class=\"type\">Utils</span>.getIteratorSize _).sum</span><br></pre></td></tr></table></figure>\n<p>可以看到是调用了 SparkContext 中的runJob方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">runJob</span></span>[<span class=\"type\">T</span>, <span class=\"type\">U</span>: <span class=\"type\">ClassTag</span>](</span><br><span class=\"line\">    ...</span><br><span class=\"line\">   dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)</span><br><span class=\"line\">   ...</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>而 SparkContext 中的 runJob 又继续调用了 DAGScheduler 中的 runJob 方法， 继而调用了 submitJob 方法。之后由经历了 DAGSchedulerEventProcessLoop.post DAGSchedulerEventProcessLoop.doOnReceive 等方法，最后在 submitMissingTasks 看到建立了 ResultTask 对象。由于一个分区的数据会被一个task 处理，因此我们猜测在 ＲesultTask 中会有关于对应 rdd 调用 iterator 的信息，果然，在ResultTask中找到了 runTask 方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">runTask</span></span>(context: <span class=\"type\">TaskContext</span>): <span class=\"type\">U</span> = &#123;</span><br><span class=\"line\">    <span class=\"comment\">// Deserialize the RDD and the func using the broadcast variables.</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> threadMXBean = <span class=\"type\">ManagementFactory</span>.getThreadMXBean</span><br><span class=\"line\">    <span class=\"keyword\">val</span> deserializeStartTime = <span class=\"type\">System</span>.currentTimeMillis()</span><br><span class=\"line\">    <span class=\"keyword\">val</span> deserializeStartCpuTime = <span class=\"keyword\">if</span> (threadMXBean.isCurrentThreadCpuTimeSupported) &#123;</span><br><span class=\"line\">      threadMXBean.getCurrentThreadCpuTime</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> <span class=\"number\">0</span>L</span><br><span class=\"line\">    <span class=\"keyword\">val</span> ser = <span class=\"type\">SparkEnv</span>.get.closureSerializer.newInstance()</span><br><span class=\"line\">    <span class=\"keyword\">val</span> (rdd, func) = ser.deserialize[(<span class=\"type\">RDD</span>[<span class=\"type\">T</span>], (<span class=\"type\">TaskContext</span>, <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>]) =&gt; <span class=\"type\">U</span>)](</span><br><span class=\"line\">      <span class=\"type\">ByteBuffer</span>.wrap(taskBinary.value), <span class=\"type\">Thread</span>.currentThread.getContextClassLoader)</span><br><span class=\"line\">    _executorDeserializeTime = <span class=\"type\">System</span>.currentTimeMillis() - deserializeStartTime</span><br><span class=\"line\">    _executorDeserializeCpuTime = <span class=\"keyword\">if</span> (threadMXBean.isCurrentThreadCpuTimeSupported) &#123;</span><br><span class=\"line\">      threadMXBean.getCurrentThreadCpuTime - deserializeStartCpuTime</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> <span class=\"number\">0</span>L</span><br><span class=\"line\"></span><br><span class=\"line\">    func(context, rdd.iterator(partition, context))<span class=\"comment\">//这里就 b 的iterator被真正调用的地方</span></span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>那么我们可以看到最后一行有一个rdd的调用，而这个rdd 是反徐序列化得的，很明显，这个rdd就是 b。此时，b 调用 iterator 方法的地方找到了，但是分区对象partition依然没有找到从哪里传入。由于 partition 是 ResultTask 构造时传入的，我们回到 submitMissingTasks 中，创建ResultTask时分区参数对应的为变量 part 。 </p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//从需要计算的分区map中获取分区，并生成task</span></span><br><span class=\"line\"> partitionsToCompute.map &#123; id =&gt;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> p: <span class=\"type\">Int</span> = stage.partitions(id)</span><br><span class=\"line\">      <span class=\"keyword\">val</span> part = partitions(p)</span><br><span class=\"line\">      <span class=\"keyword\">val</span> locs = taskIdToLocations(id)</span><br><span class=\"line\">      <span class=\"keyword\">new</span> <span class=\"type\">ResultTask</span>(stage.id, stage.latestInfo.attemptNumber,</span><br><span class=\"line\">        taskBinary, part, locs, id, properties, serializedTaskMetrics,</span><br><span class=\"line\">        <span class=\"type\">Option</span>(jobId), <span class=\"type\">Option</span>(sc.applicationId), sc.applicationAttemptId)</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>继续往上看，可以看到 part 是 从 partitions 中获取的。这里我们也可以看出 <strong>一个任务对应一个分区数据</strong>。继续往上看，发现<code>partitions = stage.rdd.partitions</code>。实际上来自于 Stage 中的 rdd 中的 partitions。那么看Stage 对 rdd 这个属性的描述:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> *</span><br><span class=\"line\"> * <span class=\"meta\">@param</span> rdd <span class=\"type\">RDD</span> that <span class=\"keyword\">this</span> stage runs on: <span class=\"keyword\">for</span> a shuffle map stage, it<span class=\"symbol\">'s</span> the <span class=\"type\">RDD</span> we run map tasks</span><br><span class=\"line\"> *   on, <span class=\"keyword\">while</span> <span class=\"keyword\">for</span> a result stage, it<span class=\"symbol\">'s</span> the target <span class=\"type\">RDD</span> that we ran an action on</span><br><span class=\"line\"> *   这里的意思是 rdd 表示调用 action 算子的 rdd 也就是 b </span><br><span class=\"line\"> */</span><br><span class=\"line\"><span class=\"keyword\">private</span>[scheduler] <span class=\"keyword\">abstract</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Stage</span>(<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val id: <span class=\"type\">Int</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val rdd: <span class=\"type\">RDD</span>[_],</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val numTasks: <span class=\"type\">Int</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val parents: <span class=\"type\">List</span>[<span class=\"type\">Stage</span>],</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val firstJobId: <span class=\"type\">Int</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val callSite: <span class=\"type\">CallSite</span></span>)</span></span><br><span class=\"line\"><span class=\"class\">  <span class=\"keyword\">extends</span> <span class=\"title\">Logging</span> </span>&#123;</span><br></pre></td></tr></table></figure>\n<p>我们发现一个惊人的事实 ，这个 rdd 居然也是 b。也就是说，我们在 runTask函数中实际调用的方法是这样的:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func(context, b.iterator(b.partitions(index), context))<span class=\"comment\">//这里就 b 的iterator被真正调用的地方</span></span><br></pre></td></tr></table></figure>\n<p>index 表示该task对应需要执行的分区标号。随即我们继续看 b 中的 partitions 的定义:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPartitions</span></span>: <span class=\"type\">Array</span>[<span class=\"type\">Partition</span>] = firstParent[<span class=\"type\">T</span>].partitions</span><br></pre></td></tr></table></figure>\n<p>说明这个 partitions 来自于 a。 而 a 中的 partitions 实际上又来自于:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPartitions</span></span>: <span class=\"type\">Array</span>[<span class=\"type\">Partition</span>] = &#123;</span><br><span class=\"line\">   <span class=\"keyword\">val</span> slices = <span class=\"type\">ParallelCollectionRDD</span>.slice(data, numSlices).toArray</span><br><span class=\"line\">   slices.indices.map(i =&gt; <span class=\"keyword\">new</span> <span class=\"type\">ParallelCollectionPartition</span>(id, i, slices(i))).toArray</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>这样最终 runTask 函数实际调用的方法为:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func(context, b.iterator(a.partitions(index), context))<span class=\"comment\">//这里就 b 的iterator被真正调用的地方</span></span><br></pre></td></tr></table></figure>\n<p>这和我们猜测的相同，这个分区确实为 ParallelCollectionPartition 对象，并且也持有了源数据的迭代器。这里其实有一个很巧妙的地方，虽然 RDD  的迭代计算是从后往前调用，但是传入源 RDD 的分区对象依然来自于源 RDD 自身。</p>\n<p>到了这里，我们也就明白分区对象是如何和数据关联起来的。ParallelCollectionPartition 对象中存在分区编号 index 以及 源数据的迭代器，通过分区编号就能获取到源数据的迭代器。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>经过如此大篇幅的讲解，终于对 spark 的迭代计算以及分区做了一个简单的分析。虽然还有很多地方不完善，例如还有其它类型的分区对象没有讲解，但是我相信你能完全看懂这篇文章所讲的内容，其他类型的分区对象对于你来说也很简单了。作者水平有限，并且这也是作者的第一篇关于 spark 源码阅读的博客，难免有遗漏和错误。如果想如给作者提建议和意见的话，请联系作者的微信或直接在 github 上提 issue 。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>首先立一个flag，这将是一个长期更新的版块。</p>\n<h2 id=\"写在最开始\"><a href=\"#写在最开始\" class=\"headerlink\" title=\"写在最开始\"></a>写在最开始</h2><p>在我使用<code>spark</code>进行日志分析的时候感受到了<code>spark</code>的便捷与强大。在学习<code>spark</code>初期，我阅读了许多与<code>spark</code>相关的文档，在这个过程中了解了<code>RDD</code>，<code>分区</code>，<code>shuffle</code>等概念，但是我并没有对这些概念有更多具体的认识。由于不了解<code>spark</code>的基本运作方式使得我在编写代码的时候十分困扰。由于这个原因，我准备阅读<code>spark</code>的源代码来解决我对基本概念的认识。</p>\n<h2 id=\"本部分主要内容\"><a href=\"#本部分主要内容\" class=\"headerlink\" title=\"本部分主要内容\"></a>本部分主要内容</h2><p>虽然该章节的名字叫做<strong>迭代计算</strong>，但是本章会讨论包括<strong>RDD、分区、Job、迭代计算</strong>等相关的内容，其中会重点讨论分区与迭代计算。因此在阅读本章之前你至少需要对这些提到的概念有一定的了解。</p>\n<h2 id=\"准备工作\"><a href=\"#准备工作\" class=\"headerlink\" title=\"准备工作\"></a>准备工作</h2><p>你需要准备一份 Spark 的源代码以及一个便于全局搜索的编辑器|IDE。</p>\n<h2 id=\"一个简单的例子\"><a href=\"#一个简单的例子\" class=\"headerlink\" title=\"一个简单的例子\"></a>一个简单的例子</h2><p>假设现在我们有这样一个需求 : 寻找从 0 to 100 的偶数并输出其总数。利用 spark 编写代码如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sc.parallelize(<span class=\"number\">0</span> to <span class=\"number\">100</span>).filter(_%<span class=\"number\">2</span> == <span class=\"number\">0</span>).count</span><br><span class=\"line\"><span class=\"comment\">//res0: Long = 51</span></span><br></pre></td></tr></table></figure>\n<p>这是一个非常简单的例子，但是里面却蕴涵了许多基础的知识。这一章的内容也是从这个例子展开的。</p>\n<h2 id=\"迭代计算\"><a href=\"#迭代计算\" class=\"headerlink\" title=\"迭代计算\"></a>迭代计算</h2><h3 id=\"comput-方法与-itertor\"><a href=\"#comput-方法与-itertor\" class=\"headerlink\" title=\"comput 方法与 itertor\"></a>comput 方法与 itertor</h3><p>在RDD.scala 定义的类 RDD中，有两个实现迭代计算的核心方法，分别是<code>compute</code>以及<code>itertor</code>方法。其中<code>itertor</code>方法如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">final</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">iterator</span></span>(split: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>] = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (storageLevel != <span class=\"type\">StorageLevel</span>.<span class=\"type\">NONE</span>) &#123;<span class=\"comment\">//如果有缓存</span></span><br><span class=\"line\">    getOrCompute(split, context)</span><br><span class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">    computeOrReadCheckpoint(split, context)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>我们只关心这个方法的第一个参数<strong>分区</strong>，以及返回的迭代器。忽略有缓存的情况，我们继续看<code>computeOrReadCheckpoint</code>这个方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span>[spark] <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">computeOrReadCheckpoint</span></span>(split: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>] =</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (isCheckpointedAndMaterialized) &#123;<span class=\"comment\">//如果有checkpoint</span></span><br><span class=\"line\">    firstParent[<span class=\"type\">T</span>].iterator(split, context)</span><br><span class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">    compute(split, context)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可以看到，在没有缓存和 checkpoint 的情况下，itertor 方法直接调用了 compute 方法。而compute方法定义如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\">  * Implemented by subclasses to compute a given partition.</span></span><br><span class=\"line\"><span class=\"comment\">  */</span></span><br><span class=\"line\"> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span></span>(split: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>]</span><br></pre></td></tr></table></figure>\n<p>从注释可以看出，compute 方法应该由子类实现，用以计算给定分区并生成一个迭代器。如果不考虑缓存和 checkpoint 的情况下，简化一下代码:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">final</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">iterator</span></span>(split: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>] = &#123;</span><br><span class=\"line\">   compute(split,context)</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>那么实际上 iterator 的功能是: <strong>接受一个分区，对这个分区进行计算，并返回计算结果的迭代器</strong>。而之所以 compute 需要子类来实现，是因为只需要在子类中实现不同的 compute 方法，就能产生不同类型的 RDD。既然不同的RDD 有不同的功能，我们想知道之前的例子是如何进行迭代计算的，就需要知道这个例子中涉及到了哪些 RDD。</p>\n<p>首先查看<code>SparkContext</code>中与<code>parallelize</code>相关的部分代码:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">parallelize</span></span>[<span class=\"type\">T</span>: <span class=\"type\">ClassTag</span>](</span><br><span class=\"line\">    seq: <span class=\"type\">Seq</span>[<span class=\"type\">T</span>],</span><br><span class=\"line\">    numSlices: <span class=\"type\">Int</span> = defaultParallelism): <span class=\"type\">RDD</span>[<span class=\"type\">T</span>] = withScope &#123;</span><br><span class=\"line\">  assertNotStopped()</span><br><span class=\"line\">  <span class=\"keyword\">new</span> <span class=\"type\">ParallelCollectionRDD</span>[<span class=\"type\">T</span>](<span class=\"keyword\">this</span>, seq, numSlices, <span class=\"type\">Map</span>[<span class=\"type\">Int</span>, <span class=\"type\">Seq</span>[<span class=\"type\">String</span>]]())</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可以看到，<code>parallelize</code>实际返回了一个<code>ParallelCollectionRDD</code>。在<code>ParallelCollectionRDD</code>中并没有对<code>filter</code>方法进行重写，因此我们查看<code>RDD</code>中的<code>filter</code>方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">filter</span></span>(f: <span class=\"type\">T</span> =&gt; <span class=\"type\">Boolean</span>): <span class=\"type\">RDD</span>[<span class=\"type\">T</span>] = withScope &#123;</span><br><span class=\"line\">   <span class=\"keyword\">val</span> cleanF = sc.clean(f)</span><br><span class=\"line\">   <span class=\"keyword\">new</span> <span class=\"type\">MapPartitionsRDD</span>[<span class=\"type\">T</span>, <span class=\"type\">T</span>](</span><br><span class=\"line\">     <span class=\"keyword\">this</span>,</span><br><span class=\"line\">     (context, pid, iter) =&gt; iter.filter(cleanF),</span><br><span class=\"line\">     preservesPartitioning = <span class=\"literal\">true</span>)</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>filter 方法返回了<code>MapPartitionsRDD</code>。为了方便起见，我将 ParallelCollectionRDD 类型的 RDD 称为 a，MapPartitionsRDD 类型的 RDD 成为B。那么先看一下 b 的 compute 方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span>[spark] <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MapPartitionsRDD</span>[<span class=\"type\">U</span>: <span class=\"type\">ClassTag</span>, <span class=\"type\">T</span>: <span class=\"type\">ClassTag</span>](<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    var prev: <span class=\"type\">RDD</span>[<span class=\"type\">T</span>],</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    f: (<span class=\"type\">TaskContext</span>, <span class=\"type\">Int</span>, <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>]</span>) <span class=\"title\">=&gt;</span> <span class=\"title\">Iterator</span>[<span class=\"type\">U</span>],  <span class=\"title\">//</span> (<span class=\"params\"><span class=\"type\">TaskContext</span>, partition index, iterator</span>)</span></span><br><span class=\"line\"><span class=\"class\">    <span class=\"title\">preservesPartitioning</span></span>: <span class=\"type\">Boolean</span> = <span class=\"literal\">false</span>)</span><br><span class=\"line\">  <span class=\"keyword\">extends</span> <span class=\"type\">RDD</span>[<span class=\"type\">U</span>](prev) &#123;</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"keyword\">val</span> partitioner = <span class=\"keyword\">if</span> (preservesPartitioning) firstParent[<span class=\"type\">T</span>].partitioner <span class=\"keyword\">else</span> <span class=\"type\">None</span></span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPartitions</span></span>: <span class=\"type\">Array</span>[<span class=\"type\">Partition</span>] = firstParent[<span class=\"type\">T</span>].partitions</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span></span>(split: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">U</span>] =</span><br><span class=\"line\">    f(context, split.index, firstParent[<span class=\"type\">T</span>].iterator(split, context))</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">clearDependencies</span></span>() &#123;</span><br><span class=\"line\">    <span class=\"keyword\">super</span>.clearDependencies()</span><br><span class=\"line\">    prev = <span class=\"literal\">null</span></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>这里可以看到 compute 方法实际上调用 f 这个函数。而 f 这个函数是在 filter 方法中传入的<code>(context, pid, iter) =&gt; iter.filter(cleanF)</code> 。那么实际上 compute 进行的计算为:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(context, split.index, firstParent[<span class=\"type\">T</span>].iterator(split, context)) =&gt; firstParent[<span class=\"type\">T</span>].iterator(split, context).filter(cleanF)</span><br></pre></td></tr></table></figure>\n<p>前两个参数并没有用到，也就是最终的方法可以简化为:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">firstParent[<span class=\"type\">T</span>].iterator(split, context) =&gt; firstParent[<span class=\"type\">T</span>].iterator(split, context).filter(cleanF)</span><br></pre></td></tr></table></figure>\n<p>这里出现了一个<code>firstParent</code>，我们知道 RDD 之间存在依赖关系，如果rdd3 依赖 rdd2，rdd2 依赖 rdd1。rdd2 与 rdd3 都是rdd1 的一个父rdd, spark也将其称为血缘关系。而故名意思 rdd2 是 rdd3 的 firstParent。在这个例子中 a 是 b的 firstParent。那么在b 的 compute 中又调用了 a 的 iterator 方法。而 a 的 iterator 方法又会调用a 的 comput 方法。用箭头表示调用关系:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">b.iterotr -&gt; b.compute -&gt; a.iterotr -&gt; a.compute -&gt;| 调用</span><br><span class=\"line\">b.iterotr &lt;- b.compute &lt;- a.iterotr &lt;- a.compute &lt;-| 返回</span><br></pre></td></tr></table></figure>\n<p>而这里我们也可以看出来，b 调用 iterotr 返回的数据，是使用 b 中的方法对 a 返回数据进行处理。也就是b 的计算结果依赖于a的计算结果。如果将一个rdd比作一个函数，大概就是 <code>fb(fa())</code>，这样的。此时，我们已经得到了关于 spark 迭代的最简单的模型，假如我们有 n 个 rdd。那么之间的调用依赖关系便是:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">n.iterotr -&gt; n.compute -&gt; (n-1).iterotr -&gt; (n-1).compute -&gt;...-&gt; 1.iterotr -&gt; 1.compute-&gt;| </span><br><span class=\"line\">n.iterotr &lt;- n.compute &lt;- (n-1).iterotr &lt;- (n-1).compute &lt;-...&lt;- 1.iterotr &lt;- 1.compute&lt;-|</span><br></pre></td></tr></table></figure>\n<p>那么细心的同学应该注意到了，所有的数据都是源自于第一个 RDD。这里把它称为源 RDD。例如本例中产生的 RDD 以及 textFile 方法产生的HadoopRDD都属于源RDD。既然属于源 RDD ，那么这个 RDD 一定会与内存或磁盘中的源数据产生交互，否则就没有真实的数据来源。这里的源数据是指内存的数组、本地文件、或者是HDFS上的分布式文件等。</p>\n<p>那么我们再继续看 a 的 compute 方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span></span>(s: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>] = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">new</span> <span class=\"type\">InterruptibleIterator</span>(context,s.asInstanceOf[<span class=\"type\">ParallelCollectionPartition</span>[<span class=\"type\">T</span>]].iterator)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可以看到 a 的 compute 根据 ParallelCollectionPartition 类的 iterator直接生成了一个迭代器。再看看ParallelCollectionPartition 的定义:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rivate[spark] <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ParallelCollectionPartition</span>[<span class=\"type\">T</span>: <span class=\"type\">ClassTag</span>](<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    var rddId: <span class=\"type\">Long</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    var slice: <span class=\"type\">Int</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    var values: <span class=\"type\">Seq</span>[<span class=\"type\">T</span>]</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">  </span>) <span class=\"keyword\">extends</span> <span class=\"title\">Partition</span> <span class=\"keyword\">with</span> <span class=\"title\">Serializable</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">iterator</span></span>: <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>] = values.iterator</span><br><span class=\"line\">  ...</span><br></pre></td></tr></table></figure>\n<p>iterator 是来自于 values 的一个迭代器，显然，values是内存中的数据。这说明 a 的 compute 方法直接返回了一个与源数据相关的迭代器。而这里 ParallelCollectionPartition 的数据是如何传入的，又是在什么时候被传入的呢?在<strong>分区</strong>小节将会提到。</p>\n<p>到这里，我们已经基本了解了 spark 的迭代计算的原理以及数据来源。但是仍然还有许多不了解的地方，例如 iterotr 会接受一个  Partition 作为参数。那么这个Partition参数到底起到了什么作用。其次，我们知道 rdd 的 iterotr 方法 是从后开始往前调用，那么又是谁调用的最后一个 rdd 的 itertor 方法呢。接下来我们就探索一下与分区相关的内容.</p>\n<h3 id=\"分区\"><a href=\"#分区\" class=\"headerlink\" title=\"分区\"></a>分区</h3><p>提到分区，大部分都是类似于“RDD 中最小的数据单元”，“只是数据的抽象分区，而不是真正的持有数据”等等这样的描述。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * An identifier for a partition in an RDD.</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">trait</span> <span class=\"title\">Partition</span> <span class=\"keyword\">extends</span> <span class=\"title\">Serializable</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">index</span></span>: <span class=\"type\">Int</span></span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hashCode</span></span>(): <span class=\"type\">Int</span> = index</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">equals</span></span>(other: <span class=\"type\">Any</span>): <span class=\"type\">Boolean</span> = <span class=\"keyword\">super</span>.equals(other)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可以看到分区的类十分的简单，只有一个 index 属性。 当将分区这个参数传入 itertor 时表示我们需要 index 对应的分区的数据经过一系列计算的之后的迭代器。那么显然，在源 RDD 中一定会有更据分区的index获取对应数据的代码部分。而我们又知道，对于<code>ParallelCollectionPartition</code>这个分区类是直接持有数据的迭代器，因此我们只需要知道这个类如何被建立，便知道了是如何根据分区获取数据的。我们在整个工程中搜索 ParallelCollectionPartition，发现:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPartitions</span></span>: <span class=\"type\">Array</span>[<span class=\"type\">Partition</span>] = &#123;</span><br><span class=\"line\">   <span class=\"keyword\">val</span> slices = <span class=\"type\">ParallelCollectionRDD</span>.slice(data, numSlices).toArray</span><br><span class=\"line\">   slices.indices.map(i =&gt; <span class=\"keyword\">new</span> <span class=\"type\">ParallelCollectionPartition</span>(id, i, slices(i))).toArray</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>中生成了一系列的分区对象。其中 ParallelCollectionRDD.slice() 方法根据总共有多少个分区来将 data  进行切分。而 data 我们在调用 parallelize() 方法时传入的。看到这里就应该明白了，<strong>ParallelCollectionRDD 会将我们输入的数组进行切分，然后将利用分区对象持有数据的迭代器，当调用到 itertor 方法时就返回该分区的迭代器，供后续的RDD</strong>进行计算。虽然这里我们只看了 ParallelCollectionRDD 的原代码，但是其他例如 HadoopRDD 的远离基本相同。 只不过一个是从内存中获取数据进行切分，另一个是从磁盘上获取数据进行切分。</p>\n<p>但是直到这里，我们仍然不知道 a 中的 itertor 方法的分区对象是如何传入的，因为这个方法间接被 b 的 itertor 方法调用。 而 b 的  itertor 方法同样也需要在外部被调用 ，因此要解决这个问题只需要找到 b 的 itertor 被调用的地方。不过我们首先可以根据代码猜测一下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute</span></span>(s: <span class=\"type\">Partition</span>, context: <span class=\"type\">TaskContext</span>): <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>] = &#123;</span><br><span class=\"line\">  <span class=\"keyword\">new</span> <span class=\"type\">InterruptibleIterator</span>(context,s.asInstanceOf[<span class=\"type\">ParallelCollectionPartition</span>[<span class=\"type\">T</span>]].iterator)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>a 中的 compute 方法直接将传入的分区对象转为了 ParallelCollectionPartition 并获取了对应的迭代器。根据对象间强转的</p>\n<p>规则，传入的分区对象只能是 ParallelCollectionPartition 类型 ，因为其父类 RDD 并没有 iterator 方法。 并且传入的ParallelCollectionPartition 一定是持有源数据迭代器的对象，否则在 a 的 compute 中就无法向后返回迭代器。而且我们知道，spark 的计算是<strong>惰性计算</strong>，在调用 action 算子之后才会真正的开始计算。那么可以猜测，b 的 iterator 也是在调用了 count 方法之后才被调用的。为了证明这一点，我们继续查看代码:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">count</span></span>(): <span class=\"type\">Long</span> = sc.runJob(<span class=\"keyword\">this</span>, <span class=\"type\">Utils</span>.getIteratorSize _).sum</span><br></pre></td></tr></table></figure>\n<p>可以看到是调用了 SparkContext 中的runJob方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">runJob</span></span>[<span class=\"type\">T</span>, <span class=\"type\">U</span>: <span class=\"type\">ClassTag</span>](</span><br><span class=\"line\">    ...</span><br><span class=\"line\">   dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)</span><br><span class=\"line\">   ...</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>而 SparkContext 中的 runJob 又继续调用了 DAGScheduler 中的 runJob 方法， 继而调用了 submitJob 方法。之后由经历了 DAGSchedulerEventProcessLoop.post DAGSchedulerEventProcessLoop.doOnReceive 等方法，最后在 submitMissingTasks 看到建立了 ResultTask 对象。由于一个分区的数据会被一个task 处理，因此我们猜测在 ＲesultTask 中会有关于对应 rdd 调用 iterator 的信息，果然，在ResultTask中找到了 runTask 方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">runTask</span></span>(context: <span class=\"type\">TaskContext</span>): <span class=\"type\">U</span> = &#123;</span><br><span class=\"line\">    <span class=\"comment\">// Deserialize the RDD and the func using the broadcast variables.</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> threadMXBean = <span class=\"type\">ManagementFactory</span>.getThreadMXBean</span><br><span class=\"line\">    <span class=\"keyword\">val</span> deserializeStartTime = <span class=\"type\">System</span>.currentTimeMillis()</span><br><span class=\"line\">    <span class=\"keyword\">val</span> deserializeStartCpuTime = <span class=\"keyword\">if</span> (threadMXBean.isCurrentThreadCpuTimeSupported) &#123;</span><br><span class=\"line\">      threadMXBean.getCurrentThreadCpuTime</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> <span class=\"number\">0</span>L</span><br><span class=\"line\">    <span class=\"keyword\">val</span> ser = <span class=\"type\">SparkEnv</span>.get.closureSerializer.newInstance()</span><br><span class=\"line\">    <span class=\"keyword\">val</span> (rdd, func) = ser.deserialize[(<span class=\"type\">RDD</span>[<span class=\"type\">T</span>], (<span class=\"type\">TaskContext</span>, <span class=\"type\">Iterator</span>[<span class=\"type\">T</span>]) =&gt; <span class=\"type\">U</span>)](</span><br><span class=\"line\">      <span class=\"type\">ByteBuffer</span>.wrap(taskBinary.value), <span class=\"type\">Thread</span>.currentThread.getContextClassLoader)</span><br><span class=\"line\">    _executorDeserializeTime = <span class=\"type\">System</span>.currentTimeMillis() - deserializeStartTime</span><br><span class=\"line\">    _executorDeserializeCpuTime = <span class=\"keyword\">if</span> (threadMXBean.isCurrentThreadCpuTimeSupported) &#123;</span><br><span class=\"line\">      threadMXBean.getCurrentThreadCpuTime - deserializeStartCpuTime</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> <span class=\"number\">0</span>L</span><br><span class=\"line\"></span><br><span class=\"line\">    func(context, rdd.iterator(partition, context))<span class=\"comment\">//这里就 b 的iterator被真正调用的地方</span></span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>那么我们可以看到最后一行有一个rdd的调用，而这个rdd 是反徐序列化得的，很明显，这个rdd就是 b。此时，b 调用 iterator 方法的地方找到了，但是分区对象partition依然没有找到从哪里传入。由于 partition 是 ResultTask 构造时传入的，我们回到 submitMissingTasks 中，创建ResultTask时分区参数对应的为变量 part 。 </p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//从需要计算的分区map中获取分区，并生成task</span></span><br><span class=\"line\"> partitionsToCompute.map &#123; id =&gt;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> p: <span class=\"type\">Int</span> = stage.partitions(id)</span><br><span class=\"line\">      <span class=\"keyword\">val</span> part = partitions(p)</span><br><span class=\"line\">      <span class=\"keyword\">val</span> locs = taskIdToLocations(id)</span><br><span class=\"line\">      <span class=\"keyword\">new</span> <span class=\"type\">ResultTask</span>(stage.id, stage.latestInfo.attemptNumber,</span><br><span class=\"line\">        taskBinary, part, locs, id, properties, serializedTaskMetrics,</span><br><span class=\"line\">        <span class=\"type\">Option</span>(jobId), <span class=\"type\">Option</span>(sc.applicationId), sc.applicationAttemptId)</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>继续往上看，可以看到 part 是 从 partitions 中获取的。这里我们也可以看出 <strong>一个任务对应一个分区数据</strong>。继续往上看，发现<code>partitions = stage.rdd.partitions</code>。实际上来自于 Stage 中的 rdd 中的 partitions。那么看Stage 对 rdd 这个属性的描述:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> *</span><br><span class=\"line\"> * <span class=\"meta\">@param</span> rdd <span class=\"type\">RDD</span> that <span class=\"keyword\">this</span> stage runs on: <span class=\"keyword\">for</span> a shuffle map stage, it<span class=\"symbol\">'s</span> the <span class=\"type\">RDD</span> we run map tasks</span><br><span class=\"line\"> *   on, <span class=\"keyword\">while</span> <span class=\"keyword\">for</span> a result stage, it<span class=\"symbol\">'s</span> the target <span class=\"type\">RDD</span> that we ran an action on</span><br><span class=\"line\"> *   这里的意思是 rdd 表示调用 action 算子的 rdd 也就是 b </span><br><span class=\"line\"> */</span><br><span class=\"line\"><span class=\"keyword\">private</span>[scheduler] <span class=\"keyword\">abstract</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Stage</span>(<span class=\"params\"></span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val id: <span class=\"type\">Int</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val rdd: <span class=\"type\">RDD</span>[_],</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val numTasks: <span class=\"type\">Int</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val parents: <span class=\"type\">List</span>[<span class=\"type\">Stage</span>],</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val firstJobId: <span class=\"type\">Int</span>,</span></span></span><br><span class=\"line\"><span class=\"class\"><span class=\"params\">    val callSite: <span class=\"type\">CallSite</span></span>)</span></span><br><span class=\"line\"><span class=\"class\">  <span class=\"keyword\">extends</span> <span class=\"title\">Logging</span> </span>&#123;</span><br></pre></td></tr></table></figure>\n<p>我们发现一个惊人的事实 ，这个 rdd 居然也是 b。也就是说，我们在 runTask函数中实际调用的方法是这样的:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func(context, b.iterator(b.partitions(index), context))<span class=\"comment\">//这里就 b 的iterator被真正调用的地方</span></span><br></pre></td></tr></table></figure>\n<p>index 表示该task对应需要执行的分区标号。随即我们继续看 b 中的 partitions 的定义:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPartitions</span></span>: <span class=\"type\">Array</span>[<span class=\"type\">Partition</span>] = firstParent[<span class=\"type\">T</span>].partitions</span><br></pre></td></tr></table></figure>\n<p>说明这个 partitions 来自于 a。 而 a 中的 partitions 实际上又来自于:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPartitions</span></span>: <span class=\"type\">Array</span>[<span class=\"type\">Partition</span>] = &#123;</span><br><span class=\"line\">   <span class=\"keyword\">val</span> slices = <span class=\"type\">ParallelCollectionRDD</span>.slice(data, numSlices).toArray</span><br><span class=\"line\">   slices.indices.map(i =&gt; <span class=\"keyword\">new</span> <span class=\"type\">ParallelCollectionPartition</span>(id, i, slices(i))).toArray</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>这样最终 runTask 函数实际调用的方法为:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func(context, b.iterator(a.partitions(index), context))<span class=\"comment\">//这里就 b 的iterator被真正调用的地方</span></span><br></pre></td></tr></table></figure>\n<p>这和我们猜测的相同，这个分区确实为 ParallelCollectionPartition 对象，并且也持有了源数据的迭代器。这里其实有一个很巧妙的地方，虽然 RDD  的迭代计算是从后往前调用，但是传入源 RDD 的分区对象依然来自于源 RDD 自身。</p>\n<p>到了这里，我们也就明白分区对象是如何和数据关联起来的。ParallelCollectionPartition 对象中存在分区编号 index 以及 源数据的迭代器，通过分区编号就能获取到源数据的迭代器。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>经过如此大篇幅的讲解，终于对 spark 的迭代计算以及分区做了一个简单的分析。虽然还有很多地方不完善，例如还有其它类型的分区对象没有讲解，但是我相信你能完全看懂这篇文章所讲的内容，其他类型的分区对象对于你来说也很简单了。作者水平有限，并且这也是作者的第一篇关于 spark 源码阅读的博客，难免有遗漏和错误。如果想如给作者提建议和意见的话，请联系作者的微信或直接在 github 上提 issue 。</p>\n"},{"title":"spark源码阅读计划 - 第二部分 - shuffle write","date":"2018-06-08T05:46:45.000Z","author":"lishion","toc":true,"_content":"## 写在开始的话\n\nshuffle 作为 spark 最重要的一部分，也是很多初学者感到头痛的一部分。简单来说，shuffle 分为两个部分 : shuffle write 和 shuffle read。shuffle write 在 map 端进行，而shuffle read 在 reduce 端进行。本章就准备就 shuffle write 做一个详细的分析。\n\n## 为什么需要shuffle\n\n在上一章[Spark-源码阅读计划-第一部分-迭代计算](https://lishion.github.io/2018/06/06/Spark-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E8%AE%A1%E5%88%92-%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86-%E8%BF%AD%E4%BB%A3%E8%AE%A1%E7%AE%97/)提到过每一个分区的数据最后都会由一个task来处理，那么当task 执行的类似于 reduceByKey 这种算子的时候一定需要满足相同的 key 在同一个分区这个条件，否则就不能按照 key 进行reduce了。但是当我们从数据源获取数据并进行处理后，相同的 key 不一定在同一个分区。那么在一个 map 端将相同的key 使用某种方式聚集在同一个分区并写入临时文件的过程就称为 shuffle write；而在 reduce 从 map 拉去执行 task 所需要的数据就是 shuffle read；这两个步骤组成了shuffle。\n\n## 一个小例子\n\n本章同样以一个小例子作为本章讲解的中心，例如一个统计词频的代码:\n\n```scala\nsc.parallelize(List(\"hello\",\"world\",\"hello\",\"spark\")).map(x=>(x,1)).reduceByKey(_+_)\n```\n\n相信有一点基础的同学都知道 reduceByKey 这算子会产生 shuffle。但是大部分的同学都不知 shuffle 是什么时候产生的，shuffle 到底做了些什么，这也是这一章的重点。\n\n## shuffle task\n\n在上一章提到过当 job 提交后会生成 result task。而在需要 shuffle 则会生成 ShuffleMapTask。这里涉及到 job 提交以及 stage 生成的的知识，将会在下一章提到。这里我们直接开始看 ShuffleMapTask 中的 runtask :\n\n```scala\n override def runTask(context: TaskContext): MapStatus = {\n    // Deserialize the RDD using the broadcast variable.\n ...\n    var writer: ShuffleWriter[Any, Any] = null\n    try {\n      val manager = SparkEnv.get.shuffleManager\n      writer = manager.getWriter[Any, Any](dep.shuffleHandle, partitionId, context)\n       // 这里依然是rdd调用iterator方法的地方\n      writer.write(rdd.iterator(partition, context).asInstanceOf[Iterator[_ <: Product2[Any, Any]]])\n      writer.stop(success = true).get\n    } catch {\n      case e: Exception =>\n        try {\n          if (writer != null) {\n            writer.stop(success = false)\n          }\n        } catch {\n          case e: Exception =>\n            log.debug(\"Could not stop writer\", e)\n        }\n        throw e\n    }\n  }\n```\n\n可以看到首先获得一个 shuffleWriter。spark 中有许多不同类型的 writer ，默认使用的是 SortShuffleWriter。SortShuffleWriter 中的 write 方法实现了 shuffle write。源代码如下:\n\n```scala\n  override def write(records: Iterator[Product2[K, V]]): Unit = {\n      //如果需要 map 端聚\n    sorter = if (dep.mapSideCombine) {\n      new ExternalSorter[K, V, C](\n        context, dep.aggregator, Some(dep.partitioner), dep.keyOrdering, dep.serializer)\n    } else {\n      new ExternalSorter[K, V, V](\n        context, aggregator = None, Some(dep.partitioner), ordering = None, dep.serializer)\n    }\n    sorter.insertAll(records)\n    val output = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)\n    val tmp = Utils.tempFileWith(output)\n    try { \n      val blockId = ShuffleBlockId(dep.shuffleId, mapId, IndexShuffleBlockResolver.NOOP_REDUCE_ID)\n      val partitionLengths = sorter.writePartitionedFile(blockId, tmp)\n      shuffleBlockResolver.writeIndexFileAndCommit(dep.shuffleId, mapId, partitionLengths, tmp)\n      mapStatus = MapStatus(blockManager.shuffleServerId, partitionLengths)\n    } finally {\n      if (tmp.exists() && !tmp.delete()) {\n        logError(s\"Error while deleting temp file ${tmp.getAbsolutePath}\")\n      }\n    }\n  }\n```\n\n使用了 ExternalSorter 的 insertAll 方法插入了上一个 RDD 生成的数据的迭代器。\n\n#### insertAll\n\n```scala\ndef insertAll(records: Iterator[Product2[K, V]]): Unit = {\n    // TODO: stop combining if we find that the reduction factor isn't high\n    val shouldCombine = aggregator.isDefined\n    // 直接看需要 map 端聚合的情况\n    // 不需要聚合的情况类似\n    if (shouldCombine) {\n      // Combine values in-memory first using our AppendOnlyMap\n       \n      val mergeValue = aggregator.get.mergeValue \n      val createCombiner = aggregator.get.createCombiner\n      var kv: Product2[K, V] = null\n      // 如果 key 不存在则直接创建 Combiner ，否则使用 mergeValue 将 value 添加到创建的 Combiner中\n      val update = (hadValue: Boolean, oldValue: C) => {\n        if (hadValue) mergeValue(oldValue, kv._2) else createCombiner(kv._2)\n      }\n      while (records.hasNext) {\n        addElementsRead()\n        kv = records.next()\n        // 更新值\n        // 这里需要注意的是之前的数据是(k,v)类型，这里转换成了((part,k),v)\n        // 其中 part 是 key 对应的分区\n        map.changeValue((getPartition(kv._1), kv._1), update)\n        maybeSpillCollection(usingMap = true)\n      }\n    } else {\n      // Stick values into our buffer\n      while (records.hasNext) {\n        addElementsRead()\n        val kv = records.next()\n        buffer.insert(getPartition(kv._1), kv._1, kv._2.asInstanceOf[C])\n        maybeSpillCollection(usingMap = false)\n      }\n    }\n  }\n```\n\n在这里使用 getPartition() 进行了数据的**分区**，也就是将不同的 key 与其分区对应起来。默认使用的是 HashPartitioner ，有兴趣的同学可以去阅读源码。再继续看 changeValue 方法。在这里使用非常重要的数据结构来存放分区后的数据，也就是代码中的 map 类型为 PartitionedAppendOnlyMap。其中是使用hask表存放数据，并且存放的方式为表中的偶数索index引存放key, index + 1 存放对应的value。例如:\n\n```\nk1|v1|null|k2|v2|k3|v3|null|....\n```\n\n之所以里面由空值，是因为在插入的时候使用了二次探测法。来看一看 changeValue 方法:\n\nPartitionedAppendOnlyMap 中的  changeValue 方法继承自 \n\n```scala\n  def changeValue(key: K, updateFunc: (Boolean, V) => V): V = {\n    assert(!destroyed, destructionMessage)\n    val k = key.asInstanceOf[AnyRef]\n    if (k.eq(null)) {\n      if (!haveNullValue) {\n        incrementSize()\n      }\n      nullValue = updateFunc(haveNullValue, nullValue)\n      haveNullValue = true\n      return nullValue\n    }\n    var pos = rehash(k.hashCode) & mask\n    var i = 1\n    while (true) {\n      // 偶数位置为 key\n      val curKey = data(2 * pos)\n       // 如果该key不存在，就直接插入\n      if (curKey.eq(null)) {\n        val newValue = updateFunc(false, null.asInstanceOf[V])\n        data(2 * pos) = k\n        data(2 * pos + 1) = newValue.asInstanceOf[AnyRef]\n        incrementSize()\n        return newValue\n      } else if (k.eq(curKey) || k.equals(curKey)) {//如果 key 存在，则进行聚合\n        \n        val newValue = updateFunc(true, data(2 * pos + 1).asInstanceOf[V])\n        data(2 * pos + 1) = newValue.asInstanceOf[AnyRef]\n        return newValue\n      } else { // 否则进行下一次探测\n        val delta = i\n        pos = (pos + delta) & mask\n        i += 1\n      }\n    }\n    null.asInstanceOf[V] // Never reached but needed to keep compiler happy\n  }\n```\n\n再调用 insertAll 进行了数据的聚合|插入之后，调用了 maybeSpillCollection 方法。这个方法的作用是判断 map 占用了内存，如果内存达到一定的值，则将内存中的数据 spill 到临时文件中。\n\n```scala\nprivate def maybeSpillCollection(usingMap: Boolean): Unit = {\n    var estimatedSize = 0L\n    if (usingMap) {\n      estimatedSize = map.estimateSize()\n      if (maybeSpill(map, estimatedSize)) {\n        map = new PartitionedAppendOnlyMap[K, C]\n      }\n    } else {\n      estimatedSize = buffer.estimateSize()\n      if (maybeSpill(buffer, estimatedSize)) {\n        buffer = new PartitionedPairBuffer[K, C]\n      }\n    }\n\n    if (estimatedSize > _peakMemoryUsedBytes) {\n      _peakMemoryUsedBytes = estimatedSize\n    }\n  }\n```\n\nmaybeSpillCollection 继续调用了 maybeSpill 方法，这个方法继承自 Spillable 中\n\n```scala\nprotected def maybeSpill(collection: C, currentMemory: Long): Boolean = {\n    var shouldSpill = false\n    if (elementsRead % 32 == 0 && currentMemory >= myMemoryThreshold) {\n      // Claim up to double our current memory from the shuffle memory pool\n      val amountToRequest = 2 * currentMemory - myMemoryThreshold\n      val granted = acquireMemory(amountToRequest)\n      myMemoryThreshold += granted\n      // If we were granted too little memory to grow further (either tryToAcquire returned 0,\n      // or we already had more memory than myMemoryThreshold), spill the current collection\n      shouldSpill = currentMemory >= myMemoryThreshold\n    }\n    shouldSpill = shouldSpill || _elementsRead > numElementsForceSpillThreshold\n    // Actually spill\n    if (shouldSpill) {\n      _spillCount += 1\n      logSpillage(currentMemory)\n      spill(collection)\n      _elementsRead = 0\n      _memoryBytesSpilled += currentMemory\n      releaseMemory()\n    }\n    shouldSpill\n  }\n```\n\n该方法首先判断了是否需要 spill，如果需要则调用 spill() 方法将内存中的数据写入临时文件中。spill 方法如下:\n\n```scala\n override protected[this] def spill(collection: WritablePartitionedPairCollection[K, C]): Unit = {\n    val inMemoryIterator = collection.destructiveSortedWritablePartitionedIterator(comparator)\n    val spillFile = spillMemoryIteratorToDisk(inMemoryIterator)\n    spills += spillFile\n  }\n```\n\n其中的  comparator 如下:\n\n```scala\n private def comparator: Option[Comparator[K]] = {\n    if (ordering.isDefined || aggregator.isDefined) {\n      Some(keyComparator)\n    } else {\n      None\n    }\n  }\n```\n\n也就是:\n\n```scala\n  private val keyComparator: Comparator[K] = ordering.getOrElse(new Comparator[K] {\n    override def compare(a: K, b: K): Int = {\n      val h1 = if (a == null) 0 else a.hashCode()\n      val h2 = if (b == null) 0 else b.hashCode()\n      if (h1 < h2) -1 else if (h1 == h2) 0 else 1\n    }\n  })\n```\n\ndestructiveSortedWritablePartitionedIterator 位于 WritablePartitionedPairCollection 类中，实现如下:\n\n```scala\ndef destructiveSortedWritablePartitionedIterator(keyComparator: Option[Comparator[K]])\n    : WritablePartitionedIterator = {\n    //这里调用的其实是　PartitionedAppendOnlyMap　中重写的　partitionedDestructiveSortedIterator    \n    val it = partitionedDestructiveSortedIterator(keyComparator)\n    // 这里还实现了一个　writeNext　的方法，后面会用到\n    new WritablePartitionedIterator {\n      private[this] var cur = if (it.hasNext) it.next() else null\n       // 在map 中的数据其实是((partition,k),v)\n       // 这里只写入了(k,v)\n      def writeNext(writer: DiskBlockObjectWriter): Unit = {\n        writer.write(cur._1._2, cur._2)\n        cur = if (it.hasNext) it.next() else null\n      }\n      def hasNext(): Boolean = cur != null\n      def nextPartition(): Int = cur._1._1\n    }\n  }\n```\n\n接下来看看partitionedDestructiveSortedIterator:\n\n```scala\ndef partitionedDestructiveSortedIterator(keyComparator: Option[Comparator[K]])\n    : Iterator[((Int, K), V)] = {\n    val comparator = keyComparator.map(partitionKeyComparator).getOrElse(partitionComparator)\n    destructiveSortedIterator(comparator)\n }\n```\n\nkeyComparator.map(partitionKeyComparator)　等于 partitionKeyComparator(keyComparator)，再看下面的partitionKeyComparator，说明比较器是先对分区进行比较，如果分区相同，再对key的hash值进行比较:\n\n```scala\n def partitionKeyComparator[K](keyComparator: Comparator[K]): Comparator[(Int, K)] = {\n    new Comparator[(Int, K)] {\n      override def compare(a: (Int, K), b: (Int, K)): Int = {\n        val partitionDiff = a._1 - b._1\n        if (partitionDiff != 0) {\n          partitionDiff\n        } else {\n          keyComparator.compare(a._2, b._2)\n        }\n      }\n    }\n  }\n```\n\n然后看destructiveSortedIterator(),这是在 PartitionedAppendOnlyMap 的父类 destructiveSortedIterator 中实现的:\n\n```scala\n def destructiveSortedIterator(keyComparator: Comparator[K]): Iterator[(K, V)] = {\n    destroyed = true\n    // Pack KV pairs into the front of the underlying array\n    // 这里是因为　PartitionedAppendOnlyMap　采用的二次探测法　,kv对之间存在　null值，所以先把非空的kv对全部移动到hash表的前端\n    var keyIndex, newIndex = 0\n    while (keyIndex < capacity) {\n      if (data(2 * keyIndex) != null) {\n        data(2 * newIndex) = data(2 * keyIndex)\n        data(2 * newIndex + 1) = data(2 * keyIndex + 1)\n        newIndex += 1\n      }\n      keyIndex += 1\n    }\n    assert(curSize == newIndex + (if (haveNullValue) 1 else 0))\n    // 这里对给定数据范围为 0 to newIndex，利用　keyComparator　比较器进行排序\n    // 也就是对 map 中的数据根据　(partition,key) 进行排序\n    new Sorter(new KVArraySortDataFormat[K, AnyRef]).sort(data, 0, newIndex, keyComparator)\n\n    // 定义迭代器\n    new Iterator[(K, V)] {\n      var i = 0\n      var nullValueReady = haveNullValue\n      def hasNext: Boolean = (i < newIndex || nullValueReady)\n      def next(): (K, V) = {\n        if (nullValueReady) {\n          nullValueReady = false\n          (null.asInstanceOf[K], nullValue)\n        } else {\n          val item = (data(2 * i).asInstanceOf[K], data(2 * i + 1).asInstanceOf[V])\n          i += 1\n          item\n        }\n      }\n    }\n  }\n```\n\n可以看出整个　`val inMemoryIterator = collection.destructiveSortedWritablePartitionedIterator(comparator)`　这一步其实就是在对　map　或 buffer 进行排序，并且实现了一个 writeNext() 方法供后续调调用。随后调用`val spillFile = spillMemoryIteratorToDisk(inMemoryIterator)`将排序后的缓存写入文件中:\n\n```scala\nprivate[this] def spillMemoryIteratorToDisk(inMemoryIterator: WritablePartitionedIterator)\n      : SpilledFile = {\n   \n    val (blockId, file) = diskBlockManager.createTempShuffleBlock()\n    // These variables are reset after each flush\n    var objectsWritten: Long = 0\n    val spillMetrics: ShuffleWriteMetrics = new ShuffleWriteMetrics\n    val writer: DiskBlockObjectWriter =\n      blockManager.getDiskWriter(blockId, file, serInstance, fileBufferSize, spillMetrics)\n    // List of batch sizes (bytes) in the order they are written to disk\n    val batchSizes = new ArrayBuffer[Long]\n    // How many elements we have in each partition\n    // 用于记录每一个分区有多少条数据\n    // 由于刚才数据写入文件没有写入key对应的分区，因此需要记录每一个分区有多少条数据，然后根据数据的偏移量就可以判断该数据属于哪个分区　\n    val elementsPerPartition = new Array[Long](numPartitions)\n    // Flush the disk writer's contents to disk, and update relevant variables.\n    // The writer is committed at the end of this process.\n    def flush(): Unit = {\n      val segment = writer.commitAndGet()\n      batchSizes += segment.length\n      _diskBytesSpilled += segment.length\n      objectsWritten = 0\n    }\n    var success = false\n    try {\n      while (inMemoryIterator.hasNext) {\n        val partitionId = inMemoryIterator.nextPartition()\n        require(partitionId >= 0 && partitionId < numPartitions,\n          s\"partition Id: ${partitionId} should be in the range [0, ${numPartitions})\")\n        inMemoryIterator.writeNext(writer)\n        elementsPerPartition(partitionId) += 1\n        objectsWritten += 1\n        if (objectsWritten == serializerBatchSize) {　//写入　serializerBatchSize　条数据便刷新一次缓存\n          // batchSize 在类中定义的如下:\n          // 可以看出如果不存在配置默认为　10000　条\n          // private val serializerBatchSize = conf.getLong(\"spark.shuffle.spill.batchSize\", 10000)\n          flush()\n        }\n      }\n      if (objectsWritten > 0) {\n        flush()\n      } else {\n        writer.revertPartialWritesAndClose()\n      }\n      success = true\n    } finally {\n      if (success) {\n        writer.close()\n      } else {\n        // This code path only happens if an exception was thrown above before we set success;\n        // close our stuff and let the exception be thrown further\n        writer.revertPartialWritesAndClose()\n        if (file.exists()) {\n          if (!file.delete()) {\n            logWarning(s\"Error deleting ${file}\")\n          }\n        }\n      }\n    }\n    //最后记录临时文件的信息\n    SpilledFile(file, blockId, batchSizes.toArray, elementsPerPartition)\n\n  }\n```\n\n这就是整个　insertAll()　方法:\n\n merge 达到溢出值后进行排序并写入临时文件，这里要注意的是并非所有的缓存文件都被写入到临时文件中，所以接下来需要将临时文件中的数据加上内存中的数据进行一次排序写入到一个大文件中。\n\n```scala\n//首先获取需要写入的文件:　\n   val output = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)\n　　val tmp = Utils.tempFileWith(output)\n    try { \n      val blockId = ShuffleBlockId(dep.shuffleId, mapId, IndexShuffleBlockResolver.NOOP_REDUCE_ID)\n      val partitionLengths = sorter.writePartitionedFile(blockId, tmp)//这一句是重点 下面会讲解\n      shuffleBlockResolver.writeIndexFileAndCommit(dep.shuffleId, mapId, partitionLengths, tmp)\n      mapStatus = MapStatus(blockManager.shuffleServerId, partitionLengths)\n    } finally {\n      if (tmp.exists() && !tmp.delete()) {\n        logError(s\"Error while deleting temp file ${tmp.getAbsolutePath}\")\n    }\n```\n#### writePartitionedFile\n\n继续看 writePartitionedFile :\n\n```scala\n//  ExternalSorter 中的　writePartitionedFile　方法\n   def writePartitionedFile(\n      blockId: BlockId,\n      outputFile: File): Array[Long] = {\n\n    // Track location of each range in the output file\n    val lengths = new Array[Long](numPartitions)\n    val writer = blockManager.getDiskWriter(blockId, outputFile, serInstance, fileBufferSize,\n      context.taskMetrics().shuffleWriteMetrics)\n\n    // 这里的spills 就是　刚才产生的临时文件的数据　如果为空逻辑就比较简单，直接进行排序并写入文件\n    if (spills.isEmpty) {\n      // Case where we only have in-memory data\n      val collection = if (aggregator.isDefined) map else buffer\n      val it = collection.destructiveSortedWritablePartitionedIterator(comparator)　//这里同样使用　destructiveSortedWritablePartitionedIterator　进行排序\n      while (it.hasNext) {\n        val partitionId = it.nextPartition()\n        while (it.hasNext && it.nextPartition() == partitionId) {\n          it.writeNext(writer)\n        }\n        val segment = writer.commitAndGet()\n        lengths(partitionId) = segment.length\n      }\n    } else {//重点看不为空的时候，这里调用了　partitionedIterator　方法\n      // We must perform merge-sort; get an iterator by partition and write everything directly.\n      for ((id, elements) <- this.partitionedIterator) {\n        if (elements.hasNext) {\n          for (elem <- elements) {\n            writer.write(elem._1, elem._2)\n          }\n          val segment = writer.commitAndGet()\n          lengths(id) = segment.length\n        }\n      }\n    }\n\n    writer.close()\n    context.taskMetrics().incMemoryBytesSpilled(memoryBytesSpilled)\n    context.taskMetrics().incDiskBytesSpilled(diskBytesSpilled)\n    context.taskMetrics().incPeakExecutionMemory(peakMemoryUsedBytes)\n\n    lengths\n  }\n```\n\n然后是 partitionedIterator 方法:\n\n```scala\ndef partitionedIterator: Iterator[(Int, Iterator[Product2[K, C]])] = {\n    val usingMap = aggregator.isDefined\n    val collection: WritablePartitionedPairCollection[K, C] = if (usingMap) map else buffer\n    if (spills.isEmpty) {// 这里又一次判断了是否为空，直接看有临时文件的部分\n      // Special case: if we have only in-memory data, we don't need to merge streams, and perhaps\n      // we don't even need to sort by anything other than partition ID\n      if (!ordering.isDefined) {\n        // The user hasn't requested sorted keys, so only sort by partition ID, not key\n   groupByPartition(destructiveIterator(collection.partitionedDestructiveSortedIterator(None)))\n      } else {\n        // We do need to sort by both partition ID and key\n        groupByPartition(destructiveIterator(\n          collection.partitionedDestructiveSortedIterator(Some(keyComparator))))\n      }\n    } else {\n      // Merge spilled and in-memory data\n      // 这里传入了临时文件　spills　和　排序后的缓存文件\n      merge(spills, destructiveIterator(\n        collection.partitionedDestructiveSortedIterator(comparator)))\n    }\n  }\n```\n\nmerge 方法:\n\n```scala\n // merge方法\n   // inMemory　是根据(partion,hash(k)) 排序后的内存数据\n   private def merge(spills: Seq[SpilledFile], inMemory: Iterator[((Int, K), C)])\n      : Iterator[(Int, Iterator[Product2[K, C]])] = {\n    //　将所有缓存文件转化为 SpillReader \n    val readers = spills.map(new SpillReader(_))\n    // buffered方法只是比普通的　itertor 方法多提供一个　head　方法，例如:\n    // val a = List(1,2,3,4,5)\n    // val b = a.iterator.buffered\n    // b.head : 1\n    // b.next : 1\n    // b.head : 2\n    // b.next : 2\n    val inMemBuffered = inMemory.buffered\n    (0 until numPartitions).iterator.map { p =>\n      // 获得分区对应数据的迭代器 \n      // 这里的　IteratorForPartition　就是得到分区 p 对应的数据的迭代器，逻辑比较简单，就不单独拿出来分析\n      val inMemIterator = new IteratorForPartition(p, inMemBuffered)\n      \n      //这里获取所有文件的第　p　个分区　并与内存中的第　p 个分区进行合并\n      val iterators = readers.map(_.readNextPartition()) ++ Seq(inMemIterator)\n　　　// 这里的　iterators　是由　多个 iterator 组成的，其中每一个都是关于 p 分区数据的 iterator\n      if (aggregator.isDefined) {\n        // Perform partial aggregation across partitions\n        (p, mergeWithAggregation(\n          iterators, aggregator.get.mergeCombiners, keyComparator, ordering.isDefined))\n      } else if (ordering.isDefined) {\n        // No aggregator given, but we have an ordering (e.g. used by reduce tasks in sortByKey);\n        // sort the elements without trying to merge them\n        (p, mergeSort(iterators, ordering.get))\n      } else {\n        (p, iterators.iterator.flatten)\n      }\n    }\n```\nmergeWithAggregation\n```scala\n\n      private def mergeWithAggregation(\n      iterators: Seq[Iterator[Product2[K, C]]],\n      mergeCombiners: (C, C) => C,\n      comparator: Comparator[K],\n      totalOrder: Boolean)\n      : Iterator[Product2[K, C]] =\n  {\n    if (!totalOrder) {\n      new Iterator[Iterator[Product2[K, C]]] {\n        val sorted = mergeSort(iterators, comparator).buffered\n        val keys = new ArrayBuffer[K]\n        val combiners = new ArrayBuffer[C]\n        override def hasNext: Boolean = sorted.hasNext\n        override def next(): Iterator[Product2[K, C]] = {\n          if (!hasNext) {\n            throw new NoSuchElementException\n          }\n          keys.clear()\n          combiners.clear()\n          val firstPair = sorted.next()\n          keys += firstPair._1\n          combiners += firstPair._2\n          val key = firstPair._1\n          while (sorted.hasNext && comparator.compare(sorted.head._1, key) == 0) {\n            val pair = sorted.next()\n            var i = 0\n            var foundKey = false\n            while (i < keys.size && !foundKey) {\n              if (keys(i) == pair._1) {\n                combiners(i) = mergeCombiners(combiners(i), pair._2)\n                foundKey = true\n              }\n              i += 1\n            }\n            if (!foundKey) {\n              keys += pair._1\n              combiners += pair._2\n            }\n          }\n          keys.iterator.zip(combiners.iterator)\n        }\n      }.flatMap(i => i)\n    } else {\n      // We have a total ordering, so the objects with the same key are sequential.\n      new Iterator[Product2[K, C]] {\n        val sorted = mergeSort(iterators, comparator).buffered\n        override def hasNext: Boolean = sorted.hasNext\n        override def next(): Product2[K, C] = {\n          if (!hasNext) {\n            throw new NoSuchElementException\n          }\n          val elem = sorted.next()\n          val k = elem._1\n          var c = elem._2\n          // 这里的意思是可能存在多个相同的key 分布在不同临时文件或内存中\n          // 所以还需要将不同的 key 对应的值进行合并 \n          while (sorted.hasNext && sorted.head._1 == k) {\n            val pair = sorted.next()\n            c = mergeCombiners(c, pair._2)\n          }\n          (k, c)//返回\n        }\n      }\n    }\n  }\n```\n\n继续看 mergeSort 方法，首先选除头元素最小对应那个分区，然后选出这个分区的第一个元素，那么这个元素一定是最小的，然后将剩余的元素放回去:\n\n```scala\n\n  private def mergeSort(iterators: Seq[Iterator[Product2[K, C]]], comparator: Comparator[K])\n      : Iterator[Product2[K, C]] =\n  {\n    val bufferedIters = iterators.filter(_.hasNext).map(_.buffered)\n    type Iter = BufferedIterator[Product2[K, C]]\n    //选取头元素最小的分区\n    val heap = new mutable.PriorityQueue[Iter]()(new Ordering[Iter] {\n      // Use the reverse of comparator.compare because PriorityQueue dequeues the max\n      override def compare(x: Iter, y: Iter): Int = -comparator.compare(x.head._1, y.head._1)\n    })\n    heap.enqueue(bufferedIters: _*) \n    new Iterator[Product2[K, C]] {\n      override def hasNext: Boolean = !heap.isEmpty\n      override def next(): Product2[K, C] = {\n        if (!hasNext) {\n          throw new NoSuchElementException\n        }\n        val firstBuf = heap.dequeue()\n        val firstPair = firstBuf.next()\n        if (firstBuf.hasNext) {\n          heap.enqueue(firstBuf)\n        }\n        firstPair\n      }\n    }\n  }\n```\n\n接下来就使用 writeIndexFileAndCommit 将每一个分区的数据条数写入索引文件，便于 reduce 端获取。\n\n## 总结\n\nshuflle write 相比起 shuffe read 要复杂很多，因此本文的篇幅也较长。其实在 shuffle write 有两个较为重要的方法，其中一个是 insertAll。这个方法的作用就是将数据缓存到一个可以添加的 hash 表中。如果表占用的内存达到的一定值，就对其进行排序。排序方式先按照分区进行排序，如果分区相同则按key的hash进行排序，随后将排序后的数据写入临时文件中，这个过程可能会生成多个临时文件。\n\n然后 writePartitionedFile 的作用是最后将内存中的数据和临时文件中的数据进行全部排序，写入到一个大文件中。\n\n最后将每一个分区对应的索引数据写入文件。整个shuffle write 阶段就完成了\n","source":"_posts/源码阅读计划-第二部分-shuffle-write.md","raw":"---\ntitle: spark源码阅读计划 - 第二部分 - shuffle write\ndate: 2018-06-08 13:46:45\ntags:\n  - Spark\n  - 编程\ncategories: Spark源码阅读计划\nauthor: lishion\ntoc: true\n---\n## 写在开始的话\n\nshuffle 作为 spark 最重要的一部分，也是很多初学者感到头痛的一部分。简单来说，shuffle 分为两个部分 : shuffle write 和 shuffle read。shuffle write 在 map 端进行，而shuffle read 在 reduce 端进行。本章就准备就 shuffle write 做一个详细的分析。\n\n## 为什么需要shuffle\n\n在上一章[Spark-源码阅读计划-第一部分-迭代计算](https://lishion.github.io/2018/06/06/Spark-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E8%AE%A1%E5%88%92-%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86-%E8%BF%AD%E4%BB%A3%E8%AE%A1%E7%AE%97/)提到过每一个分区的数据最后都会由一个task来处理，那么当task 执行的类似于 reduceByKey 这种算子的时候一定需要满足相同的 key 在同一个分区这个条件，否则就不能按照 key 进行reduce了。但是当我们从数据源获取数据并进行处理后，相同的 key 不一定在同一个分区。那么在一个 map 端将相同的key 使用某种方式聚集在同一个分区并写入临时文件的过程就称为 shuffle write；而在 reduce 从 map 拉去执行 task 所需要的数据就是 shuffle read；这两个步骤组成了shuffle。\n\n## 一个小例子\n\n本章同样以一个小例子作为本章讲解的中心，例如一个统计词频的代码:\n\n```scala\nsc.parallelize(List(\"hello\",\"world\",\"hello\",\"spark\")).map(x=>(x,1)).reduceByKey(_+_)\n```\n\n相信有一点基础的同学都知道 reduceByKey 这算子会产生 shuffle。但是大部分的同学都不知 shuffle 是什么时候产生的，shuffle 到底做了些什么，这也是这一章的重点。\n\n## shuffle task\n\n在上一章提到过当 job 提交后会生成 result task。而在需要 shuffle 则会生成 ShuffleMapTask。这里涉及到 job 提交以及 stage 生成的的知识，将会在下一章提到。这里我们直接开始看 ShuffleMapTask 中的 runtask :\n\n```scala\n override def runTask(context: TaskContext): MapStatus = {\n    // Deserialize the RDD using the broadcast variable.\n ...\n    var writer: ShuffleWriter[Any, Any] = null\n    try {\n      val manager = SparkEnv.get.shuffleManager\n      writer = manager.getWriter[Any, Any](dep.shuffleHandle, partitionId, context)\n       // 这里依然是rdd调用iterator方法的地方\n      writer.write(rdd.iterator(partition, context).asInstanceOf[Iterator[_ <: Product2[Any, Any]]])\n      writer.stop(success = true).get\n    } catch {\n      case e: Exception =>\n        try {\n          if (writer != null) {\n            writer.stop(success = false)\n          }\n        } catch {\n          case e: Exception =>\n            log.debug(\"Could not stop writer\", e)\n        }\n        throw e\n    }\n  }\n```\n\n可以看到首先获得一个 shuffleWriter。spark 中有许多不同类型的 writer ，默认使用的是 SortShuffleWriter。SortShuffleWriter 中的 write 方法实现了 shuffle write。源代码如下:\n\n```scala\n  override def write(records: Iterator[Product2[K, V]]): Unit = {\n      //如果需要 map 端聚\n    sorter = if (dep.mapSideCombine) {\n      new ExternalSorter[K, V, C](\n        context, dep.aggregator, Some(dep.partitioner), dep.keyOrdering, dep.serializer)\n    } else {\n      new ExternalSorter[K, V, V](\n        context, aggregator = None, Some(dep.partitioner), ordering = None, dep.serializer)\n    }\n    sorter.insertAll(records)\n    val output = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)\n    val tmp = Utils.tempFileWith(output)\n    try { \n      val blockId = ShuffleBlockId(dep.shuffleId, mapId, IndexShuffleBlockResolver.NOOP_REDUCE_ID)\n      val partitionLengths = sorter.writePartitionedFile(blockId, tmp)\n      shuffleBlockResolver.writeIndexFileAndCommit(dep.shuffleId, mapId, partitionLengths, tmp)\n      mapStatus = MapStatus(blockManager.shuffleServerId, partitionLengths)\n    } finally {\n      if (tmp.exists() && !tmp.delete()) {\n        logError(s\"Error while deleting temp file ${tmp.getAbsolutePath}\")\n      }\n    }\n  }\n```\n\n使用了 ExternalSorter 的 insertAll 方法插入了上一个 RDD 生成的数据的迭代器。\n\n#### insertAll\n\n```scala\ndef insertAll(records: Iterator[Product2[K, V]]): Unit = {\n    // TODO: stop combining if we find that the reduction factor isn't high\n    val shouldCombine = aggregator.isDefined\n    // 直接看需要 map 端聚合的情况\n    // 不需要聚合的情况类似\n    if (shouldCombine) {\n      // Combine values in-memory first using our AppendOnlyMap\n       \n      val mergeValue = aggregator.get.mergeValue \n      val createCombiner = aggregator.get.createCombiner\n      var kv: Product2[K, V] = null\n      // 如果 key 不存在则直接创建 Combiner ，否则使用 mergeValue 将 value 添加到创建的 Combiner中\n      val update = (hadValue: Boolean, oldValue: C) => {\n        if (hadValue) mergeValue(oldValue, kv._2) else createCombiner(kv._2)\n      }\n      while (records.hasNext) {\n        addElementsRead()\n        kv = records.next()\n        // 更新值\n        // 这里需要注意的是之前的数据是(k,v)类型，这里转换成了((part,k),v)\n        // 其中 part 是 key 对应的分区\n        map.changeValue((getPartition(kv._1), kv._1), update)\n        maybeSpillCollection(usingMap = true)\n      }\n    } else {\n      // Stick values into our buffer\n      while (records.hasNext) {\n        addElementsRead()\n        val kv = records.next()\n        buffer.insert(getPartition(kv._1), kv._1, kv._2.asInstanceOf[C])\n        maybeSpillCollection(usingMap = false)\n      }\n    }\n  }\n```\n\n在这里使用 getPartition() 进行了数据的**分区**，也就是将不同的 key 与其分区对应起来。默认使用的是 HashPartitioner ，有兴趣的同学可以去阅读源码。再继续看 changeValue 方法。在这里使用非常重要的数据结构来存放分区后的数据，也就是代码中的 map 类型为 PartitionedAppendOnlyMap。其中是使用hask表存放数据，并且存放的方式为表中的偶数索index引存放key, index + 1 存放对应的value。例如:\n\n```\nk1|v1|null|k2|v2|k3|v3|null|....\n```\n\n之所以里面由空值，是因为在插入的时候使用了二次探测法。来看一看 changeValue 方法:\n\nPartitionedAppendOnlyMap 中的  changeValue 方法继承自 \n\n```scala\n  def changeValue(key: K, updateFunc: (Boolean, V) => V): V = {\n    assert(!destroyed, destructionMessage)\n    val k = key.asInstanceOf[AnyRef]\n    if (k.eq(null)) {\n      if (!haveNullValue) {\n        incrementSize()\n      }\n      nullValue = updateFunc(haveNullValue, nullValue)\n      haveNullValue = true\n      return nullValue\n    }\n    var pos = rehash(k.hashCode) & mask\n    var i = 1\n    while (true) {\n      // 偶数位置为 key\n      val curKey = data(2 * pos)\n       // 如果该key不存在，就直接插入\n      if (curKey.eq(null)) {\n        val newValue = updateFunc(false, null.asInstanceOf[V])\n        data(2 * pos) = k\n        data(2 * pos + 1) = newValue.asInstanceOf[AnyRef]\n        incrementSize()\n        return newValue\n      } else if (k.eq(curKey) || k.equals(curKey)) {//如果 key 存在，则进行聚合\n        \n        val newValue = updateFunc(true, data(2 * pos + 1).asInstanceOf[V])\n        data(2 * pos + 1) = newValue.asInstanceOf[AnyRef]\n        return newValue\n      } else { // 否则进行下一次探测\n        val delta = i\n        pos = (pos + delta) & mask\n        i += 1\n      }\n    }\n    null.asInstanceOf[V] // Never reached but needed to keep compiler happy\n  }\n```\n\n再调用 insertAll 进行了数据的聚合|插入之后，调用了 maybeSpillCollection 方法。这个方法的作用是判断 map 占用了内存，如果内存达到一定的值，则将内存中的数据 spill 到临时文件中。\n\n```scala\nprivate def maybeSpillCollection(usingMap: Boolean): Unit = {\n    var estimatedSize = 0L\n    if (usingMap) {\n      estimatedSize = map.estimateSize()\n      if (maybeSpill(map, estimatedSize)) {\n        map = new PartitionedAppendOnlyMap[K, C]\n      }\n    } else {\n      estimatedSize = buffer.estimateSize()\n      if (maybeSpill(buffer, estimatedSize)) {\n        buffer = new PartitionedPairBuffer[K, C]\n      }\n    }\n\n    if (estimatedSize > _peakMemoryUsedBytes) {\n      _peakMemoryUsedBytes = estimatedSize\n    }\n  }\n```\n\nmaybeSpillCollection 继续调用了 maybeSpill 方法，这个方法继承自 Spillable 中\n\n```scala\nprotected def maybeSpill(collection: C, currentMemory: Long): Boolean = {\n    var shouldSpill = false\n    if (elementsRead % 32 == 0 && currentMemory >= myMemoryThreshold) {\n      // Claim up to double our current memory from the shuffle memory pool\n      val amountToRequest = 2 * currentMemory - myMemoryThreshold\n      val granted = acquireMemory(amountToRequest)\n      myMemoryThreshold += granted\n      // If we were granted too little memory to grow further (either tryToAcquire returned 0,\n      // or we already had more memory than myMemoryThreshold), spill the current collection\n      shouldSpill = currentMemory >= myMemoryThreshold\n    }\n    shouldSpill = shouldSpill || _elementsRead > numElementsForceSpillThreshold\n    // Actually spill\n    if (shouldSpill) {\n      _spillCount += 1\n      logSpillage(currentMemory)\n      spill(collection)\n      _elementsRead = 0\n      _memoryBytesSpilled += currentMemory\n      releaseMemory()\n    }\n    shouldSpill\n  }\n```\n\n该方法首先判断了是否需要 spill，如果需要则调用 spill() 方法将内存中的数据写入临时文件中。spill 方法如下:\n\n```scala\n override protected[this] def spill(collection: WritablePartitionedPairCollection[K, C]): Unit = {\n    val inMemoryIterator = collection.destructiveSortedWritablePartitionedIterator(comparator)\n    val spillFile = spillMemoryIteratorToDisk(inMemoryIterator)\n    spills += spillFile\n  }\n```\n\n其中的  comparator 如下:\n\n```scala\n private def comparator: Option[Comparator[K]] = {\n    if (ordering.isDefined || aggregator.isDefined) {\n      Some(keyComparator)\n    } else {\n      None\n    }\n  }\n```\n\n也就是:\n\n```scala\n  private val keyComparator: Comparator[K] = ordering.getOrElse(new Comparator[K] {\n    override def compare(a: K, b: K): Int = {\n      val h1 = if (a == null) 0 else a.hashCode()\n      val h2 = if (b == null) 0 else b.hashCode()\n      if (h1 < h2) -1 else if (h1 == h2) 0 else 1\n    }\n  })\n```\n\ndestructiveSortedWritablePartitionedIterator 位于 WritablePartitionedPairCollection 类中，实现如下:\n\n```scala\ndef destructiveSortedWritablePartitionedIterator(keyComparator: Option[Comparator[K]])\n    : WritablePartitionedIterator = {\n    //这里调用的其实是　PartitionedAppendOnlyMap　中重写的　partitionedDestructiveSortedIterator    \n    val it = partitionedDestructiveSortedIterator(keyComparator)\n    // 这里还实现了一个　writeNext　的方法，后面会用到\n    new WritablePartitionedIterator {\n      private[this] var cur = if (it.hasNext) it.next() else null\n       // 在map 中的数据其实是((partition,k),v)\n       // 这里只写入了(k,v)\n      def writeNext(writer: DiskBlockObjectWriter): Unit = {\n        writer.write(cur._1._2, cur._2)\n        cur = if (it.hasNext) it.next() else null\n      }\n      def hasNext(): Boolean = cur != null\n      def nextPartition(): Int = cur._1._1\n    }\n  }\n```\n\n接下来看看partitionedDestructiveSortedIterator:\n\n```scala\ndef partitionedDestructiveSortedIterator(keyComparator: Option[Comparator[K]])\n    : Iterator[((Int, K), V)] = {\n    val comparator = keyComparator.map(partitionKeyComparator).getOrElse(partitionComparator)\n    destructiveSortedIterator(comparator)\n }\n```\n\nkeyComparator.map(partitionKeyComparator)　等于 partitionKeyComparator(keyComparator)，再看下面的partitionKeyComparator，说明比较器是先对分区进行比较，如果分区相同，再对key的hash值进行比较:\n\n```scala\n def partitionKeyComparator[K](keyComparator: Comparator[K]): Comparator[(Int, K)] = {\n    new Comparator[(Int, K)] {\n      override def compare(a: (Int, K), b: (Int, K)): Int = {\n        val partitionDiff = a._1 - b._1\n        if (partitionDiff != 0) {\n          partitionDiff\n        } else {\n          keyComparator.compare(a._2, b._2)\n        }\n      }\n    }\n  }\n```\n\n然后看destructiveSortedIterator(),这是在 PartitionedAppendOnlyMap 的父类 destructiveSortedIterator 中实现的:\n\n```scala\n def destructiveSortedIterator(keyComparator: Comparator[K]): Iterator[(K, V)] = {\n    destroyed = true\n    // Pack KV pairs into the front of the underlying array\n    // 这里是因为　PartitionedAppendOnlyMap　采用的二次探测法　,kv对之间存在　null值，所以先把非空的kv对全部移动到hash表的前端\n    var keyIndex, newIndex = 0\n    while (keyIndex < capacity) {\n      if (data(2 * keyIndex) != null) {\n        data(2 * newIndex) = data(2 * keyIndex)\n        data(2 * newIndex + 1) = data(2 * keyIndex + 1)\n        newIndex += 1\n      }\n      keyIndex += 1\n    }\n    assert(curSize == newIndex + (if (haveNullValue) 1 else 0))\n    // 这里对给定数据范围为 0 to newIndex，利用　keyComparator　比较器进行排序\n    // 也就是对 map 中的数据根据　(partition,key) 进行排序\n    new Sorter(new KVArraySortDataFormat[K, AnyRef]).sort(data, 0, newIndex, keyComparator)\n\n    // 定义迭代器\n    new Iterator[(K, V)] {\n      var i = 0\n      var nullValueReady = haveNullValue\n      def hasNext: Boolean = (i < newIndex || nullValueReady)\n      def next(): (K, V) = {\n        if (nullValueReady) {\n          nullValueReady = false\n          (null.asInstanceOf[K], nullValue)\n        } else {\n          val item = (data(2 * i).asInstanceOf[K], data(2 * i + 1).asInstanceOf[V])\n          i += 1\n          item\n        }\n      }\n    }\n  }\n```\n\n可以看出整个　`val inMemoryIterator = collection.destructiveSortedWritablePartitionedIterator(comparator)`　这一步其实就是在对　map　或 buffer 进行排序，并且实现了一个 writeNext() 方法供后续调调用。随后调用`val spillFile = spillMemoryIteratorToDisk(inMemoryIterator)`将排序后的缓存写入文件中:\n\n```scala\nprivate[this] def spillMemoryIteratorToDisk(inMemoryIterator: WritablePartitionedIterator)\n      : SpilledFile = {\n   \n    val (blockId, file) = diskBlockManager.createTempShuffleBlock()\n    // These variables are reset after each flush\n    var objectsWritten: Long = 0\n    val spillMetrics: ShuffleWriteMetrics = new ShuffleWriteMetrics\n    val writer: DiskBlockObjectWriter =\n      blockManager.getDiskWriter(blockId, file, serInstance, fileBufferSize, spillMetrics)\n    // List of batch sizes (bytes) in the order they are written to disk\n    val batchSizes = new ArrayBuffer[Long]\n    // How many elements we have in each partition\n    // 用于记录每一个分区有多少条数据\n    // 由于刚才数据写入文件没有写入key对应的分区，因此需要记录每一个分区有多少条数据，然后根据数据的偏移量就可以判断该数据属于哪个分区　\n    val elementsPerPartition = new Array[Long](numPartitions)\n    // Flush the disk writer's contents to disk, and update relevant variables.\n    // The writer is committed at the end of this process.\n    def flush(): Unit = {\n      val segment = writer.commitAndGet()\n      batchSizes += segment.length\n      _diskBytesSpilled += segment.length\n      objectsWritten = 0\n    }\n    var success = false\n    try {\n      while (inMemoryIterator.hasNext) {\n        val partitionId = inMemoryIterator.nextPartition()\n        require(partitionId >= 0 && partitionId < numPartitions,\n          s\"partition Id: ${partitionId} should be in the range [0, ${numPartitions})\")\n        inMemoryIterator.writeNext(writer)\n        elementsPerPartition(partitionId) += 1\n        objectsWritten += 1\n        if (objectsWritten == serializerBatchSize) {　//写入　serializerBatchSize　条数据便刷新一次缓存\n          // batchSize 在类中定义的如下:\n          // 可以看出如果不存在配置默认为　10000　条\n          // private val serializerBatchSize = conf.getLong(\"spark.shuffle.spill.batchSize\", 10000)\n          flush()\n        }\n      }\n      if (objectsWritten > 0) {\n        flush()\n      } else {\n        writer.revertPartialWritesAndClose()\n      }\n      success = true\n    } finally {\n      if (success) {\n        writer.close()\n      } else {\n        // This code path only happens if an exception was thrown above before we set success;\n        // close our stuff and let the exception be thrown further\n        writer.revertPartialWritesAndClose()\n        if (file.exists()) {\n          if (!file.delete()) {\n            logWarning(s\"Error deleting ${file}\")\n          }\n        }\n      }\n    }\n    //最后记录临时文件的信息\n    SpilledFile(file, blockId, batchSizes.toArray, elementsPerPartition)\n\n  }\n```\n\n这就是整个　insertAll()　方法:\n\n merge 达到溢出值后进行排序并写入临时文件，这里要注意的是并非所有的缓存文件都被写入到临时文件中，所以接下来需要将临时文件中的数据加上内存中的数据进行一次排序写入到一个大文件中。\n\n```scala\n//首先获取需要写入的文件:　\n   val output = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)\n　　val tmp = Utils.tempFileWith(output)\n    try { \n      val blockId = ShuffleBlockId(dep.shuffleId, mapId, IndexShuffleBlockResolver.NOOP_REDUCE_ID)\n      val partitionLengths = sorter.writePartitionedFile(blockId, tmp)//这一句是重点 下面会讲解\n      shuffleBlockResolver.writeIndexFileAndCommit(dep.shuffleId, mapId, partitionLengths, tmp)\n      mapStatus = MapStatus(blockManager.shuffleServerId, partitionLengths)\n    } finally {\n      if (tmp.exists() && !tmp.delete()) {\n        logError(s\"Error while deleting temp file ${tmp.getAbsolutePath}\")\n    }\n```\n#### writePartitionedFile\n\n继续看 writePartitionedFile :\n\n```scala\n//  ExternalSorter 中的　writePartitionedFile　方法\n   def writePartitionedFile(\n      blockId: BlockId,\n      outputFile: File): Array[Long] = {\n\n    // Track location of each range in the output file\n    val lengths = new Array[Long](numPartitions)\n    val writer = blockManager.getDiskWriter(blockId, outputFile, serInstance, fileBufferSize,\n      context.taskMetrics().shuffleWriteMetrics)\n\n    // 这里的spills 就是　刚才产生的临时文件的数据　如果为空逻辑就比较简单，直接进行排序并写入文件\n    if (spills.isEmpty) {\n      // Case where we only have in-memory data\n      val collection = if (aggregator.isDefined) map else buffer\n      val it = collection.destructiveSortedWritablePartitionedIterator(comparator)　//这里同样使用　destructiveSortedWritablePartitionedIterator　进行排序\n      while (it.hasNext) {\n        val partitionId = it.nextPartition()\n        while (it.hasNext && it.nextPartition() == partitionId) {\n          it.writeNext(writer)\n        }\n        val segment = writer.commitAndGet()\n        lengths(partitionId) = segment.length\n      }\n    } else {//重点看不为空的时候，这里调用了　partitionedIterator　方法\n      // We must perform merge-sort; get an iterator by partition and write everything directly.\n      for ((id, elements) <- this.partitionedIterator) {\n        if (elements.hasNext) {\n          for (elem <- elements) {\n            writer.write(elem._1, elem._2)\n          }\n          val segment = writer.commitAndGet()\n          lengths(id) = segment.length\n        }\n      }\n    }\n\n    writer.close()\n    context.taskMetrics().incMemoryBytesSpilled(memoryBytesSpilled)\n    context.taskMetrics().incDiskBytesSpilled(diskBytesSpilled)\n    context.taskMetrics().incPeakExecutionMemory(peakMemoryUsedBytes)\n\n    lengths\n  }\n```\n\n然后是 partitionedIterator 方法:\n\n```scala\ndef partitionedIterator: Iterator[(Int, Iterator[Product2[K, C]])] = {\n    val usingMap = aggregator.isDefined\n    val collection: WritablePartitionedPairCollection[K, C] = if (usingMap) map else buffer\n    if (spills.isEmpty) {// 这里又一次判断了是否为空，直接看有临时文件的部分\n      // Special case: if we have only in-memory data, we don't need to merge streams, and perhaps\n      // we don't even need to sort by anything other than partition ID\n      if (!ordering.isDefined) {\n        // The user hasn't requested sorted keys, so only sort by partition ID, not key\n   groupByPartition(destructiveIterator(collection.partitionedDestructiveSortedIterator(None)))\n      } else {\n        // We do need to sort by both partition ID and key\n        groupByPartition(destructiveIterator(\n          collection.partitionedDestructiveSortedIterator(Some(keyComparator))))\n      }\n    } else {\n      // Merge spilled and in-memory data\n      // 这里传入了临时文件　spills　和　排序后的缓存文件\n      merge(spills, destructiveIterator(\n        collection.partitionedDestructiveSortedIterator(comparator)))\n    }\n  }\n```\n\nmerge 方法:\n\n```scala\n // merge方法\n   // inMemory　是根据(partion,hash(k)) 排序后的内存数据\n   private def merge(spills: Seq[SpilledFile], inMemory: Iterator[((Int, K), C)])\n      : Iterator[(Int, Iterator[Product2[K, C]])] = {\n    //　将所有缓存文件转化为 SpillReader \n    val readers = spills.map(new SpillReader(_))\n    // buffered方法只是比普通的　itertor 方法多提供一个　head　方法，例如:\n    // val a = List(1,2,3,4,5)\n    // val b = a.iterator.buffered\n    // b.head : 1\n    // b.next : 1\n    // b.head : 2\n    // b.next : 2\n    val inMemBuffered = inMemory.buffered\n    (0 until numPartitions).iterator.map { p =>\n      // 获得分区对应数据的迭代器 \n      // 这里的　IteratorForPartition　就是得到分区 p 对应的数据的迭代器，逻辑比较简单，就不单独拿出来分析\n      val inMemIterator = new IteratorForPartition(p, inMemBuffered)\n      \n      //这里获取所有文件的第　p　个分区　并与内存中的第　p 个分区进行合并\n      val iterators = readers.map(_.readNextPartition()) ++ Seq(inMemIterator)\n　　　// 这里的　iterators　是由　多个 iterator 组成的，其中每一个都是关于 p 分区数据的 iterator\n      if (aggregator.isDefined) {\n        // Perform partial aggregation across partitions\n        (p, mergeWithAggregation(\n          iterators, aggregator.get.mergeCombiners, keyComparator, ordering.isDefined))\n      } else if (ordering.isDefined) {\n        // No aggregator given, but we have an ordering (e.g. used by reduce tasks in sortByKey);\n        // sort the elements without trying to merge them\n        (p, mergeSort(iterators, ordering.get))\n      } else {\n        (p, iterators.iterator.flatten)\n      }\n    }\n```\nmergeWithAggregation\n```scala\n\n      private def mergeWithAggregation(\n      iterators: Seq[Iterator[Product2[K, C]]],\n      mergeCombiners: (C, C) => C,\n      comparator: Comparator[K],\n      totalOrder: Boolean)\n      : Iterator[Product2[K, C]] =\n  {\n    if (!totalOrder) {\n      new Iterator[Iterator[Product2[K, C]]] {\n        val sorted = mergeSort(iterators, comparator).buffered\n        val keys = new ArrayBuffer[K]\n        val combiners = new ArrayBuffer[C]\n        override def hasNext: Boolean = sorted.hasNext\n        override def next(): Iterator[Product2[K, C]] = {\n          if (!hasNext) {\n            throw new NoSuchElementException\n          }\n          keys.clear()\n          combiners.clear()\n          val firstPair = sorted.next()\n          keys += firstPair._1\n          combiners += firstPair._2\n          val key = firstPair._1\n          while (sorted.hasNext && comparator.compare(sorted.head._1, key) == 0) {\n            val pair = sorted.next()\n            var i = 0\n            var foundKey = false\n            while (i < keys.size && !foundKey) {\n              if (keys(i) == pair._1) {\n                combiners(i) = mergeCombiners(combiners(i), pair._2)\n                foundKey = true\n              }\n              i += 1\n            }\n            if (!foundKey) {\n              keys += pair._1\n              combiners += pair._2\n            }\n          }\n          keys.iterator.zip(combiners.iterator)\n        }\n      }.flatMap(i => i)\n    } else {\n      // We have a total ordering, so the objects with the same key are sequential.\n      new Iterator[Product2[K, C]] {\n        val sorted = mergeSort(iterators, comparator).buffered\n        override def hasNext: Boolean = sorted.hasNext\n        override def next(): Product2[K, C] = {\n          if (!hasNext) {\n            throw new NoSuchElementException\n          }\n          val elem = sorted.next()\n          val k = elem._1\n          var c = elem._2\n          // 这里的意思是可能存在多个相同的key 分布在不同临时文件或内存中\n          // 所以还需要将不同的 key 对应的值进行合并 \n          while (sorted.hasNext && sorted.head._1 == k) {\n            val pair = sorted.next()\n            c = mergeCombiners(c, pair._2)\n          }\n          (k, c)//返回\n        }\n      }\n    }\n  }\n```\n\n继续看 mergeSort 方法，首先选除头元素最小对应那个分区，然后选出这个分区的第一个元素，那么这个元素一定是最小的，然后将剩余的元素放回去:\n\n```scala\n\n  private def mergeSort(iterators: Seq[Iterator[Product2[K, C]]], comparator: Comparator[K])\n      : Iterator[Product2[K, C]] =\n  {\n    val bufferedIters = iterators.filter(_.hasNext).map(_.buffered)\n    type Iter = BufferedIterator[Product2[K, C]]\n    //选取头元素最小的分区\n    val heap = new mutable.PriorityQueue[Iter]()(new Ordering[Iter] {\n      // Use the reverse of comparator.compare because PriorityQueue dequeues the max\n      override def compare(x: Iter, y: Iter): Int = -comparator.compare(x.head._1, y.head._1)\n    })\n    heap.enqueue(bufferedIters: _*) \n    new Iterator[Product2[K, C]] {\n      override def hasNext: Boolean = !heap.isEmpty\n      override def next(): Product2[K, C] = {\n        if (!hasNext) {\n          throw new NoSuchElementException\n        }\n        val firstBuf = heap.dequeue()\n        val firstPair = firstBuf.next()\n        if (firstBuf.hasNext) {\n          heap.enqueue(firstBuf)\n        }\n        firstPair\n      }\n    }\n  }\n```\n\n接下来就使用 writeIndexFileAndCommit 将每一个分区的数据条数写入索引文件，便于 reduce 端获取。\n\n## 总结\n\nshuflle write 相比起 shuffe read 要复杂很多，因此本文的篇幅也较长。其实在 shuffle write 有两个较为重要的方法，其中一个是 insertAll。这个方法的作用就是将数据缓存到一个可以添加的 hash 表中。如果表占用的内存达到的一定值，就对其进行排序。排序方式先按照分区进行排序，如果分区相同则按key的hash进行排序，随后将排序后的数据写入临时文件中，这个过程可能会生成多个临时文件。\n\n然后 writePartitionedFile 的作用是最后将内存中的数据和临时文件中的数据进行全部排序，写入到一个大文件中。\n\n最后将每一个分区对应的索引数据写入文件。整个shuffle write 阶段就完成了\n","slug":"源码阅读计划-第二部分-shuffle-write","published":1,"updated":"2018-06-20T06:22:05.783Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjjhx8v930036b8tgh128nfr2","content":"<h2 id=\"写在开始的话\"><a href=\"#写在开始的话\" class=\"headerlink\" title=\"写在开始的话\"></a>写在开始的话</h2><p>shuffle 作为 spark 最重要的一部分，也是很多初学者感到头痛的一部分。简单来说，shuffle 分为两个部分 : shuffle write 和 shuffle read。shuffle write 在 map 端进行，而shuffle read 在 reduce 端进行。本章就准备就 shuffle write 做一个详细的分析。</p>\n<h2 id=\"为什么需要shuffle\"><a href=\"#为什么需要shuffle\" class=\"headerlink\" title=\"为什么需要shuffle\"></a>为什么需要shuffle</h2><p>在上一章<a href=\"https://lishion.github.io/2018/06/06/Spark-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E8%AE%A1%E5%88%92-%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86-%E8%BF%AD%E4%BB%A3%E8%AE%A1%E7%AE%97/\" target=\"_blank\" rel=\"noopener\">Spark-源码阅读计划-第一部分-迭代计算</a>提到过每一个分区的数据最后都会由一个task来处理，那么当task 执行的类似于 reduceByKey 这种算子的时候一定需要满足相同的 key 在同一个分区这个条件，否则就不能按照 key 进行reduce了。但是当我们从数据源获取数据并进行处理后，相同的 key 不一定在同一个分区。那么在一个 map 端将相同的key 使用某种方式聚集在同一个分区并写入临时文件的过程就称为 shuffle write；而在 reduce 从 map 拉去执行 task 所需要的数据就是 shuffle read；这两个步骤组成了shuffle。</p>\n<h2 id=\"一个小例子\"><a href=\"#一个小例子\" class=\"headerlink\" title=\"一个小例子\"></a>一个小例子</h2><p>本章同样以一个小例子作为本章讲解的中心，例如一个统计词频的代码:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sc.parallelize(<span class=\"type\">List</span>(<span class=\"string\">\"hello\"</span>,<span class=\"string\">\"world\"</span>,<span class=\"string\">\"hello\"</span>,<span class=\"string\">\"spark\"</span>)).map(x=&gt;(x,<span class=\"number\">1</span>)).reduceByKey(_+_)</span><br></pre></td></tr></table></figure>\n<p>相信有一点基础的同学都知道 reduceByKey 这算子会产生 shuffle。但是大部分的同学都不知 shuffle 是什么时候产生的，shuffle 到底做了些什么，这也是这一章的重点。</p>\n<h2 id=\"shuffle-task\"><a href=\"#shuffle-task\" class=\"headerlink\" title=\"shuffle task\"></a>shuffle task</h2><p>在上一章提到过当 job 提交后会生成 result task。而在需要 shuffle 则会生成 ShuffleMapTask。这里涉及到 job 提交以及 stage 生成的的知识，将会在下一章提到。这里我们直接开始看 ShuffleMapTask 中的 runtask :</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">runTask</span></span>(context: <span class=\"type\">TaskContext</span>): <span class=\"type\">MapStatus</span> = &#123;</span><br><span class=\"line\">   <span class=\"comment\">// Deserialize the RDD using the broadcast variable.</span></span><br><span class=\"line\">...</span><br><span class=\"line\">   <span class=\"keyword\">var</span> writer: <span class=\"type\">ShuffleWriter</span>[<span class=\"type\">Any</span>, <span class=\"type\">Any</span>] = <span class=\"literal\">null</span></span><br><span class=\"line\">   <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">     <span class=\"keyword\">val</span> manager = <span class=\"type\">SparkEnv</span>.get.shuffleManager</span><br><span class=\"line\">     writer = manager.getWriter[<span class=\"type\">Any</span>, <span class=\"type\">Any</span>](dep.shuffleHandle, partitionId, context)</span><br><span class=\"line\">      <span class=\"comment\">// 这里依然是rdd调用iterator方法的地方</span></span><br><span class=\"line\">     writer.write(rdd.iterator(partition, context).asInstanceOf[<span class=\"type\">Iterator</span>[_ &lt;: <span class=\"type\">Product2</span>[<span class=\"type\">Any</span>, <span class=\"type\">Any</span>]]])</span><br><span class=\"line\">     writer.stop(success = <span class=\"literal\">true</span>).get</span><br><span class=\"line\">   &#125; <span class=\"keyword\">catch</span> &#123;</span><br><span class=\"line\">     <span class=\"keyword\">case</span> e: <span class=\"type\">Exception</span> =&gt;</span><br><span class=\"line\">       <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">         <span class=\"keyword\">if</span> (writer != <span class=\"literal\">null</span>) &#123;</span><br><span class=\"line\">           writer.stop(success = <span class=\"literal\">false</span>)</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">       &#125; <span class=\"keyword\">catch</span> &#123;</span><br><span class=\"line\">         <span class=\"keyword\">case</span> e: <span class=\"type\">Exception</span> =&gt;</span><br><span class=\"line\">           log.debug(<span class=\"string\">\"Could not stop writer\"</span>, e)</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">       <span class=\"keyword\">throw</span> e</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>可以看到首先获得一个 shuffleWriter。spark 中有许多不同类型的 writer ，默认使用的是 SortShuffleWriter。SortShuffleWriter 中的 write 方法实现了 shuffle write。源代码如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">write</span></span>(records: <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>]]): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">    <span class=\"comment\">//如果需要 map 端聚</span></span><br><span class=\"line\">  sorter = <span class=\"keyword\">if</span> (dep.mapSideCombine) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">ExternalSorter</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>, <span class=\"type\">C</span>](</span><br><span class=\"line\">      context, dep.aggregator, <span class=\"type\">Some</span>(dep.partitioner), dep.keyOrdering, dep.serializer)</span><br><span class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">ExternalSorter</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>, <span class=\"type\">V</span>](</span><br><span class=\"line\">      context, aggregator = <span class=\"type\">None</span>, <span class=\"type\">Some</span>(dep.partitioner), ordering = <span class=\"type\">None</span>, dep.serializer)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  sorter.insertAll(records)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> output = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> tmp = <span class=\"type\">Utils</span>.tempFileWith(output)</span><br><span class=\"line\">  <span class=\"keyword\">try</span> &#123; </span><br><span class=\"line\">    <span class=\"keyword\">val</span> blockId = <span class=\"type\">ShuffleBlockId</span>(dep.shuffleId, mapId, <span class=\"type\">IndexShuffleBlockResolver</span>.<span class=\"type\">NOOP_REDUCE_ID</span>)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> partitionLengths = sorter.writePartitionedFile(blockId, tmp)</span><br><span class=\"line\">    shuffleBlockResolver.writeIndexFileAndCommit(dep.shuffleId, mapId, partitionLengths, tmp)</span><br><span class=\"line\">    mapStatus = <span class=\"type\">MapStatus</span>(blockManager.shuffleServerId, partitionLengths)</span><br><span class=\"line\">  &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (tmp.exists() &amp;&amp; !tmp.delete()) &#123;</span><br><span class=\"line\">      logError(<span class=\"string\">s\"Error while deleting temp file <span class=\"subst\">$&#123;tmp.getAbsolutePath&#125;</span>\"</span>)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>使用了 ExternalSorter 的 insertAll 方法插入了上一个 RDD 生成的数据的迭代器。</p>\n<h4 id=\"insertAll\"><a href=\"#insertAll\" class=\"headerlink\" title=\"insertAll\"></a>insertAll</h4><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">insertAll</span></span>(records: <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>]]): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">    <span class=\"comment\">// <span class=\"doctag\">TODO:</span> stop combining if we find that the reduction factor isn't high</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> shouldCombine = aggregator.isDefined</span><br><span class=\"line\">    <span class=\"comment\">// 直接看需要 map 端聚合的情况</span></span><br><span class=\"line\">    <span class=\"comment\">// 不需要聚合的情况类似</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (shouldCombine) &#123;</span><br><span class=\"line\">      <span class=\"comment\">// Combine values in-memory first using our AppendOnlyMap</span></span><br><span class=\"line\">       </span><br><span class=\"line\">      <span class=\"keyword\">val</span> mergeValue = aggregator.get.mergeValue </span><br><span class=\"line\">      <span class=\"keyword\">val</span> createCombiner = aggregator.get.createCombiner</span><br><span class=\"line\">      <span class=\"keyword\">var</span> kv: <span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>] = <span class=\"literal\">null</span></span><br><span class=\"line\">      <span class=\"comment\">// 如果 key 不存在则直接创建 Combiner ，否则使用 mergeValue 将 value 添加到创建的 Combiner中</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> update = (hadValue: <span class=\"type\">Boolean</span>, oldValue: <span class=\"type\">C</span>) =&gt; &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (hadValue) mergeValue(oldValue, kv._2) <span class=\"keyword\">else</span> createCombiner(kv._2)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      <span class=\"keyword\">while</span> (records.hasNext) &#123;</span><br><span class=\"line\">        addElementsRead()</span><br><span class=\"line\">        kv = records.next()</span><br><span class=\"line\">        <span class=\"comment\">// 更新值</span></span><br><span class=\"line\">        <span class=\"comment\">// 这里需要注意的是之前的数据是(k,v)类型，这里转换成了((part,k),v)</span></span><br><span class=\"line\">        <span class=\"comment\">// 其中 part 是 key 对应的分区</span></span><br><span class=\"line\">        map.changeValue((getPartition(kv._1), kv._1), update)</span><br><span class=\"line\">        maybeSpillCollection(usingMap = <span class=\"literal\">true</span>)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">      <span class=\"comment\">// Stick values into our buffer</span></span><br><span class=\"line\">      <span class=\"keyword\">while</span> (records.hasNext) &#123;</span><br><span class=\"line\">        addElementsRead()</span><br><span class=\"line\">        <span class=\"keyword\">val</span> kv = records.next()</span><br><span class=\"line\">        buffer.insert(getPartition(kv._1), kv._1, kv._2.asInstanceOf[<span class=\"type\">C</span>])</span><br><span class=\"line\">        maybeSpillCollection(usingMap = <span class=\"literal\">false</span>)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>在这里使用 getPartition() 进行了数据的<strong>分区</strong>，也就是将不同的 key 与其分区对应起来。默认使用的是 HashPartitioner ，有兴趣的同学可以去阅读源码。再继续看 changeValue 方法。在这里使用非常重要的数据结构来存放分区后的数据，也就是代码中的 map 类型为 PartitionedAppendOnlyMap。其中是使用hask表存放数据，并且存放的方式为表中的偶数索index引存放key, index + 1 存放对应的value。例如:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">k1|v1|null|k2|v2|k3|v3|null|....</span><br></pre></td></tr></table></figure>\n<p>之所以里面由空值，是因为在插入的时候使用了二次探测法。来看一看 changeValue 方法:</p>\n<p>PartitionedAppendOnlyMap 中的  changeValue 方法继承自 </p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">changeValue</span></span>(key: <span class=\"type\">K</span>, updateFunc: (<span class=\"type\">Boolean</span>, <span class=\"type\">V</span>) =&gt; <span class=\"type\">V</span>): <span class=\"type\">V</span> = &#123;</span><br><span class=\"line\">  assert(!destroyed, destructionMessage)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> k = key.asInstanceOf[<span class=\"type\">AnyRef</span>]</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (k.eq(<span class=\"literal\">null</span>)) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (!haveNullValue) &#123;</span><br><span class=\"line\">      incrementSize()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    nullValue = updateFunc(haveNullValue, nullValue)</span><br><span class=\"line\">    haveNullValue = <span class=\"literal\">true</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> nullValue</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"keyword\">var</span> pos = rehash(k.hashCode) &amp; mask</span><br><span class=\"line\">  <span class=\"keyword\">var</span> i = <span class=\"number\">1</span></span><br><span class=\"line\">  <span class=\"keyword\">while</span> (<span class=\"literal\">true</span>) &#123;</span><br><span class=\"line\">    <span class=\"comment\">// 偶数位置为 key</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> curKey = data(<span class=\"number\">2</span> * pos)</span><br><span class=\"line\">     <span class=\"comment\">// 如果该key不存在，就直接插入</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (curKey.eq(<span class=\"literal\">null</span>)) &#123;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> newValue = updateFunc(<span class=\"literal\">false</span>, <span class=\"literal\">null</span>.asInstanceOf[<span class=\"type\">V</span>])</span><br><span class=\"line\">      data(<span class=\"number\">2</span> * pos) = k</span><br><span class=\"line\">      data(<span class=\"number\">2</span> * pos + <span class=\"number\">1</span>) = newValue.asInstanceOf[<span class=\"type\">AnyRef</span>]</span><br><span class=\"line\">      incrementSize()</span><br><span class=\"line\">      <span class=\"keyword\">return</span> newValue</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (k.eq(curKey) || k.equals(curKey)) &#123;<span class=\"comment\">//如果 key 存在，则进行聚合</span></span><br><span class=\"line\">      </span><br><span class=\"line\">      <span class=\"keyword\">val</span> newValue = updateFunc(<span class=\"literal\">true</span>, data(<span class=\"number\">2</span> * pos + <span class=\"number\">1</span>).asInstanceOf[<span class=\"type\">V</span>])</span><br><span class=\"line\">      data(<span class=\"number\">2</span> * pos + <span class=\"number\">1</span>) = newValue.asInstanceOf[<span class=\"type\">AnyRef</span>]</span><br><span class=\"line\">      <span class=\"keyword\">return</span> newValue</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123; <span class=\"comment\">// 否则进行下一次探测</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> delta = i</span><br><span class=\"line\">      pos = (pos + delta) &amp; mask</span><br><span class=\"line\">      i += <span class=\"number\">1</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"literal\">null</span>.asInstanceOf[<span class=\"type\">V</span>] <span class=\"comment\">// Never reached but needed to keep compiler happy</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>再调用 insertAll 进行了数据的聚合|插入之后，调用了 maybeSpillCollection 方法。这个方法的作用是判断 map 占用了内存，如果内存达到一定的值，则将内存中的数据 spill 到临时文件中。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">maybeSpillCollection</span></span>(usingMap: <span class=\"type\">Boolean</span>): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">var</span> estimatedSize = <span class=\"number\">0</span>L</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (usingMap) &#123;</span><br><span class=\"line\">      estimatedSize = map.estimateSize()</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (maybeSpill(map, estimatedSize)) &#123;</span><br><span class=\"line\">        map = <span class=\"keyword\">new</span> <span class=\"type\">PartitionedAppendOnlyMap</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">      estimatedSize = buffer.estimateSize()</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (maybeSpill(buffer, estimatedSize)) &#123;</span><br><span class=\"line\">        buffer = <span class=\"keyword\">new</span> <span class=\"type\">PartitionedPairBuffer</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (estimatedSize &gt; _peakMemoryUsedBytes) &#123;</span><br><span class=\"line\">      _peakMemoryUsedBytes = estimatedSize</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>maybeSpillCollection 继续调用了 maybeSpill 方法，这个方法继承自 Spillable 中</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">protected</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">maybeSpill</span></span>(collection: <span class=\"type\">C</span>, currentMemory: <span class=\"type\">Long</span>): <span class=\"type\">Boolean</span> = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">var</span> shouldSpill = <span class=\"literal\">false</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (elementsRead % <span class=\"number\">32</span> == <span class=\"number\">0</span> &amp;&amp; currentMemory &gt;= myMemoryThreshold) &#123;</span><br><span class=\"line\">      <span class=\"comment\">// Claim up to double our current memory from the shuffle memory pool</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> amountToRequest = <span class=\"number\">2</span> * currentMemory - myMemoryThreshold</span><br><span class=\"line\">      <span class=\"keyword\">val</span> granted = acquireMemory(amountToRequest)</span><br><span class=\"line\">      myMemoryThreshold += granted</span><br><span class=\"line\">      <span class=\"comment\">// If we were granted too little memory to grow further (either tryToAcquire returned 0,</span></span><br><span class=\"line\">      <span class=\"comment\">// or we already had more memory than myMemoryThreshold), spill the current collection</span></span><br><span class=\"line\">      shouldSpill = currentMemory &gt;= myMemoryThreshold</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    shouldSpill = shouldSpill || _elementsRead &gt; numElementsForceSpillThreshold</span><br><span class=\"line\">    <span class=\"comment\">// Actually spill</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (shouldSpill) &#123;</span><br><span class=\"line\">      _spillCount += <span class=\"number\">1</span></span><br><span class=\"line\">      logSpillage(currentMemory)</span><br><span class=\"line\">      spill(collection)</span><br><span class=\"line\">      _elementsRead = <span class=\"number\">0</span></span><br><span class=\"line\">      _memoryBytesSpilled += currentMemory</span><br><span class=\"line\">      releaseMemory()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    shouldSpill</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>该方法首先判断了是否需要 spill，如果需要则调用 spill() 方法将内存中的数据写入临时文件中。spill 方法如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"keyword\">protected</span>[<span class=\"keyword\">this</span>] <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">spill</span></span>(collection: <span class=\"type\">WritablePartitionedPairCollection</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">   <span class=\"keyword\">val</span> inMemoryIterator = collection.destructiveSortedWritablePartitionedIterator(comparator)</span><br><span class=\"line\">   <span class=\"keyword\">val</span> spillFile = spillMemoryIteratorToDisk(inMemoryIterator)</span><br><span class=\"line\">   spills += spillFile</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>其中的  comparator 如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">comparator</span></span>: <span class=\"type\">Option</span>[<span class=\"type\">Comparator</span>[<span class=\"type\">K</span>]] = &#123;</span><br><span class=\"line\">   <span class=\"keyword\">if</span> (ordering.isDefined || aggregator.isDefined) &#123;</span><br><span class=\"line\">     <span class=\"type\">Some</span>(keyComparator)</span><br><span class=\"line\">   &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">     <span class=\"type\">None</span></span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>也就是:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"keyword\">val</span> keyComparator: <span class=\"type\">Comparator</span>[<span class=\"type\">K</span>] = ordering.getOrElse(<span class=\"keyword\">new</span> <span class=\"type\">Comparator</span>[<span class=\"type\">K</span>] &#123;</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compare</span></span>(a: <span class=\"type\">K</span>, b: <span class=\"type\">K</span>): <span class=\"type\">Int</span> = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> h1 = <span class=\"keyword\">if</span> (a == <span class=\"literal\">null</span>) <span class=\"number\">0</span> <span class=\"keyword\">else</span> a.hashCode()</span><br><span class=\"line\">    <span class=\"keyword\">val</span> h2 = <span class=\"keyword\">if</span> (b == <span class=\"literal\">null</span>) <span class=\"number\">0</span> <span class=\"keyword\">else</span> b.hashCode()</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (h1 &lt; h2) <span class=\"number\">-1</span> <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (h1 == h2) <span class=\"number\">0</span> <span class=\"keyword\">else</span> <span class=\"number\">1</span></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;)</span><br></pre></td></tr></table></figure>\n<p>destructiveSortedWritablePartitionedIterator 位于 WritablePartitionedPairCollection 类中，实现如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">destructiveSortedWritablePartitionedIterator</span></span>(keyComparator: <span class=\"type\">Option</span>[<span class=\"type\">Comparator</span>[<span class=\"type\">K</span>]])</span><br><span class=\"line\">    : <span class=\"type\">WritablePartitionedIterator</span> = &#123;</span><br><span class=\"line\">    <span class=\"comment\">//这里调用的其实是　PartitionedAppendOnlyMap　中重写的　partitionedDestructiveSortedIterator    </span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> it = partitionedDestructiveSortedIterator(keyComparator)</span><br><span class=\"line\">    <span class=\"comment\">// 这里还实现了一个　writeNext　的方法，后面会用到</span></span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">WritablePartitionedIterator</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">private</span>[<span class=\"keyword\">this</span>] <span class=\"keyword\">var</span> cur = <span class=\"keyword\">if</span> (it.hasNext) it.next() <span class=\"keyword\">else</span> <span class=\"literal\">null</span></span><br><span class=\"line\">       <span class=\"comment\">// 在map 中的数据其实是((partition,k),v)</span></span><br><span class=\"line\">       <span class=\"comment\">// 这里只写入了(k,v)</span></span><br><span class=\"line\">      <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">writeNext</span></span>(writer: <span class=\"type\">DiskBlockObjectWriter</span>): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">        writer.write(cur._1._2, cur._2)</span><br><span class=\"line\">        cur = <span class=\"keyword\">if</span> (it.hasNext) it.next() <span class=\"keyword\">else</span> <span class=\"literal\">null</span></span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hasNext</span></span>(): <span class=\"type\">Boolean</span> = cur != <span class=\"literal\">null</span></span><br><span class=\"line\">      <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">nextPartition</span></span>(): <span class=\"type\">Int</span> = cur._1._1</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>接下来看看partitionedDestructiveSortedIterator:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">partitionedDestructiveSortedIterator</span></span>(keyComparator: <span class=\"type\">Option</span>[<span class=\"type\">Comparator</span>[<span class=\"type\">K</span>]])</span><br><span class=\"line\">    : <span class=\"type\">Iterator</span>[((<span class=\"type\">Int</span>, <span class=\"type\">K</span>), <span class=\"type\">V</span>)] = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> comparator = keyComparator.map(partitionKeyComparator).getOrElse(partitionComparator)</span><br><span class=\"line\">    destructiveSortedIterator(comparator)</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>keyComparator.map(partitionKeyComparator)　等于 partitionKeyComparator(keyComparator)，再看下面的partitionKeyComparator，说明比较器是先对分区进行比较，如果分区相同，再对key的hash值进行比较:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">partitionKeyComparator</span></span>[<span class=\"type\">K</span>](keyComparator: <span class=\"type\">Comparator</span>[<span class=\"type\">K</span>]): <span class=\"type\">Comparator</span>[(<span class=\"type\">Int</span>, <span class=\"type\">K</span>)] = &#123;</span><br><span class=\"line\">   <span class=\"keyword\">new</span> <span class=\"type\">Comparator</span>[(<span class=\"type\">Int</span>, <span class=\"type\">K</span>)] &#123;</span><br><span class=\"line\">     <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compare</span></span>(a: (<span class=\"type\">Int</span>, <span class=\"type\">K</span>), b: (<span class=\"type\">Int</span>, <span class=\"type\">K</span>)): <span class=\"type\">Int</span> = &#123;</span><br><span class=\"line\">       <span class=\"keyword\">val</span> partitionDiff = a._1 - b._1</span><br><span class=\"line\">       <span class=\"keyword\">if</span> (partitionDiff != <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">         partitionDiff</span><br><span class=\"line\">       &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">         keyComparator.compare(a._2, b._2)</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>然后看destructiveSortedIterator(),这是在 PartitionedAppendOnlyMap 的父类 destructiveSortedIterator 中实现的:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">destructiveSortedIterator</span></span>(keyComparator: <span class=\"type\">Comparator</span>[<span class=\"type\">K</span>]): <span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">V</span>)] = &#123;</span><br><span class=\"line\">   destroyed = <span class=\"literal\">true</span></span><br><span class=\"line\">   <span class=\"comment\">// Pack KV pairs into the front of the underlying array</span></span><br><span class=\"line\">   <span class=\"comment\">// 这里是因为　PartitionedAppendOnlyMap　采用的二次探测法　,kv对之间存在　null值，所以先把非空的kv对全部移动到hash表的前端</span></span><br><span class=\"line\">   <span class=\"keyword\">var</span> keyIndex, newIndex = <span class=\"number\">0</span></span><br><span class=\"line\">   <span class=\"keyword\">while</span> (keyIndex &lt; capacity) &#123;</span><br><span class=\"line\">     <span class=\"keyword\">if</span> (data(<span class=\"number\">2</span> * keyIndex) != <span class=\"literal\">null</span>) &#123;</span><br><span class=\"line\">       data(<span class=\"number\">2</span> * newIndex) = data(<span class=\"number\">2</span> * keyIndex)</span><br><span class=\"line\">       data(<span class=\"number\">2</span> * newIndex + <span class=\"number\">1</span>) = data(<span class=\"number\">2</span> * keyIndex + <span class=\"number\">1</span>)</span><br><span class=\"line\">       newIndex += <span class=\"number\">1</span></span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">     keyIndex += <span class=\"number\">1</span></span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   assert(curSize == newIndex + (<span class=\"keyword\">if</span> (haveNullValue) <span class=\"number\">1</span> <span class=\"keyword\">else</span> <span class=\"number\">0</span>))</span><br><span class=\"line\">   <span class=\"comment\">// 这里对给定数据范围为 0 to newIndex，利用　keyComparator　比较器进行排序</span></span><br><span class=\"line\">   <span class=\"comment\">// 也就是对 map 中的数据根据　(partition,key) 进行排序</span></span><br><span class=\"line\">   <span class=\"keyword\">new</span> <span class=\"type\">Sorter</span>(<span class=\"keyword\">new</span> <span class=\"type\">KVArraySortDataFormat</span>[<span class=\"type\">K</span>, <span class=\"type\">AnyRef</span>]).sort(data, <span class=\"number\">0</span>, newIndex, keyComparator)</span><br><span class=\"line\"></span><br><span class=\"line\">   <span class=\"comment\">// 定义迭代器</span></span><br><span class=\"line\">   <span class=\"keyword\">new</span> <span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">V</span>)] &#123;</span><br><span class=\"line\">     <span class=\"keyword\">var</span> i = <span class=\"number\">0</span></span><br><span class=\"line\">     <span class=\"keyword\">var</span> nullValueReady = haveNullValue</span><br><span class=\"line\">     <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hasNext</span></span>: <span class=\"type\">Boolean</span> = (i &lt; newIndex || nullValueReady)</span><br><span class=\"line\">     <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">next</span></span>(): (<span class=\"type\">K</span>, <span class=\"type\">V</span>) = &#123;</span><br><span class=\"line\">       <span class=\"keyword\">if</span> (nullValueReady) &#123;</span><br><span class=\"line\">         nullValueReady = <span class=\"literal\">false</span></span><br><span class=\"line\">         (<span class=\"literal\">null</span>.asInstanceOf[<span class=\"type\">K</span>], nullValue)</span><br><span class=\"line\">       &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">         <span class=\"keyword\">val</span> item = (data(<span class=\"number\">2</span> * i).asInstanceOf[<span class=\"type\">K</span>], data(<span class=\"number\">2</span> * i + <span class=\"number\">1</span>).asInstanceOf[<span class=\"type\">V</span>])</span><br><span class=\"line\">         i += <span class=\"number\">1</span></span><br><span class=\"line\">         item</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>可以看出整个　<code>val inMemoryIterator = collection.destructiveSortedWritablePartitionedIterator(comparator)</code>　这一步其实就是在对　map　或 buffer 进行排序，并且实现了一个 writeNext() 方法供后续调调用。随后调用<code>val spillFile = spillMemoryIteratorToDisk(inMemoryIterator)</code>将排序后的缓存写入文件中:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span>[<span class=\"keyword\">this</span>] <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">spillMemoryIteratorToDisk</span></span>(inMemoryIterator: <span class=\"type\">WritablePartitionedIterator</span>)</span><br><span class=\"line\">      : <span class=\"type\">SpilledFile</span> = &#123;</span><br><span class=\"line\">   </span><br><span class=\"line\">    <span class=\"keyword\">val</span> (blockId, file) = diskBlockManager.createTempShuffleBlock()</span><br><span class=\"line\">    <span class=\"comment\">// These variables are reset after each flush</span></span><br><span class=\"line\">    <span class=\"keyword\">var</span> objectsWritten: <span class=\"type\">Long</span> = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> spillMetrics: <span class=\"type\">ShuffleWriteMetrics</span> = <span class=\"keyword\">new</span> <span class=\"type\">ShuffleWriteMetrics</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> writer: <span class=\"type\">DiskBlockObjectWriter</span> =</span><br><span class=\"line\">      blockManager.getDiskWriter(blockId, file, serInstance, fileBufferSize, spillMetrics)</span><br><span class=\"line\">    <span class=\"comment\">// List of batch sizes (bytes) in the order they are written to disk</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> batchSizes = <span class=\"keyword\">new</span> <span class=\"type\">ArrayBuffer</span>[<span class=\"type\">Long</span>]</span><br><span class=\"line\">    <span class=\"comment\">// How many elements we have in each partition</span></span><br><span class=\"line\">    <span class=\"comment\">// 用于记录每一个分区有多少条数据</span></span><br><span class=\"line\">    <span class=\"comment\">// 由于刚才数据写入文件没有写入key对应的分区，因此需要记录每一个分区有多少条数据，然后根据数据的偏移量就可以判断该数据属于哪个分区　</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> elementsPerPartition = <span class=\"keyword\">new</span> <span class=\"type\">Array</span>[<span class=\"type\">Long</span>](numPartitions)</span><br><span class=\"line\">    <span class=\"comment\">// Flush the disk writer's contents to disk, and update relevant variables.</span></span><br><span class=\"line\">    <span class=\"comment\">// The writer is committed at the end of this process.</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">flush</span></span>(): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> segment = writer.commitAndGet()</span><br><span class=\"line\">      batchSizes += segment.length</span><br><span class=\"line\">      _diskBytesSpilled += segment.length</span><br><span class=\"line\">      objectsWritten = <span class=\"number\">0</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">var</span> success = <span class=\"literal\">false</span></span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">while</span> (inMemoryIterator.hasNext) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">val</span> partitionId = inMemoryIterator.nextPartition()</span><br><span class=\"line\">        require(partitionId &gt;= <span class=\"number\">0</span> &amp;&amp; partitionId &lt; numPartitions,</span><br><span class=\"line\">          <span class=\"string\">s\"partition Id: <span class=\"subst\">$&#123;partitionId&#125;</span> should be in the range [0, <span class=\"subst\">$&#123;numPartitions&#125;</span>)\"</span>)</span><br><span class=\"line\">        inMemoryIterator.writeNext(writer)</span><br><span class=\"line\">        elementsPerPartition(partitionId) += <span class=\"number\">1</span></span><br><span class=\"line\">        objectsWritten += <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (objectsWritten == serializerBatchSize) &#123;　<span class=\"comment\">//写入　serializerBatchSize　条数据便刷新一次缓存</span></span><br><span class=\"line\">          <span class=\"comment\">// batchSize 在类中定义的如下:</span></span><br><span class=\"line\">          <span class=\"comment\">// 可以看出如果不存在配置默认为　10000　条</span></span><br><span class=\"line\">          <span class=\"comment\">// private val serializerBatchSize = conf.getLong(\"spark.shuffle.spill.batchSize\", 10000)</span></span><br><span class=\"line\">          flush()</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (objectsWritten &gt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">        flush()</span><br><span class=\"line\">      &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        writer.revertPartialWritesAndClose()</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      success = <span class=\"literal\">true</span></span><br><span class=\"line\">    &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (success) &#123;</span><br><span class=\"line\">        writer.close()</span><br><span class=\"line\">      &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// This code path only happens if an exception was thrown above before we set success;</span></span><br><span class=\"line\">        <span class=\"comment\">// close our stuff and let the exception be thrown further</span></span><br><span class=\"line\">        writer.revertPartialWritesAndClose()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (file.exists()) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">if</span> (!file.delete()) &#123;</span><br><span class=\"line\">            logWarning(<span class=\"string\">s\"Error deleting <span class=\"subst\">$&#123;file&#125;</span>\"</span>)</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">//最后记录临时文件的信息</span></span><br><span class=\"line\">    <span class=\"type\">SpilledFile</span>(file, blockId, batchSizes.toArray, elementsPerPartition)</span><br><span class=\"line\"></span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>这就是整个　insertAll()　方法:</p>\n<p> merge 达到溢出值后进行排序并写入临时文件，这里要注意的是并非所有的缓存文件都被写入到临时文件中，所以接下来需要将临时文件中的数据加上内存中的数据进行一次排序写入到一个大文件中。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//首先获取需要写入的文件:　</span></span><br><span class=\"line\">   <span class=\"keyword\">val</span> output = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)</span><br><span class=\"line\">　　<span class=\"keyword\">val</span> tmp = <span class=\"type\">Utils</span>.tempFileWith(output)</span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123; </span><br><span class=\"line\">      <span class=\"keyword\">val</span> blockId = <span class=\"type\">ShuffleBlockId</span>(dep.shuffleId, mapId, <span class=\"type\">IndexShuffleBlockResolver</span>.<span class=\"type\">NOOP_REDUCE_ID</span>)</span><br><span class=\"line\">      <span class=\"keyword\">val</span> partitionLengths = sorter.writePartitionedFile(blockId, tmp)<span class=\"comment\">//这一句是重点 下面会讲解</span></span><br><span class=\"line\">      shuffleBlockResolver.writeIndexFileAndCommit(dep.shuffleId, mapId, partitionLengths, tmp)</span><br><span class=\"line\">      mapStatus = <span class=\"type\">MapStatus</span>(blockManager.shuffleServerId, partitionLengths)</span><br><span class=\"line\">    &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (tmp.exists() &amp;&amp; !tmp.delete()) &#123;</span><br><span class=\"line\">        logError(<span class=\"string\">s\"Error while deleting temp file <span class=\"subst\">$&#123;tmp.getAbsolutePath&#125;</span>\"</span>)</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"writePartitionedFile\"><a href=\"#writePartitionedFile\" class=\"headerlink\" title=\"writePartitionedFile\"></a>writePartitionedFile</h4><p>继续看 writePartitionedFile :</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//  ExternalSorter 中的　writePartitionedFile　方法</span></span><br><span class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">writePartitionedFile</span></span>(</span><br><span class=\"line\">      blockId: <span class=\"type\">BlockId</span>,</span><br><span class=\"line\">      outputFile: <span class=\"type\">File</span>): <span class=\"type\">Array</span>[<span class=\"type\">Long</span>] = &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// Track location of each range in the output file</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> lengths = <span class=\"keyword\">new</span> <span class=\"type\">Array</span>[<span class=\"type\">Long</span>](numPartitions)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> writer = blockManager.getDiskWriter(blockId, outputFile, serInstance, fileBufferSize,</span><br><span class=\"line\">      context.taskMetrics().shuffleWriteMetrics)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 这里的spills 就是　刚才产生的临时文件的数据　如果为空逻辑就比较简单，直接进行排序并写入文件</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (spills.isEmpty) &#123;</span><br><span class=\"line\">      <span class=\"comment\">// Case where we only have in-memory data</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> collection = <span class=\"keyword\">if</span> (aggregator.isDefined) map <span class=\"keyword\">else</span> buffer</span><br><span class=\"line\">      <span class=\"keyword\">val</span> it = collection.destructiveSortedWritablePartitionedIterator(comparator)　<span class=\"comment\">//这里同样使用　destructiveSortedWritablePartitionedIterator　进行排序</span></span><br><span class=\"line\">      <span class=\"keyword\">while</span> (it.hasNext) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">val</span> partitionId = it.nextPartition()</span><br><span class=\"line\">        <span class=\"keyword\">while</span> (it.hasNext &amp;&amp; it.nextPartition() == partitionId) &#123;</span><br><span class=\"line\">          it.writeNext(writer)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">val</span> segment = writer.commitAndGet()</span><br><span class=\"line\">        lengths(partitionId) = segment.length</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;<span class=\"comment\">//重点看不为空的时候，这里调用了　partitionedIterator　方法</span></span><br><span class=\"line\">      <span class=\"comment\">// We must perform merge-sort; get an iterator by partition and write everything directly.</span></span><br><span class=\"line\">      <span class=\"keyword\">for</span> ((id, elements) &lt;- <span class=\"keyword\">this</span>.partitionedIterator) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (elements.hasNext) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">for</span> (elem &lt;- elements) &#123;</span><br><span class=\"line\">            writer.write(elem._1, elem._2)</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">          <span class=\"keyword\">val</span> segment = writer.commitAndGet()</span><br><span class=\"line\">          lengths(id) = segment.length</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    writer.close()</span><br><span class=\"line\">    context.taskMetrics().incMemoryBytesSpilled(memoryBytesSpilled)</span><br><span class=\"line\">    context.taskMetrics().incDiskBytesSpilled(diskBytesSpilled)</span><br><span class=\"line\">    context.taskMetrics().incPeakExecutionMemory(peakMemoryUsedBytes)</span><br><span class=\"line\"></span><br><span class=\"line\">    lengths</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>然后是 partitionedIterator 方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">partitionedIterator</span></span>: <span class=\"type\">Iterator</span>[(<span class=\"type\">Int</span>, <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]])] = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> usingMap = aggregator.isDefined</span><br><span class=\"line\">    <span class=\"keyword\">val</span> collection: <span class=\"type\">WritablePartitionedPairCollection</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>] = <span class=\"keyword\">if</span> (usingMap) map <span class=\"keyword\">else</span> buffer</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (spills.isEmpty) &#123;<span class=\"comment\">// 这里又一次判断了是否为空，直接看有临时文件的部分</span></span><br><span class=\"line\">      <span class=\"comment\">// Special case: if we have only in-memory data, we don't need to merge streams, and perhaps</span></span><br><span class=\"line\">      <span class=\"comment\">// we don't even need to sort by anything other than partition ID</span></span><br><span class=\"line\">      <span class=\"keyword\">if</span> (!ordering.isDefined) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// The user hasn't requested sorted keys, so only sort by partition ID, not key</span></span><br><span class=\"line\">   groupByPartition(destructiveIterator(collection.partitionedDestructiveSortedIterator(<span class=\"type\">None</span>)))</span><br><span class=\"line\">      &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// We do need to sort by both partition ID and key</span></span><br><span class=\"line\">        groupByPartition(destructiveIterator(</span><br><span class=\"line\">          collection.partitionedDestructiveSortedIterator(<span class=\"type\">Some</span>(keyComparator))))</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">      <span class=\"comment\">// Merge spilled and in-memory data</span></span><br><span class=\"line\">      <span class=\"comment\">// 这里传入了临时文件　spills　和　排序后的缓存文件</span></span><br><span class=\"line\">      merge(spills, destructiveIterator(</span><br><span class=\"line\">        collection.partitionedDestructiveSortedIterator(comparator)))</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>merge 方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// merge方法</span></span><br><span class=\"line\">  <span class=\"comment\">// inMemory　是根据(partion,hash(k)) 排序后的内存数据</span></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">merge</span></span>(spills: <span class=\"type\">Seq</span>[<span class=\"type\">SpilledFile</span>], inMemory: <span class=\"type\">Iterator</span>[((<span class=\"type\">Int</span>, <span class=\"type\">K</span>), <span class=\"type\">C</span>)])</span><br><span class=\"line\">     : <span class=\"type\">Iterator</span>[(<span class=\"type\">Int</span>, <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]])] = &#123;</span><br><span class=\"line\">   <span class=\"comment\">//　将所有缓存文件转化为 SpillReader </span></span><br><span class=\"line\">   <span class=\"keyword\">val</span> readers = spills.map(<span class=\"keyword\">new</span> <span class=\"type\">SpillReader</span>(_))</span><br><span class=\"line\">   <span class=\"comment\">// buffered方法只是比普通的　itertor 方法多提供一个　head　方法，例如:</span></span><br><span class=\"line\">   <span class=\"comment\">// val a = List(1,2,3,4,5)</span></span><br><span class=\"line\">   <span class=\"comment\">// val b = a.iterator.buffered</span></span><br><span class=\"line\">   <span class=\"comment\">// b.head : 1</span></span><br><span class=\"line\">   <span class=\"comment\">// b.next : 1</span></span><br><span class=\"line\">   <span class=\"comment\">// b.head : 2</span></span><br><span class=\"line\">   <span class=\"comment\">// b.next : 2</span></span><br><span class=\"line\">   <span class=\"keyword\">val</span> inMemBuffered = inMemory.buffered</span><br><span class=\"line\">   (<span class=\"number\">0</span> until numPartitions).iterator.map &#123; p =&gt;</span><br><span class=\"line\">     <span class=\"comment\">// 获得分区对应数据的迭代器 </span></span><br><span class=\"line\">     <span class=\"comment\">// 这里的　IteratorForPartition　就是得到分区 p 对应的数据的迭代器，逻辑比较简单，就不单独拿出来分析</span></span><br><span class=\"line\">     <span class=\"keyword\">val</span> inMemIterator = <span class=\"keyword\">new</span> <span class=\"type\">IteratorForPartition</span>(p, inMemBuffered)</span><br><span class=\"line\">     </span><br><span class=\"line\">     <span class=\"comment\">//这里获取所有文件的第　p　个分区　并与内存中的第　p 个分区进行合并</span></span><br><span class=\"line\">     <span class=\"keyword\">val</span> iterators = readers.map(_.readNextPartition()) ++ <span class=\"type\">Seq</span>(inMemIterator)</span><br><span class=\"line\">　　　<span class=\"comment\">// 这里的　iterators　是由　多个 iterator 组成的，其中每一个都是关于 p 分区数据的 iterator</span></span><br><span class=\"line\">     <span class=\"keyword\">if</span> (aggregator.isDefined) &#123;</span><br><span class=\"line\">       <span class=\"comment\">// Perform partial aggregation across partitions</span></span><br><span class=\"line\">       (p, mergeWithAggregation(</span><br><span class=\"line\">         iterators, aggregator.get.mergeCombiners, keyComparator, ordering.isDefined))</span><br><span class=\"line\">     &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (ordering.isDefined) &#123;</span><br><span class=\"line\">       <span class=\"comment\">// No aggregator given, but we have an ordering (e.g. used by reduce tasks in sortByKey);</span></span><br><span class=\"line\">       <span class=\"comment\">// sort the elements without trying to merge them</span></span><br><span class=\"line\">       (p, mergeSort(iterators, ordering.get))</span><br><span class=\"line\">     &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">       (p, iterators.iterator.flatten)</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">   &#125;</span><br></pre></td></tr></table></figure>\n<p>mergeWithAggregation<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">mergeWithAggregation</span></span>(</span><br><span class=\"line\">    iterators: <span class=\"type\">Seq</span>[<span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]]],</span><br><span class=\"line\">    mergeCombiners: (<span class=\"type\">C</span>, <span class=\"type\">C</span>) =&gt; <span class=\"type\">C</span>,</span><br><span class=\"line\">    comparator: <span class=\"type\">Comparator</span>[<span class=\"type\">K</span>],</span><br><span class=\"line\">    totalOrder: <span class=\"type\">Boolean</span>)</span><br><span class=\"line\">    : <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] =</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (!totalOrder) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">Iterator</span>[<span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]]] &#123;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> sorted = mergeSort(iterators, comparator).buffered</span><br><span class=\"line\">      <span class=\"keyword\">val</span> keys = <span class=\"keyword\">new</span> <span class=\"type\">ArrayBuffer</span>[<span class=\"type\">K</span>]</span><br><span class=\"line\">      <span class=\"keyword\">val</span> combiners = <span class=\"keyword\">new</span> <span class=\"type\">ArrayBuffer</span>[<span class=\"type\">C</span>]</span><br><span class=\"line\">      <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hasNext</span></span>: <span class=\"type\">Boolean</span> = sorted.hasNext</span><br><span class=\"line\">      <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">next</span></span>(): <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] = &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!hasNext) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">NoSuchElementException</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        keys.clear()</span><br><span class=\"line\">        combiners.clear()</span><br><span class=\"line\">        <span class=\"keyword\">val</span> firstPair = sorted.next()</span><br><span class=\"line\">        keys += firstPair._1</span><br><span class=\"line\">        combiners += firstPair._2</span><br><span class=\"line\">        <span class=\"keyword\">val</span> key = firstPair._1</span><br><span class=\"line\">        <span class=\"keyword\">while</span> (sorted.hasNext &amp;&amp; comparator.compare(sorted.head._1, key) == <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">val</span> pair = sorted.next()</span><br><span class=\"line\">          <span class=\"keyword\">var</span> i = <span class=\"number\">0</span></span><br><span class=\"line\">          <span class=\"keyword\">var</span> foundKey = <span class=\"literal\">false</span></span><br><span class=\"line\">          <span class=\"keyword\">while</span> (i &lt; keys.size &amp;&amp; !foundKey) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (keys(i) == pair._1) &#123;</span><br><span class=\"line\">              combiners(i) = mergeCombiners(combiners(i), pair._2)</span><br><span class=\"line\">              foundKey = <span class=\"literal\">true</span></span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            i += <span class=\"number\">1</span></span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">          <span class=\"keyword\">if</span> (!foundKey) &#123;</span><br><span class=\"line\">            keys += pair._1</span><br><span class=\"line\">            combiners += pair._2</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        keys.iterator.zip(combiners.iterator)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;.flatMap(i =&gt; i)</span><br><span class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">// We have a total ordering, so the objects with the same key are sequential.</span></span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] &#123;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> sorted = mergeSort(iterators, comparator).buffered</span><br><span class=\"line\">      <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hasNext</span></span>: <span class=\"type\">Boolean</span> = sorted.hasNext</span><br><span class=\"line\">      <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">next</span></span>(): <span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>] = &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!hasNext) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">NoSuchElementException</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">val</span> elem = sorted.next()</span><br><span class=\"line\">        <span class=\"keyword\">val</span> k = elem._1</span><br><span class=\"line\">        <span class=\"keyword\">var</span> c = elem._2</span><br><span class=\"line\">        <span class=\"comment\">// 这里的意思是可能存在多个相同的key 分布在不同临时文件或内存中</span></span><br><span class=\"line\">        <span class=\"comment\">// 所以还需要将不同的 key 对应的值进行合并 </span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> (sorted.hasNext &amp;&amp; sorted.head._1 == k) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">val</span> pair = sorted.next()</span><br><span class=\"line\">          c = mergeCombiners(c, pair._2)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        (k, c)<span class=\"comment\">//返回</span></span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>继续看 mergeSort 方法，首先选除头元素最小对应那个分区，然后选出这个分区的第一个元素，那么这个元素一定是最小的，然后将剩余的元素放回去:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">mergeSort</span></span>(iterators: <span class=\"type\">Seq</span>[<span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]]], comparator: <span class=\"type\">Comparator</span>[<span class=\"type\">K</span>])</span><br><span class=\"line\">    : <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] =</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  <span class=\"keyword\">val</span> bufferedIters = iterators.filter(_.hasNext).map(_.buffered)</span><br><span class=\"line\">  <span class=\"class\"><span class=\"keyword\">type</span> <span class=\"title\">Iter</span> </span>= <span class=\"type\">BufferedIterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]]</span><br><span class=\"line\">  <span class=\"comment\">//选取头元素最小的分区</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> heap = <span class=\"keyword\">new</span> mutable.<span class=\"type\">PriorityQueue</span>[<span class=\"type\">Iter</span>]()(<span class=\"keyword\">new</span> <span class=\"type\">Ordering</span>[<span class=\"type\">Iter</span>] &#123;</span><br><span class=\"line\">    <span class=\"comment\">// Use the reverse of comparator.compare because PriorityQueue dequeues the max</span></span><br><span class=\"line\">    <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compare</span></span>(x: <span class=\"type\">Iter</span>, y: <span class=\"type\">Iter</span>): <span class=\"type\">Int</span> = -comparator.compare(x.head._1, y.head._1)</span><br><span class=\"line\">  &#125;)</span><br><span class=\"line\">  heap.enqueue(bufferedIters: _*) </span><br><span class=\"line\">  <span class=\"keyword\">new</span> <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] &#123;</span><br><span class=\"line\">    <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hasNext</span></span>: <span class=\"type\">Boolean</span> = !heap.isEmpty</span><br><span class=\"line\">    <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">next</span></span>(): <span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>] = &#123;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (!hasNext) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">NoSuchElementException</span></span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> firstBuf = heap.dequeue()</span><br><span class=\"line\">      <span class=\"keyword\">val</span> firstPair = firstBuf.next()</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (firstBuf.hasNext) &#123;</span><br><span class=\"line\">        heap.enqueue(firstBuf)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      firstPair</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>接下来就使用 writeIndexFileAndCommit 将每一个分区的数据条数写入索引文件，便于 reduce 端获取。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>shuflle write 相比起 shuffe read 要复杂很多，因此本文的篇幅也较长。其实在 shuffle write 有两个较为重要的方法，其中一个是 insertAll。这个方法的作用就是将数据缓存到一个可以添加的 hash 表中。如果表占用的内存达到的一定值，就对其进行排序。排序方式先按照分区进行排序，如果分区相同则按key的hash进行排序，随后将排序后的数据写入临时文件中，这个过程可能会生成多个临时文件。</p>\n<p>然后 writePartitionedFile 的作用是最后将内存中的数据和临时文件中的数据进行全部排序，写入到一个大文件中。</p>\n<p>最后将每一个分区对应的索引数据写入文件。整个shuffle write 阶段就完成了</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"写在开始的话\"><a href=\"#写在开始的话\" class=\"headerlink\" title=\"写在开始的话\"></a>写在开始的话</h2><p>shuffle 作为 spark 最重要的一部分，也是很多初学者感到头痛的一部分。简单来说，shuffle 分为两个部分 : shuffle write 和 shuffle read。shuffle write 在 map 端进行，而shuffle read 在 reduce 端进行。本章就准备就 shuffle write 做一个详细的分析。</p>\n<h2 id=\"为什么需要shuffle\"><a href=\"#为什么需要shuffle\" class=\"headerlink\" title=\"为什么需要shuffle\"></a>为什么需要shuffle</h2><p>在上一章<a href=\"https://lishion.github.io/2018/06/06/Spark-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E8%AE%A1%E5%88%92-%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86-%E8%BF%AD%E4%BB%A3%E8%AE%A1%E7%AE%97/\" target=\"_blank\" rel=\"noopener\">Spark-源码阅读计划-第一部分-迭代计算</a>提到过每一个分区的数据最后都会由一个task来处理，那么当task 执行的类似于 reduceByKey 这种算子的时候一定需要满足相同的 key 在同一个分区这个条件，否则就不能按照 key 进行reduce了。但是当我们从数据源获取数据并进行处理后，相同的 key 不一定在同一个分区。那么在一个 map 端将相同的key 使用某种方式聚集在同一个分区并写入临时文件的过程就称为 shuffle write；而在 reduce 从 map 拉去执行 task 所需要的数据就是 shuffle read；这两个步骤组成了shuffle。</p>\n<h2 id=\"一个小例子\"><a href=\"#一个小例子\" class=\"headerlink\" title=\"一个小例子\"></a>一个小例子</h2><p>本章同样以一个小例子作为本章讲解的中心，例如一个统计词频的代码:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sc.parallelize(<span class=\"type\">List</span>(<span class=\"string\">\"hello\"</span>,<span class=\"string\">\"world\"</span>,<span class=\"string\">\"hello\"</span>,<span class=\"string\">\"spark\"</span>)).map(x=&gt;(x,<span class=\"number\">1</span>)).reduceByKey(_+_)</span><br></pre></td></tr></table></figure>\n<p>相信有一点基础的同学都知道 reduceByKey 这算子会产生 shuffle。但是大部分的同学都不知 shuffle 是什么时候产生的，shuffle 到底做了些什么，这也是这一章的重点。</p>\n<h2 id=\"shuffle-task\"><a href=\"#shuffle-task\" class=\"headerlink\" title=\"shuffle task\"></a>shuffle task</h2><p>在上一章提到过当 job 提交后会生成 result task。而在需要 shuffle 则会生成 ShuffleMapTask。这里涉及到 job 提交以及 stage 生成的的知识，将会在下一章提到。这里我们直接开始看 ShuffleMapTask 中的 runtask :</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">runTask</span></span>(context: <span class=\"type\">TaskContext</span>): <span class=\"type\">MapStatus</span> = &#123;</span><br><span class=\"line\">   <span class=\"comment\">// Deserialize the RDD using the broadcast variable.</span></span><br><span class=\"line\">...</span><br><span class=\"line\">   <span class=\"keyword\">var</span> writer: <span class=\"type\">ShuffleWriter</span>[<span class=\"type\">Any</span>, <span class=\"type\">Any</span>] = <span class=\"literal\">null</span></span><br><span class=\"line\">   <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">     <span class=\"keyword\">val</span> manager = <span class=\"type\">SparkEnv</span>.get.shuffleManager</span><br><span class=\"line\">     writer = manager.getWriter[<span class=\"type\">Any</span>, <span class=\"type\">Any</span>](dep.shuffleHandle, partitionId, context)</span><br><span class=\"line\">      <span class=\"comment\">// 这里依然是rdd调用iterator方法的地方</span></span><br><span class=\"line\">     writer.write(rdd.iterator(partition, context).asInstanceOf[<span class=\"type\">Iterator</span>[_ &lt;: <span class=\"type\">Product2</span>[<span class=\"type\">Any</span>, <span class=\"type\">Any</span>]]])</span><br><span class=\"line\">     writer.stop(success = <span class=\"literal\">true</span>).get</span><br><span class=\"line\">   &#125; <span class=\"keyword\">catch</span> &#123;</span><br><span class=\"line\">     <span class=\"keyword\">case</span> e: <span class=\"type\">Exception</span> =&gt;</span><br><span class=\"line\">       <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">         <span class=\"keyword\">if</span> (writer != <span class=\"literal\">null</span>) &#123;</span><br><span class=\"line\">           writer.stop(success = <span class=\"literal\">false</span>)</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">       &#125; <span class=\"keyword\">catch</span> &#123;</span><br><span class=\"line\">         <span class=\"keyword\">case</span> e: <span class=\"type\">Exception</span> =&gt;</span><br><span class=\"line\">           log.debug(<span class=\"string\">\"Could not stop writer\"</span>, e)</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">       <span class=\"keyword\">throw</span> e</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>可以看到首先获得一个 shuffleWriter。spark 中有许多不同类型的 writer ，默认使用的是 SortShuffleWriter。SortShuffleWriter 中的 write 方法实现了 shuffle write。源代码如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">write</span></span>(records: <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>]]): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">    <span class=\"comment\">//如果需要 map 端聚</span></span><br><span class=\"line\">  sorter = <span class=\"keyword\">if</span> (dep.mapSideCombine) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">ExternalSorter</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>, <span class=\"type\">C</span>](</span><br><span class=\"line\">      context, dep.aggregator, <span class=\"type\">Some</span>(dep.partitioner), dep.keyOrdering, dep.serializer)</span><br><span class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">ExternalSorter</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>, <span class=\"type\">V</span>](</span><br><span class=\"line\">      context, aggregator = <span class=\"type\">None</span>, <span class=\"type\">Some</span>(dep.partitioner), ordering = <span class=\"type\">None</span>, dep.serializer)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  sorter.insertAll(records)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> output = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> tmp = <span class=\"type\">Utils</span>.tempFileWith(output)</span><br><span class=\"line\">  <span class=\"keyword\">try</span> &#123; </span><br><span class=\"line\">    <span class=\"keyword\">val</span> blockId = <span class=\"type\">ShuffleBlockId</span>(dep.shuffleId, mapId, <span class=\"type\">IndexShuffleBlockResolver</span>.<span class=\"type\">NOOP_REDUCE_ID</span>)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> partitionLengths = sorter.writePartitionedFile(blockId, tmp)</span><br><span class=\"line\">    shuffleBlockResolver.writeIndexFileAndCommit(dep.shuffleId, mapId, partitionLengths, tmp)</span><br><span class=\"line\">    mapStatus = <span class=\"type\">MapStatus</span>(blockManager.shuffleServerId, partitionLengths)</span><br><span class=\"line\">  &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (tmp.exists() &amp;&amp; !tmp.delete()) &#123;</span><br><span class=\"line\">      logError(<span class=\"string\">s\"Error while deleting temp file <span class=\"subst\">$&#123;tmp.getAbsolutePath&#125;</span>\"</span>)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>使用了 ExternalSorter 的 insertAll 方法插入了上一个 RDD 生成的数据的迭代器。</p>\n<h4 id=\"insertAll\"><a href=\"#insertAll\" class=\"headerlink\" title=\"insertAll\"></a>insertAll</h4><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">insertAll</span></span>(records: <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>]]): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">    <span class=\"comment\">// <span class=\"doctag\">TODO:</span> stop combining if we find that the reduction factor isn't high</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> shouldCombine = aggregator.isDefined</span><br><span class=\"line\">    <span class=\"comment\">// 直接看需要 map 端聚合的情况</span></span><br><span class=\"line\">    <span class=\"comment\">// 不需要聚合的情况类似</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (shouldCombine) &#123;</span><br><span class=\"line\">      <span class=\"comment\">// Combine values in-memory first using our AppendOnlyMap</span></span><br><span class=\"line\">       </span><br><span class=\"line\">      <span class=\"keyword\">val</span> mergeValue = aggregator.get.mergeValue </span><br><span class=\"line\">      <span class=\"keyword\">val</span> createCombiner = aggregator.get.createCombiner</span><br><span class=\"line\">      <span class=\"keyword\">var</span> kv: <span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>] = <span class=\"literal\">null</span></span><br><span class=\"line\">      <span class=\"comment\">// 如果 key 不存在则直接创建 Combiner ，否则使用 mergeValue 将 value 添加到创建的 Combiner中</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> update = (hadValue: <span class=\"type\">Boolean</span>, oldValue: <span class=\"type\">C</span>) =&gt; &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (hadValue) mergeValue(oldValue, kv._2) <span class=\"keyword\">else</span> createCombiner(kv._2)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      <span class=\"keyword\">while</span> (records.hasNext) &#123;</span><br><span class=\"line\">        addElementsRead()</span><br><span class=\"line\">        kv = records.next()</span><br><span class=\"line\">        <span class=\"comment\">// 更新值</span></span><br><span class=\"line\">        <span class=\"comment\">// 这里需要注意的是之前的数据是(k,v)类型，这里转换成了((part,k),v)</span></span><br><span class=\"line\">        <span class=\"comment\">// 其中 part 是 key 对应的分区</span></span><br><span class=\"line\">        map.changeValue((getPartition(kv._1), kv._1), update)</span><br><span class=\"line\">        maybeSpillCollection(usingMap = <span class=\"literal\">true</span>)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">      <span class=\"comment\">// Stick values into our buffer</span></span><br><span class=\"line\">      <span class=\"keyword\">while</span> (records.hasNext) &#123;</span><br><span class=\"line\">        addElementsRead()</span><br><span class=\"line\">        <span class=\"keyword\">val</span> kv = records.next()</span><br><span class=\"line\">        buffer.insert(getPartition(kv._1), kv._1, kv._2.asInstanceOf[<span class=\"type\">C</span>])</span><br><span class=\"line\">        maybeSpillCollection(usingMap = <span class=\"literal\">false</span>)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>在这里使用 getPartition() 进行了数据的<strong>分区</strong>，也就是将不同的 key 与其分区对应起来。默认使用的是 HashPartitioner ，有兴趣的同学可以去阅读源码。再继续看 changeValue 方法。在这里使用非常重要的数据结构来存放分区后的数据，也就是代码中的 map 类型为 PartitionedAppendOnlyMap。其中是使用hask表存放数据，并且存放的方式为表中的偶数索index引存放key, index + 1 存放对应的value。例如:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">k1|v1|null|k2|v2|k3|v3|null|....</span><br></pre></td></tr></table></figure>\n<p>之所以里面由空值，是因为在插入的时候使用了二次探测法。来看一看 changeValue 方法:</p>\n<p>PartitionedAppendOnlyMap 中的  changeValue 方法继承自 </p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">changeValue</span></span>(key: <span class=\"type\">K</span>, updateFunc: (<span class=\"type\">Boolean</span>, <span class=\"type\">V</span>) =&gt; <span class=\"type\">V</span>): <span class=\"type\">V</span> = &#123;</span><br><span class=\"line\">  assert(!destroyed, destructionMessage)</span><br><span class=\"line\">  <span class=\"keyword\">val</span> k = key.asInstanceOf[<span class=\"type\">AnyRef</span>]</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (k.eq(<span class=\"literal\">null</span>)) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (!haveNullValue) &#123;</span><br><span class=\"line\">      incrementSize()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    nullValue = updateFunc(haveNullValue, nullValue)</span><br><span class=\"line\">    haveNullValue = <span class=\"literal\">true</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> nullValue</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"keyword\">var</span> pos = rehash(k.hashCode) &amp; mask</span><br><span class=\"line\">  <span class=\"keyword\">var</span> i = <span class=\"number\">1</span></span><br><span class=\"line\">  <span class=\"keyword\">while</span> (<span class=\"literal\">true</span>) &#123;</span><br><span class=\"line\">    <span class=\"comment\">// 偶数位置为 key</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> curKey = data(<span class=\"number\">2</span> * pos)</span><br><span class=\"line\">     <span class=\"comment\">// 如果该key不存在，就直接插入</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (curKey.eq(<span class=\"literal\">null</span>)) &#123;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> newValue = updateFunc(<span class=\"literal\">false</span>, <span class=\"literal\">null</span>.asInstanceOf[<span class=\"type\">V</span>])</span><br><span class=\"line\">      data(<span class=\"number\">2</span> * pos) = k</span><br><span class=\"line\">      data(<span class=\"number\">2</span> * pos + <span class=\"number\">1</span>) = newValue.asInstanceOf[<span class=\"type\">AnyRef</span>]</span><br><span class=\"line\">      incrementSize()</span><br><span class=\"line\">      <span class=\"keyword\">return</span> newValue</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (k.eq(curKey) || k.equals(curKey)) &#123;<span class=\"comment\">//如果 key 存在，则进行聚合</span></span><br><span class=\"line\">      </span><br><span class=\"line\">      <span class=\"keyword\">val</span> newValue = updateFunc(<span class=\"literal\">true</span>, data(<span class=\"number\">2</span> * pos + <span class=\"number\">1</span>).asInstanceOf[<span class=\"type\">V</span>])</span><br><span class=\"line\">      data(<span class=\"number\">2</span> * pos + <span class=\"number\">1</span>) = newValue.asInstanceOf[<span class=\"type\">AnyRef</span>]</span><br><span class=\"line\">      <span class=\"keyword\">return</span> newValue</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123; <span class=\"comment\">// 否则进行下一次探测</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> delta = i</span><br><span class=\"line\">      pos = (pos + delta) &amp; mask</span><br><span class=\"line\">      i += <span class=\"number\">1</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"literal\">null</span>.asInstanceOf[<span class=\"type\">V</span>] <span class=\"comment\">// Never reached but needed to keep compiler happy</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>再调用 insertAll 进行了数据的聚合|插入之后，调用了 maybeSpillCollection 方法。这个方法的作用是判断 map 占用了内存，如果内存达到一定的值，则将内存中的数据 spill 到临时文件中。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">maybeSpillCollection</span></span>(usingMap: <span class=\"type\">Boolean</span>): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">var</span> estimatedSize = <span class=\"number\">0</span>L</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (usingMap) &#123;</span><br><span class=\"line\">      estimatedSize = map.estimateSize()</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (maybeSpill(map, estimatedSize)) &#123;</span><br><span class=\"line\">        map = <span class=\"keyword\">new</span> <span class=\"type\">PartitionedAppendOnlyMap</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">      estimatedSize = buffer.estimateSize()</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (maybeSpill(buffer, estimatedSize)) &#123;</span><br><span class=\"line\">        buffer = <span class=\"keyword\">new</span> <span class=\"type\">PartitionedPairBuffer</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (estimatedSize &gt; _peakMemoryUsedBytes) &#123;</span><br><span class=\"line\">      _peakMemoryUsedBytes = estimatedSize</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>maybeSpillCollection 继续调用了 maybeSpill 方法，这个方法继承自 Spillable 中</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">protected</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">maybeSpill</span></span>(collection: <span class=\"type\">C</span>, currentMemory: <span class=\"type\">Long</span>): <span class=\"type\">Boolean</span> = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">var</span> shouldSpill = <span class=\"literal\">false</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (elementsRead % <span class=\"number\">32</span> == <span class=\"number\">0</span> &amp;&amp; currentMemory &gt;= myMemoryThreshold) &#123;</span><br><span class=\"line\">      <span class=\"comment\">// Claim up to double our current memory from the shuffle memory pool</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> amountToRequest = <span class=\"number\">2</span> * currentMemory - myMemoryThreshold</span><br><span class=\"line\">      <span class=\"keyword\">val</span> granted = acquireMemory(amountToRequest)</span><br><span class=\"line\">      myMemoryThreshold += granted</span><br><span class=\"line\">      <span class=\"comment\">// If we were granted too little memory to grow further (either tryToAcquire returned 0,</span></span><br><span class=\"line\">      <span class=\"comment\">// or we already had more memory than myMemoryThreshold), spill the current collection</span></span><br><span class=\"line\">      shouldSpill = currentMemory &gt;= myMemoryThreshold</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    shouldSpill = shouldSpill || _elementsRead &gt; numElementsForceSpillThreshold</span><br><span class=\"line\">    <span class=\"comment\">// Actually spill</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (shouldSpill) &#123;</span><br><span class=\"line\">      _spillCount += <span class=\"number\">1</span></span><br><span class=\"line\">      logSpillage(currentMemory)</span><br><span class=\"line\">      spill(collection)</span><br><span class=\"line\">      _elementsRead = <span class=\"number\">0</span></span><br><span class=\"line\">      _memoryBytesSpilled += currentMemory</span><br><span class=\"line\">      releaseMemory()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    shouldSpill</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>该方法首先判断了是否需要 spill，如果需要则调用 spill() 方法将内存中的数据写入临时文件中。spill 方法如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">override</span> <span class=\"keyword\">protected</span>[<span class=\"keyword\">this</span>] <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">spill</span></span>(collection: <span class=\"type\">WritablePartitionedPairCollection</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">   <span class=\"keyword\">val</span> inMemoryIterator = collection.destructiveSortedWritablePartitionedIterator(comparator)</span><br><span class=\"line\">   <span class=\"keyword\">val</span> spillFile = spillMemoryIteratorToDisk(inMemoryIterator)</span><br><span class=\"line\">   spills += spillFile</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>其中的  comparator 如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">comparator</span></span>: <span class=\"type\">Option</span>[<span class=\"type\">Comparator</span>[<span class=\"type\">K</span>]] = &#123;</span><br><span class=\"line\">   <span class=\"keyword\">if</span> (ordering.isDefined || aggregator.isDefined) &#123;</span><br><span class=\"line\">     <span class=\"type\">Some</span>(keyComparator)</span><br><span class=\"line\">   &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">     <span class=\"type\">None</span></span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>也就是:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"keyword\">val</span> keyComparator: <span class=\"type\">Comparator</span>[<span class=\"type\">K</span>] = ordering.getOrElse(<span class=\"keyword\">new</span> <span class=\"type\">Comparator</span>[<span class=\"type\">K</span>] &#123;</span><br><span class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compare</span></span>(a: <span class=\"type\">K</span>, b: <span class=\"type\">K</span>): <span class=\"type\">Int</span> = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> h1 = <span class=\"keyword\">if</span> (a == <span class=\"literal\">null</span>) <span class=\"number\">0</span> <span class=\"keyword\">else</span> a.hashCode()</span><br><span class=\"line\">    <span class=\"keyword\">val</span> h2 = <span class=\"keyword\">if</span> (b == <span class=\"literal\">null</span>) <span class=\"number\">0</span> <span class=\"keyword\">else</span> b.hashCode()</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (h1 &lt; h2) <span class=\"number\">-1</span> <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (h1 == h2) <span class=\"number\">0</span> <span class=\"keyword\">else</span> <span class=\"number\">1</span></span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;)</span><br></pre></td></tr></table></figure>\n<p>destructiveSortedWritablePartitionedIterator 位于 WritablePartitionedPairCollection 类中，实现如下:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">destructiveSortedWritablePartitionedIterator</span></span>(keyComparator: <span class=\"type\">Option</span>[<span class=\"type\">Comparator</span>[<span class=\"type\">K</span>]])</span><br><span class=\"line\">    : <span class=\"type\">WritablePartitionedIterator</span> = &#123;</span><br><span class=\"line\">    <span class=\"comment\">//这里调用的其实是　PartitionedAppendOnlyMap　中重写的　partitionedDestructiveSortedIterator    </span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> it = partitionedDestructiveSortedIterator(keyComparator)</span><br><span class=\"line\">    <span class=\"comment\">// 这里还实现了一个　writeNext　的方法，后面会用到</span></span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">WritablePartitionedIterator</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">private</span>[<span class=\"keyword\">this</span>] <span class=\"keyword\">var</span> cur = <span class=\"keyword\">if</span> (it.hasNext) it.next() <span class=\"keyword\">else</span> <span class=\"literal\">null</span></span><br><span class=\"line\">       <span class=\"comment\">// 在map 中的数据其实是((partition,k),v)</span></span><br><span class=\"line\">       <span class=\"comment\">// 这里只写入了(k,v)</span></span><br><span class=\"line\">      <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">writeNext</span></span>(writer: <span class=\"type\">DiskBlockObjectWriter</span>): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">        writer.write(cur._1._2, cur._2)</span><br><span class=\"line\">        cur = <span class=\"keyword\">if</span> (it.hasNext) it.next() <span class=\"keyword\">else</span> <span class=\"literal\">null</span></span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hasNext</span></span>(): <span class=\"type\">Boolean</span> = cur != <span class=\"literal\">null</span></span><br><span class=\"line\">      <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">nextPartition</span></span>(): <span class=\"type\">Int</span> = cur._1._1</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>接下来看看partitionedDestructiveSortedIterator:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">partitionedDestructiveSortedIterator</span></span>(keyComparator: <span class=\"type\">Option</span>[<span class=\"type\">Comparator</span>[<span class=\"type\">K</span>]])</span><br><span class=\"line\">    : <span class=\"type\">Iterator</span>[((<span class=\"type\">Int</span>, <span class=\"type\">K</span>), <span class=\"type\">V</span>)] = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> comparator = keyComparator.map(partitionKeyComparator).getOrElse(partitionComparator)</span><br><span class=\"line\">    destructiveSortedIterator(comparator)</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>keyComparator.map(partitionKeyComparator)　等于 partitionKeyComparator(keyComparator)，再看下面的partitionKeyComparator，说明比较器是先对分区进行比较，如果分区相同，再对key的hash值进行比较:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">partitionKeyComparator</span></span>[<span class=\"type\">K</span>](keyComparator: <span class=\"type\">Comparator</span>[<span class=\"type\">K</span>]): <span class=\"type\">Comparator</span>[(<span class=\"type\">Int</span>, <span class=\"type\">K</span>)] = &#123;</span><br><span class=\"line\">   <span class=\"keyword\">new</span> <span class=\"type\">Comparator</span>[(<span class=\"type\">Int</span>, <span class=\"type\">K</span>)] &#123;</span><br><span class=\"line\">     <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compare</span></span>(a: (<span class=\"type\">Int</span>, <span class=\"type\">K</span>), b: (<span class=\"type\">Int</span>, <span class=\"type\">K</span>)): <span class=\"type\">Int</span> = &#123;</span><br><span class=\"line\">       <span class=\"keyword\">val</span> partitionDiff = a._1 - b._1</span><br><span class=\"line\">       <span class=\"keyword\">if</span> (partitionDiff != <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">         partitionDiff</span><br><span class=\"line\">       &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">         keyComparator.compare(a._2, b._2)</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>然后看destructiveSortedIterator(),这是在 PartitionedAppendOnlyMap 的父类 destructiveSortedIterator 中实现的:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">destructiveSortedIterator</span></span>(keyComparator: <span class=\"type\">Comparator</span>[<span class=\"type\">K</span>]): <span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">V</span>)] = &#123;</span><br><span class=\"line\">   destroyed = <span class=\"literal\">true</span></span><br><span class=\"line\">   <span class=\"comment\">// Pack KV pairs into the front of the underlying array</span></span><br><span class=\"line\">   <span class=\"comment\">// 这里是因为　PartitionedAppendOnlyMap　采用的二次探测法　,kv对之间存在　null值，所以先把非空的kv对全部移动到hash表的前端</span></span><br><span class=\"line\">   <span class=\"keyword\">var</span> keyIndex, newIndex = <span class=\"number\">0</span></span><br><span class=\"line\">   <span class=\"keyword\">while</span> (keyIndex &lt; capacity) &#123;</span><br><span class=\"line\">     <span class=\"keyword\">if</span> (data(<span class=\"number\">2</span> * keyIndex) != <span class=\"literal\">null</span>) &#123;</span><br><span class=\"line\">       data(<span class=\"number\">2</span> * newIndex) = data(<span class=\"number\">2</span> * keyIndex)</span><br><span class=\"line\">       data(<span class=\"number\">2</span> * newIndex + <span class=\"number\">1</span>) = data(<span class=\"number\">2</span> * keyIndex + <span class=\"number\">1</span>)</span><br><span class=\"line\">       newIndex += <span class=\"number\">1</span></span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">     keyIndex += <span class=\"number\">1</span></span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">   assert(curSize == newIndex + (<span class=\"keyword\">if</span> (haveNullValue) <span class=\"number\">1</span> <span class=\"keyword\">else</span> <span class=\"number\">0</span>))</span><br><span class=\"line\">   <span class=\"comment\">// 这里对给定数据范围为 0 to newIndex，利用　keyComparator　比较器进行排序</span></span><br><span class=\"line\">   <span class=\"comment\">// 也就是对 map 中的数据根据　(partition,key) 进行排序</span></span><br><span class=\"line\">   <span class=\"keyword\">new</span> <span class=\"type\">Sorter</span>(<span class=\"keyword\">new</span> <span class=\"type\">KVArraySortDataFormat</span>[<span class=\"type\">K</span>, <span class=\"type\">AnyRef</span>]).sort(data, <span class=\"number\">0</span>, newIndex, keyComparator)</span><br><span class=\"line\"></span><br><span class=\"line\">   <span class=\"comment\">// 定义迭代器</span></span><br><span class=\"line\">   <span class=\"keyword\">new</span> <span class=\"type\">Iterator</span>[(<span class=\"type\">K</span>, <span class=\"type\">V</span>)] &#123;</span><br><span class=\"line\">     <span class=\"keyword\">var</span> i = <span class=\"number\">0</span></span><br><span class=\"line\">     <span class=\"keyword\">var</span> nullValueReady = haveNullValue</span><br><span class=\"line\">     <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hasNext</span></span>: <span class=\"type\">Boolean</span> = (i &lt; newIndex || nullValueReady)</span><br><span class=\"line\">     <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">next</span></span>(): (<span class=\"type\">K</span>, <span class=\"type\">V</span>) = &#123;</span><br><span class=\"line\">       <span class=\"keyword\">if</span> (nullValueReady) &#123;</span><br><span class=\"line\">         nullValueReady = <span class=\"literal\">false</span></span><br><span class=\"line\">         (<span class=\"literal\">null</span>.asInstanceOf[<span class=\"type\">K</span>], nullValue)</span><br><span class=\"line\">       &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">         <span class=\"keyword\">val</span> item = (data(<span class=\"number\">2</span> * i).asInstanceOf[<span class=\"type\">K</span>], data(<span class=\"number\">2</span> * i + <span class=\"number\">1</span>).asInstanceOf[<span class=\"type\">V</span>])</span><br><span class=\"line\">         i += <span class=\"number\">1</span></span><br><span class=\"line\">         item</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">   &#125;</span><br><span class=\"line\"> &#125;</span><br></pre></td></tr></table></figure>\n<p>可以看出整个　<code>val inMemoryIterator = collection.destructiveSortedWritablePartitionedIterator(comparator)</code>　这一步其实就是在对　map　或 buffer 进行排序，并且实现了一个 writeNext() 方法供后续调调用。随后调用<code>val spillFile = spillMemoryIteratorToDisk(inMemoryIterator)</code>将排序后的缓存写入文件中:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span>[<span class=\"keyword\">this</span>] <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">spillMemoryIteratorToDisk</span></span>(inMemoryIterator: <span class=\"type\">WritablePartitionedIterator</span>)</span><br><span class=\"line\">      : <span class=\"type\">SpilledFile</span> = &#123;</span><br><span class=\"line\">   </span><br><span class=\"line\">    <span class=\"keyword\">val</span> (blockId, file) = diskBlockManager.createTempShuffleBlock()</span><br><span class=\"line\">    <span class=\"comment\">// These variables are reset after each flush</span></span><br><span class=\"line\">    <span class=\"keyword\">var</span> objectsWritten: <span class=\"type\">Long</span> = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> spillMetrics: <span class=\"type\">ShuffleWriteMetrics</span> = <span class=\"keyword\">new</span> <span class=\"type\">ShuffleWriteMetrics</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> writer: <span class=\"type\">DiskBlockObjectWriter</span> =</span><br><span class=\"line\">      blockManager.getDiskWriter(blockId, file, serInstance, fileBufferSize, spillMetrics)</span><br><span class=\"line\">    <span class=\"comment\">// List of batch sizes (bytes) in the order they are written to disk</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> batchSizes = <span class=\"keyword\">new</span> <span class=\"type\">ArrayBuffer</span>[<span class=\"type\">Long</span>]</span><br><span class=\"line\">    <span class=\"comment\">// How many elements we have in each partition</span></span><br><span class=\"line\">    <span class=\"comment\">// 用于记录每一个分区有多少条数据</span></span><br><span class=\"line\">    <span class=\"comment\">// 由于刚才数据写入文件没有写入key对应的分区，因此需要记录每一个分区有多少条数据，然后根据数据的偏移量就可以判断该数据属于哪个分区　</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> elementsPerPartition = <span class=\"keyword\">new</span> <span class=\"type\">Array</span>[<span class=\"type\">Long</span>](numPartitions)</span><br><span class=\"line\">    <span class=\"comment\">// Flush the disk writer's contents to disk, and update relevant variables.</span></span><br><span class=\"line\">    <span class=\"comment\">// The writer is committed at the end of this process.</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">flush</span></span>(): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> segment = writer.commitAndGet()</span><br><span class=\"line\">      batchSizes += segment.length</span><br><span class=\"line\">      _diskBytesSpilled += segment.length</span><br><span class=\"line\">      objectsWritten = <span class=\"number\">0</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">var</span> success = <span class=\"literal\">false</span></span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">while</span> (inMemoryIterator.hasNext) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">val</span> partitionId = inMemoryIterator.nextPartition()</span><br><span class=\"line\">        require(partitionId &gt;= <span class=\"number\">0</span> &amp;&amp; partitionId &lt; numPartitions,</span><br><span class=\"line\">          <span class=\"string\">s\"partition Id: <span class=\"subst\">$&#123;partitionId&#125;</span> should be in the range [0, <span class=\"subst\">$&#123;numPartitions&#125;</span>)\"</span>)</span><br><span class=\"line\">        inMemoryIterator.writeNext(writer)</span><br><span class=\"line\">        elementsPerPartition(partitionId) += <span class=\"number\">1</span></span><br><span class=\"line\">        objectsWritten += <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (objectsWritten == serializerBatchSize) &#123;　<span class=\"comment\">//写入　serializerBatchSize　条数据便刷新一次缓存</span></span><br><span class=\"line\">          <span class=\"comment\">// batchSize 在类中定义的如下:</span></span><br><span class=\"line\">          <span class=\"comment\">// 可以看出如果不存在配置默认为　10000　条</span></span><br><span class=\"line\">          <span class=\"comment\">// private val serializerBatchSize = conf.getLong(\"spark.shuffle.spill.batchSize\", 10000)</span></span><br><span class=\"line\">          flush()</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (objectsWritten &gt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">        flush()</span><br><span class=\"line\">      &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        writer.revertPartialWritesAndClose()</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      success = <span class=\"literal\">true</span></span><br><span class=\"line\">    &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (success) &#123;</span><br><span class=\"line\">        writer.close()</span><br><span class=\"line\">      &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// This code path only happens if an exception was thrown above before we set success;</span></span><br><span class=\"line\">        <span class=\"comment\">// close our stuff and let the exception be thrown further</span></span><br><span class=\"line\">        writer.revertPartialWritesAndClose()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (file.exists()) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">if</span> (!file.delete()) &#123;</span><br><span class=\"line\">            logWarning(<span class=\"string\">s\"Error deleting <span class=\"subst\">$&#123;file&#125;</span>\"</span>)</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">//最后记录临时文件的信息</span></span><br><span class=\"line\">    <span class=\"type\">SpilledFile</span>(file, blockId, batchSizes.toArray, elementsPerPartition)</span><br><span class=\"line\"></span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>这就是整个　insertAll()　方法:</p>\n<p> merge 达到溢出值后进行排序并写入临时文件，这里要注意的是并非所有的缓存文件都被写入到临时文件中，所以接下来需要将临时文件中的数据加上内存中的数据进行一次排序写入到一个大文件中。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//首先获取需要写入的文件:　</span></span><br><span class=\"line\">   <span class=\"keyword\">val</span> output = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)</span><br><span class=\"line\">　　<span class=\"keyword\">val</span> tmp = <span class=\"type\">Utils</span>.tempFileWith(output)</span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123; </span><br><span class=\"line\">      <span class=\"keyword\">val</span> blockId = <span class=\"type\">ShuffleBlockId</span>(dep.shuffleId, mapId, <span class=\"type\">IndexShuffleBlockResolver</span>.<span class=\"type\">NOOP_REDUCE_ID</span>)</span><br><span class=\"line\">      <span class=\"keyword\">val</span> partitionLengths = sorter.writePartitionedFile(blockId, tmp)<span class=\"comment\">//这一句是重点 下面会讲解</span></span><br><span class=\"line\">      shuffleBlockResolver.writeIndexFileAndCommit(dep.shuffleId, mapId, partitionLengths, tmp)</span><br><span class=\"line\">      mapStatus = <span class=\"type\">MapStatus</span>(blockManager.shuffleServerId, partitionLengths)</span><br><span class=\"line\">    &#125; <span class=\"keyword\">finally</span> &#123;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (tmp.exists() &amp;&amp; !tmp.delete()) &#123;</span><br><span class=\"line\">        logError(<span class=\"string\">s\"Error while deleting temp file <span class=\"subst\">$&#123;tmp.getAbsolutePath&#125;</span>\"</span>)</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"writePartitionedFile\"><a href=\"#writePartitionedFile\" class=\"headerlink\" title=\"writePartitionedFile\"></a>writePartitionedFile</h4><p>继续看 writePartitionedFile :</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//  ExternalSorter 中的　writePartitionedFile　方法</span></span><br><span class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">writePartitionedFile</span></span>(</span><br><span class=\"line\">      blockId: <span class=\"type\">BlockId</span>,</span><br><span class=\"line\">      outputFile: <span class=\"type\">File</span>): <span class=\"type\">Array</span>[<span class=\"type\">Long</span>] = &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// Track location of each range in the output file</span></span><br><span class=\"line\">    <span class=\"keyword\">val</span> lengths = <span class=\"keyword\">new</span> <span class=\"type\">Array</span>[<span class=\"type\">Long</span>](numPartitions)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> writer = blockManager.getDiskWriter(blockId, outputFile, serInstance, fileBufferSize,</span><br><span class=\"line\">      context.taskMetrics().shuffleWriteMetrics)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 这里的spills 就是　刚才产生的临时文件的数据　如果为空逻辑就比较简单，直接进行排序并写入文件</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (spills.isEmpty) &#123;</span><br><span class=\"line\">      <span class=\"comment\">// Case where we only have in-memory data</span></span><br><span class=\"line\">      <span class=\"keyword\">val</span> collection = <span class=\"keyword\">if</span> (aggregator.isDefined) map <span class=\"keyword\">else</span> buffer</span><br><span class=\"line\">      <span class=\"keyword\">val</span> it = collection.destructiveSortedWritablePartitionedIterator(comparator)　<span class=\"comment\">//这里同样使用　destructiveSortedWritablePartitionedIterator　进行排序</span></span><br><span class=\"line\">      <span class=\"keyword\">while</span> (it.hasNext) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">val</span> partitionId = it.nextPartition()</span><br><span class=\"line\">        <span class=\"keyword\">while</span> (it.hasNext &amp;&amp; it.nextPartition() == partitionId) &#123;</span><br><span class=\"line\">          it.writeNext(writer)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">val</span> segment = writer.commitAndGet()</span><br><span class=\"line\">        lengths(partitionId) = segment.length</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;<span class=\"comment\">//重点看不为空的时候，这里调用了　partitionedIterator　方法</span></span><br><span class=\"line\">      <span class=\"comment\">// We must perform merge-sort; get an iterator by partition and write everything directly.</span></span><br><span class=\"line\">      <span class=\"keyword\">for</span> ((id, elements) &lt;- <span class=\"keyword\">this</span>.partitionedIterator) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (elements.hasNext) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">for</span> (elem &lt;- elements) &#123;</span><br><span class=\"line\">            writer.write(elem._1, elem._2)</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">          <span class=\"keyword\">val</span> segment = writer.commitAndGet()</span><br><span class=\"line\">          lengths(id) = segment.length</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    writer.close()</span><br><span class=\"line\">    context.taskMetrics().incMemoryBytesSpilled(memoryBytesSpilled)</span><br><span class=\"line\">    context.taskMetrics().incDiskBytesSpilled(diskBytesSpilled)</span><br><span class=\"line\">    context.taskMetrics().incPeakExecutionMemory(peakMemoryUsedBytes)</span><br><span class=\"line\"></span><br><span class=\"line\">    lengths</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>然后是 partitionedIterator 方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">partitionedIterator</span></span>: <span class=\"type\">Iterator</span>[(<span class=\"type\">Int</span>, <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]])] = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> usingMap = aggregator.isDefined</span><br><span class=\"line\">    <span class=\"keyword\">val</span> collection: <span class=\"type\">WritablePartitionedPairCollection</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>] = <span class=\"keyword\">if</span> (usingMap) map <span class=\"keyword\">else</span> buffer</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (spills.isEmpty) &#123;<span class=\"comment\">// 这里又一次判断了是否为空，直接看有临时文件的部分</span></span><br><span class=\"line\">      <span class=\"comment\">// Special case: if we have only in-memory data, we don't need to merge streams, and perhaps</span></span><br><span class=\"line\">      <span class=\"comment\">// we don't even need to sort by anything other than partition ID</span></span><br><span class=\"line\">      <span class=\"keyword\">if</span> (!ordering.isDefined) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// The user hasn't requested sorted keys, so only sort by partition ID, not key</span></span><br><span class=\"line\">   groupByPartition(destructiveIterator(collection.partitionedDestructiveSortedIterator(<span class=\"type\">None</span>)))</span><br><span class=\"line\">      &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        <span class=\"comment\">// We do need to sort by both partition ID and key</span></span><br><span class=\"line\">        groupByPartition(destructiveIterator(</span><br><span class=\"line\">          collection.partitionedDestructiveSortedIterator(<span class=\"type\">Some</span>(keyComparator))))</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">      <span class=\"comment\">// Merge spilled and in-memory data</span></span><br><span class=\"line\">      <span class=\"comment\">// 这里传入了临时文件　spills　和　排序后的缓存文件</span></span><br><span class=\"line\">      merge(spills, destructiveIterator(</span><br><span class=\"line\">        collection.partitionedDestructiveSortedIterator(comparator)))</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br></pre></td></tr></table></figure>\n<p>merge 方法:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// merge方法</span></span><br><span class=\"line\">  <span class=\"comment\">// inMemory　是根据(partion,hash(k)) 排序后的内存数据</span></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">merge</span></span>(spills: <span class=\"type\">Seq</span>[<span class=\"type\">SpilledFile</span>], inMemory: <span class=\"type\">Iterator</span>[((<span class=\"type\">Int</span>, <span class=\"type\">K</span>), <span class=\"type\">C</span>)])</span><br><span class=\"line\">     : <span class=\"type\">Iterator</span>[(<span class=\"type\">Int</span>, <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]])] = &#123;</span><br><span class=\"line\">   <span class=\"comment\">//　将所有缓存文件转化为 SpillReader </span></span><br><span class=\"line\">   <span class=\"keyword\">val</span> readers = spills.map(<span class=\"keyword\">new</span> <span class=\"type\">SpillReader</span>(_))</span><br><span class=\"line\">   <span class=\"comment\">// buffered方法只是比普通的　itertor 方法多提供一个　head　方法，例如:</span></span><br><span class=\"line\">   <span class=\"comment\">// val a = List(1,2,3,4,5)</span></span><br><span class=\"line\">   <span class=\"comment\">// val b = a.iterator.buffered</span></span><br><span class=\"line\">   <span class=\"comment\">// b.head : 1</span></span><br><span class=\"line\">   <span class=\"comment\">// b.next : 1</span></span><br><span class=\"line\">   <span class=\"comment\">// b.head : 2</span></span><br><span class=\"line\">   <span class=\"comment\">// b.next : 2</span></span><br><span class=\"line\">   <span class=\"keyword\">val</span> inMemBuffered = inMemory.buffered</span><br><span class=\"line\">   (<span class=\"number\">0</span> until numPartitions).iterator.map &#123; p =&gt;</span><br><span class=\"line\">     <span class=\"comment\">// 获得分区对应数据的迭代器 </span></span><br><span class=\"line\">     <span class=\"comment\">// 这里的　IteratorForPartition　就是得到分区 p 对应的数据的迭代器，逻辑比较简单，就不单独拿出来分析</span></span><br><span class=\"line\">     <span class=\"keyword\">val</span> inMemIterator = <span class=\"keyword\">new</span> <span class=\"type\">IteratorForPartition</span>(p, inMemBuffered)</span><br><span class=\"line\">     </span><br><span class=\"line\">     <span class=\"comment\">//这里获取所有文件的第　p　个分区　并与内存中的第　p 个分区进行合并</span></span><br><span class=\"line\">     <span class=\"keyword\">val</span> iterators = readers.map(_.readNextPartition()) ++ <span class=\"type\">Seq</span>(inMemIterator)</span><br><span class=\"line\">　　　<span class=\"comment\">// 这里的　iterators　是由　多个 iterator 组成的，其中每一个都是关于 p 分区数据的 iterator</span></span><br><span class=\"line\">     <span class=\"keyword\">if</span> (aggregator.isDefined) &#123;</span><br><span class=\"line\">       <span class=\"comment\">// Perform partial aggregation across partitions</span></span><br><span class=\"line\">       (p, mergeWithAggregation(</span><br><span class=\"line\">         iterators, aggregator.get.mergeCombiners, keyComparator, ordering.isDefined))</span><br><span class=\"line\">     &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (ordering.isDefined) &#123;</span><br><span class=\"line\">       <span class=\"comment\">// No aggregator given, but we have an ordering (e.g. used by reduce tasks in sortByKey);</span></span><br><span class=\"line\">       <span class=\"comment\">// sort the elements without trying to merge them</span></span><br><span class=\"line\">       (p, mergeSort(iterators, ordering.get))</span><br><span class=\"line\">     &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">       (p, iterators.iterator.flatten)</span><br><span class=\"line\">     &#125;</span><br><span class=\"line\">   &#125;</span><br></pre></td></tr></table></figure>\n<p>mergeWithAggregation<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">mergeWithAggregation</span></span>(</span><br><span class=\"line\">    iterators: <span class=\"type\">Seq</span>[<span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]]],</span><br><span class=\"line\">    mergeCombiners: (<span class=\"type\">C</span>, <span class=\"type\">C</span>) =&gt; <span class=\"type\">C</span>,</span><br><span class=\"line\">    comparator: <span class=\"type\">Comparator</span>[<span class=\"type\">K</span>],</span><br><span class=\"line\">    totalOrder: <span class=\"type\">Boolean</span>)</span><br><span class=\"line\">    : <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] =</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (!totalOrder) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">Iterator</span>[<span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]]] &#123;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> sorted = mergeSort(iterators, comparator).buffered</span><br><span class=\"line\">      <span class=\"keyword\">val</span> keys = <span class=\"keyword\">new</span> <span class=\"type\">ArrayBuffer</span>[<span class=\"type\">K</span>]</span><br><span class=\"line\">      <span class=\"keyword\">val</span> combiners = <span class=\"keyword\">new</span> <span class=\"type\">ArrayBuffer</span>[<span class=\"type\">C</span>]</span><br><span class=\"line\">      <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hasNext</span></span>: <span class=\"type\">Boolean</span> = sorted.hasNext</span><br><span class=\"line\">      <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">next</span></span>(): <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] = &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!hasNext) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">NoSuchElementException</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        keys.clear()</span><br><span class=\"line\">        combiners.clear()</span><br><span class=\"line\">        <span class=\"keyword\">val</span> firstPair = sorted.next()</span><br><span class=\"line\">        keys += firstPair._1</span><br><span class=\"line\">        combiners += firstPair._2</span><br><span class=\"line\">        <span class=\"keyword\">val</span> key = firstPair._1</span><br><span class=\"line\">        <span class=\"keyword\">while</span> (sorted.hasNext &amp;&amp; comparator.compare(sorted.head._1, key) == <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">val</span> pair = sorted.next()</span><br><span class=\"line\">          <span class=\"keyword\">var</span> i = <span class=\"number\">0</span></span><br><span class=\"line\">          <span class=\"keyword\">var</span> foundKey = <span class=\"literal\">false</span></span><br><span class=\"line\">          <span class=\"keyword\">while</span> (i &lt; keys.size &amp;&amp; !foundKey) &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (keys(i) == pair._1) &#123;</span><br><span class=\"line\">              combiners(i) = mergeCombiners(combiners(i), pair._2)</span><br><span class=\"line\">              foundKey = <span class=\"literal\">true</span></span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            i += <span class=\"number\">1</span></span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">          <span class=\"keyword\">if</span> (!foundKey) &#123;</span><br><span class=\"line\">            keys += pair._1</span><br><span class=\"line\">            combiners += pair._2</span><br><span class=\"line\">          &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        keys.iterator.zip(combiners.iterator)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;.flatMap(i =&gt; i)</span><br><span class=\"line\">  &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">// We have a total ordering, so the objects with the same key are sequential.</span></span><br><span class=\"line\">    <span class=\"keyword\">new</span> <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] &#123;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> sorted = mergeSort(iterators, comparator).buffered</span><br><span class=\"line\">      <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hasNext</span></span>: <span class=\"type\">Boolean</span> = sorted.hasNext</span><br><span class=\"line\">      <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">next</span></span>(): <span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>] = &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!hasNext) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">NoSuchElementException</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">val</span> elem = sorted.next()</span><br><span class=\"line\">        <span class=\"keyword\">val</span> k = elem._1</span><br><span class=\"line\">        <span class=\"keyword\">var</span> c = elem._2</span><br><span class=\"line\">        <span class=\"comment\">// 这里的意思是可能存在多个相同的key 分布在不同临时文件或内存中</span></span><br><span class=\"line\">        <span class=\"comment\">// 所以还需要将不同的 key 对应的值进行合并 </span></span><br><span class=\"line\">        <span class=\"keyword\">while</span> (sorted.hasNext &amp;&amp; sorted.head._1 == k) &#123;</span><br><span class=\"line\">          <span class=\"keyword\">val</span> pair = sorted.next()</span><br><span class=\"line\">          c = mergeCombiners(c, pair._2)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        (k, c)<span class=\"comment\">//返回</span></span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>继续看 mergeSort 方法，首先选除头元素最小对应那个分区，然后选出这个分区的第一个元素，那么这个元素一定是最小的，然后将剩余的元素放回去:</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">mergeSort</span></span>(iterators: <span class=\"type\">Seq</span>[<span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]]], comparator: <span class=\"type\">Comparator</span>[<span class=\"type\">K</span>])</span><br><span class=\"line\">    : <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] =</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  <span class=\"keyword\">val</span> bufferedIters = iterators.filter(_.hasNext).map(_.buffered)</span><br><span class=\"line\">  <span class=\"class\"><span class=\"keyword\">type</span> <span class=\"title\">Iter</span> </span>= <span class=\"type\">BufferedIterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]]</span><br><span class=\"line\">  <span class=\"comment\">//选取头元素最小的分区</span></span><br><span class=\"line\">  <span class=\"keyword\">val</span> heap = <span class=\"keyword\">new</span> mutable.<span class=\"type\">PriorityQueue</span>[<span class=\"type\">Iter</span>]()(<span class=\"keyword\">new</span> <span class=\"type\">Ordering</span>[<span class=\"type\">Iter</span>] &#123;</span><br><span class=\"line\">    <span class=\"comment\">// Use the reverse of comparator.compare because PriorityQueue dequeues the max</span></span><br><span class=\"line\">    <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compare</span></span>(x: <span class=\"type\">Iter</span>, y: <span class=\"type\">Iter</span>): <span class=\"type\">Int</span> = -comparator.compare(x.head._1, y.head._1)</span><br><span class=\"line\">  &#125;)</span><br><span class=\"line\">  heap.enqueue(bufferedIters: _*) </span><br><span class=\"line\">  <span class=\"keyword\">new</span> <span class=\"type\">Iterator</span>[<span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>]] &#123;</span><br><span class=\"line\">    <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hasNext</span></span>: <span class=\"type\">Boolean</span> = !heap.isEmpty</span><br><span class=\"line\">    <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">next</span></span>(): <span class=\"type\">Product2</span>[<span class=\"type\">K</span>, <span class=\"type\">C</span>] = &#123;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (!hasNext) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> <span class=\"type\">NoSuchElementException</span></span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      <span class=\"keyword\">val</span> firstBuf = heap.dequeue()</span><br><span class=\"line\">      <span class=\"keyword\">val</span> firstPair = firstBuf.next()</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (firstBuf.hasNext) &#123;</span><br><span class=\"line\">        heap.enqueue(firstBuf)</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      firstPair</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>接下来就使用 writeIndexFileAndCommit 将每一个分区的数据条数写入索引文件，便于 reduce 端获取。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>shuflle write 相比起 shuffe read 要复杂很多，因此本文的篇幅也较长。其实在 shuffle write 有两个较为重要的方法，其中一个是 insertAll。这个方法的作用就是将数据缓存到一个可以添加的 hash 表中。如果表占用的内存达到的一定值，就对其进行排序。排序方式先按照分区进行排序，如果分区相同则按key的hash进行排序，随后将排序后的数据写入临时文件中，这个过程可能会生成多个临时文件。</p>\n<p>然后 writePartitionedFile 的作用是最后将内存中的数据和临时文件中的数据进行全部排序，写入到一个大文件中。</p>\n<p>最后将每一个分区对应的索引数据写入文件。整个shuffle write 阶段就完成了</p>\n"}],"PostAsset":[{"_id":"source/_posts/异常检测-体验孤立森林/normal_data.png","slug":"normal_data.png","post":"cjjhx8v3e000gb8tgnh9dvicw","modified":0,"renderable":0},{"_id":"source/_posts/数据挖掘基础-最大似然估计与最大后验估计/func_img.png","slug":"func_img.png","post":"cjjhx8v3n000sb8tgkinolr95","modified":0,"renderable":0},{"_id":"source/_posts/使用travis自动部署hexo到github/author-code.png","slug":"author-code.png","post":"cjjhx8v330005b8tgzoedeabn","modified":0,"renderable":0},{"_id":"source/_posts/使用travis自动部署hexo到github/author.png","slug":"author.png","post":"cjjhx8v330005b8tgzoedeabn","modified":0,"renderable":0},{"_id":"source/_posts/使用travis自动部署hexo到github/token-add.png","slug":"token-add.png","post":"cjjhx8v330005b8tgzoedeabn","modified":0,"renderable":0},{"_id":"source/_posts/使用travis自动部署hexo到github/travis-add.png","slug":"travis-add.png","post":"cjjhx8v330005b8tgzoedeabn","modified":0,"renderable":0},{"_id":"source/_posts/使用travis自动部署hexo到github/travis-mainpage.png","slug":"travis-mainpage.png","post":"cjjhx8v330005b8tgzoedeabn","modified":0,"renderable":0},{"_id":"source/_posts/数据挖掘基础-决策树/continue.png","slug":"continue.png","post":"cjjhx8v7c002tb8tgld0ujngr","modified":0,"renderable":0},{"_id":"source/_posts/数据挖掘基础-决策树/cut.png","slug":"cut.png","post":"cjjhx8v7c002tb8tgld0ujngr","modified":0,"renderable":0},{"_id":"source/_posts/数据挖掘基础-决策树/discrete.png","slug":"discrete.png","post":"cjjhx8v7c002tb8tgld0ujngr","modified":0,"renderable":0},{"_id":"source/_posts/数据挖掘基础-决策树/tree.png","slug":"tree.png","post":"cjjhx8v7c002tb8tgld0ujngr","modified":0,"renderable":0},{"_id":"source/_posts/数据挖掘基础-决策树/未命名文件 (1).png","slug":"未命名文件 (1).png","post":"cjjhx8v7c002tb8tgld0ujngr","modified":0,"renderable":0},{"_id":"source/_posts/异常检测-体验孤立森林/normal_scores_dis.png","slug":"normal_scores_dis.png","post":"cjjhx8v3e000gb8tgnh9dvicw","modified":0,"renderable":0},{"_id":"source/_posts/异常检测-体验孤立森林/absolute_data.png","slug":"absolute_data.png","post":"cjjhx8v3e000gb8tgnh9dvicw","modified":1,"renderable":0},{"_id":"source/_posts/异常检测-体验孤立森林/absolute_score_dis.png","slug":"absolute_score_dis.png","post":"cjjhx8v3e000gb8tgnh9dvicw","modified":1,"renderable":0},{"_id":"source/_posts/异常检测-体验孤立森林/compare.png","slug":"compare.png","post":"cjjhx8v3e000gb8tgnh9dvicw","modified":1,"renderable":0},{"_id":"source/_posts/异常检测-体验孤立森林/normal_result.png","slug":"normal_result.png","post":"cjjhx8v3e000gb8tgnh9dvicw","modified":1,"renderable":0},{"_id":"source/_posts/异常检测-体验孤立森林/uniform_scores_dis.png","slug":"uniform_scores_dis.png","post":"cjjhx8v3e000gb8tgnh9dvicw","modified":1,"renderable":0},{"_id":"source/_posts/异常检测-体验孤立森林/uniform_data.png","slug":"uniform_data.png","post":"cjjhx8v3e000gb8tgnh9dvicw","modified":1,"renderable":0}],"PostCategory":[{"post_id":"cjjhx8v2p0000b8tgrunuz6oj","category_id":"cjjhx8v2y0002b8tgb058bzb6","_id":"cjjhx8v39000bb8tgc4k29298"},{"post_id":"cjjhx8v370009b8tgd7hyxjer","category_id":"cjjhx8v2y0002b8tgb058bzb6","_id":"cjjhx8v3g000hb8tgkicoexp9"},{"post_id":"cjjhx8v2v0001b8tguq1mby4v","category_id":"cjjhx8v360007b8tg9l2zh3gw","_id":"cjjhx8v3j000mb8tg9f34ai39"},{"post_id":"cjjhx8v310004b8tgipujopgx","category_id":"cjjhx8v3a000cb8tgyftr6vto","_id":"cjjhx8v3m000qb8tgtc9ffpae"},{"post_id":"cjjhx8v330005b8tgzoedeabn","category_id":"cjjhx8v3a000cb8tgyftr6vto","_id":"cjjhx8v3q000vb8tgowcjwb13"},{"post_id":"cjjhx8v340006b8tgq16c9fkl","category_id":"cjjhx8v3a000cb8tgyftr6vto","_id":"cjjhx8v3r000yb8tg74w8dmkb"},{"post_id":"cjjhx8v38000ab8tg5zm33gv8","category_id":"cjjhx8v3p000ub8tgq59y1ruy","_id":"cjjhx8v3s0012b8tg9p152qdv"},{"post_id":"cjjhx8v3b000eb8tg79yrbxlx","category_id":"cjjhx8v3r0010b8tgnolrjl6q","_id":"cjjhx8v3u0019b8tgk54irjah"},{"post_id":"cjjhx8v3i000lb8tgg2r8xyzb","category_id":"cjjhx8v3r0010b8tgnolrjl6q","_id":"cjjhx8v3w001fb8tg46uc3jvk"},{"post_id":"cjjhx8v3k000ob8tgeoi1qbba","category_id":"cjjhx8v3r0010b8tgnolrjl6q","_id":"cjjhx8v3x001jb8tg9o9we2a0"},{"post_id":"cjjhx8v3n000sb8tgkinolr95","category_id":"cjjhx8v3w001eb8tg3hf98jhz","_id":"cjjhx8v3z001ob8tgszm1kmwy"},{"post_id":"cjjhx8v3o000tb8tgfls8b2g2","category_id":"cjjhx8v3w001eb8tg3hf98jhz","_id":"cjjhx8v3z001sb8tgrywmgp17"},{"post_id":"cjjhx8v79002rb8tg2feixeg6","category_id":"cjjhx8v2y0002b8tgb058bzb6","_id":"cjjhx8v7g002yb8tgsjopfv9g"},{"post_id":"cjjhx8v7c002tb8tgld0ujngr","category_id":"cjjhx8v3w001eb8tg3hf98jhz","_id":"cjjhx8v7h0030b8tgk6wch0am"},{"post_id":"cjjhx8v800032b8tg5hkbpul2","category_id":"cjjhx8v2y0002b8tgb058bzb6","_id":"cjjhx8v820035b8tgp4k1s4rj"},{"post_id":"cjjhx8v930036b8tgh128nfr2","category_id":"cjjhx8v2y0002b8tgb058bzb6","_id":"cjjhx8v970039b8tgts52ih3t"},{"post_id":"cjjhx8v3e000gb8tgnh9dvicw","category_id":"cjjhx8v3r0010b8tgnolrjl6q","_id":"cjji864wn0002i7tg9590sjnp"}],"PostTag":[{"post_id":"cjjhx8v2p0000b8tgrunuz6oj","tag_id":"cjjhx8v300003b8tgdepwuh6y","_id":"cjjhx8v3e000fb8tgi4s7m5xm"},{"post_id":"cjjhx8v2p0000b8tgrunuz6oj","tag_id":"cjjhx8v360008b8tg81nqc6u0","_id":"cjjhx8v3h000ib8tggd59m7wt"},{"post_id":"cjjhx8v2v0001b8tguq1mby4v","tag_id":"cjjhx8v300003b8tgdepwuh6y","_id":"cjjhx8v3j000nb8tgcrc131rz"},{"post_id":"cjjhx8v310004b8tgipujopgx","tag_id":"cjjhx8v3h000jb8tg3982wxvm","_id":"cjjhx8v3q000xb8tgb8zq5m53"},{"post_id":"cjjhx8v310004b8tgipujopgx","tag_id":"cjjhx8v3m000rb8tg71b37jxh","_id":"cjjhx8v3r000zb8tg5nb8ew9h"},{"post_id":"cjjhx8v330005b8tgzoedeabn","tag_id":"cjjhx8v3h000jb8tg3982wxvm","_id":"cjjhx8v3t0015b8tggmyu5buv"},{"post_id":"cjjhx8v330005b8tgzoedeabn","tag_id":"cjjhx8v3m000rb8tg71b37jxh","_id":"cjjhx8v3t0016b8tg7nc9g94x"},{"post_id":"cjjhx8v340006b8tgq16c9fkl","tag_id":"cjjhx8v3h000jb8tg3982wxvm","_id":"cjjhx8v3x001gb8tg23izky7t"},{"post_id":"cjjhx8v340006b8tgq16c9fkl","tag_id":"cjjhx8v3m000rb8tg71b37jxh","_id":"cjjhx8v3x001hb8tglggne2r1"},{"post_id":"cjjhx8v340006b8tgq16c9fkl","tag_id":"cjjhx8v360008b8tg81nqc6u0","_id":"cjjhx8v3y001lb8tg5puaf2lg"},{"post_id":"cjjhx8v340006b8tgq16c9fkl","tag_id":"cjjhx8v300003b8tgdepwuh6y","_id":"cjjhx8v3y001mb8tg37mvnyrj"},{"post_id":"cjjhx8v370009b8tgd7hyxjer","tag_id":"cjjhx8v3m000rb8tg71b37jxh","_id":"cjjhx8v3z001pb8tg9l0htied"},{"post_id":"cjjhx8v370009b8tgd7hyxjer","tag_id":"cjjhx8v3h000jb8tg3982wxvm","_id":"cjjhx8v3z001qb8tg210muqic"},{"post_id":"cjjhx8v38000ab8tg5zm33gv8","tag_id":"cjjhx8v3y001nb8tg9ye5bexs","_id":"cjjhx8v3z001tb8tgu6yptw1k"},{"post_id":"cjjhx8v38000ab8tg5zm33gv8","tag_id":"cjjhx8v360008b8tg81nqc6u0","_id":"cjjhx8v40001ub8tgl7o4v8vw"},{"post_id":"cjjhx8v3b000eb8tg79yrbxlx","tag_id":"cjjhx8v3z001rb8tgkpbvy2sc","_id":"cjjhx8v41001yb8tgfmyjpb8p"},{"post_id":"cjjhx8v3b000eb8tg79yrbxlx","tag_id":"cjjhx8v40001vb8tg4elemgqb","_id":"cjjhx8v41001zb8tgu7v5fegn"},{"post_id":"cjjhx8v3b000eb8tg79yrbxlx","tag_id":"cjjhx8v40001wb8tg4v8ppemc","_id":"cjjhx8v410021b8tgqapemchq"},{"post_id":"cjjhx8v3i000lb8tgg2r8xyzb","tag_id":"cjjhx8v3z001rb8tgkpbvy2sc","_id":"cjjhx8v44002bb8tgxg7eq3bj"},{"post_id":"cjjhx8v3i000lb8tgg2r8xyzb","tag_id":"cjjhx8v40001vb8tg4elemgqb","_id":"cjjhx8v44002cb8tgq57rc1sy"},{"post_id":"cjjhx8v3i000lb8tgg2r8xyzb","tag_id":"cjjhx8v40001wb8tg4v8ppemc","_id":"cjjhx8v45002eb8tg9w4tperp"},{"post_id":"cjjhx8v3k000ob8tgeoi1qbba","tag_id":"cjjhx8v3z001rb8tgkpbvy2sc","_id":"cjjhx8v46002hb8tg6j9ez3c3"},{"post_id":"cjjhx8v3k000ob8tgeoi1qbba","tag_id":"cjjhx8v40001vb8tg4elemgqb","_id":"cjjhx8v46002ib8tgq3xwo8a6"},{"post_id":"cjjhx8v3k000ob8tgeoi1qbba","tag_id":"cjjhx8v40001wb8tg4v8ppemc","_id":"cjjhx8v46002kb8tgnr48dp41"},{"post_id":"cjjhx8v3n000sb8tgkinolr95","tag_id":"cjjhx8v3z001rb8tgkpbvy2sc","_id":"cjjhx8v47002mb8tggqk67d4j"},{"post_id":"cjjhx8v3n000sb8tgkinolr95","tag_id":"cjjhx8v40001vb8tg4elemgqb","_id":"cjjhx8v47002nb8tgaczv1tqh"},{"post_id":"cjjhx8v3o000tb8tgfls8b2g2","tag_id":"cjjhx8v3z001rb8tgkpbvy2sc","_id":"cjjhx8v48002pb8tgbkw9pish"},{"post_id":"cjjhx8v3o000tb8tgfls8b2g2","tag_id":"cjjhx8v40001vb8tg4elemgqb","_id":"cjjhx8v48002qb8tgtzpjwozy"},{"post_id":"cjjhx8v79002rb8tg2feixeg6","tag_id":"cjjhx8v300003b8tgdepwuh6y","_id":"cjjhx8v7f002vb8tgf7b5m5ph"},{"post_id":"cjjhx8v79002rb8tg2feixeg6","tag_id":"cjjhx8v360008b8tg81nqc6u0","_id":"cjjhx8v7g002xb8tgutpqbtyf"},{"post_id":"cjjhx8v7c002tb8tgld0ujngr","tag_id":"cjjhx8v3z001rb8tgkpbvy2sc","_id":"cjjhx8v7h002zb8tgiyzzrg2o"},{"post_id":"cjjhx8v7c002tb8tgld0ujngr","tag_id":"cjjhx8v40001vb8tg4elemgqb","_id":"cjjhx8v7h0031b8tg1iki2aci"},{"post_id":"cjjhx8v800032b8tg5hkbpul2","tag_id":"cjjhx8v300003b8tgdepwuh6y","_id":"cjjhx8v820033b8tgzbct7arw"},{"post_id":"cjjhx8v800032b8tg5hkbpul2","tag_id":"cjjhx8v360008b8tg81nqc6u0","_id":"cjjhx8v820034b8tg1qi03ccq"},{"post_id":"cjjhx8v930036b8tgh128nfr2","tag_id":"cjjhx8v300003b8tgdepwuh6y","_id":"cjjhx8v960037b8tgzu8x2c0s"},{"post_id":"cjjhx8v930036b8tgh128nfr2","tag_id":"cjjhx8v360008b8tg81nqc6u0","_id":"cjjhx8v970038b8tg47q2sc27"},{"post_id":"cjjhx8v3e000gb8tgnh9dvicw","tag_id":"cjjhx8v3z001rb8tgkpbvy2sc","_id":"cjji864wn0000i7tg47dw2e3q"},{"post_id":"cjjhx8v3e000gb8tgnh9dvicw","tag_id":"cjjhx8v40001vb8tg4elemgqb","_id":"cjji864wn0001i7tgszbfsdzs"},{"post_id":"cjjhx8v3e000gb8tgnh9dvicw","tag_id":"cjjhx8v40001wb8tg4v8ppemc","_id":"cjji864wn0003i7tg1nlvghav"},{"post_id":"cjjhx8v3e000gb8tgnh9dvicw","tag_id":"cjjhx8v360008b8tg81nqc6u0","_id":"cjji864wn0004i7tgppwuzupv"}],"Tag":[{"name":"Spark","_id":"cjjhx8v300003b8tgdepwuh6y"},{"name":"编程","_id":"cjjhx8v360008b8tg81nqc6u0"},{"name":"工具","_id":"cjjhx8v3h000jb8tg3982wxvm"},{"name":"教程","_id":"cjjhx8v3m000rb8tg71b37jxh"},{"name":"Linux","_id":"cjjhx8v3y001nb8tg9ye5bexs"},{"name":"数据挖掘","_id":"cjjhx8v3z001rb8tgkpbvy2sc"},{"name":"机器学习","_id":"cjjhx8v40001vb8tg4elemgqb"},{"name":"异常检测","_id":"cjjhx8v40001wb8tg4v8ppemc"}]}}